[
  {
    "context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n",
    "topic": "Open-domain evaluations, such as those provided by AlpacaEval and ArenaHard, offer broader assessments of LLM performance.",
    "response": "<document_analysis>\nThe context discusses various benchmarks used to evaluate Large Language Models (LLMs), categorized into \u2018Math\u2019 and \u2018Knowledge\u2019 benchmarks. It highlights specific datasets like GSM8K, MATH, NaturalQuestions, TriviaQA, MMLU, BBH, and AGI Eval. Furthermore, it mentions newer benchmarks such as MMLU-Redux, MMLU-Pro, ControlBench, FRAMES, and GPQA Diamond. Crucially, it introduces open-domain evaluations through AlpacaEval and ArenaHard. The difficulty level is high, requiring a nuanced understanding of the landscape of LLM evaluation methodologies. The core challenge lies in distinguishing between the specific datasets and the broader evaluation frameworks they represent. </document_analysis>\n\n<output_format>\nQuestion: Beyond their individual datasets, what is the primary distinguishing characteristic of open-domain evaluations like AlpacaEval and ArenaHard within the broader context of LLM benchmarking?\nA: They exclusively focus on assessing mathematical reasoning capabilities.\nB: They utilize a standardized, closed-domain format to ensure consistent evaluation metrics.\nC: They employ a multi-turn, adversarial interaction paradigm to probe LLM robustness and generalization.\nD: They are primarily designed to evaluate the factual accuracy of knowledge retrieval systems.\nExplanation: The correct answer (C) accurately reflects the described methodology of AlpacaEval and ArenaHard, which involve multi-turn interactions and adversarial prompts to assess a model's broader capabilities.\nCorrect Answer: C\n</output_format>",
    "question": "Beyond their individual datasets, what is the primary distinguishing characteristic of open-domain evaluations like AlpacaEval and ArenaHard within the broader context of LLM benchmarking?",
    "choices": {
      "A": "They exclusively focus on assessing mathematical reasoning capabilities.",
      "B": "They utilize a standardized, closed-domain format to ensure consistent evaluation metrics.",
      "C": "They employ a multi-turn, adversarial interaction paradigm to probe LLM robustness and generalization.",
      "D": "They are primarily designed to evaluate the factual accuracy of knowledge retrieval systems."
    },
    "answer": "C",
    "explanation": "The correct answer (C) accurately reflects the described methodology of AlpacaEval and ArenaHard, which involve multi-turn interactions and adversarial prompts to assess a model's broader capabilities.",
    "question_token_count": 31,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 14
  },
  {
    "context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n",
    "topic": "Understanding the implications of relying on memorized information versus reasoning during LLM inference is crucial for developing robust evaluation strategies.",
    "response": "```\n<document_analysis>\nThe context discusses the problem of \"data contamination\" in evaluating Large Language Models (LLMs). Specifically, it highlights \"syntactic contamination\" \u2013 where test data is created by rephrasing training data with a prefix string. The core argument is whether this constitutes true contamination, as it blurs the line between memorization and reasoning. The text emphasizes the significance of addressing this issue to avoid misleading benchmark results and ensure accurate assessments of LLM capabilities. It also states that without robust methods, evaluations may overestimate a model\u2019s true capabilities. The difficulty lies in distinguishing between memorization and genuine reasoning.\n</document_analysis>\n\n<output_format>\nQuestion: Considering the described challenge of syntactic contamination in LLM benchmarking, what is the primary concern regarding its impact on evaluation accuracy?\nA: It primarily indicates a weakness in the LLM\u2019s syntactic processing abilities.\nB: It obscures the distinction between an LLM\u2019s ability to recall memorized information and its capacity for reasoning, potentially inflating performance estimates.\nC: It necessitates a complete overhaul of existing LLM training methodologies.\nD: It solely impacts the reliability of benchmarks used in academic research.\nExplanation: The text explicitly states that understanding and mitigating data contamination is significant because it can \u201coverestimate a model\u2019s true capabilities\u201d and \u201cundermines the validity of benchmarks.\u201d Option B directly reflects this concern.\nCorrect Answer: B\n</output_format>\n```",
    "question": "Considering the described challenge of syntactic contamination in LLM benchmarking, what is the primary concern regarding its impact on evaluation accuracy?",
    "choices": {
      "A": "It primarily indicates a weakness in the LLM\u2019s syntactic processing abilities.",
      "B": "It obscures the distinction between an LLM\u2019s ability to recall memorized information and its capacity for reasoning, potentially inflating performance estimates.",
      "C": "It necessitates a complete overhaul of existing LLM training methodologies.",
      "D": "It solely impacts the reliability of benchmarks used in academic research."
    },
    "answer": "B",
    "explanation": "The text explicitly states that understanding and mitigating data contamination is significant because it can \u201coverestimate a model\u2019s true capabilities\u201d and \u201cundermines the validity of benchmarks.\u201d Option B directly reflects this concern.",
    "question_token_count": 24,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 4,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 17
  },
  {
    "context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n",
    "topic": "Describe three specific scenarios that commonly lead to data contamination in LLM training datasets.",
    "response": "`<document_analysis>`\nThe context describes data contamination in LLM training datasets, differentiating between exact and syntactic contamination. It highlights common scenarios like verbatim test examples, code snippets, and documentation leaks. The key takeaway is that contamination undermines performance evaluation. The difficulty level is moderate, requiring a basic understanding of LLM training and evaluation. The intent is to assess understanding of the types of contamination and their implications.\n`</document_analysis>`\n\n`<output_format>`\nQuestion: Which of the following scenarios most directly exemplifies \u2018exact contamination\u2019 in the context of LLM training data?\nA: A test prompt containing a paraphrased version of a training example.\nB: A verbatim instance of a test question appearing within a training dataset.\nC: A code snippet from a benchmark implementation being included in the training data.\nD: A documentation excerpt describing the benchmark dataset being used for training.\nExplanation: Exact contamination occurs when there is an identical data point present in both the training and test datasets. Option B accurately describes this scenario.\nCorrect Answer: B\n</output_format>`",
    "question": "Which of the following scenarios most directly exemplifies \u2018exact contamination\u2019 in the context of LLM training data?",
    "choices": {
      "A": "A test prompt containing a paraphrased version of a training example.",
      "B": "A verbatim instance of a test question appearing within a training dataset.",
      "C": "A code snippet from a benchmark implementation being included in the training data.",
      "D": "A documentation excerpt describing the benchmark dataset being used for training."
    },
    "answer": "B",
    "explanation": "Exact contamination occurs when there is an identical data point present in both the training and test datasets. Option B accurately describes this scenario.",
    "question_token_count": 22,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 14
  },
  {
    "context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n",
    "topic": "Explain the interpretation of the equation presented in the text: \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 / \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225.",
    "response": "```\n<document_analysis>\nThe context describes a metric called \"scalability\" in the context of dynamic benchmarking. It's defined as the ratio of the size of the transformed dataset to the size of the original dataset, considering a cost function. The equation \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 / \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 represents this ratio, taking into account the cost associated with the transformation process. The key takeaway is that this equation quantifies how efficiently a benchmark generates data relative to the resources expended. This is a complex relationship, requiring an understanding of both data size and transformation cost. The equation\u2019s purpose is to evaluate how well a dynamic benchmark scales.\n\nDifficulty Rating: 8/10. This requires understanding of mathematical ratios, cost functions, and the concept of benchmarking. It\u2019s not immediately obvious and requires some conceptual grasp beyond simple reading.\n</document_analysis>\n\n<output_format>\nQuestion:  In the context of dynamic benchmarking, what does the equation \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 / \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 primarily represent?\nA:  The statistical distribution of the original dataset.\nB:  The ratio of the cost of data transformation to the size of the original dataset.\nC:  The proportional increase in data size after transformation, independent of cost.\nD:  The efficiency of data generation relative to resource expenditure.\nExplanation: The equation explicitly states it represents the \"proportion of data that can be generated per unit cost.\"  Option D accurately captures this nuanced interpretation.\nCorrect Answer: D\n</output_format>\n```",
    "question": "In the context of dynamic benchmarking, what does the equation \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 / \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 primarily represent?",
    "choices": {
      "A": "The statistical distribution of the original dataset.",
      "B": "The ratio of the cost of data transformation to the size of the original dataset.",
      "C": "The proportional increase in data size after transformation, independent of cost.",
      "D": "The efficiency of data generation relative to resource expenditure."
    },
    "answer": "D",
    "explanation": "The equation explicitly states it represents the \"proportion of data that can be generated per unit cost.\"  Option D accurately captures this nuanced interpretation.",
    "question_token_count": 89,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 1,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 12
  },
  {
    "context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n",
    "topic": "The decomposition of benchmark creation into specialized agent roles \u2013 planning, generation, verification, and evaluation \u2013 and its impact on overall efficiency.",
    "response": "`<document_analysis>`\nThe provided text discusses methods for evaluating Large Language Models (LLMs) using benchmarks. It highlights two main approaches: interactive evaluation (simulating human interviews) and multi-agent evaluation (using multiple LLM agents to create benchmarks). A key aspect of the multi-agent approach, specifically detailed in the \u201cBenchmark Self-Evolving\u201d paper, is the division of the benchmark creation process into distinct agent roles \u2013 planning, generation, verification, and evaluation. The text emphasizes the benefits of this specialized agent architecture, including scalability, diversity, and quality. The central theme revolves around the decomposition of benchmark creation into a structured, agent-based workflow.\n\nStrategic Complexity Calibration: 8/10 \u2013 This requires understanding the nuances of how benchmarks are constructed and the value of specialized roles within that process. It goes beyond simply recognizing the terms; it demands an appreciation for the strategic rationale behind the architecture.\n\nIntentional Question Planning: The question should probe the core rationale for this agent decomposition and its implications for benchmark quality. </document_analysis>`\n\n`<output_format>`\nQuestion: Within the context of automated benchmark creation using the BENCHAGENTS framework, what is the primary justification for dividing the process into distinct agent roles (planning, generation, verification, and evaluation)?\nA: To simplify the evaluation process for human annotators.\nB: To leverage the inherent limitations of single-agent LLMs in complex tasks.\nC: To enable a more scalable and adaptable benchmark generation process, improving diversity and quality.\nD: To reduce the computational cost associated with benchmark creation.\nExplanation: The text explicitly states that the BENCHAGENTS framework, employing a multi-agent framework, \u201csplits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent. This coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\u201d\nCorrect Answer: C </output_format>`",
    "question": "Within the context of automated benchmark creation using the BENCHAGENTS framework, what is the primary justification for dividing the process into distinct agent roles (planning, generation, verification, and evaluation)?",
    "choices": {
      "A": "To simplify the evaluation process for human annotators.",
      "B": "To leverage the inherent limitations of single-agent LLMs in complex tasks.",
      "C": "To enable a more scalable and adaptable benchmark generation process, improving diversity and quality.",
      "D": "To reduce the computational cost associated with benchmark creation."
    },
    "answer": "C",
    "explanation": "The text explicitly states that the BENCHAGENTS framework, employing a multi-agent framework, \u201csplits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent. This coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\u201d",
    "question_token_count": 39,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 13
  },
  {
    "context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n",
    "topic": "Current static benchmarking methods suffer from a lack of transparency and rely on assumptions about contaminated models, highlighting a key limitation.",
    "response": "`<document_analysis>`\nThe context discusses limitations of current static benchmarking methods in LLMs, specifically highlighting a lack of transparency and reliance on assumptions about contaminated models. It introduces the concept of data contamination and how it increases with the size of training datasets. Furthermore, the text identifies challenges with dynamic benchmarks, including issues with scalability and a lack of standardized evaluation criteria. The core issue revolves around the increasing difficulty of ensuring the integrity of benchmarks as LLMs become larger and more complex. The difficulty lies in the lack of clear methods to assess contamination and the emergence of new challenges with dynamic benchmarks.\n\nStrategic Complexity Calibration: 8/10 \u2013 This requires an understanding of how LLM training and evaluation are intertwined and the inherent difficulties in assessing model integrity.\n\nIntentional Question Planning: The question will probe the reader\u2019s understanding of the trade-offs between static and dynamic benchmarks and the underlying reasons for their limitations. It will require them to consider the implications of data contamination beyond simple statistical probabilities.\n</document_analysis>`\n\n`<output_format>`\nQuestion: Considering the observed inverse relationship between training dataset size and data contamination probability, what fundamental characteristic of LLM training processes most directly contributes to this escalating issue?\nA: The inherent biases present within initial training data sets.\nB: The increasing computational demands of training larger models, leading to reliance on less curated data sources.\nC: The limitations of post-hoc contamination detection methods, which are unable to effectively identify subtle forms of data corruption.\nD: The stochastic nature of the training process, introducing unpredictable variations in model behavior.\nExplanation: The correct answer is C. The text explicitly states that static benchmarks rely on assumptions about contaminated models and that post-hoc detection methods are insufficient. The other options represent contributing factors but don\u2019t directly address the core problem outlined in the text.\nCorrect Answer: C\n</output_format>`",
    "question": "Considering the observed inverse relationship between training dataset size and data contamination probability, what fundamental characteristic of LLM training processes most directly contributes to this escalating issue?",
    "choices": {
      "A": "The inherent biases present within initial training data sets.",
      "B": "The increasing computational demands of training larger models, leading to reliance on less curated data sources.",
      "C": "The limitations of post-hoc contamination detection methods, which are unable to effectively identify subtle forms of data corruption.",
      "D": "The stochastic nature of the training process, introducing unpredictable variations in model behavior."
    },
    "answer": "C",
    "explanation": "The correct answer is C. The text explicitly states that static benchmarks rely on assumptions about contaminated models and that post-hoc detection methods are insufficient. The other options represent contributing factors but don\u2019t directly address the core problem outlined in the text.",
    "question_token_count": 31,
    "answer_correctness_score": 8,
    "explanation_validity_score": 8,
    "question_clarity_score": 8,
    "question_difficulty_score": 8,
    "question_groundedness_score": 6,
    "avg_answer_token_count": 17
  },
  {
    "context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n",
    "topic": "What does the \"Repeat Trials\" metric indicate about the benchmark\u2019s ability to generate novel test cases?",
    "response": "```\n<document_analysis>\nThe context introduces the \"Repeat Trials\" metric as a measure of a benchmark\u2019s ability to generate novel test cases. It defines \"collision\" as overlap between transformed datasets and uses \"Repeat Trials\" to quantify the expected number of transformations needed to regenerate a dataset. The core idea is to assess how well a dynamic benchmark can produce diverse variations, mitigating the risk of contamination from training data. The difficulty of the metric is moderate, requiring an understanding of the concept of \"transformation\" and \"novelty\" in the context of benchmarks. The question aims to assess whether the expert understands the implications of the metric beyond a superficial level.\n</document_analysis>\n<output_format>\nQuestion: What does the \u201cRepeat Trials\u201d metric primarily indicate about the benchmark\u2019s capability to generate diverse test cases?\nA: It measures the rate of data overlap between different benchmark versions.\nB: It estimates the number of transformations needed to completely replicate an existing transformed dataset, indicating the benchmark\u2019s capacity to produce novel variations.\nC: It assesses the susceptibility of the benchmark to data contamination during training.\nD: It determines the initial difficulty level of the benchmark.\nExplanation: The context explicitly states that \u201cRepeat Trials\u201d quantifies the \u201cexpected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D )\u201d, directly indicating its relevance to generating novel variations.\nCorrect Answer: B\n</output_format>\n```",
    "question": "What does the \u201cRepeat Trials\u201d metric primarily indicate about the benchmark\u2019s capability to generate diverse test cases?",
    "choices": {
      "A": "It measures the rate of data overlap between different benchmark versions.",
      "B": "It estimates the number of transformations needed to completely replicate an existing transformed dataset, indicating the benchmark\u2019s capacity to produce novel variations.",
      "C": "It assesses the susceptibility of the benchmark to data contamination during training.",
      "D": "It determines the initial difficulty level of the benchmark."
    },
    "answer": "B",
    "explanation": "The context explicitly states that \u201cRepeat Trials\u201d quantifies the \u201cexpected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D )\u201d, directly indicating its relevance to generating novel variations.",
    "question_token_count": 22,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 16
  },
  {
    "context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n",
    "topic": "How are the various tasks evaluated within the framework of static benchmarks?",
    "response": "<document_analysis>\nThe provided context describes static benchmarks used to evaluate LLM performance across diverse tasks. It outlines the components of a static benchmark \u2013 the seed dataset (prompts and expected outputs) and a scoring function. The core purpose is to provide standardized evaluation tools. The text focuses on defining the structure and elements of static benchmarks, rather than detailing *how* the various tasks are evaluated. Therefore, generating a deeply challenging question requires inferring the criteria used in the scoring function.\n\nStrategic Complexity Calibration: 8/10 \u2013 This requires understanding the concept of a scoring function and applying it to the broader context of benchmark evaluation. It\u2019s not a simple factual recall but demands an understanding of the underlying evaluation process.\n</document_analysis>\n<output_format>\nQuestion: Within the framework of static benchmarking, what is the primary role of the scoring function (\ud835\udcae(\u22c5))?\nA: To generate new input prompts for the LLM.\nB: To define the set of expected outputs (\ud835\udcb4) for each task.\nC: To assess the quality of an LLM\u2019s output by comparing it against the expected outputs.\nD: To categorize the tasks being evaluated into specific categories.\nExplanation: The text explicitly states that the scoring function \u201cevaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4.\u201d\nCorrect Answer: C\n</output_format>",
    "question": "Within the framework of static benchmarking, what is the primary role of the scoring function (\ud835\udcae(\u22c5))?",
    "choices": {
      "A": "To generate new input prompts for the LLM.",
      "B": "To define the set of expected outputs (\ud835\udcb4) for each task.",
      "C": "To assess the quality of an LLM\u2019s output by comparing it against the expected outputs.",
      "D": "To categorize the tasks being evaluated into specific categories."
    },
    "answer": "C",
    "explanation": "The text explicitly states that the scoring function \u201cevaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4.\u201d",
    "question_token_count": 24,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 13
  },
  {
    "context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n",
    "topic": "What role does the \"oracle\" function play in evaluating the correctness of a dynamic benchmark?",
    "response": "`<document_analysis>`\nThe context describes a proposed evaluation criterion for dynamic benchmarking algorithms, specifically focusing on \"Correctness.\" It introduces an \"oracle\" function (\ud835\udca2\u2062(\u22c5)) as a crucial component \u2013 a reference point that provides the ground truth for assessing the benchmark\u2019s output. The oracle is described as potentially being a domain-specific annotator. The core concept is that the benchmark\u2019s correctness is quantified by comparing its transformed dataset\u2019s outputs to the ground truth, using a scoring function \ud835\udcae\u2062(\u22c5).  The difficulty lies in understanding the role and implementation of this oracle function within the broader evaluation framework.\n\nStrategic Complexity Calibration: 8/10 \u2013 Requires understanding of the evaluation process and the function of an oracle, which is not immediately obvious.\n\nIntentional Question Planning: The question will assess understanding of the oracle's function as a grounding reference for correctness, requiring the ability to infer its purpose and potential implementation beyond the explicit description.\n`</document_analysis>`\n\n<output_format>\nQuestion: In the context of evaluating dynamic benchmarks, what is the primary role of the \u201coracle\u201d function?\nA: To generate the initial dataset used for benchmarking.\nB: To provide the ground truth for assessing the correctness of the benchmark\u2019s output.\nC: To calculate the scoring function used to measure alignment.\nD: To identify potential biases within the benchmark\u2019s transformation process.\nExplanation: The text explicitly states that the \u201coracle\u201d function \u201creturns the ground truth of its input,\u201d serving as an objective reference for evaluating correctness.\nCorrect Answer: B\n</output_format>",
    "question": "In the context of evaluating dynamic benchmarks, what is the primary role of the \u201coracle\u201d function?",
    "choices": {
      "A": "To generate the initial dataset used for benchmarking.",
      "B": "To provide the ground truth for assessing the correctness of the benchmark\u2019s output.",
      "C": "To calculate the scoring function used to measure alignment.",
      "D": "To identify potential biases within the benchmark\u2019s transformation process."
    },
    "answer": "B",
    "explanation": "The text explicitly states that the \u201coracle\u201d function \u201creturns the ground truth of its input,\u201d serving as an objective reference for evaluating correctness.",
    "question_token_count": 21,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 12
  },
  {
    "context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n",
    "topic": "Fairness, accountability, and privacy must be central design principles when developing benchmarking frameworks for LLMs.",
    "response": "<document_analysis>\nThe context highlights the critical importance of fairness, accountability, and privacy when designing LLM benchmarking frameworks. It specifically addresses the potential for static benchmarks to perpetuate bias and dynamic benchmarks to raise privacy concerns. The core argument emphasizes the need for proactive ethical design principles to prevent misuse and negative societal impacts. The difficulty level is high, requiring an understanding of the nuanced risks associated with different benchmarking approaches. Strategic complexity is calibrated to 8/10, as the question requires synthesis of multiple ethical considerations.\n</document_analysis>\n\n<output_format>\nQuestion: Considering the potential for dynamic LLM benchmarks to compromise user privacy, what is the most prudent approach for mitigating this risk within a benchmarking framework\u2019s design?\nA: Prioritize the collection of vast datasets to improve benchmark adaptability.\nB: Implement differential privacy techniques to obscure individual data contributions.\nC: Rely exclusively on static benchmarks to guarantee data immutability.\nD: Minimize data collection and focus solely on model performance metrics.\nExplanation: The correct answer is B. The context explicitly states the privacy concerns associated with dynamic benchmarks and suggests differential privacy as a solution. Options A, C, and D are flawed because they either exacerbate the privacy risk or ignore the fundamental issue.\nCorrect Answer: B\n</output_format>",
    "question": "Considering the potential for dynamic LLM benchmarks to compromise user privacy, what is the most prudent approach for mitigating this risk within a benchmarking framework\u2019s design?",
    "choices": {
      "A": "Prioritize the collection of vast datasets to improve benchmark adaptability.",
      "B": "Implement differential privacy techniques to obscure individual data contributions.",
      "C": "Rely exclusively on static benchmarks to guarantee data immutability.",
      "D": "Minimize data collection and focus solely on model performance metrics."
    },
    "answer": "B",
    "explanation": "The correct answer is B. The context explicitly states the privacy concerns associated with dynamic benchmarks and suggests differential privacy as a solution. Options A, C, and D are flawed because they either exacerbate the privacy risk or ignore the fundamental issue.",
    "question_token_count": 30,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 12
  },
  {
    "context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n",
    "topic": "ITD\u2019s approach to identifying and rewriting contaminated samples from static benchmarks while preserving difficulty levels.",
    "response": "`<document_analysis>`\nThe context describes several methods for addressing contamination in static benchmarks used for training LLMs. It highlights the potential risks of in-distribution contamination and introduces approaches like Auto-Dataset, StructEval, ITD, and VarBench. ITD\u2019s core methodology involves a contamination detector followed by an LLM-driven rewriting process, with a specific focus on maintaining the original difficulty level of the samples. The key challenge lies in accurately identifying contamination and preserving the difficulty level during the rewriting process. The information provided is technical and focused on the specific techniques employed.\n\nStrategic Complexity Calibration: 8/10 \u2013 Requires understanding of LLM training, benchmark contamination, and the interplay between difficulty preservation and rewriting.\n\nIntentional Question Planning: The question should probe the nuances of the ITD approach, particularly the interaction between the contamination detector and the LLM rewriting process, and how difficulty is assessed and maintained.\n</document_analysis>`\n\n`<output_format>`\nQuestion: Within the ITD framework, how does the contamination detector\u2019s output directly influence the subsequent LLM rewriting process, specifically concerning the preservation of difficulty levels?\nA: The contamination detector simply flags samples as contaminated, and the LLM then generates completely new samples without considering the original difficulty.\nB: The contamination detector\u2019s output is used to adjust the LLM\u2019s prompt, guiding it to rewrite the sample while maintaining a similar difficulty level, as determined by a separate metric.\nC: The contamination detector\u2019s output is used to identify the most easily rewritten samples, prioritizing those for modification regardless of difficulty.\nD: The contamination detector\u2019s output is discarded, and the LLM rewrites all samples based on a pre-defined difficulty scale.\nExplanation: The correct answer is B. The context explicitly states that ITD utilizes a contamination detector and then prompts an LLM to rewrite the contaminated samples *while preserving their difficulty levels*. The prompt adjustment is implied by this process.\nCorrect Answer: B\n</output_format>`",
    "question": "Within the ITD framework, how does the contamination detector\u2019s output directly influence the subsequent LLM rewriting process, specifically concerning the preservation of difficulty levels?",
    "choices": {
      "A": "The contamination detector simply flags samples as contaminated, and the LLM then generates completely new samples without considering the original difficulty.",
      "B": "The contamination detector\u2019s output is used to adjust the LLM\u2019s prompt, guiding it to rewrite the sample while maintaining a similar difficulty level, as determined by a separate metric.",
      "C": "The contamination detector\u2019s output is used to identify the most easily rewritten samples, prioritizing those for modification regardless of difficulty.",
      "D": "The contamination detector\u2019s output is discarded, and the LLM rewrites all samples based on a pre-defined difficulty scale."
    },
    "answer": "B",
    "explanation": "The correct answer is B. The context explicitly states that ITD utilizes a contamination detector and then prompts an LLM to rewrite the contaminated samples *while preserving their difficulty levels*. The prompt adjustment is implied by this process.",
    "question_token_count": 30,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 5,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 26
  },
  {
    "context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n",
    "topic": "Describe the mathematical notation used to represent a static benchmark and its components.",
    "response": "<document_analysis>\nThe provided text describes a \"static benchmark\" and its components. The notation used is \ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.)). The key elements are: \ud835\udcb3 (input prompts), \ud835\udcb4 (expected outputs), and \ud835\udcae(\u22c5) (scoring function). The question needs to assess understanding of this notation and its implications. The difficulty level should be high, requiring the ability to interpret the mathematical representation and its significance. It\u2019s crucial to avoid simplistic questions that can be answered by directly extracting information from the text. Instead, the question should probe the understanding of how the notation defines the benchmark and its purpose.\n</document_analysis>\n<output_format>\nQuestion: What does the symbol \ud835\udcae(\u22c5) in the notation \ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(\u22c5)) represent within the context of a static benchmark?\nA: The set of input prompts.\nB: The scoring function used to evaluate model outputs.\nC: The expected output for each input prompt.\nD: The dataset used to train the model.\nExplanation: The text explicitly states that \ud835\udcae(\u22c5) is the \u201cscoring function that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4.\u201d\nCorrect Answer: B\n</output_format>",
    "question": "What does the symbol \ud835\udcae(\u22c5) in the notation \ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(\u22c5)) represent within the context of a static benchmark?",
    "choices": {
      "A": "The set of input prompts.",
      "B": "The scoring function used to evaluate model outputs.",
      "C": "The expected output for each input prompt.",
      "D": "The dataset used to train the model."
    },
    "answer": "B",
    "explanation": "The text explicitly states that \ud835\udcae(\u22c5) is the \u201cscoring function that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4.\u201d",
    "question_token_count": 36,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 1,
    "question_difficulty_score": 5,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 8
  },
  {
    "context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n",
    "topic": "What does the external diversity formula represent in the context of transformed datasets?",
    "response": "`<document_analysis>`\nThe context defines \"external diversity\" as a measure of variation between a transformed dataset and the original seed dataset. It's represented by an expected value (\ud835\udd3c) across multiple transformed datasets (\ud835\udc9fi) and the original dataset (\ud835\udc9f), where \u0398(\ud835\udc9fi, \ud835\udc9f) calculates the diversity between them. The formula explicitly states that this is an expected value across N trials. The purpose of this definition is to quantify how much the transformation process has changed the data compared to the starting point. The diversity function (\u0398) itself is described as a metric like N-gram or BLEU scores.\n\nThe difficulty of this concept is moderate \u2013 a domain expert should understand the basic principles of statistical expectation and the concept of dataset variation. However, the precise mathematical notation and the requirement to consider multiple transformations adds a layer of complexity.\n\nStrategic complexity: 7/10.\n</document_analysis>\n\n`<output_format>`\nQuestion: What is the primary mathematical representation of \"external diversity\" as described within the provided context?\nA: It is a measure of the absolute difference between the mean of the transformed dataset and the mean of the seed dataset.\nB: It is the expected value, across N transformations, of a diversity function (\u0398) that quantifies the difference between each transformed dataset and the original seed dataset.\nC: It is the variance of the transformed dataset, calculated by subtracting the seed dataset from each transformed dataset.\nD: It is the correlation coefficient between the transformed dataset and the seed dataset.\nExplanation: The correct answer (B) accurately reflects the formula provided in the text: \ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f. The other options misinterpret or misrepresent the mathematical operation involved.\nCorrect Answer: B\n</output_format>",
    "question": "What is the primary mathematical representation of \"external diversity\" as described within the provided context?",
    "choices": {
      "A": "It is a measure of the absolute difference between the mean of the transformed dataset and the mean of the seed dataset.",
      "B": "It is the expected value, across N transformations, of a diversity function (\u0398) that quantifies the difference between each transformed dataset and the original seed dataset.",
      "C": "It is the variance of the transformed dataset, calculated by subtracting the seed dataset from each transformed dataset.",
      "D": "It is the correlation coefficient between the transformed dataset and the seed dataset."
    },
    "answer": "B",
    "explanation": "The correct answer (B) accurately reflects the formula provided in the text: \ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f. The other options misinterpret or misrepresent the mathematical operation involved.",
    "question_token_count": 19,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 5,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 23
  },
  {
    "context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n",
    "topic": "How do the proposed metrics, \"Collision Rate\" and \"Repeat Trials,\" specifically address the risk of data contamination in LLM evaluation?",
    "response": "<document_analysis>\nThe context introduces two new metrics, \"Collision Rate\" and \"Repeat Trials,\" designed to mitigate the risk of data contamination in LLM evaluation. The core idea is to assess how well a dynamic benchmark can generate novel test cases and avoid producing redundant or overlapping data. The metrics are presented as tools for evaluating benchmark robustness against potential training data contamination. The difficulty level is high as it requires understanding the nuanced implications of data contamination and how these metrics address it. Strategic complexity calibration: 8/10.\n</document_analysis>\n<output_format>\nQuestion: How do the proposed \"Collision Rate\" and \"Repeat Trials\" metrics fundamentally differ in their approach to evaluating the potential for data contamination in LLM evaluation benchmarks?\nA: Collision Rate measures the diversity of the benchmark dataset, while Repeat Trials assesses the benchmark\u2019s ability to generate entirely new transformations.\nB: Repeat Trials quantifies the number of transformations needed to fully regenerate a dataset, whereas Collision Rate measures the degree of overlap between transformed datasets.\nC: Both metrics are directly correlated and provide identical information about benchmark robustness.\nD: Collision Rate assesses the frequency of specific data points, while Repeat Trials evaluates the overall dataset size.\nExplanation: The context explicitly states that Collision Rate measures the percentage of overlap between transformed datasets, and Repeat Trials quantifies the number of transformations needed to regenerate a dataset.\nCorrect Answer: B\n</output_format>",
    "question": "How do the proposed \"Collision Rate\" and \"Repeat Trials\" metrics fundamentally differ in their approach to evaluating the potential for data contamination in LLM evaluation benchmarks?",
    "choices": {
      "A": "Collision Rate measures the diversity of the benchmark dataset, while Repeat Trials assesses the benchmark\u2019s ability to generate entirely new transformations.",
      "B": "Repeat Trials quantifies the number of transformations needed to fully regenerate a dataset, whereas Collision Rate measures the degree of overlap between transformed datasets.",
      "C": "Both metrics are directly correlated and provide identical information about benchmark robustness.",
      "D": "Collision Rate assesses the frequency of specific data points, while Repeat Trials evaluates the overall dataset size."
    },
    "answer": "B",
    "explanation": "The context explicitly states that Collision Rate measures the percentage of overlap between transformed datasets, and Repeat Trials quantifies the number of transformations needed to regenerate a dataset.",
    "question_token_count": 32,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 21
  },
  {
    "context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n",
    "topic": "The proposed design principles for dynamic benchmarking aim to overcome the limitations of existing methods and provide a more robust assessment of LLM data contamination risks.",
    "response": "```\n<document_analysis>\nThe context describes a shift from static to dynamic benchmarking for Large Language Models (LLMs) to address data contamination risks. It identifies limitations in existing static benchmarks and highlights the need for standardized criteria in dynamic benchmarks. The text proposes design principles for dynamic benchmarking and references a GitHub repository containing benchmarking methods. The core challenge lies in the evaluation of dynamic benchmarks, a critical gap in current research. The difficulty level is high, requiring a nuanced understanding of LLM benchmarking methodologies and the inherent complexities of data contamination.\n\n</document_analysis>\n\n<output_format>\nQuestion: What is the primary limitation identified within the context regarding existing dynamic benchmarking methods for mitigating LLM data contamination?\nA: A lack of standardized criteria for evaluating their effectiveness.\nB: An over-reliance on static benchmarks, leading to inaccurate assessments.\nC: The inherent difficulty in simulating real-world data contamination scenarios.\nD: The computational cost associated with dynamic benchmarking procedures.\nExplanation: The text explicitly states \u201cwe then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks.\u201d\nCorrect Answer: A\n</output_format>\n```",
    "question": "What is the primary limitation identified within the context regarding existing dynamic benchmarking methods for mitigating LLM data contamination?",
    "choices": {
      "A": "A lack of standardized criteria for evaluating their effectiveness.",
      "B": "An over-reliance on static benchmarks, leading to inaccurate assessments.",
      "C": "The inherent difficulty in simulating real-world data contamination scenarios.",
      "D": "The computational cost associated with dynamic benchmarking procedures."
    },
    "answer": "A",
    "explanation": "The text explicitly states \u201cwe then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks.\u201d",
    "question_token_count": 21,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 12
  },
  {
    "context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n",
    "topic": "The primary challenge addressed by post-hoc detection methods is ensuring the reliability of performance metrics in machine learning.",
    "response": "`<document_analysis>`\nThe provided text discusses the limitations of label protection and highlights post-hoc detection methods as a solution to data contamination. It details how these methods, utilizing techniques like n-gram matching and embedding similarity, identify overlaps between training and testing datasets. Furthermore, the text extends beyond simple overlap detection, incorporating analysis of model behavior under various conditions to detect memorization or preference biases. The core challenge addressed is ensuring reliable performance metrics, particularly in the face of potential data contamination. The difficulty lies in the nuanced understanding of these techniques and their application to complex model behavior. </document_analysis>\n\n`<output_format>`\nQuestion: Dekoninck et al. (2024) propose a method for detecting contamination that primarily relies on what mechanism?\nA: Direct overlap detection through n-gram matching.\nB: Analyzing model behavior under conditions of partial completions.\nC: Comparing model performance across multiple benchmarks.\nD: Embedding-based similarity analysis.\nExplanation: The text explicitly states that CONSTAT, proposed by Dekoninck et al. (2024), \u201cdetects contamination by comparing model performance across benchmarks.\u201d\nCorrect Answer: C\n</output_format>`",
    "question": "Dekoninck et al. (2024) propose a method for detecting contamination that primarily relies on what mechanism?",
    "choices": {
      "A": "Direct overlap detection through n-gram matching.",
      "B": "Analyzing model behavior under conditions of partial completions.",
      "C": "Comparing model performance across multiple benchmarks.",
      "D": "Embedding-based similarity analysis."
    },
    "answer": "C",
    "explanation": "The text explicitly states that CONSTAT, proposed by Dekoninck et al. (2024), \u201cdetects contamination by comparing model performance across benchmarks.\u201d",
    "question_token_count": 25,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 8
  },
  {
    "context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n",
    "topic": "The increasing probability of data contamination in LLMs necessitates a shift towards dynamic benchmarks due to the limitations of static benchmarks.",
    "response": "`<document_analysis>`\nThe text discusses the increasing problem of data contamination in Large Language Models (LLMs) and highlights the inadequacy of static benchmarks in addressing this issue. It introduces several benchmark generation methods like LatestEval, DARG, and C2LEVA, all aimed at mitigating contamination. A key argument is that as training datasets grow, the probability of contamination increases, rendering static benchmarks obsolete. The text also points out a lack of standardized criteria for evaluating dynamic benchmarks, presenting a significant challenge for future development. The core problem is that current benchmarks don't adequately account for the evolving nature of LLM training data and the resulting contamination.\n\nStrategic Complexity Calibration: 8/10 \u2013 The question needs to delve into the nuances of benchmark design and the inherent difficulties in assessing LLM performance reliably. It requires an understanding of both static and dynamic benchmarking approaches and the trade-offs involved.\n\nIntentional Question Planning: The question will explore the fundamental limitations of current evaluation strategies and the need for a more adaptive approach, prompting reflection on the broader implications for LLM development and deployment.\n</document_analysis>`\n\n`<output_format>`\nQuestion: Considering the exponential growth of training datasets and the observed increase in data contamination rates, what is the most critical deficiency currently hindering the effective evaluation of LLMs?\nA: The lack of human annotators for evaluating generated samples.\nB: The inherent difficulty in scaling dynamic benchmark generation methods to accommodate diverse LLM architectures.\nC: The absence of standardized criteria for assessing the robustness and reliability of dynamic benchmarks.\nD: The limited computational resources available for conducting extensive post-hoc contamination detection.\nExplanation: The text explicitly states a lack of standardized criteria for evaluating dynamic benchmarks as a key challenge. Options A, B, and D address secondary issues, while option C directly reflects the central argument presented in the passage.\nCorrect Answer: C\n</output_format>`",
    "question": "Considering the exponential growth of training datasets and the observed increase in data contamination rates, what is the most critical deficiency currently hindering the effective evaluation of LLMs?",
    "choices": {
      "A": "The lack of human annotators for evaluating generated samples.",
      "B": "The inherent difficulty in scaling dynamic benchmark generation methods to accommodate diverse LLM architectures.",
      "C": "The absence of standardized criteria for assessing the robustness and reliability of dynamic benchmarks.",
      "D": "The limited computational resources available for conducting extensive post-hoc contamination detection."
    },
    "answer": "C",
    "explanation": "The text explicitly states a lack of standardized criteria for evaluating dynamic benchmarks as a key challenge. Options A, B, and D address secondary issues, while option C directly reflects the central argument presented in the passage.",
    "question_token_count": 33,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 4,
    "avg_answer_token_count": 14
  },
  {
    "context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n",
    "topic": "What are the potential benefits of employing hybrid approaches to dynamic benchmark construction?",
    "response": "<document_analysis>\nThe context describes different types of dynamic benchmarks for evaluating Large Language Models (LLMs). It outlines temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches. The core focus is on the benefits of hybrid approaches \u2013 combining elements of other methods \u2013 specifically highlighting their potential to address the challenges of interpretability and validation in dynamic benchmark construction. The text emphasizes the need for interpretable transformations to reduce manual verification costs, especially when using LLM-assisted methods. The difficulty level is high because it requires understanding the nuances of combining different benchmark construction techniques and their implications for validation.\n</document_analysis>\n<output_format>\nQuestion: In the context of dynamic benchmark construction, what is the primary rationale for utilizing hybrid approaches, and what specific challenge do they aim to mitigate?\nA: Hybrid approaches are unnecessary for dynamic benchmarks, as rule-based generation is the most efficient method.\nB: Hybrid approaches leverage LLM-based generation exclusively to maximize data volume.\nC: Hybrid approaches combine different benchmark construction techniques to address the challenges of interpretability and validation, particularly when using LLM-assisted transformations.\nD: Hybrid approaches solely rely on temporal cutoff methods for data collection.\nExplanation: The text explicitly states that hybrid approaches \"combine the idea of these different approaches\" and are used to address the challenges of interpretability and validation, specifically when LLMs are involved.\nCorrect Answer: C\n</output_format>",
    "question": "In the context of dynamic benchmark construction, what is the primary rationale for utilizing hybrid approaches, and what specific challenge do they aim to mitigate?",
    "choices": {
      "A": "Hybrid approaches are unnecessary for dynamic benchmarks, as rule-based generation is the most efficient method.",
      "B": "Hybrid approaches leverage LLM-based generation exclusively to maximize data volume.",
      "C": "Hybrid approaches combine different benchmark construction techniques to address the challenges of interpretability and validation, particularly when using LLM-assisted transformations.",
      "D": "Hybrid approaches solely rely on temporal cutoff methods for data collection."
    },
    "answer": "C",
    "explanation": "The text explicitly states that hybrid approaches \"combine the idea of these different approaches\" and are used to address the challenges of interpretability and validation, specifically when LLMs are involved.",
    "question_token_count": 29,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 4,
    "question_difficulty_score": 8,
    "question_groundedness_score": 6,
    "avg_answer_token_count": 17
  },
  {
    "context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n",
    "topic": "Embedding-based similarity offers a more robust approach to post-hoc detection compared to exact matching.",
    "response": "`<document_analysis>`\nThe text discusses the limitations of label protection and highlights post-hoc detection as a solution to data contamination. It contrasts exact matching with embedding-based similarity, noting that embedding similarity offers a more robust approach. The passage also introduces broader analyses of model behavior beyond simple overlap detection, including memorization and preference analysis. The core idea is the shift towards more sophisticated techniques to identify contamination. The difficulty level is high, requiring an understanding of the nuances in detection methods and their relative strengths. Strategic complexity calibration: 8/10.\n`</document_analysis>`\n\n`<output_format>`\nQuestion: Dekoninck et al. (2024) utilize which methodology to detect contamination by comparing model performance across benchmarks?\nA: N-gram matching at various levels, such as tokens.\nB: Embedding-based similarity, specifically comparing model performance across benchmarks.\nC: Analyzing model behavior under different conditions, including memorization through masked inputs.\nD: Direct overlap detection using exact matching.\nExplanation: The answer is B because the text explicitly states that Dekoninck et al. (2024) propose CONSTAT, which \u201cdetects contamination by comparing model performance across benchmarks.\u201d\nCorrect Answer: B\n</output_format>`",
    "question": "Dekoninck et al. (2024) utilize which methodology to detect contamination by comparing model performance across benchmarks?",
    "choices": {
      "A": "N-gram matching at various levels, such as tokens.",
      "B": "Embedding-based similarity, specifically comparing model performance across benchmarks.",
      "C": "Analyzing model behavior under different conditions, including memorization through masked inputs.",
      "D": "Direct overlap detection using exact matching."
    },
    "answer": "B",
    "explanation": "The answer is B because the text explicitly states that Dekoninck et al. (2024) propose CONSTAT, which \u201cdetects contamination by comparing model performance across benchmarks.\u201d",
    "question_token_count": 25,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 4,
    "question_difficulty_score": 6,
    "question_groundedness_score": 6,
    "avg_answer_token_count": 12
  },
  {
    "context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n",
    "topic": "The conceptual definition of stability in dynamic benchmarking, focusing on variance across trials as an indicator of robustness.",
    "response": "```\n<document_analysis>\nThe context discusses the concept of stability in dynamic benchmarking, specifically relating it to variance in complexity across trials. It introduces a formal definition of stability using a complexity measurement function and highlights that high variance indicates instability. The key takeaway is the use of variance as an indicator of robustness. The difficulty level is moderately high as it requires understanding of a specific metric (variance) and its application within the broader context of dynamic benchmarking. Strategic complexity calibration: 7/10.\n\nIrrelevant elements: None.\nBogus Information: None.\n</document_analysis>\n\n<output_format>\nQuestion: According to the provided text, what is the primary interpretation of high variance in the complexity measurement function within the context of dynamic benchmarking stability?\nA: It signifies a decrease in task complexity.\nB: It indicates a lack of data contamination.\nC: It represents a high degree of robustness and stability.\nD: It suggests a significant performance improvement.\nExplanation: The text states that \u201chigh variance indicates that the dynamic benchmarking method is not stable.\u201d Therefore, high variance represents instability, not stability.\nCorrect Answer: C\n</output_format>\n```",
    "question": "According to the provided text, what is the primary interpretation of high variance in the complexity measurement function within the context of dynamic benchmarking stability?",
    "choices": {
      "A": "It signifies a decrease in task complexity.",
      "B": "It indicates a lack of data contamination.",
      "C": "It represents a high degree of robustness and stability.",
      "D": "It suggests a significant performance improvement."
    },
    "answer": "C",
    "explanation": "The text states that \u201chigh variance indicates that the dynamic benchmarking method is not stable.\u201d Therefore, high variance represents instability, not stability.",
    "question_token_count": 28,
    "answer_correctness_score": 1,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 9
  },
  {
    "context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n",
    "topic": "How does data contamination impact the reliability of performance metrics for LLMs, and why is this a significant concern?",
    "response": "`<document_analysis>`\nThe provided context details the concept of \"data contamination\" in the context of Large Language Models (LLMs) and its impact on performance metrics. It distinguishes between exact contamination (identical data points) and syntactic contamination (data points transformed but retaining meaning). The core issue is that if the evaluation data is present in the training data, the performance metrics will be artificially inflated, giving a misleading picture of the model\u2019s true capabilities. The analysis focuses on identifying the types of contamination and the mechanisms by which they can occur.\n\nThe difficulty level is high, requiring a nuanced understanding of how LLMs are trained and evaluated, and the potential for subtle forms of contamination to skew results. The question should probe beyond a simple definition and explore the practical implications.\n\nThe intent is to assess the understanding of the potential pitfalls in LLM evaluation and the different ways in which these pitfalls can manifest.\n</document_analysis>\n`<output_format>`\nQuestion: Syntactic contamination, as defined in the context, poses a significant challenge to LLM evaluation primarily because it:\nA: Results in a complete loss of information, rendering the benchmark data unusable.\nB: Introduces a direct and easily detectable duplicate of test data within the training set.\nC: Creates a scenario where a transformed version of a test example exists in the training data, potentially inflating performance metrics without reflecting genuine model capability.\nD: Only affects the accuracy of metrics related to factual recall.\nExplanation: The correct answer is C. Syntactic contamination involves transforming test data into a form that exists within the training data. This artificially boosts performance metrics, providing a misleading assessment of the LLM\u2019s true capabilities.\nCorrect Answer: C\n</output_format>`",
    "question": "Syntactic contamination, as defined in the context, poses a significant challenge to LLM evaluation primarily because it:",
    "choices": {
      "A": "Results in a complete loss of information, rendering the benchmark data unusable.",
      "B": "Introduces a direct and easily detectable duplicate of test data within the training set.",
      "C": "Creates a scenario where a transformed version of a test example exists in the training data, potentially inflating performance metrics without reflecting genuine model capability.",
      "D": "Only affects the accuracy of metrics related to factual recall."
    },
    "answer": "C",
    "explanation": "The correct answer is C. Syntactic contamination involves transforming test data into a form that exists within the training data. This artificially boosts performance metrics, providing a misleading assessment of the LLM\u2019s true capabilities.",
    "question_token_count": 23,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 18
  },
  {
    "context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n",
    "topic": "The methodology employed by LiveBench for constructing a reliable benchmark to evaluate LLM performance.",
    "response": "<document_analysis>\nThe context describes several benchmarks designed to evaluate LLM performance, specifically addressing the issue of data contamination due to models\u2019 knowledge cutoffs. Each benchmark utilizes a distinct strategy: LiveBench focuses on recent math competitions, AntiLeak-Bench on newly emerged knowledge, AcademicEval on arXiv papers, LiveCodeBench on coding problems, LiveAoPSBench on math problems from a forum, and Forecastbench on daily forecasting questions. The core problem being addressed is the risk of models being trained on or accessing information beyond their cutoff date, which would artificially inflate their performance. The difficulty level is high, requiring an understanding of the nuances of LLM evaluation and the potential for data contamination. Strategic complexity calibration: 8/10.\n</document_analysis>\n<output_format>\nQuestion: Considering the challenges presented by LLM knowledge cutoffs, which of the following benchmarks *most directly* addresses the risk of data contamination by utilizing information exclusively from sources after the model\u2019s training cutoff?\nA: AcademicEval, which focuses on evaluating LLM performance through academic writing tasks.\nB: LiveCodeBench, which leverages human-written coding problems from online competition platforms.\nC: AntiLeak-Bench, which generates queries about newly emerged knowledge unknown prior to the model's cutoff.\nD: LiveBench, which employs math competitions from the preceding 12 months.\nExplanation: AntiLeak-Bench\u2019s explicit design goal is to assess knowledge *beyond* the model\u2019s cutoff date, directly mitigating data contamination. The other benchmarks focus on evaluating performance within the known knowledge domain.\nCorrect Answer: C\n</output_format>",
    "question": "Considering the challenges presented by LLM knowledge cutoffs, which of the following benchmarks *most directly* addresses the risk of data contamination by utilizing information exclusively from sources after the model\u2019s training cutoff?",
    "choices": {
      "A": "AcademicEval, which focuses on evaluating LLM performance through academic writing tasks.",
      "B": "LiveCodeBench, which leverages human-written coding problems from online competition platforms.",
      "C": "AntiLeak-Bench, which generates queries about newly emerged knowledge unknown prior to the model's cutoff.",
      "D": "LiveBench, which employs math competitions from the preceding 12 months."
    },
    "answer": "C",
    "explanation": "AntiLeak-Bench\u2019s explicit design goal is to assess knowledge *beyond* the model\u2019s cutoff date, directly mitigating data contamination. The other benchmarks focus on evaluating performance within the known knowledge domain.",
    "question_token_count": 39,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 16
  },
  {
    "context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n",
    "topic": "The potential for contaminated benchmarks to distort evaluations of LLM generalization and robustness presents a significant challenge to the field.",
    "response": "<document_analysis>\nThe context discusses the problem of \"contaminated benchmarks\" in evaluating Large Language Models (LLMs). It highlights two types of contamination: syntactic (rephrasing training data) and a broader debate about whether such transformations truly represent contamination. The significance of this issue is emphasized \u2013 contaminated benchmarks can lead to inflated performance estimates, hindering accurate assessments of generalization, robustness, and real-world applicability. The core concern is that benchmarks might be assessing memorization rather than genuine reasoning. The difficulty in distinguishing between these two capabilities is a key element of the problem.\n\nStrategic Complexity Calibration: 8/10 \u2013 This requires a nuanced understanding of LLM evaluation methodologies and the potential for subtle biases introduced through data manipulation. It\u2019s not a straightforward factual question.\n\nIntentional Question Planning: The question should probe the expert\u2019s understanding of the *nature* of contamination, not just its existence. It should also require them to consider the practical implications of failing to address this problem.\n</document_analysis>\n<output_format>\nQuestion: Considering the described debate surrounding syntactic contamination, what fundamental limitation does the current inability to definitively distinguish between memorization and reasoning pose to the reliability of LLM benchmark evaluations?\nA: It primarily restricts the types of NLP applications that can benefit from LLM evaluations.\nB: It introduces a systematic bias favoring models with superior recall capabilities over those with genuine reasoning skills.\nC: It necessitates the development of entirely new evaluation metrics, independent of existing benchmarks.\nD: It only affects the evaluation of models trained on smaller datasets.\nExplanation: The correct answer is B. The text explicitly states that contaminated benchmarks can lead to an overestimation of a model\u2019s true capabilities by inadvertently testing it on data it has already seen, thus favoring models that simply memorize data.\nCorrect Answer: B\n</output_format>",
    "question": "Considering the described debate surrounding syntactic contamination, what fundamental limitation does the current inability to definitively distinguish between memorization and reasoning pose to the reliability of LLM benchmark evaluations?",
    "choices": {
      "A": "It primarily restricts the types of NLP applications that can benefit from LLM evaluations.",
      "B": "It introduces a systematic bias favoring models with superior recall capabilities over those with genuine reasoning skills.",
      "C": "It necessitates the development of entirely new evaluation metrics, independent of existing benchmarks.",
      "D": "It only affects the evaluation of models trained on smaller datasets."
    },
    "answer": "B",
    "explanation": "The correct answer is B. The text explicitly states that contaminated benchmarks can lead to an overestimation of a model\u2019s true capabilities by inadvertently testing it on data it has already seen, thus favoring models that simply memorize data.",
    "question_token_count": 34,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 16
  },
  {
    "context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n",
    "topic": "The significance of temporal cutoff dates in the context of evaluating Large Language Models.",
    "response": "<document_analysis>\nThe context details several benchmark datasets designed to evaluate Large Language Models (LLMs) by incorporating temporal cutoff dates. These benchmarks aim to prevent data contamination \u2013 the issue of LLMs being trained on data after their cutoff date, leading to artificially inflated performance metrics. The examples provided (LiveBench, AntiLeak-Bench, etc.) illustrate different strategies for creating these datasets, focusing on recent events, newly emerged knowledge, and current academic research. The core concept is to isolate the model\u2019s ability to reason about information *before* its training data became outdated.\n\nStrategic Complexity Calibration: 8/10 \u2013 This requires understanding the nuances of data contamination and the various approaches to mitigate it. It\u2019s not a simple factual recall question.\n\nIntentional Question Planning: The question should assess the understanding of *why* these temporal cutoff benchmarks are valuable, not just *what* they are. It should encourage reflection on the broader implications of LLM knowledge limitations.\n</document_analysis>\n\n<output_format>\nQuestion: What is the primary methodological justification for the development of benchmarks like AntiLeak-Bench and LiveBench in the context of evaluating Large Language Models?\nA: To assess the model\u2019s ability to accurately predict future events based on historical trends.\nB: To evaluate the model\u2019s performance on tasks requiring knowledge exclusively from its training data, regardless of recency.\nC: To mitigate the risk of data contamination by utilizing information exclusively from sources after the model's knowledge cutoff date.\nD: To identify potential biases in the model\u2019s responses related to current social or political events.\nExplanation: The correct answer is C. The context explicitly states that AntiLeak-Bench generates queries about \u201cnewly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date\u201d to eliminate data contamination.\nCorrect Answer: C\n</output_format>",
    "question": "What is the primary methodological justification for the development of benchmarks like AntiLeak-Bench and LiveBench in the context of evaluating Large Language Models?",
    "choices": {
      "A": "To assess the model\u2019s ability to accurately predict future events based on historical trends.",
      "B": "To evaluate the model\u2019s performance on tasks requiring knowledge exclusively from its training data, regardless of recency.",
      "C": "To mitigate the risk of data contamination by utilizing information exclusively from sources after the model's knowledge cutoff date.",
      "D": "To identify potential biases in the model\u2019s responses related to current social or political events."
    },
    "answer": "C",
    "explanation": "The correct answer is C. The context explicitly states that AntiLeak-Bench generates queries about \u201cnewly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date\u201d to eliminate data contamination.",
    "question_token_count": 30,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 19
  },
  {
    "context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n",
    "topic": "The creation and maintenance of a GitHub repository for collecting static and dynamic LLM benchmarking methods represent a valuable resource for the research community.",
    "response": "```\n<document_analysis>\nThe context describes the creation of a GitHub repository for collecting static and dynamic LLM benchmarking methods. The primary motivation is to address the issue of data contamination in LLMs. The text highlights a gap in the evaluation of dynamic benchmarks and proposes design principles. The repository link is provided. The content is focused on the establishment and purpose of a resource for benchmarking methods.\n\nStrategic Complexity Calibration: 7/10 \u2013 Requires understanding of the broader context of LLM data contamination and benchmarking, and the rationale for a centralized repository.\n\nThought Process: The question needs to go beyond simply stating the existence of the repository. It should probe the expert\u2019s understanding of *why* such a repository is strategically important and what challenges are involved in maintaining it effectively. The question should also implicitly require knowledge of the data contamination problem.\n</document_analysis>\n<output_format>\nQuestion: Considering the documented limitations of existing dynamic LLM benchmarks and the risk of data contamination, what constitutes the most critical operational challenge in maintaining the proposed GitHub repository of benchmarking methods?\nA: Ensuring the repository solely contains static benchmarks to avoid any potential contamination.\nB: Maintaining a comprehensive and continuously updated catalog of diverse dynamic benchmarking approaches, addressing the identified gap in evaluation criteria.\nC: Primarily focusing on attracting a large number of contributors to the repository, regardless of the quality of the submitted methods.\nD: Implementing strict access controls to limit the repository's visibility and prevent unauthorized modifications.\nExplanation: The correct answer (B) directly addresses the identified gap \u2013 the lack of standardized criteria for evaluating dynamic benchmarks \u2013 which is the core problem the repository aims to solve. The other options represent misinterpretations or tangential concerns.\nCorrect Answer: B\n</output_format>\n```",
    "question": "Considering the documented limitations of existing dynamic LLM benchmarks and the risk of data contamination, what constitutes the most critical operational challenge in maintaining the proposed GitHub repository of benchmarking methods?",
    "choices": {
      "A": "Ensuring the repository solely contains static benchmarks to avoid any potential contamination.",
      "B": "Maintaining a comprehensive and continuously updated catalog of diverse dynamic benchmarking approaches, addressing the identified gap in evaluation criteria.",
      "C": "Primarily focusing on attracting a large number of contributors to the repository, regardless of the quality of the submitted methods.",
      "D": "Implementing strict access controls to limit the repository's visibility and prevent unauthorized modifications."
    },
    "answer": "B",
    "explanation": "The correct answer (B) directly addresses the identified gap \u2013 the lack of standardized criteria for evaluating dynamic benchmarks \u2013 which is the core problem the repository aims to solve. The other options represent misinterpretations or tangential concerns.",
    "question_token_count": 35,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 4,
    "question_difficulty_score": 7,
    "question_groundedness_score": 3,
    "avg_answer_token_count": 19
  },
  {
    "context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n",
    "topic": "The proprietary nature of many LLM training datasets complicates accurate performance assessment and benchmark development, further compounding the contamination problem.",
    "response": "<document_analysis>\nThe core issue presented is the difficulty in accurately assessing LLM performance due to the proprietary nature of training datasets. This opacity hinders verification and mitigation of data contamination. The text highlights a significant challenge: the lack of transparency surrounding training data makes it nearly impossible to definitively determine if evaluation data has inadvertently been included. The difficulty in detection is compounded by the scale of the datasets. The question should probe the implications of this opacity beyond simple detection, exploring the broader consequences for benchmark reliability and model evaluation.\n\nStrategic Complexity Calibration: 8/10 \u2013 This requires an understanding of the broader implications of data contamination and the challenges it poses to the field. It\u2019s not a simple factual recall question.\n\n</document_analysis>\n<output_format>\nQuestion: Considering the proprietary nature of LLM training datasets, what is the most significant impediment to establishing truly reliable and unbiased benchmarks for evaluating model performance?\nA: The inherent randomness of the web scraping process.\nB: The difficulty in implementing retrieval-based detection methods.\nC: The lack of transparency regarding training data, hindering verification of data contamination.\nD: The limited availability of human-annotated datasets for fine-tuning.\nExplanation: The correct answer is C. The text explicitly states that \u201cmany LLMs keep their training data proprietary,\u201d which directly impedes the ability to verify and mitigate data contamination \u2013 the core issue of benchmark reliability.\nCorrect Answer: C\n</output_format>",
    "question": "Considering the proprietary nature of LLM training datasets, what is the most significant impediment to establishing truly reliable and unbiased benchmarks for evaluating model performance?",
    "choices": {
      "A": "The inherent randomness of the web scraping process.",
      "B": "The difficulty in implementing retrieval-based detection methods.",
      "C": "The lack of transparency regarding training data, hindering verification of data contamination.",
      "D": "The limited availability of human-annotated datasets for fine-tuning."
    },
    "answer": "C",
    "explanation": "The correct answer is C. The text explicitly states that \u201cmany LLMs keep their training data proprietary,\u201d which directly impedes the ability to verify and mitigate data contamination \u2013 the core issue of benchmark reliability.",
    "question_token_count": 29,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 12
  },
  {
    "context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n",
    "topic": "What are the key differences between evaluating LLMs on SQL tables (S3Eval) versus randomly generated graphs (DyVal, NPHardEval, Xie et al.)?",
    "response": "`<document_analysis>`\nThe context provides a comparison between two methods for evaluating Large Language Models (LLMs) \u2013 S3Eval, which uses SQL tables, and graph-based methods like DyVal, NPHardEval, and Xie et al.\u2019s approach. The core distinction lies in the data structure used for evaluation: SQL tables versus randomly generated graphs (specifically DAGs and puzzles). The text highlights the differences in how these data structures are created and how LLMs are prompted to solve them. The difficulty of the tasks is controlled by varying the complexity of the graphs. There is a clear emphasis on the evolution of evaluation methods, moving from simpler SQL tables to more complex graph-based representations of reasoning problems.\n</document_analysis>\n\n`<output_format>`\nQuestion: What is a fundamental difference in the data structure utilized by S3Eval for LLM evaluation compared to the graph-based approaches (DyVal, NPHardEval, Xie et al.) described in the context?\nA: S3Eval utilizes naturally occurring SQL databases, while graph-based methods generate synthetic data.\nB: S3Eval employs randomly generated SQL tables, whereas graph-based methods utilize randomly generated directed acyclic graphs (DAGs) or puzzle structures.\nC: S3Eval focuses on evaluating LLMs' ability to perform mathematical calculations, while graph-based methods assess logical reasoning.\nD: S3Eval is designed for evaluating LLMs on simple arithmetic problems, while graph-based methods are used for complex combinatorial problems.\nExplanation: The context explicitly states that S3Eval evaluates LLMs on \u201crandomly generated SQL tables\u201d and that graph-based methods utilize \u201crandomly generated directed acyclic graphs (DAGs).\u201d This directly differentiates the core data structures used in each evaluation framework.\nCorrect Answer: B\n</output_format>`",
    "question": "What is a fundamental difference in the data structure utilized by S3Eval for LLM evaluation compared to the graph-based approaches (DyVal, NPHardEval, Xie et al.) described in the context?",
    "choices": {
      "A": "S3Eval utilizes naturally occurring SQL databases, while graph-based methods generate synthetic data.",
      "B": "S3Eval employs randomly generated SQL tables, whereas graph-based methods utilize randomly generated directed acyclic graphs (DAGs) or puzzle structures.",
      "C": "S3Eval focuses on evaluating LLMs' ability to perform mathematical calculations, while graph-based methods assess logical reasoning.",
      "D": "S3Eval is designed for evaluating LLMs on simple arithmetic problems, while graph-based methods are used for complex combinatorial problems."
    },
    "answer": "B",
    "explanation": "The context explicitly states that S3Eval evaluates LLMs on \u201crandomly generated SQL tables\u201d and that graph-based methods utilize \u201crandomly generated directed acyclic graphs (DAGs).\u201d This directly differentiates the core data structures used in each evaluation framework.",
    "question_token_count": 42,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 24
  },
  {
    "context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n",
    "topic": "The evolution of benchmarking strategies, from static to dynamic approaches, reflects the growing difficulty of ensuring the integrity of LLM evaluations as training datasets expand.",
    "response": "`<document_analysis>`\nThe text discusses the increasing difficulty of evaluating Large Language Models (LLMs) due to data contamination as training datasets grow. It highlights the limitations of static benchmarks and the emergence of dynamic approaches. A key concern identified is the lack of standardized criteria for evaluating dynamic benchmarks. The document details specific examples of benchmark generation techniques (LatestEval, DARG, C2LEVA) and outlines challenges related to transparency, scalability, and complexity control in dynamic benchmarking. The core issue revolves around the shift from static to dynamic evaluation strategies necessitated by the scale of modern LLM training data. The difficulty lies in establishing reliable evaluation methods for these dynamic approaches.\n\nStrategic Complexity Calibration: 8/10 (Requires understanding of benchmark evolution, data contamination, and the trade-offs between static and dynamic approaches.)\n</document_analysis>`\n\n`<output_format>`\nQuestion: Considering the observed increase in data contamination as LLM training datasets expand, what fundamental characteristic of static benchmarks, as outlined in the text, necessitates the adoption of dynamic evaluation strategies?\nA: Static benchmarks are inherently resistant to contamination due to their reliance on curated, isolated datasets.\nB: Static benchmarks provide a perfectly transparent and reproducible evaluation process, eliminating the risk of bias.\nC: Static benchmarks\u2019 effectiveness diminishes proportionally with the size of the training dataset, making them increasingly susceptible to contamination.\nD: Static benchmarks are universally accepted as the gold standard for LLM evaluation, regardless of training data scale.\nExplanation: The text explicitly states that \u201cstatic benchmarks become less effective as training corpora grow\u201d and that \u201cthe probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121\u201d. Option C accurately reflects this causal relationship.\nCorrect Answer: C\n</output_format>`",
    "question": "Considering the observed increase in data contamination as LLM training datasets expand, what fundamental characteristic of static benchmarks, as outlined in the text, necessitates the adoption of dynamic evaluation strategies?",
    "choices": {
      "A": "Static benchmarks are inherently resistant to contamination due to their reliance on curated, isolated datasets.",
      "B": "Static benchmarks provide a perfectly transparent and reproducible evaluation process, eliminating the risk of bias.",
      "C": "Static benchmarks\u2019 effectiveness diminishes proportionally with the size of the training dataset, making them increasingly susceptible to contamination.",
      "D": "Static benchmarks are universally accepted as the gold standard for LLM evaluation, regardless of training data scale."
    },
    "answer": "C",
    "explanation": "The text explicitly states that \u201cstatic benchmarks become less effective as training corpora grow\u201d and that \u201cthe probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121\u201d. Option C accurately reflects this causal relationship.",
    "question_token_count": 36,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 20
  },
  {
    "context": "###### Safety\nSafety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content. Datasets such as RealToxicityPrompts\u00a0(Gehman et\u00a0al., 2020) and ToxiGen\u00a0(Hartvigsen et\u00a0al., 2022) assess resilience against producing harmful outputs.\n\nBy providing a controlled environment to measure these aspects, safety benchmarks play a critical role in guiding the development of models that are not only powerful outputs but also responsible and trustworthy for real-world applications.\n\n\n###### Language\nLanguage benchmarks assess the LLMs\u2019 proficiency in specific languages.\n\nGLUE\u00a0(Wang, 2018) and SuperGLUE\u00a0(Wang et\u00a0al., 2019) cover tasks from sentiment analysis to language inference, while CLUE\u00a0(Xu et\u00a0al., 2020) targets Chinese language.\n\nTypo-fixing\u00a0(Suzgun et\u00a0al., 2022) is also widely used.\n\n\n###### Reading Comprehension\nReading comprehension tasks test a model\u2019s ability to extract and infer information from text. Benchmarks like SQuAD\u00a0(Rajpurkar et\u00a0al., 2018), QuAC\u00a0(Choi et\u00a0al., 2018), and BoolQ\u00a0(Clark et\u00a0al., 2019) challenge models to understand passages and draw logical conclusions.\n",
    "topic": "The significance of benchmark datasets like Typo-fixing in assessing language proficiency within LLMs.",
    "response": "<document_analysis>\nThe provided context discusses various benchmarks used to evaluate Large Language Models (LLMs). It highlights the importance of safety benchmarks (RealToxicityPrompts, ToxiGen) for assessing ethical considerations and robustness against harmful outputs, language benchmarks (GLUE, SuperGLUE, CLUE), and reading comprehension benchmarks (SQuAD, QuAC, BoolQ). Typo-fixing is explicitly mentioned as a widely used language benchmark. The difficulty level is moderate, requiring familiarity with the landscape of LLM evaluation. The strategic complexity calibration is set to 7/10, as it requires understanding the *purpose* of benchmarks beyond just recognizing their names.\n\n</document_analysis>\n<output_format>\nQuestion: Considering the diverse range of benchmarks presented, what is the primary differentiating factor in the design and purpose of the Typo-fixing dataset compared to benchmarks like SQuAD or GLUE?\nA: Typo-fixing focuses on assessing a model's ability to generate creative narratives.\nB: Typo-fixing specifically evaluates a model\u2019s proficiency in Chinese language understanding.\nC: Typo-fixing assesses a model\u2019s ability to correct typographical errors, distinguishing it from tasks involving complex reasoning or language generation.\nD: Typo-fixing is designed to measure a model\u2019s resistance to generating toxic outputs, similar to RealToxicityPrompts.\nExplanation: The correct answer is C. The context states that Typo-fixing is \"widely used,\" implying a specific, focused purpose \u2013 correcting typographical errors \u2013 which contrasts with the broader scope of benchmarks like SQuAD (reading comprehension) or GLUE (general language understanding).\nCorrect Answer: C\n</output_format>",
    "question": "Considering the diverse range of benchmarks presented, what is the primary differentiating factor in the design and purpose of the Typo-fixing dataset compared to benchmarks like SQuAD or GLUE?",
    "choices": {
      "A": "Typo-fixing focuses on assessing a model's ability to generate creative narratives.",
      "B": "Typo-fixing specifically evaluates a model\u2019s proficiency in Chinese language understanding.",
      "C": "Typo-fixing assesses a model\u2019s ability to correct typographical errors, distinguishing it from tasks involving complex reasoning or language generation.",
      "D": "Typo-fixing is designed to measure a model\u2019s resistance to generating toxic outputs, similar to RealToxicityPrompts."
    },
    "answer": "C",
    "explanation": "The correct answer is C. The context states that Typo-fixing is \"widely used,\" implying a specific, focused purpose \u2013 correcting typographical errors \u2013 which contrasts with the broader scope of benchmarks like SQuAD (reading comprehension) or GLUE (general language understanding).",
    "question_token_count": 38,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 5,
    "avg_answer_token_count": 21
  },
  {
    "context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n",
    "topic": "VarBench\u2019s method for replacing variables in existing benchmarks to generate novel training data.",
    "response": "<document_analysis>\nThe context describes several methods for generating novel training data from existing benchmarks. These methods include benchmark rewriting (Auto-Dataset, StructEval, ITD) and VarBench. VarBench is specifically described as prompting LLMs to replace variables within existing benchmarks. The text highlights a limitation of these methods \u2013 the potential for contamination, particularly with publicly available rule-generated data.  The difficulty lies in ensuring the generated data truly represents a shift in knowledge and doesn't simply replicate the original dataset. I rate the difficulty as 8/10, as it requires understanding the nuances of data contamination and the strategic application of LLMs.\n\n</document_analysis>\n<output_format>\nQuestion: According to the text, what is the primary mechanism employed by VarBench to generate new training data?\nA: Rewriting existing benchmark samples using an LLM to create related questions.\nB: Identifying and replacing variables within existing benchmark samples using an LLM.\nC: Expanding on examined concepts from the original benchmark using LLMs and knowledge graphs.\nD: Utilizing a contamination detector to identify and correct contaminated samples in static benchmarks.\nExplanation: The text explicitly states, \u201cVarBench (Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\u201d\nCorrect Answer: B\n</output_format>",
    "question": "According to the text, what is the primary mechanism employed by VarBench to generate new training data?",
    "choices": {
      "A": "Rewriting existing benchmark samples using an LLM to create related questions.",
      "B": "Identifying and replacing variables within existing benchmark samples using an LLM.",
      "C": "Expanding on examined concepts from the original benchmark using LLMs and knowledge graphs.",
      "D": "Utilizing a contamination detector to identify and correct contaminated samples in static benchmarks."
    },
    "answer": "B",
    "explanation": "The text explicitly states, \u201cVarBench (Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\u201d",
    "question_token_count": 21,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 9,
    "question_difficulty_score": 2,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 14
  },
  {
    "context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n",
    "topic": "The primary driver for the transition from static to dynamic benchmarking in LLMs is the increasing risk of data contamination due to reliance on large, internet-derived training datasets.",
    "response": "`<document_analysis>`\nThe core issue addressed is the increasing risk of data contamination in LLMs stemming from their training data. The context highlights a shift from static to dynamic benchmarking as a response to this problem. A key challenge identified is the absence of standardized evaluation criteria for dynamic benchmarks. The document outlines a need for improved design principles and a comprehensive overview of existing methods. The link provided is for a GitHub repository containing benchmarking methods, but it's external and not directly relevant to the question itself.\n\nStrategic Complexity Calibration: 8/10 \u2013 This requires understanding the nuances of data contamination and the rationale behind dynamic benchmarking, demanding more than a simple recall of facts.\n\nIntentional Question Planning: The question should probe the understanding of *why* dynamic benchmarking is superior to static, and the specific challenges in evaluating dynamic methods. It should require the ability to synthesize the presented arguments.\n</document_analysis>`\n\n`<output_format>`\nQuestion: What fundamental limitation of static benchmarking, as identified within the text, directly motivates the transition to dynamic benchmarking approaches for LLMs?\nA: The reliance on smaller, curated datasets.\nB: The inherent inability to detect subtle instances of data contamination.\nC: The lack of standardized evaluation criteria, hindering accurate assessment of effectiveness.\nD: The computational expense associated with static benchmark construction.\nExplanation: The text explicitly states a \u201ccritical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks.\u201d This directly addresses the reason for the shift.\nCorrect Answer: C\n</output_format>`",
    "question": "What fundamental limitation of static benchmarking, as identified within the text, directly motivates the transition to dynamic benchmarking approaches for LLMs?",
    "choices": {
      "A": "The reliance on smaller, curated datasets.",
      "B": "The inherent inability to detect subtle instances of data contamination.",
      "C": "The lack of standardized evaluation criteria, hindering accurate assessment of effectiveness.",
      "D": "The computational expense associated with static benchmark construction."
    },
    "answer": "C",
    "explanation": "The text explicitly states a \u201ccritical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks.\u201d This directly addresses the reason for the shift.",
    "question_token_count": 26,
    "answer_correctness_score": 4,
    "explanation_validity_score": 2,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 8,
    "avg_answer_token_count": 11
  },
  {
    "context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n",
    "topic": "A key gap in current LLM benchmarking research is the absence of standardized criteria for evaluating the effectiveness of dynamic benchmarks.",
    "response": "<document_analysis>\nThe core issue highlighted is the lack of standardized evaluation criteria for dynamic LLM benchmarks, a significant gap in the field. The text emphasizes this deficiency and proposes design principles to address it. The provided context is focused on the evolution of benchmarking from static to dynamic methods to combat data contamination. The difficulty level is high as it requires understanding the nuances of benchmarking methodologies and recognizing a critical research gap. The intent is to provoke thought about the challenges in rigorously assessing dynamic benchmarks, going beyond simply knowing they exist. It requires an understanding of the broader research landscape and the need for improved evaluation standards.\n\n</document_analysis>\n<output_format>\nQuestion: What is the primary limitation identified in the research regarding current LLM benchmarking practices?\nA: The reliance on internet-derived training data is a significant concern.\nB: The transition from static to dynamic benchmarks represents a successful mitigation strategy for data contamination.\nC: The absence of standardized criteria for evaluating dynamic benchmarks.\nD: The proposed design principles for dynamic benchmarking are overly complex.\nExplanation: The text explicitly states \u201ca critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks.\u201d\nCorrect Answer: C\n</output_format>",
    "question": "What is the primary limitation identified in the research regarding current LLM benchmarking practices?",
    "choices": {
      "A": "The reliance on internet-derived training data is a significant concern.",
      "B": "The transition from static to dynamic benchmarks represents a successful mitigation strategy for data contamination.",
      "C": "The absence of standardized criteria for evaluating dynamic benchmarks.",
      "D": "The proposed design principles for dynamic benchmarking are overly complex."
    },
    "answer": "C",
    "explanation": "The text explicitly states \u201ca critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks.\u201d",
    "question_token_count": 16,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 4,
    "question_difficulty_score": 7,
    "question_groundedness_score": 2,
    "avg_answer_token_count": 13
  },
  {
    "context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n",
    "topic": "What is the primary concern regarding the correctness of dynamic benchmarking algorithms as presented in the text?",
    "response": "`<document_analysis>`\nThe primary concern highlighted in the text is the potential for dynamic benchmarking algorithms to provide a \u201cfalse sense of reliability\u201d if the correctness of the generated dataset cannot be guaranteed. This stems from the reliance on an oracle function (\ud835\udca2\u2062(\u22c5)) to establish ground truth, and the potential for errors in this oracle to skew the evaluation of LLMs. The text emphasizes that a lack of guaranteed correctness undermines the value of the benchmark. The difficulty lies in the fact that establishing a reliable oracle is a significant challenge in itself.\n\nStrategic Complexity Calibration: 8/10 \u2013 The question requires understanding of a complex evaluation metric and its implications. It\u2019s not immediately obvious why correctness is a primary concern. It demands an understanding of the role of an oracle and the potential for error propagation. </document_analysis>\n\n`<output_format>`\nQuestion: What is the most significant risk associated with dynamic benchmarking algorithms, as described in the text, concerning the reliability of LLM evaluations?\nA: The reliance on a domain-specific annotator as an oracle inevitably introduces subjective bias into the evaluation process.\nB: The computational cost of generating large datasets for dynamic benchmarking is prohibitively expensive for many researchers.\nC: The potential for the generated dataset's outputs to diverge significantly from the ground truth, leading to a false sense of reliability, undermines the benchmark's utility.\nD: The difficulty in defining a universally applicable scoring function (\ud835\udcae\u2062(\u22c5)) prevents consistent evaluation across different LLMs.\nExplanation: The correct answer is C. The text explicitly states that \u201cIf the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability\u2026\u201d This highlights the core concern about the benchmark\u2019s validity.\nCorrect Answer: C\n</output_format>`",
    "question": "What is the most significant risk associated with dynamic benchmarking algorithms, as described in the text, concerning the reliability of LLM evaluations?",
    "choices": {
      "A": "The reliance on a domain-specific annotator as an oracle inevitably introduces subjective bias into the evaluation process.",
      "B": "The computational cost of generating large datasets for dynamic benchmarking is prohibitively expensive for many researchers.",
      "C": "The potential for the generated dataset's outputs to diverge significantly from the ground truth, leading to a false sense of reliability, undermines the benchmark's utility.",
      "D": "The difficulty in defining a universally applicable scoring function (\ud835\udcae\u2062(\u22c5)) prevents consistent evaluation across different LLMs."
    },
    "answer": "C",
    "explanation": "The correct answer is C. The text explicitly states that \u201cIf the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability\u2026\u201d This highlights the core concern about the benchmark\u2019s validity.",
    "question_token_count": 26,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 8,
    "avg_answer_token_count": 24
  },
  {
    "context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n",
    "topic": "The distinctions between HumanEval, MBPP, and SWE-Bench benchmarks in evaluating code synthesis and debugging capabilities.",
    "response": "`<document_analysis>`\nThe context differentiates between several benchmarks used to evaluate language models, specifically focusing on coding, instruction following, and reasoning capabilities. It outlines the purpose and characteristics of HumanEval, MBPP, SWE-Bench, and several others. The key distinction lies in the type of challenge each benchmark presents \u2013 code synthesis/debugging, instruction comprehension, and general reasoning skills. The text also highlights the increasing complexity of these benchmarks, with SWE-Bench representing a more advanced set of challenges. There is no immediately obvious, simple question that can be derived. The text is dense and requires a nuanced understanding of the landscape of benchmarks.\n\nStrategic Complexity Calibration: 8/10 \u2013 Requires understanding of benchmark design and purpose, going beyond a simple factual recall.\n\nIntentional Question Planning: The question should probe the subtle differences in the *nature* of the challenges posed by these benchmarks, requiring the reader to consider the underlying goals of each.\n</document_analysis>`\n\n`<output_format>`\nQuestion: Considering the evolution of coding benchmarks, what is the primary differentiating factor between SWE-Bench and HumanEval/MBPP regarding the types of errors a model might exhibit?\nA: SWE-Bench focuses exclusively on syntax errors, while HumanEval and MBPP primarily assess semantic correctness.\nB: HumanEval and MBPP evaluate code functionality, whereas SWE-Bench prioritizes adherence to specific coding style guidelines.\nC: SWE-Bench assesses the ability to generate complete, functional programs, while HumanEval and MBPP concentrate on debugging existing code.\nD: HumanEval and MBPP are designed for simpler coding tasks, whereas SWE-Bench presents more complex, multi-step problems.\nExplanation: The correct answer (D) highlights that SWE-Bench is intended for more advanced challenges, requiring multi-step problem solving, differentiating it from the more focused tasks of HumanEval and MBPP. The other options misrepresent the core distinctions between the benchmarks.\nCorrect Answer: D\n</output_format>`",
    "question": "Considering the evolution of coding benchmarks, what is the primary differentiating factor between SWE-Bench and HumanEval/MBPP regarding the types of errors a model might exhibit?",
    "choices": {
      "A": "SWE-Bench focuses exclusively on syntax errors, while HumanEval and MBPP primarily assess semantic correctness.",
      "B": "HumanEval and MBPP evaluate code functionality, whereas SWE-Bench prioritizes adherence to specific coding style guidelines.",
      "C": "SWE-Bench assesses the ability to generate complete, functional programs, while HumanEval and MBPP concentrate on debugging existing code.",
      "D": "HumanEval and MBPP are designed for simpler coding tasks, whereas SWE-Bench presents more complex, multi-step problems."
    },
    "answer": "D",
    "explanation": "The correct answer (D) highlights that SWE-Bench is intended for more advanced challenges, requiring multi-step problem solving, differentiating it from the more focused tasks of HumanEval and MBPP. The other options misrepresent the core distinctions between the benchmarks.",
    "question_token_count": 34,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 8,
    "avg_answer_token_count": 24
  },
  {
    "context": "###### Safety\nSafety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content. Datasets such as RealToxicityPrompts\u00a0(Gehman et\u00a0al., 2020) and ToxiGen\u00a0(Hartvigsen et\u00a0al., 2022) assess resilience against producing harmful outputs.\n\nBy providing a controlled environment to measure these aspects, safety benchmarks play a critical role in guiding the development of models that are not only powerful outputs but also responsible and trustworthy for real-world applications.\n\n\n###### Language\nLanguage benchmarks assess the LLMs\u2019 proficiency in specific languages.\n\nGLUE\u00a0(Wang, 2018) and SuperGLUE\u00a0(Wang et\u00a0al., 2019) cover tasks from sentiment analysis to language inference, while CLUE\u00a0(Xu et\u00a0al., 2020) targets Chinese language.\n\nTypo-fixing\u00a0(Suzgun et\u00a0al., 2022) is also widely used.\n\n\n###### Reading Comprehension\nReading comprehension tasks test a model\u2019s ability to extract and infer information from text. Benchmarks like SQuAD\u00a0(Rajpurkar et\u00a0al., 2018), QuAC\u00a0(Choi et\u00a0al., 2018), and BoolQ\u00a0(Clark et\u00a0al., 2019) challenge models to understand passages and draw logical conclusions.\n",
    "topic": "The role of safety benchmarks in evaluating LLM robustness and ethical alignment, specifically referencing datasets like RealToxicityPrompts and ToxiGen.",
    "response": "<document_analysis>\nThe context focuses on various language benchmarks used to evaluate Large Language Models (LLMs). It highlights benchmarks for safety, language proficiency, and reading comprehension. Specifically, it mentions RealToxicityPrompts and ToxiGen as safety benchmarks. The analysis reveals a need to probe the expert\u2019s understanding of the purpose and construction of these benchmarks, going beyond simply naming them. The difficulty level should be high, requiring an understanding of the underlying methodology and implications. The question should not explicitly reference the text, but rather assess general knowledge about the benchmarks.\n</document_analysis>\n<output_format>\nQuestion: Considering the design goals of datasets like RealToxicityPrompts and ToxiGen, what is the primary methodological difference between them in assessing LLM safety?\nA: Both datasets rely on human annotation of toxicity levels for every generated response.\nB: ToxiGen focuses on detecting subtle biases, while RealToxicityPrompts prioritizes identifying overtly harmful outputs.\nC: RealToxicityPrompts uses a single, pre-defined toxicity threshold, whereas ToxiGen employs a dynamically adjusted threshold based on model output.\nD: Both datasets are entirely based on automated analysis of generated text without any human input.\nExplanation: The correct answer (C) reflects the nuanced approach of ToxiGen, which adapts its toxicity detection based on the model's output, distinguishing it from RealToxicityPrompts, which uses a static threshold.  Options A and D are incorrect because both datasets incorporate human input. Option B misrepresents the core focus of each dataset.\nCorrect Answer: C\n</output_format>",
    "question": "Considering the design goals of datasets like RealToxicityPrompts and ToxiGen, what is the primary methodological difference between them in assessing LLM safety?",
    "choices": {
      "A": "Both datasets rely on human annotation of toxicity levels for every generated response.",
      "B": "ToxiGen focuses on detecting subtle biases, while RealToxicityPrompts prioritizes identifying overtly harmful outputs.",
      "C": "RealToxicityPrompts uses a single, pre-defined toxicity threshold, whereas ToxiGen employs a dynamically adjusted threshold based on model output.",
      "D": "Both datasets are entirely based on automated analysis of generated text without any human input."
    },
    "answer": "C",
    "explanation": "The correct answer (C) reflects the nuanced approach of ToxiGen, which adapts its toxicity detection based on the model's output, distinguishing it from RealToxicityPrompts, which uses a static threshold.  Options A and D are incorrect because both datasets incorporate human input. Option B misrepresents the core focus of each dataset.",
    "question_token_count": 32,
    "answer_correctness_score": 8,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 4,
    "avg_answer_token_count": 21
  },
  {
    "context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n",
    "topic": "What are the primary distinctions between encryption and label protection as strategies for safeguarding evaluation data?",
    "response": "<document_analysis>\nThe context distinguishes between encryption and label protection as data safeguarding strategies. Encryption focuses on making data inaccessible, while label protection focuses on preventing models from learning the true answers. Both methods have limitations, particularly concerning computational overhead and the risk of compromise. The core difference lies in the mechanism: encryption secures the data itself, while label protection secures the evaluation targets. The text highlights the vulnerability of encryption if keys are compromised and the integrity maintained by label protection.\n\nStrategic Complexity Calibration: 8/10 \u2013 This requires understanding the nuances of both techniques and their respective weaknesses, going beyond a simple definition. It probes the expert's understanding of the underlying principles and potential vulnerabilities.\n</document_analysis>\n<output_format>\nQuestion: What is the fundamental distinction between encryption and label protection as methods for preserving the integrity of evaluation datasets?\nA: Encryption focuses on preventing unauthorized access to the data itself, while label protection prevents models from learning the correct answers.\nB: Both methods achieve the same goal \u2013 ensuring data confidentiality \u2013 but employ different techniques.\nC: Encryption primarily addresses the risk of data leakage, whereas label protection mitigates the risk of model memorization.\nD: Label protection involves concealing the answers, while encryption involves scrambling the data.\nExplanation: The correct answer (A) accurately captures the core difference: encryption secures the data itself, preventing access, while label protection shields the answers from model exposure.\nCorrect Answer: A\n</output_format>",
    "question": "What is the fundamental distinction between encryption and label protection as methods for preserving the integrity of evaluation datasets?",
    "choices": {
      "A": "Encryption focuses on preventing unauthorized access to the data itself, while label protection prevents models from learning the correct answers.",
      "B": "Both methods achieve the same goal \u2013 ensuring data confidentiality \u2013 but employ different techniques.",
      "C": "Encryption primarily addresses the risk of data leakage, whereas label protection mitigates the risk of model memorization.",
      "D": "Label protection involves concealing the answers, while encryption involves scrambling the data."
    },
    "answer": "A",
    "explanation": "The correct answer (A) accurately captures the core difference: encryption secures the data itself, preventing access, while label protection shields the answers from model exposure.",
    "question_token_count": 21,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 19
  },
  {
    "context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n",
    "topic": "The AIME and CNMO challenges represent recent benchmarks designed to rigorously test a model's mathematical problem-solving skills.",
    "response": "`<document_analysis>`\nThe context describes different types of benchmarks used to evaluate large language models (LLMs), specifically focusing on mathematical problem-solving skills (AIME and CNMO challenges) and general knowledge. It details datasets like GSM8K and MATH, as well as benchmarks for knowledge retrieval and multi-domain tasks. The text also highlights newer benchmarks designed for technical and long-context challenges. The core theme is the evolution of benchmarks to assess increasingly complex capabilities of LLMs.\n\nStrategic Complexity Calibration: 8/10 \u2013 The question needs to probe beyond simple recall and require an understanding of the *purpose* and *relative importance* of these different benchmark types. It should also assess the expert's grasp of the evolving landscape of LLM evaluation.\n\nIntentional Question Planning: The question will be designed to assess an understanding of how benchmarks are used to evaluate different aspects of LLM capabilities, and to prompt reflection on the criteria for judging a model's performance. </document_analysis>\n\n<output_format>\nQuestion: Considering the described benchmarks (AIME, CNMO, GSM8K, MATH, etc.), which of the following best characterizes the primary distinction between mathematical problem-solving benchmarks and knowledge benchmarks?\nA: Mathematical benchmarks exclusively assess factual recall, while knowledge benchmarks focus on creative problem generation.\nB: Mathematical benchmarks evaluate a model\u2019s ability to apply learned concepts to novel, multi-step problems, whereas knowledge benchmarks primarily test the breadth of the model\u2019s internal knowledge base.\nC: Both types of benchmarks are fundamentally equivalent in their evaluation criteria, differing only in the specific datasets used.\nD: Mathematical benchmarks assess the model's reasoning process, while knowledge benchmarks evaluate the model's ability to synthesize information from multiple sources.\nExplanation: The correct answer (B) accurately reflects the core distinction highlighted in the text \u2013 mathematical benchmarks prioritize applying existing knowledge to solve complex, novel problems, whereas knowledge benchmarks focus on demonstrating a broad understanding of factual information. Option A is incorrect as mathematical benchmarks are not just about recall. Option C is incorrect as the benchmarks have distinct goals. Option D is incorrect as the benchmarks evaluate different aspects of an LLM's capabilities.\nCorrect Answer: B\n</output_format>",
    "question": "Considering the described benchmarks (AIME, CNMO, GSM8K, MATH, etc.), which of the following best characterizes the primary distinction between mathematical problem-solving benchmarks and knowledge benchmarks?",
    "choices": {
      "A": "Mathematical benchmarks exclusively assess factual recall, while knowledge benchmarks focus on creative problem generation.",
      "B": "Mathematical benchmarks evaluate a model\u2019s ability to apply learned concepts to novel, multi-step problems, whereas knowledge benchmarks primarily test the breadth of the model\u2019s internal knowledge base.",
      "C": "Both types of benchmarks are fundamentally equivalent in their evaluation criteria, differing only in the specific datasets used.",
      "D": "Mathematical benchmarks assess the model's reasoning process, while knowledge benchmarks evaluate the model's ability to synthesize information from multiple sources."
    },
    "answer": "B",
    "explanation": "The correct answer (B) accurately reflects the core distinction highlighted in the text \u2013 mathematical benchmarks prioritize applying existing knowledge to solve complex, novel problems, whereas knowledge benchmarks focus on demonstrating a broad understanding of factual information. Option A is incorrect as mathematical benchmarks are not just about recall. Option C is incorrect as the benchmarks have distinct goals. Option D is incorrect as the benchmarks evaluate different aspects of an LLM's capabilities.",
    "question_token_count": 38,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 23
  },
  {
    "context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n",
    "topic": "The lack of transparency regarding training data impedes community efforts to verify and mitigate data contamination, highlighting the need for improved evaluation practices.",
    "response": "`<document_analysis>`\nThe context discusses the significant challenge of data contamination in Large Language Models (LLMs) due to the lack of transparency surrounding their training data. It highlights the difficulty in verifying and mitigating contamination because of the vast, often proprietary, datasets used for training, coupled with the potential for overlap between training and evaluation data. The core problem is the opacity of the training process, which hinders community efforts to ensure reliable benchmarks. The difficulty in detecting contamination is compounded by the existence of retrieval-based detection methods that are not entirely effective given the scale of the training data. The passage emphasizes the need for improved evaluation practices and benchmarks.\n\nStrategic Complexity Calibration: 8/10 \u2013 This requires an understanding of LLM training methodologies, data contamination risks, and the implications of data opacity. It\u2019s not a straightforward factual recall but demands critical thinking about the broader challenges in the field.\n\nIntentional Question Planning: I will craft a question that probes the implications of this lack of transparency, forcing the expert to consider the wider consequences beyond simply identifying contamination. </document_analysis>\n\n`<output_format>`\nQuestion: Considering the inherent opacity surrounding LLM training data, what is the most significant impediment to establishing truly unbiased and reliable benchmarks for evaluating model performance?\nA: The difficulty in implementing retrieval-based detection methods.\nB: The potential for overlap between training and evaluation datasets, hindering accurate performance assessment.\nC: The reliance on human-annotated datasets for fine-tuning.\nD: The prevalence of synthetic datasets used in training.\nExplanation: The text explicitly states that \u201cThis opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\u201d Option B directly reflects this core argument.\nCorrect Answer: B\n</output_format>`",
    "question": "Considering the inherent opacity surrounding LLM training data, what is the most significant impediment to establishing truly unbiased and reliable benchmarks for evaluating model performance?",
    "choices": {
      "A": "The difficulty in implementing retrieval-based detection methods.",
      "B": "The potential for overlap between training and evaluation datasets, hindering accurate performance assessment.",
      "C": "The reliance on human-annotated datasets for fine-tuning.",
      "D": "The prevalence of synthetic datasets used in training."
    },
    "answer": "B",
    "explanation": "The text explicitly states that \u201cThis opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\u201d Option B directly reflects this core argument.",
    "question_token_count": 29,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 12
  },
  {
    "context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n",
    "topic": "Provide an example of what the \u201coracle\u201d function could be, as described in the text.",
    "response": "`<document_analysis>`\nThe text describes an \"oracle\" function as a component of a dynamic benchmarking algorithm. Specifically, it\u2019s defined as a function that returns the ground truth for a given input. The example provided is a domain-specific annotator. The key takeaway is that the oracle provides an objective reference for evaluating the correctness of the generated dataset. The text emphasizes that the oracle ensures alignment between transformed dataset outputs and ground truth values. The difficulty level is moderate, requiring an understanding of benchmarking concepts and the role of reference points.\n\n</document_analysis>\n`<output_format>`\nQuestion: In the context of dynamic benchmarking, what is the primary function of the \u201coracle\u201d described in the text?\nA: To generate synthetic data for evaluating LLMs.\nB: To provide a ground truth reference for assessing the correctness of a dynamic benchmark.\nC: To automatically score the performance of LLMs on a given dataset.\nD: To modify the input data to improve the accuracy of the benchmark.\nExplanation: The text explicitly states that the oracle \u201creturns the ground truth of its input,\u201d serving as an objective reference for correctness evaluation.\nCorrect Answer: B\n</output_format>",
    "question": "In the context of dynamic benchmarking, what is the primary function of the \u201coracle\u201d described in the text?",
    "choices": {
      "A": "To generate synthetic data for evaluating LLMs.",
      "B": "To provide a ground truth reference for assessing the correctness of a dynamic benchmark.",
      "C": "To automatically score the performance of LLMs on a given dataset.",
      "D": "To modify the input data to improve the accuracy of the benchmark."
    },
    "answer": "B",
    "explanation": "The text explicitly states that the oracle \u201creturns the ground truth of its input,\u201d serving as an objective reference for correctness evaluation.",
    "question_token_count": 23,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 13
  },
  {
    "context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n",
    "topic": "The subsequent fine-tuning process on human-annotated or synthetic datasets exacerbates the contamination risk, mirroring potential evaluation tasks.",
    "response": "<document_analysis>\nThe context discusses the significant risk of contamination in Large Language Model (LLM) evaluation due to the way they are trained. Specifically, fine-tuning on datasets resembling evaluation tasks, often sourced from human annotations or synthetic data, amplifies this risk. The core challenge is the difficulty in completely excluding evaluation data from the massive and often proprietary training corpora. The opacity surrounding training data adds another layer of complexity.\n\nStrategic Complexity Calibration: 8/10 \u2013 This requires understanding the nuances of LLM training, the nature of contamination, and the challenges of data provenance. It\u2019s not a straightforward factual question.\n\nIntentional Question Planning: I will craft a question that probes the expert's understanding of *why* fine-tuning on data resembling evaluation tasks is particularly problematic, moving beyond simply stating that it increases contamination. The question will require an inference about the underlying mechanisms.\n</document_analysis>\n<output_format>\nQuestion: Considering the challenges of evaluating LLMs, why does fine-tuning on datasets designed to mimic evaluation tasks represent a heightened risk of contamination, beyond simply increasing the volume of potentially overlapping data?\nA: Because synthetic datasets invariably lack the biases present in human-annotated data, reducing the likelihood of contamination.\nB: Because the process of fine-tuning inherently alters the model\u2019s parameters, effectively removing any overlap with the original training data.\nC: Because such fine-tuning replicates the evaluation process itself, introducing systematic errors that are difficult to detect.\nD: Because the similarity between fine-tuning datasets and evaluation tasks creates a feedback loop, reinforcing biases and amplifying contamination.\nExplanation: The correct answer (D) highlights the core issue: the feedback loop created by mimicking evaluation tasks during fine-tuning. This process perpetuates and intensifies the contamination, making it a more significant problem than simply adding more potentially contaminated data.\nCorrect Answer: D\n</output_format>",
    "question": "Considering the challenges of evaluating LLMs, why does fine-tuning on datasets designed to mimic evaluation tasks represent a heightened risk of contamination, beyond simply increasing the volume of potentially overlapping data?",
    "choices": {
      "A": "Because synthetic datasets invariably lack the biases present in human-annotated data, reducing the likelihood of contamination.",
      "B": "Because the process of fine-tuning inherently alters the model\u2019s parameters, effectively removing any overlap with the original training data.",
      "C": "Because such fine-tuning replicates the evaluation process itself, introducing systematic errors that are difficult to detect.",
      "D": "Because the similarity between fine-tuning datasets and evaluation tasks creates a feedback loop, reinforcing biases and amplifying contamination."
    },
    "answer": "D",
    "explanation": "The correct answer (D) highlights the core issue: the feedback loop created by mimicking evaluation tasks during fine-tuning. This process perpetuates and intensifies the contamination, making it a more significant problem than simply adding more potentially contaminated data.",
    "question_token_count": 37,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 22
  },
  {
    "context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "The mathematical representation of a dynamic benchmark, including the roles of the dataset, transformation function, and timestamped data sets.",
    "response": "<document_analysis>\nThe context describes a dynamic benchmark, a method to evaluate LLMs that addresses the limitations of static benchmarks. Key components are the dataset (\ud835\udc9f), the transformation function (T), and timestamped datasets (\ud835\udc9ft). The transformation function modifies the dataset over time (N timestamps). The core idea is to avoid data contamination by altering the dataset. The document provides a mathematical representation of the dynamic benchmark. The difficulty is moderate, requiring understanding of mathematical notation and the concept of a dynamic dataset. The question should probe the understanding of the roles of each component and the purpose of the transformation function.\n</document_analysis>\n\n<output_format>\nQuestion: What is the primary rationale for employing a transformation function (T) within the framework of a dynamic benchmark, as described in the provided text?\nA: To ensure the dataset remains static throughout the evaluation process, minimizing the risk of data contamination.\nB: To introduce a controlled element of randomness into the benchmarking process, allowing for more robust performance analysis.\nC: To adapt the dataset over time, mitigating potential biases and ensuring a more realistic assessment of the LLM\u2019s capabilities.\nD: To simplify the dataset, reducing computational complexity and accelerating the benchmarking process.\nExplanation: The text explicitly states that the transformation function \"modifies the data set during the benchmarking to avoid possible data contamination.\"\nCorrect Answer: C\n</output_format>",
    "question": "What is the primary rationale for employing a transformation function (T) within the framework of a dynamic benchmark, as described in the provided text?",
    "choices": {
      "A": "To ensure the dataset remains static throughout the evaluation process, minimizing the risk of data contamination.",
      "B": "To introduce a controlled element of randomness into the benchmarking process, allowing for more robust performance analysis.",
      "C": "To adapt the dataset over time, mitigating potential biases and ensuring a more realistic assessment of the LLM\u2019s capabilities.",
      "D": "To simplify the dataset, reducing computational complexity and accelerating the benchmarking process."
    },
    "answer": "C",
    "explanation": "The text explicitly states that the transformation function \"modifies the data set during the benchmarking to avoid possible data contamination.\"",
    "question_token_count": 29,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 4,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 19
  },
  {
    "context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "The evolution of LLM evaluation, from static to dynamic benchmarking, and the associated research landscape.",
    "response": "<document_analysis>\nThe provided text discusses the evolution of LLM evaluation, shifting from static benchmarks to dynamic approaches due to the problem of data contamination. Key concepts include static benchmarking (e.g., HumanEval), data contamination (when benchmark data is included in training), and dynamic benchmarking methods like continuously updating datasets and regenerating benchmarks. The text highlights a gap in the literature regarding a systematic survey of dynamic benchmarking and criteria for evaluating them. The document\u2019s primary focus is the evolving landscape of LLM evaluation, specifically addressing the challenges posed by data contamination and the need for more robust benchmarking strategies. The difficulty rating is 8/10 \u2013 it requires understanding of ML principles, benchmarking methodologies, and current research trends.\n\n</document_analysis>\n<output_format>\nQuestion: Considering the inherent risk of data contamination impacting static LLM benchmarks, what fundamental methodological shift does the text primarily advocate for in the evaluation of these models?\nA: Implementing differential privacy techniques during model training to isolate benchmark data.\nB: Transitioning to continuous, real-time monitoring of LLM training data sources to identify contamination.\nC: Employing dynamic benchmarking strategies that regenerate or continuously update benchmark datasets, minimizing the risk of data leakage.\nD: Focusing exclusively on human evaluation of LLM outputs, bypassing automated benchmarking altogether.\nExplanation: The text explicitly states the move from static to dynamic benchmarking due to data contamination. Option C accurately reflects this shift, as it describes methods designed to mitigate contamination. Options A, B, and D are incorrect as they propose solutions that are not central to the argument presented in the text.\nCorrect Answer: C\n</output_format>",
    "question": "Considering the inherent risk of data contamination impacting static LLM benchmarks, what fundamental methodological shift does the text primarily advocate for in the evaluation of these models?",
    "choices": {
      "A": "Implementing differential privacy techniques during model training to isolate benchmark data.",
      "B": "Transitioning to continuous, real-time monitoring of LLM training data sources to identify contamination.",
      "C": "Employing dynamic benchmarking strategies that regenerate or continuously update benchmark datasets, minimizing the risk of data leakage.",
      "D": "Focusing exclusively on human evaluation of LLM outputs, bypassing automated benchmarking altogether."
    },
    "answer": "C",
    "explanation": "The text explicitly states the move from static to dynamic benchmarking due to data contamination. Option C accurately reflects this shift, as it describes methods designed to mitigate contamination. Options A, B, and D are incorrect as they propose solutions that are not central to the argument presented in the text.",
    "question_token_count": 30,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 17
  },
  {
    "context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n",
    "topic": "How might a high \"Collision Rate\" impact the reliability of a dynamic benchmark for evaluating LLM capabilities?",
    "response": "<document_analysis>\nThe context describes a problem \u2013 data contamination when using dynamic benchmarks \u2013 and introduces two metrics to address it: Collision Rate and Repeat Trials. The core idea is to understand how much overlap there is between different transformations of the benchmark, and how many trials are needed to generate entirely new variations. The text emphasizes the importance of these metrics for assessing the benchmark's reliability in the face of potential training data leakage.\n\nStrategic Complexity Calibration: 8/10 \u2013 This requires understanding the nuances of data contamination, the concept of transformations, and the implications of overlap in benchmark datasets. It\u2019s not a simple factual question; it demands an understanding of the underlying problem and the proposed solution.\n\nIntentional Question Planning: I want to create a question that probes the expert\u2019s understanding of how a high collision rate would fundamentally undermine the purpose of a dynamic benchmark, moving beyond a simple definition of the metrics.\n</document_analysis>\n\n<output_format>\nQuestion: In the context of dynamic benchmarking, what is the primary consequence of a high Collision Rate, and how does this impact the benchmark\u2019s utility?\nA: It indicates a high degree of diversity in the generated test cases.\nB: It suggests that the benchmark is resistant to data contamination.\nC: It implies that the benchmark\u2019s ability to generate novel test cases is compromised, potentially leading to inaccurate evaluations of LLM capabilities.\nD: It signifies that the benchmark requires more computational resources.\nExplanation: The text defines Collision Rate as the percentage of overlap between transformed datasets. High overlap suggests limited diversity and a reduced ability to generate truly novel test cases, directly undermining the benchmark\u2019s purpose.\nCorrect Answer: C\n</output_format>",
    "question": "In the context of dynamic benchmarking, what is the primary consequence of a high Collision Rate, and how does this impact the benchmark\u2019s utility?",
    "choices": {
      "A": "It indicates a high degree of diversity in the generated test cases.",
      "B": "It suggests that the benchmark is resistant to data contamination.",
      "C": "It implies that the benchmark\u2019s ability to generate novel test cases is compromised, potentially leading to inaccurate evaluations of LLM capabilities.",
      "D": "It signifies that the benchmark requires more computational resources."
    },
    "answer": "C",
    "explanation": "The text defines Collision Rate as the percentage of overlap between transformed datasets. High overlap suggests limited diversity and a reduced ability to generate truly novel test cases, directly undermining the benchmark\u2019s purpose.",
    "question_token_count": 29,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 15
  },
  {
    "context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n",
    "topic": "Explain the difference between \"exact contamination\" and \"syntactic contamination,\" providing illustrative examples of each.",
    "response": "`<document_analysis>`\nThe context differentiates between \"exact contamination\" and \"syntactic contamination,\" both forms of data contamination impacting LLM evaluation. Exact contamination involves direct duplication of data points, while syntactic contamination involves transformations of test data that result in matches within the training data. The text provides illustrative examples of each type, including verbatim examples and paraphrased versions. The difficulty level is moderate, requiring an understanding of data manipulation and its potential impact on model performance. The intent is to assess the reader\u2019s comprehension of these distinct contamination types and their implications.\n`</document_analysis>`\n\n`<output_format>`\nQuestion: Which of the following best describes the key distinction between \"exact contamination\" and \"syntactic contamination\" as presented in the text?\nA: Exact contamination refers to instances where test data is paraphrased, while syntactic contamination involves verbatim duplication.\nB: Exact contamination involves direct duplication of data points, whereas syntactic contamination involves data points that can be transformed syntactically to match training data.\nC: Both terms describe the same phenomenon, indicating a lack of diversity in the training data.\nD: Exact contamination only affects text data, while syntactic contamination exclusively impacts code snippets.\nExplanation: The correct answer highlights the fundamental difference: exact duplication versus transformation via syntactic operations.\nCorrect Answer: B\n</output_format>`",
    "question": "Which of the following best describes the key distinction between \"exact contamination\" and \"syntactic contamination\" as presented in the text?",
    "choices": {
      "A": "Exact contamination refers to instances where test data is paraphrased, while syntactic contamination involves verbatim duplication.",
      "B": "Exact contamination involves direct duplication of data points, whereas syntactic contamination involves data points that can be transformed syntactically to match training data.",
      "C": "Both terms describe the same phenomenon, indicating a lack of diversity in the training data.",
      "D": "Exact contamination only affects text data, while syntactic contamination exclusively impacts code snippets."
    },
    "answer": "B",
    "explanation": "The correct answer highlights the fundamental difference: exact duplication versus transformation via syntactic operations.",
    "question_token_count": 28,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 21
  },
  {
    "context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n",
    "topic": "Explain how TRUCE\u2019s use of confidential computing and secure multi-party computation contributes to private benchmarking, and why this is crucial for data confidentiality.",
    "response": "<document_analysis>\nThe context describes TRUCE\u2019s use of confidential computing and secure multi-party computation for private benchmarking. It highlights the importance of protecting test data and model parameters from unauthorized access. The passage also acknowledges limitations, such as computational overhead and vulnerabilities related to key management. The core concept is that TRUCE\u2019s approach allows for benchmarking without revealing the underlying data, which is crucial for maintaining data confidentiality.  The difficulty level is high because it requires understanding of complex cryptographic techniques and their application in a specific benchmarking context. The question aims to probe the expert\u2019s understanding of *why* this approach is so important beyond simply stating that it\u2019s used.\n</document_analysis>\n\n<output_format>\nQuestion: What is the primary strategic advantage of TRUCE\u2019s utilization of confidential computing and secure multi-party computation within the context of private benchmarking, as detailed in the provided text?\nA: It significantly reduces the computational cost of model evaluation.\nB: It guarantees complete immunity from all forms of data leakage, regardless of system vulnerabilities.\nC: It ensures that test data and model parameters remain confidential, preventing unauthorized access and potential data contamination during benchmarking.\nD: It automatically generates diverse test datasets, mitigating the risk of overfitting.\nExplanation: The correct answer is C. The text explicitly states that TRUCE\u2019s methods \u201censure that test data and model parameters remain confidential.\u201d The other options are either incorrect or misinterpret the core benefit of the technology.\nCorrect Answer: C\n</output_format>",
    "question": "What is the primary strategic advantage of TRUCE\u2019s utilization of confidential computing and secure multi-party computation within the context of private benchmarking, as detailed in the provided text?",
    "choices": {
      "A": "It significantly reduces the computational cost of model evaluation.",
      "B": "It guarantees complete immunity from all forms of data leakage, regardless of system vulnerabilities.",
      "C": "It ensures that test data and model parameters remain confidential, preventing unauthorized access and potential data contamination during benchmarking.",
      "D": "It automatically generates diverse test datasets, mitigating the risk of overfitting."
    },
    "answer": "C",
    "explanation": "The correct answer is C. The text explicitly states that TRUCE\u2019s methods \u201censure that test data and model parameters remain confidential.\u201d The other options are either incorrect or misinterpret the core benefit of the technology.",
    "question_token_count": 34,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 16
  },
  {
    "context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n",
    "topic": "Recent advancements in LLM evaluation are reflected in benchmarks like MMLU-Redux and MMLU-Pro, which refine existing assessments.",
    "response": "`<document_analysis>`\nThe provided text discusses recent advancements in LLM evaluation, specifically highlighting benchmarks like MMLU-Redux and MMLU-Pro. It details the evolution of math and knowledge benchmarks and introduces newer, more challenging evaluations. The core concept is the refinement of existing assessments to better gauge LLM capabilities. The difficulty level is high, requiring an understanding of the landscape of LLM evaluation methodologies. Strategic complexity calibration: 8/10.\n</document_analysis>\n\n`<output_format>`\nQuestion: MMLU-Redux and MMLU-Pro represent an iterative approach to evaluating LLMs. What is the primary distinguishing characteristic of these benchmarks compared to their predecessors like MMLU?\nA: They utilize exclusively mathematical problem-solving tasks.\nB: They incorporate a significantly reduced dataset size to improve evaluation efficiency.\nC: They represent refinements of existing assessments, incorporating more challenging question formulations and expanded datasets.\nD: They are exclusively designed for evaluating models\u2019 ability to generate creative text formats.\nExplanation: The text explicitly states that MMLU-Redux and MMLU-Pro \u201crefine these assessments further,\u201d indicating an improvement and adjustment to the original MMLU benchmark.\nCorrect Answer: C\n</output_format>",
    "question": "MMLU-Redux and MMLU-Pro represent an iterative approach to evaluating LLMs. What is the primary distinguishing characteristic of these benchmarks compared to their predecessors like MMLU?",
    "choices": {
      "A": "They utilize exclusively mathematical problem-solving tasks.",
      "B": "They incorporate a significantly reduced dataset size to improve evaluation efficiency.",
      "C": "They represent refinements of existing assessments, incorporating more challenging question formulations and expanded datasets.",
      "D": "They are exclusively designed for evaluating models\u2019 ability to generate creative text formats."
    },
    "answer": "C",
    "explanation": "The text explicitly states that MMLU-Redux and MMLU-Pro \u201crefine these assessments further,\u201d indicating an improvement and adjustment to the original MMLU benchmark.",
    "question_token_count": 38,
    "answer_correctness_score": 10,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 14
  },
  {
    "context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n",
    "topic": "Explain the rationale for utilizing directed acyclic graphs (DAGs) in the DyVal framework and how they are transformed for LLM evaluation.",
    "response": "<document_analysis>\nThe context describes the DyVal framework, which uses directed acyclic graphs (DAGs) to evaluate LLMs' reasoning abilities. The process involves constructing DAGs of varying complexity, converting them to natural language descriptions, and then querying the LLM for the root node\u2019s value. The purpose of using DAGs is to control task difficulty by adjusting the number of nodes and edges. The core idea is to assess the LLM's ability to reason through a structured, logical representation of a problem. The transformation from DAG to natural language is crucial for presenting the problem to the LLM in a format it can understand.\n\nStrategic Complexity Calibration: 8/10 \u2013 This requires an understanding of graph theory, logical reasoning, and the specifics of the DyVal framework. It\u2019s not a simple factual recall but demands application of concepts.\n</document_analysis>\n<output_format>\nQuestion: What is the primary justification for employing directed acyclic graphs (DAGs) within the DyVal framework for evaluating LLM reasoning capabilities?\nA: DAGs provide a simplified representation of complex SQL queries, enabling faster evaluation.\nB: DAGs offer a controlled mechanism for adjusting the difficulty of reasoning tasks by manipulating node and edge counts.\nC: DAGs are inherently more efficient than random graphs for generating SQL tables.\nD: DAGs guarantee that LLMs will always arrive at the correct solution.\nExplanation: The context explicitly states that DAGs are constructed with varying numbers of nodes and edges to control task difficulty.\nCorrect Answer: B\n</output_format>",
    "question": "What is the primary justification for employing directed acyclic graphs (DAGs) within the DyVal framework for evaluating LLM reasoning capabilities?",
    "choices": {
      "A": "DAGs provide a simplified representation of complex SQL queries, enabling faster evaluation.",
      "B": "DAGs offer a controlled mechanism for adjusting the difficulty of reasoning tasks by manipulating node and edge counts.",
      "C": "DAGs are inherently more efficient than random graphs for generating SQL tables.",
      "D": "DAGs guarantee that LLMs will always arrive at the correct solution."
    },
    "answer": "B",
    "explanation": "The context explicitly states that DAGs are constructed with varying numbers of nodes and edges to control task difficulty.",
    "question_token_count": 26,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 16
  },
  {
    "context": "###### Safety\nSafety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content. Datasets such as RealToxicityPrompts\u00a0(Gehman et\u00a0al., 2020) and ToxiGen\u00a0(Hartvigsen et\u00a0al., 2022) assess resilience against producing harmful outputs.\n\nBy providing a controlled environment to measure these aspects, safety benchmarks play a critical role in guiding the development of models that are not only powerful outputs but also responsible and trustworthy for real-world applications.\n\n\n###### Language\nLanguage benchmarks assess the LLMs\u2019 proficiency in specific languages.\n\nGLUE\u00a0(Wang, 2018) and SuperGLUE\u00a0(Wang et\u00a0al., 2019) cover tasks from sentiment analysis to language inference, while CLUE\u00a0(Xu et\u00a0al., 2020) targets Chinese language.\n\nTypo-fixing\u00a0(Suzgun et\u00a0al., 2022) is also widely used.\n\n\n###### Reading Comprehension\nReading comprehension tasks test a model\u2019s ability to extract and infer information from text. Benchmarks like SQuAD\u00a0(Rajpurkar et\u00a0al., 2018), QuAC\u00a0(Choi et\u00a0al., 2018), and BoolQ\u00a0(Clark et\u00a0al., 2019) challenge models to understand passages and draw logical conclusions.\n",
    "topic": "The purpose and scope of language benchmarks, including datasets such as GLUE, SuperGLUE, and CLUE, and their coverage of tasks like sentiment analysis and language inference.",
    "response": "`<document_analysis>`\nThe provided text discusses several types of benchmarks: safety benchmarks (focused on toxicity and ethics), language benchmarks (GLUE, SuperGLUE, CLUE, and Typo-fixing), and reading comprehension benchmarks (SQuAD, QuAC, BoolQ). It outlines the purpose and scope of each category, highlighting their coverage of tasks like sentiment analysis and language inference. The text doesn't delve into the underlying methodologies or theoretical justifications for these benchmarks, but rather presents them as established tools within the LLM evaluation landscape. The difficulty level is moderate, requiring a basic understanding of the different benchmark categories and their associated tasks. Strategic complexity calibration: 7/10.\n</document_analysis>\n`<output_format>`\nQuestion: Considering the diverse tasks covered by benchmarks such as GLUE and SuperGLUE, what fundamental limitation does the inclusion of datasets like Typo-fixing introduce to the overall assessment of a language model\u2019s capabilities?\nA: Typo-fixing primarily assesses a model\u2019s ability to generate creative and engaging narratives.\nB: Typo-fixing introduces a bias towards evaluating models\u2019 performance on tasks involving phonetic similarity rather than semantic understanding.\nC: Typo-fixing focuses exclusively on evaluating a model\u2019s ability to handle grammatical errors in English.\nD: Typo-fixing provides a standardized measure of a model\u2019s factual recall accuracy.\nExplanation: The correct answer is B. GLUE and SuperGLUE evaluate broader language understanding, including tasks like sentiment analysis and inference. Typo-fixing, focused on correcting typos, inherently prioritizes phonetic similarity over semantic meaning, creating a specific and potentially limited evaluation perspective.\nCorrect Answer: B\n</output_format>`",
    "question": "Considering the diverse tasks covered by benchmarks such as GLUE and SuperGLUE, what fundamental limitation does the inclusion of datasets like Typo-fixing introduce to the overall assessment of a language model\u2019s capabilities?",
    "choices": {
      "A": "Typo-fixing primarily assesses a model\u2019s ability to generate creative and engaging narratives.",
      "B": "Typo-fixing introduces a bias towards evaluating models\u2019 performance on tasks involving phonetic similarity rather than semantic understanding.",
      "C": "Typo-fixing focuses exclusively on evaluating a model\u2019s ability to handle grammatical errors in English.",
      "D": "Typo-fixing provides a standardized measure of a model\u2019s factual recall accuracy."
    },
    "answer": "B",
    "explanation": "The correct answer is B. GLUE and SuperGLUE evaluate broader language understanding, including tasks like sentiment analysis and inference. Typo-fixing, focused on correcting typos, inherently prioritizes phonetic similarity over semantic meaning, creating a specific and potentially limited evaluation perspective.",
    "question_token_count": 42,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 8,
    "avg_answer_token_count": 19
  },
  {
    "context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "What are the primary motivations for developing comprehensive benchmarks for evaluating Large Language Models?",
    "response": "<document_analysis>\nThe provided context discusses the need for comprehensive benchmarks for evaluating Large Language Models (LLMs). It highlights the motivations behind developing these benchmarks, specifically focusing on assessing different aspects of model performance like instruction-following and coding. The text also acknowledges the limitations of static benchmarks due to the rapid evolution of LLMs and the potential for data contamination.  The development of contamination detectors and dynamic benchmarks is presented as a response to these challenges.\n\nStrategic Complexity Calibration: 8/10 \u2013 The question requires understanding the broader context of LLM development and the challenges associated with evaluating them, going beyond a simple factual recall.\n\n</document_analysis>\n\n<output_format>\nQuestion: What is the primary driver behind the development of dynamic benchmarks for Large Language Models, as outlined in the text?\nA: To provide a simpler and more easily interpretable assessment of model capabilities.\nB: To address the issue of static benchmarks becoming obsolete due to the rapid advancement of LLMs and the risk of data contamination.\nC: To exclusively focus on evaluating instruction-following tasks within LLMs.\nD: To eliminate the need for human effort in benchmark creation.\nExplanation: The text explicitly states that static benchmarks face challenges as LLMs evolve and introduce data contamination risks. Dynamic benchmarks are presented as a solution to these problems.\nCorrect Answer: B\n</output_format>",
    "question": "What is the primary driver behind the development of dynamic benchmarks for Large Language Models, as outlined in the text?",
    "choices": {
      "A": "To provide a simpler and more easily interpretable assessment of model capabilities.",
      "B": "To address the issue of static benchmarks becoming obsolete due to the rapid advancement of LLMs and the risk of data contamination.",
      "C": "To exclusively focus on evaluating instruction-following tasks within LLMs.",
      "D": "To eliminate the need for human effort in benchmark creation."
    },
    "answer": "B",
    "explanation": "The text explicitly states that static benchmarks face challenges as LLMs evolve and introduce data contamination risks. Dynamic benchmarks are presented as a solution to these problems.",
    "question_token_count": 23,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 16
  },
  {
    "context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n",
    "topic": "The role of benchmarks such as PIQA, SIQA, and CommonsenseQA in assessing a model\u2019s intuitive reasoning skills and background knowledge integration.",
    "response": "<document_analysis>\nThe context discusses various benchmarks used to evaluate language models, categorized into coding, instruction following, and reasoning. Specifically, it highlights benchmarks like PIQA, SIQA, HellaSwag, and CommonsenseQA as assessing intuitive reasoning and background knowledge integration. The core focus is on how these benchmarks test a model's ability to apply common sense and integrate relevant knowledge. The difficulty level is high, requiring an understanding of the diverse benchmark landscape and the specific skills they target. The strategic complexity is calibrated to challenge an expert\u2019s understanding of *why* these benchmarks are important and the nuances of the knowledge they assess, not simply recalling their names.\n</document_analysis>\n\n<output_format>\nQuestion: CommonsenseQA distinguishes itself from other reasoning benchmarks by primarily focusing on evaluating a model's ability to:\nA: Generate executable code snippets.\nB: Accurately follow complex, multi-step instructions.\nC: Integrate background knowledge with logical reasoning to answer questions requiring common sense.\nD: Predict the next word in a sequence.\nExplanation: CommonsenseQA explicitly states it \u201crequire[s] the integration of background knowledge with logical reasoning to arrive at plausible answers.\u201d Options A, B, and D represent different benchmark categories described in the text.\nCorrect Answer: C\n</output_format>",
    "question": "CommonsenseQA distinguishes itself from other reasoning benchmarks by primarily focusing on evaluating a model's ability to:",
    "choices": {
      "A": "Generate executable code snippets.",
      "B": "Accurately follow complex, multi-step instructions.",
      "C": "Integrate background knowledge with logical reasoning to answer questions requiring common sense.",
      "D": "Predict the next word in a sequence."
    },
    "answer": "C",
    "explanation": "CommonsenseQA explicitly states it \u201crequire[s] the integration of background knowledge with logical reasoning to arrive at plausible answers.\u201d Options A, B, and D represent different benchmark categories described in the text.",
    "question_token_count": 20,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 5,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 10
  },
  {
    "context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n",
    "topic": "Analyzing model behavior under conditions like memorization through masked inputs provides an alternative method for detecting contamination.",
    "response": "`<document_analysis>`\nThe context discusses post-hoc detection methods for identifying data contamination in large language models. It highlights limitations of label protection and outlines various techniques like n-gram matching, embedding-based similarity, and improved mapping metrics. Furthermore, it introduces the concept of analyzing model behavior under specific conditions, such as memorization through masked inputs. The core idea is to observe deviations in model performance that suggest contamination. The difficulty lies in discerning subtle patterns indicative of memorization from genuine generalization. </document_analysis>\n\n`<output_format>`\nQuestion: Dekoninck et al. (2024) propose a method for detecting contamination that relies on comparing model performance across which benchmarks?\nA: Direct overlap detection using n-gram matching.\nB: Embedding-based similarity analysis of tokens.\nC: Performance across multiple benchmarks.\nD: Mapping metrics to identify exact data overlaps.\nExplanation: The text explicitly states that CONSTAT, proposed by Dekoninck et al. (2024), \u201cdetects contamination by comparing model performance across benchmarks.\u201d\nCorrect Answer: C\n</output_format>",
    "question": "Dekoninck et al. (2024) propose a method for detecting contamination that relies on comparing model performance across which benchmarks?",
    "choices": {
      "A": "Direct overlap detection using n-gram matching.",
      "B": "Embedding-based similarity analysis of tokens.",
      "C": "Performance across multiple benchmarks.",
      "D": "Mapping metrics to identify exact data overlaps."
    },
    "answer": "C",
    "explanation": "The text explicitly states that CONSTAT, proposed by Dekoninck et al. (2024), \u201cdetects contamination by comparing model performance across benchmarks.\u201d",
    "question_token_count": 28,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 4,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 8
  },
  {
    "context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n",
    "topic": "MMLU-CF\u2019s approach to generating novel multiple-choice questions by shuffling answer options and replacing incorrect choices.",
    "response": "`<document_analysis>`\nThe provided context describes the MMLU-CF method for generating multiple-choice questions. It highlights that MMLU-CF generates new samples by shuffling answer choices and replacing incorrect options with \"None of the other choices.\" The text also mentions limitations of existing methods, such as the need for human effort, potential data contamination, and overlooked verification.  The document details other methods like rule-based generation and template-based generation, providing a comparative overview. The difficulty level is high, requiring an understanding of different question generation techniques and their associated challenges. The strategic complexity calibration is 8/10.\n</document_analysis>\n`<output_format>`\nQuestion: What is the core strategy employed by MMLU-CF to generate novel multiple-choice questions, as described in the context?\nA: Utilizing temporal cutoffs to assess LLM performance.\nB: Shuffling answer choices and replacing incorrect options with \u201cNone of the other choices.\u201d\nC: Synthesizing test cases based on predefined rules with minimal collision probability.\nD: Generating queries adhering to the rules of Mathador games and varying input numbers.\nExplanation: The correct answer is B, as the context explicitly states \u201cMMLU-CF (Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \u201cNone of the other choices.\u201d\u201d.\nCorrect Answer: B\n</output_format>`",
    "question": "What is the core strategy employed by MMLU-CF to generate novel multiple-choice questions, as described in the context?",
    "choices": {
      "A": "Utilizing temporal cutoffs to assess LLM performance.",
      "B": "Shuffling answer choices and replacing incorrect options with \u201cNone of the other choices.\u201d",
      "C": "Synthesizing test cases based on predefined rules with minimal collision probability.",
      "D": "Generating queries adhering to the rules of Mathador games and varying input numbers."
    },
    "answer": "B",
    "explanation": "The correct answer is B, as the context explicitly states \u201cMMLU-CF (Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \u201cNone of the other choices.\u201d\u201d.",
    "question_token_count": 26,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 2,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 15
  },
  {
    "context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "The challenges of data contamination in LLM benchmarking and the shift towards dynamic evaluation strategies.",
    "response": "<document_analysis>\nThe text discusses the significant problem of data contamination in LLM benchmarking, highlighting the challenges posed by LLMs\u2019 reliance on internet-sourced data for training. It contrasts static benchmarks (like HumanEval) with emerging dynamic evaluation strategies designed to address contamination. The core difficulty lies in the inherent risk of benchmark data inadvertently appearing in the model\u2019s training set. The text then outlines various proposed solutions, including data encryption, post-hoc detection, and dynamic benchmarking techniques like continuous dataset updates and data regeneration. A critical gap identified is the lack of systematic research and evaluation criteria for dynamic benchmarks themselves.\n\n**Difficulty Rating:** 8/10 \u2013 Requires understanding of LLM training methodologies, benchmarking practices, and the nuanced challenges of data contamination. It demands a grasp of the evolving landscape of LLM evaluation beyond simple performance metrics.\n\n**Intentional Question Planning:** I want to craft a question that probes the expert's understanding of *why* simply regenerating benchmarks isn't a complete solution to data contamination, and forces them to consider the deeper implications for evaluating LLM capabilities.\n</document_analysis>\n<output_format>\nQuestion: Considering the described methods for mitigating data contamination in LLM benchmarks \u2013 data regeneration and continuous dataset updates \u2013 why do these approaches alone represent an insufficient strategy for truly evaluating an LLM\u2019s independent intelligence?\nA: Data regeneration inherently introduces a bias towards the original benchmark distribution, failing to capture the model\u2019s adaptability to unseen data.\nB: Continuous dataset updates do not address the fundamental problem of models learning from data they were exposed to during training, regardless of the update frequency.\nC: Both data regeneration and continuous updates are computationally infeasible for large-scale LLMs, limiting their practical application.\nD: The primary concern with data contamination is the model\u2019s ability to memorize specific examples, not its generalizability to novel situations.\nCorrect Answer: B\nExplanation: The text explicitly states that data regeneration merely reconstructs the original benchmarks, failing to address the core issue of the model already having learned from that data during training. Continuous updates, while helpful, don't fundamentally change the fact that the model's knowledge base is initially influenced by contaminated data.\n</output_format>",
    "question": "Considering the described methods for mitigating data contamination in LLM benchmarks \u2013 data regeneration and continuous dataset updates \u2013 why do these approaches alone represent an insufficient strategy for truly evaluating an LLM\u2019s independent intelligence?",
    "choices": {
      "A": "Data regeneration inherently introduces a bias towards the original benchmark distribution, failing to capture the model\u2019s adaptability to unseen data.",
      "B": "Continuous dataset updates do not address the fundamental problem of models learning from data they were exposed to during training, regardless of the update frequency.",
      "C": "Both data regeneration and continuous updates are computationally infeasible for large-scale LLMs, limiting their practical application.",
      "D": "The primary concern with data contamination is the model\u2019s ability to memorize specific examples, not its generalizability to novel situations."
    },
    "answer": "B",
    "explanation": "The text explicitly states that data regeneration merely reconstructs the original benchmarks, failing to address the core issue of the model already having learned from that data during training. Continuous updates, while helpful, don't fundamentally change the fact that the model's knowledge base is initially influenced by contaminated data.",
    "question_token_count": 38,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 4,
    "question_difficulty_score": 8,
    "question_groundedness_score": 8,
    "avg_answer_token_count": 24
  },
  {
    "context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n",
    "topic": "The approach utilized by LiveCodeBench for gathering new coding problems from online competition platforms.",
    "response": "<document_analysis>\nThe context describes several approaches to constructing benchmarks for Large Language Models (LLMs) to avoid data contamination. These approaches involve collecting data from various sources, including online competition platforms (LeetCode, Art of Problem Solving forum, prediction markets), recent academic papers (arXiv), and competitions within the past 12 months. The core challenge is to ensure the data used for evaluation is genuinely reflective of the model\u2019s capabilities and not influenced by information it might have learned during its training. LiveCodeBench specifically focuses on collecting coding problems from online competition platforms.\n\nStrategic Complexity Calibration: 8/10 \u2013 This requires understanding the nuances of data collection strategies and their implications for benchmark reliability. It\u2019s not a simple factual recall question.\n\nIntentional Question Planning: The question should probe the specific methodology employed by LiveCodeBench, going beyond simply stating that it collects problems from these platforms. It needs to assess the sophistication of their approach.\n</document_analysis>\n\n<output_format>\nQuestion: What distinguishes LiveCodeBench\u2019s methodology for acquiring coding problems compared to approaches like AntiLeak-Bench and AcademicEval, as highlighted in the provided text?\nA: LiveCodeBench exclusively utilizes data from arXiv papers to assess LLM performance.\nB: LiveCodeBench continuously gathers new problems from online coding competitions, focusing on recent challenges.\nC: LiveCodeBench relies solely on data collected from math competitions within the past 12 months.\nD: LiveCodeBench generates queries about newly emerged knowledge to avoid data contamination.\nExplanation: The text explicitly states that LiveCodeBench \u201ccontinuously collects new human-written coding problems from online coding competition platforms like LeetCode.\u201d This distinguishes it from the other approaches which focus on different data sources or strategies.\nCorrect Answer: B\n</output_format>",
    "question": "What distinguishes LiveCodeBench\u2019s methodology for acquiring coding problems compared to approaches like AntiLeak-Bench and AcademicEval, as highlighted in the provided text?",
    "choices": {
      "A": "LiveCodeBench exclusively utilizes data from arXiv papers to assess LLM performance.",
      "B": "LiveCodeBench continuously gathers new problems from online coding competitions, focusing on recent challenges.",
      "C": "LiveCodeBench relies solely on data collected from math competitions within the past 12 months.",
      "D": "LiveCodeBench generates queries about newly emerged knowledge to avoid data contamination."
    },
    "answer": "B",
    "explanation": "The text explicitly states that LiveCodeBench \u201ccontinuously collects new human-written coding problems from online coding competition platforms like LeetCode.\u201d This distinguishes it from the other approaches which focus on different data sources or strategies.",
    "question_token_count": 32,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 17
  },
  {
    "context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n",
    "topic": "Future research should prioritize the development of standardized dynamic evaluation approaches for LLMs.",
    "response": "`<document_analysis>`\nThe context discusses the limitations of current LLM benchmarking, specifically highlighting the need for standardized dynamic evaluation approaches to address the vulnerability of static methods to data contamination as datasets grow. It acknowledges challenges with reliability and reproducibility in dynamic approaches and suggests future research should focus on these aspects. The core issue revolves around the evolving landscape of LLM development and the need for robust, adaptable evaluation techniques. The difficulty level is moderate, requiring an understanding of benchmarking methodologies and their limitations. Strategic complexity calibration: 7/10.\n</document_analysis>\n`<output_format>`\nQuestion: Considering the identified limitations regarding the increasing vulnerability of static LLM benchmarks to data contamination, what is the most critical element for establishing a truly standardized dynamic evaluation approach?\nA: Implementing a single, universally applicable metric for assessing LLM performance.\nB: Developing a system capable of simulating diverse, real-world user interactions to identify subtle biases.\nC: Establishing rigorous protocols for data provenance and ensuring complete dataset transparency.\nD: Focusing solely on evaluating LLM performance on established, curated datasets.\nExplanation: The context emphasizes the need for standardized dynamic evaluation to combat data contamination. While all options represent relevant considerations, the most critical element is ensuring data provenance and transparency \u2013 a cornerstone of reliable dynamic evaluation.\nCorrect Answer: C </output_format>",
    "question": "Considering the identified limitations regarding the increasing vulnerability of static LLM benchmarks to data contamination, what is the most critical element for establishing a truly standardized dynamic evaluation approach?",
    "choices": {
      "A": "Implementing a single, universally applicable metric for assessing LLM performance.",
      "B": "Developing a system capable of simulating diverse, real-world user interactions to identify subtle biases.",
      "C": "Establishing rigorous protocols for data provenance and ensuring complete dataset transparency.",
      "D": "Focusing solely on evaluating LLM performance on established, curated datasets."
    },
    "answer": "C",
    "explanation": "The context emphasizes the need for standardized dynamic evaluation to combat data contamination. While all options represent relevant considerations, the most critical element is ensuring data provenance and transparency \u2013 a cornerstone of reliable dynamic evaluation.",
    "question_token_count": 32,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 15
  },
  {
    "context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n",
    "topic": "What types of costs, beyond monetary expenses, are explicitly mentioned as factors that should be considered when evaluating the scalability of a dynamic benchmark?",
    "response": "<document_analysis>\nThe context discusses scalability in dynamic benchmarking, focusing on generating larger datasets while minimizing costs. It introduces a formula that represents the ratio of transformed dataset size to the original dataset size, with \u201cCost\u201d representing the associated costs (monetary, time, or effort). The key takeaway is that scalability is about maximizing the data generated per unit of cost. The passage explicitly mentions \u201cmonetary cost, time spent, or manual effort\u201d as potential cost factors.\n\nStrategic Complexity Calibration: 8/10 \u2013 The question requires understanding of the core concept of scalability and the cost factors involved, necessitating more than a superficial reading of the text. It probes for an understanding of how these costs are considered within the broader framework of the benchmark.\n\n</document_analysis>\n<output_format>\nQuestion: Beyond monetary expenses, what specific types of resources are identified as contributing to the cost associated with a dynamic benchmark\u2019s scalability evaluation?\nA: Computational resources and memory allocation.\nB: Time spent on data transformation and manual effort in dataset curation.\nC: Storage capacity and network bandwidth.\nD: Software licenses and hardware maintenance fees.\nExplanation: The text explicitly states that \u201cCost\u201d encompasses \u201cmonetary cost, time spent, or manual effort.\u201d Option B directly reflects this.\nCorrect Answer: B\n</output_format>",
    "question": "Beyond monetary expenses, what specific types of resources are identified as contributing to the cost associated with a dynamic benchmark\u2019s scalability evaluation?",
    "choices": {
      "A": "Computational resources and memory allocation.",
      "B": "Time spent on data transformation and manual effort in dataset curation.",
      "C": "Storage capacity and network bandwidth.",
      "D": "Software licenses and hardware maintenance fees."
    },
    "answer": "B",
    "explanation": "The text explicitly states that \u201cCost\u201d encompasses \u201cmonetary cost, time spent, or manual effort.\u201d Option B directly reflects this.",
    "question_token_count": 26,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 9
  },
  {
    "context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n",
    "topic": "Existing static benchmarking methods suffer from inherent limitations when addressing data contamination, necessitating the development of dynamic approaches.",
    "response": "```\n<document_analysis>\nThe context discusses the shift from static to dynamic benchmarking in LLM evaluation due to the issue of data contamination. It identifies limitations in existing static methods and highlights the need for standardized criteria in dynamic benchmarks. The text also points to a GitHub repository containing benchmarking methods. The core challenge is the difficulty in reliably assessing whether an LLM\u2019s performance is genuine or influenced by contaminated data. The difficulty level is high, requiring a nuanced understanding of LLM training and evaluation methodologies. Strategic complexity is rated 8/10.\n\n</document_analysis>\n<output_format>\nQuestion: What is the primary impetus for transitioning from static to dynamic benchmarking methods for Large Language Models (LLMs)?\nA: To improve the computational efficiency of LLM evaluations.\nB: To address the inherent limitations of static benchmarks in mitigating the risks associated with data contamination.\nC: To simplify the process of creating new LLM training datasets.\nD: To reduce the reliance on Internet-derived data for LLM training.\nExplanation: The text explicitly states that data contamination is a growing concern due to the reliance of LLMs on vast Internet-derived training corpora, necessitating the shift to dynamic benchmarking.\nCorrect Answer: B\n</output_format>\n```",
    "question": "What is the primary impetus for transitioning from static to dynamic benchmarking methods for Large Language Models (LLMs)?",
    "choices": {
      "A": "To improve the computational efficiency of LLM evaluations.",
      "B": "To address the inherent limitations of static benchmarks in mitigating the risks associated with data contamination.",
      "C": "To simplify the process of creating new LLM training datasets.",
      "D": "To reduce the reliance on Internet-derived data for LLM training."
    },
    "answer": "B",
    "explanation": "The text explicitly states that data contamination is a growing concern due to the reliance of LLMs on vast Internet-derived training corpora, necessitating the shift to dynamic benchmarking.",
    "question_token_count": 23,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 13
  },
  {
    "context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n",
    "topic": "Canary strings function as unique tokens embedded within datasets to identify instances where an LLM has memorized portions of its training data, indicating potential data leakage.",
    "response": "```\n<document_analysis>\nThe context describes \"Canary Strings\" as a method to detect data leakage in LLMs. It explains that these strings are unique tokens embedded in datasets and their presence in a model's output signals memorization of training data. The document highlights a limitation: if a developer intentionally leaks benchmarking data, this method will be ineffective. The text cites Jacovi et al. (2023) as a source for the implementation of canary strings in the BIG-Bench dataset. The difficulty level is high, requiring an understanding of LLM training and potential vulnerabilities.\n</document_analysis>\n<output_format>\nQuestion: In the context of mitigating data contamination in LLMs, what is the primary purpose of embedding \"canary strings\" within a dataset?\nA: To improve the model\u2019s fluency and coherence.\nB: To serve as unique identifiers for individual training examples.\nC: To flag instances where the model has memorized portions of its training data.\nD: To optimize the model\u2019s response time during inference.\nExplanation: Canary strings are specifically designed to indicate memorization, not any other characteristic of the model\u2019s output.\nCorrect Answer: C\n</output_format>\n```",
    "question": "In the context of mitigating data contamination in LLMs, what is the primary purpose of embedding \"canary strings\" within a dataset?",
    "choices": {
      "A": "To improve the model\u2019s fluency and coherence.",
      "B": "To serve as unique identifiers for individual training examples.",
      "C": "To flag instances where the model has memorized portions of its training data.",
      "D": "To optimize the model\u2019s response time during inference."
    },
    "answer": "C",
    "explanation": "Canary strings are specifically designed to indicate memorization, not any other characteristic of the model\u2019s output.",
    "question_token_count": 28,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 12
  },
  {
    "context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n",
    "topic": "Further exploration of ethical guidelines is needed regarding data usage, model transparency, and the societal impact of AI benchmarks.",
    "response": "<document_analysis>\nThe context highlights several key ethical concerns related to AI benchmarks: bias perpetuation in static benchmarks, privacy and security risks with dynamic benchmarks, potential for misuse of results, and the broader societal impact. The core issue revolves around ensuring fairness, accountability, and privacy within benchmarking practices. The text emphasizes the need for ongoing exploration of ethical guidelines. The difficulty level is high, requiring an understanding of the nuanced risks associated with different benchmarking approaches. The question should probe the expert's ability to synthesize these concerns and articulate a comprehensive ethical framework.\n</document_analysis>\n<output_format>\nQuestion: Considering the potential for bias amplification and the inherent privacy risks associated with dynamic AI benchmarks, what is the most critical initial step in establishing an ethically sound benchmarking framework?\nA: Prioritize the rapid expansion of benchmark datasets to ensure comprehensive coverage.\nB: Implement robust data anonymization techniques and establish clear data usage policies from the outset.\nC: Focus solely on developing benchmarks that demonstrably improve model performance metrics.\nD: Adopt a purely static benchmark approach to eliminate any potential for privacy violations.\nExplanation: The correct answer is B, as the context explicitly states privacy and security concerns with dynamic benchmarks. Data anonymization and clear policies are paramount for mitigating these risks.\nCorrect Answer: B\n</output_format>",
    "question": "Considering the potential for bias amplification and the inherent privacy risks associated with dynamic AI benchmarks, what is the most critical initial step in establishing an ethically sound benchmarking framework?",
    "choices": {
      "A": "Prioritize the rapid expansion of benchmark datasets to ensure comprehensive coverage.",
      "B": "Implement robust data anonymization techniques and establish clear data usage policies from the outset.",
      "C": "Focus solely on developing benchmarks that demonstrably improve model performance metrics.",
      "D": "Adopt a purely static benchmark approach to eliminate any potential for privacy violations."
    },
    "answer": "B",
    "explanation": "The correct answer is B, as the context explicitly states privacy and security concerns with dynamic benchmarks. Data anonymization and clear policies are paramount for mitigating these risks.",
    "question_token_count": 34,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 15
  },
  {
    "context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n",
    "topic": "Standardized criteria are currently lacking for evaluating dynamic benchmarks, posing a significant challenge to their effective development and use.",
    "response": "`<document_analysis>`\nThe text highlights a critical issue: the lack of standardized evaluation criteria for dynamic benchmarks used to assess Large Language Models (LLMs). It points out that static benchmarks are becoming less reliable due to data contamination as training datasets grow and introduces dynamic benchmarks as a solution. However, it emphasizes a significant gap \u2013 the absence of clear standards for evaluating these dynamic benchmarks. The passage details the increasing contamination risk with growing training data and mentions challenges associated with both static and dynamic benchmarks, such as transparency, scalability, and complexity control. The core problem being addressed is the need for robust, objective methods to assess the performance of LLMs, particularly in the context of evolving training data. The difficulty lies in the rapidly changing landscape of LLM training and the resulting need for adaptable evaluation strategies.\n\nStrategic Complexity Calibration: 8/10 \u2013 This requires an understanding of the evolution of benchmarking methodologies and the inherent challenges in evaluating models trained on massive, constantly updated datasets.\n\nIntentional Question Planning: The question should probe the reader's understanding of the specific shortcomings of current dynamic benchmark evaluation and the implications for the field. </document_analysis>`\n\n`<output_format>`\nQuestion: Considering the documented challenges with both static and dynamic benchmarks, specifically regarding the increasing risk of data contamination and the lack of standardized evaluation criteria for dynamic benchmarks, what is the most critical prerequisite for establishing a reliable and meaningful framework for assessing LLM performance using these dynamic approaches?\nA: Implementing stricter post-hoc contamination detection methods across all benchmark datasets.\nB: Prioritizing the development of LLMs exclusively trained on curated, isolated datasets to minimize contamination risks.\nC: Establishing universally accepted metrics and protocols for assessing benchmark fidelity, scalability, and complexity within dynamic benchmark evaluations.\nD: Focusing solely on expanding the size of dynamic benchmark datasets to compensate for the limitations of static benchmarks.\nExplanation: The correct answer (C) directly addresses the core problem identified in the text: the lack of standardized criteria. The other options propose solutions that either don\u2019t address the root issue (A, B, and D) or are impractical given the nature of LLM training and data.\nCorrect Answer: C\n</output_format>`",
    "question": "Considering the documented challenges with both static and dynamic benchmarks, specifically regarding the increasing risk of data contamination and the lack of standardized evaluation criteria for dynamic benchmarks, what is the most critical prerequisite for establishing a reliable and meaningful framework for assessing LLM performance using these dynamic approaches?",
    "choices": {
      "A": "Implementing stricter post-hoc contamination detection methods across all benchmark datasets.",
      "B": "Prioritizing the development of LLMs exclusively trained on curated, isolated datasets to minimize contamination risks.",
      "C": "Establishing universally accepted metrics and protocols for assessing benchmark fidelity, scalability, and complexity within dynamic benchmark evaluations.",
      "D": "Focusing solely on expanding the size of dynamic benchmark datasets to compensate for the limitations of static benchmarks."
    },
    "answer": "C",
    "explanation": "The correct answer (C) directly addresses the core problem identified in the text: the lack of standardized criteria. The other options propose solutions that either don\u2019t address the root issue (A, B, and D) or are impractical given the nature of LLM training and data.",
    "question_token_count": 53,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 7,
    "avg_answer_token_count": 20
  },
  {
    "context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n",
    "topic": "The purpose of AntiLeak-Bench in preventing data contamination when assessing LLM knowledge.",
    "response": "<document_analysis>\nThe context discusses various benchmark datasets designed to assess LLM knowledge while minimizing data contamination. Specifically, it highlights AntiLeak-Bench, which generates queries about newly emerged knowledge to avoid using information the model wouldn't have known before its cutoff date. The core concept revolves around preventing contamination by ensuring the benchmark questions are genuinely novel and not based on the LLM's existing knowledge. The difficulty lies in understanding the *reasoning* behind this specific method \u2013 why generating new queries is crucial, and what the potential consequences are of not employing this strategy.\n\nStrategic Complexity Calibration: 8/10 \u2013 This requires an understanding of how LLMs learn and retain information, and the potential for bias introduced by using data beyond their training cutoff.\n</document_analysis>\n<output_format>\nQuestion: What is the primary mechanism employed by AntiLeak-Bench to mitigate data contamination in LLM knowledge assessments?\nA: Utilizing historical data from academic publications.\nB: Generating queries about recently emerged knowledge that the LLM would not have been exposed to during its training.\nC: Focusing on questions related to established mathematical concepts.\nD: Employing a temporal cutoff to exclude data collected after the LLM\u2019s knowledge cutoff date.\nExplanation: The context explicitly states that AntiLeak-Bench \"generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\"\nCorrect Answer: B\n</output_format>",
    "question": "What is the primary mechanism employed by AntiLeak-Bench to mitigate data contamination in LLM knowledge assessments?",
    "choices": {
      "A": "Utilizing historical data from academic publications.",
      "B": "Generating queries about recently emerged knowledge that the LLM would not have been exposed to during its training.",
      "C": "Focusing on questions related to established mathematical concepts.",
      "D": "Employing a temporal cutoff to exclude data collected after the LLM\u2019s knowledge cutoff date."
    },
    "answer": "B",
    "explanation": "The context explicitly states that AntiLeak-Bench \"generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\"",
    "question_token_count": 22,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 5,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 14
  },
  {
    "context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n",
    "topic": "The Auto-Dataset method's strategy for generating new samples that maintain stylistic elements and essential knowledge of original benchmarks.",
    "response": "<document_analysis>\nThe context describes several methods for generating new benchmark samples using LLMs, highlighting potential issues like contamination and diversity limitations. Auto-Dataset is presented as a method that generates two types of samples: those preserving original style and knowledge, and those with related questions at different cognitive levels. The text also briefly mentions StructEval, ITD, and VarBench as alternative approaches. The core challenge lies in maintaining both stylistic fidelity and knowledge preservation while generating diverse and reliable benchmark data. The difficulty lies in the nuanced understanding of how these methods balance these competing goals and the potential pitfalls associated with each.\n\nStrategic Complexity Calibration: 8/10 \u2013 This requires understanding the trade-offs between stylistic preservation, knowledge retention, and cognitive diversity, and how different methods address these.\n</document_analysis>\n<output_format>\nQuestion: According to the text, what is a primary distinction between the sample generation strategy employed by Auto-Dataset compared to other methods like StructEval and ITD?\nA: Auto-Dataset exclusively generates questions at varying cognitive levels.\nB: Auto-Dataset generates two distinct types of samples: those maintaining original style and knowledge, and those presenting related questions at different cognitive levels.\nC: Auto-Dataset relies solely on rewriting existing benchmark samples.\nD: Auto-Dataset utilizes knowledge graphs to extend benchmark concepts.\nExplanation: The text explicitly states that Auto-Dataset generates \"two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels.\"\nCorrect Answer: B\n</output_format>",
    "question": "According to the text, what is a primary distinction between the sample generation strategy employed by Auto-Dataset compared to other methods like StructEval and ITD?",
    "choices": {
      "A": "Auto-Dataset exclusively generates questions at varying cognitive levels.",
      "B": "Auto-Dataset generates two distinct types of samples: those maintaining original style and knowledge, and those presenting related questions at different cognitive levels.",
      "C": "Auto-Dataset relies solely on rewriting existing benchmark samples.",
      "D": "Auto-Dataset utilizes knowledge graphs to extend benchmark concepts."
    },
    "answer": "B",
    "explanation": "The text explicitly states that Auto-Dataset generates \"two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels.\"",
    "question_token_count": 32,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 16
  },
  {
    "context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n",
    "topic": "The potential of multi-agent frameworks for creating scalable and diverse benchmarks, exemplified by Benchmark Self-Evolving and BENCHAGENTS.",
    "response": "`<document_analysis>`\nThe context describes the evolution of benchmark creation, moving from static to interactive and now multi-agent approaches. Specifically, it highlights \"Benchmark Self-Evolving\" and \"BENCHAGENTS\" as examples of multi-agent frameworks for dynamic benchmark generation. The core idea is that these frameworks leverage multiple LLM agents working together to create more complex and diverse benchmarks than traditional methods. A key distinction is the breakdown of the benchmark creation process into distinct stages handled by specialized agents (planning, generation, verification, and evaluation), with human feedback incorporated. The difficulty lies in understanding the strategic value of this multi-agent decomposition and its impact on benchmark quality.\n</document_analysis>\n\n`<output_format>`\nQuestion: In the context of Benchmark Self-Evolving and BENCHAGENTS, what is the primary strategic advantage of decomposing the benchmark creation process into distinct agent-specific stages (planning, generation, verification, and evaluation)?\nA:  Ensuring consistent adherence to the original benchmark's structure, minimizing deviation from the initial task description.\nB:  Facilitating independent agent specialization, leading to increased efficiency in each individual stage and overall benchmark creation speed.\nC:  Introducing redundancy into the process, allowing for multiple agents to independently verify the quality of the benchmark, enhancing robustness.\nD:  Enabling the system to dynamically adapt the benchmark based on real-time performance feedback from human evaluators, optimizing for diversity.\nExplanation: The correct answer is (B). The text explicitly states that each stage is handled by a \"specialized LLM agent,\" implying independent expertise and optimized performance within that specific area.  Options A, C, and D misinterpret the purpose of the agent decomposition.\nCorrect Answer: B\n</output_format>`",
    "question": "In the context of Benchmark Self-Evolving and BENCHAGENTS, what is the primary strategic advantage of decomposing the benchmark creation process into distinct agent-specific stages (planning, generation, verification, and evaluation)?",
    "choices": {
      "A": "Ensuring consistent adherence to the original benchmark's structure, minimizing deviation from the initial task description.",
      "B": "Facilitating independent agent specialization, leading to increased efficiency in each individual stage and overall benchmark creation speed.",
      "C": "Introducing redundancy into the process, allowing for multiple agents to independently verify the quality of the benchmark, enhancing robustness.",
      "D": "Enabling the system to dynamically adapt the benchmark based on real-time performance feedback from human evaluators, optimizing for diversity."
    },
    "answer": "B",
    "explanation": "The correct answer is (B). The text explicitly states that each stage is handled by a \"specialized LLM agent,\" implying independent expertise and optimized performance within that specific area.  Options A, C, and D misinterpret the purpose of the agent decomposition.",
    "question_token_count": 43,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 22
  },
  {
    "context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n",
    "topic": "What are N-gram metrics and reference-based metrics (e.g., BLEU scores) mentioned as potential examples of functions \u0398(\ud835\udc9fi, \ud835\udc9fj)?",
    "response": "`<document_analysis>`\nThe context discusses diversity metrics within a transformed dataset, specifically external and internal diversity. It introduces the function \u0398(\ud835\udc9fi, \ud835\udc9fj) as a measure of diversity between two datasets \ud835\udc9fi and \ud835\udc9fj.  It provides examples of what this function could be \u2013 N-gram metrics and reference-based metrics like BLEU scores. The text doesn't delve into the specifics of these metrics but establishes them as potential candidates. The difficulty level is moderate, requiring an understanding of the broader concept of diversity metrics and the ability to recall examples. The question aims to assess the reader's understanding of the notation \u0398(\ud835\udc9fi, \ud835\udc9fj) and its application in the context of diversity measurement.\n\n</document_analysis>\n`<output_format>`\nQuestion: Within the context of measuring dataset diversity, as exemplified by the function \u0398(\ud835\udc9fi, \ud835\udc9fj), what categories of metrics are explicitly mentioned as potential instances of this function?\nA: Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA).\nB: N-gram metrics and reference-based metrics, such as BLEU scores.\nC: Kullback-Leibler divergence and Jensen-Shannon divergence.\nD: Cosine similarity and Euclidean distance.\nExplanation: The text explicitly states that N-gram metrics and reference-based metrics (including BLEU scores) are examples of the function \u0398(\ud835\udc9fi, \ud835\udc9fj).\nCorrect Answer: B </output_format>",
    "question": "Within the context of measuring dataset diversity, as exemplified by the function \u0398(\ud835\udc9fi, \ud835\udc9fj), what categories of metrics are explicitly mentioned as potential instances of this function?",
    "choices": {
      "A": "Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA).",
      "B": "N-gram metrics and reference-based metrics, such as BLEU scores.",
      "C": "Kullback-Leibler divergence and Jensen-Shannon divergence.",
      "D": "Cosine similarity and Euclidean distance."
    },
    "answer": "B",
    "explanation": "The text explicitly states that N-gram metrics and reference-based metrics (including BLEU scores) are examples of the function \u0398(\ud835\udc9fi, \ud835\udc9fj).",
    "question_token_count": 39,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 13
  },
  {
    "context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n",
    "topic": "The strategic advantages of employing LLMs as interviewers in evaluating other LLMs, considering the iterative nature of the process.",
    "response": "`<document_analysis>`\nThe provided text describes several methods for evaluating LLMs, primarily focusing on iterative evaluation techniques resembling human interviews and multi-agent benchmarks. Key concepts include LLM-as-an-Interviewer, TreeEval, KIEval, and Benchmark Self-Evolving, all leveraging LLMs to generate follow-up questions and assess the responses of other LLMs. The text highlights the use of multi-agent frameworks for dynamic benchmark creation, exemplified by BENCHAGENTS. The core idea is to move beyond static benchmarks and create more adaptive and comprehensive evaluation processes. The difficulty level is high, requiring a nuanced understanding of the various evaluation strategies and their underlying principles. Strategic complexity calibration: 8/10.\n</document_analysis>\n`<output_format>`\nQuestion: In the context of LLM evaluation methodologies described, what is the primary differentiating factor between the approach outlined in \"LLM-as-an-Interviewer\" and \"KIEval\"?\nA: Both methods rely solely on generating questions based on initial static benchmarks.\nB: \u201cLLM-as-an-Interviewer\u201d employs a single iterative round of questioning, while \u201cKIEval\u201d generates follow-up subtopics and questions based on the examined LLM\u2019s response.\nC: \u201cLLM-as-an-Interviewer\u201d exclusively utilizes multi-agent frameworks, unlike \u201cKIEval.\u201d\nD: Both methods are fundamentally identical in their question generation process.\nExplanation: The text explicitly states that \u201cLLM-as-an-Interviewer\u201d uses a multi-turn evaluation with follow-up questions, while \u201cKIEval\u201d generates follow-up questions based on the evaluated model\u2019s response. This distinction highlights a core difference in their approaches to iterative questioning.\nCorrect Answer: B\n</output_format>`",
    "question": "In the context of LLM evaluation methodologies described, what is the primary differentiating factor between the approach outlined in \"LLM-as-an-Interviewer\" and \"KIEval\"?",
    "choices": {
      "A": "Both methods rely solely on generating questions based on initial static benchmarks.",
      "B": "\u201cLLM-as-an-Interviewer\u201d employs a single iterative round of questioning, while \u201cKIEval\u201d generates follow-up subtopics and questions based on the examined LLM\u2019s response.",
      "C": "\u201cLLM-as-an-Interviewer\u201d exclusively utilizes multi-agent frameworks, unlike \u201cKIEval.\u201d",
      "D": "Both methods are fundamentally identical in their question generation process."
    },
    "answer": "B",
    "explanation": "The text explicitly states that \u201cLLM-as-an-Interviewer\u201d uses a multi-turn evaluation with follow-up questions, while \u201cKIEval\u201d generates follow-up questions based on the evaluated model\u2019s response. This distinction highlights a core difference in their approaches to iterative questioning.",
    "question_token_count": 36,
    "answer_correctness_score": 8,
    "explanation_validity_score": 9,
    "question_clarity_score": 5,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 21
  },
  {
    "context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n",
    "topic": "How does interpretability factor into the design and validation of transformations used in LLM-assisted benchmarks?",
    "response": "<document_analysis>\nThe context discusses the importance of interpretability in dynamic benchmarking, particularly when using LLM-assisted transformations. It highlights that while rule-based transformations are inherently interpretable, LLM-based transformations require additional validation mechanisms due to the models' opacity. The core challenge is ensuring the reliability and correctness of these transformed datasets. The text outlines four categories of dynamic benchmarks: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches. The difficulty level is high because it requires understanding the nuances of different benchmarking approaches and the inherent challenges of interpreting LLM-generated transformations.\n\n</document_analysis>\n<output_format>\nQuestion: In the context of LLM-assisted benchmarks, what is the primary justification for employing additional validation mechanisms beyond purely interpretable transformations?\nA: Rule-based transformations inherently guarantee data correctness.\nB: LLM-based transformations are always transparent and easily traceable.\nC: The scale of transformed data necessitates reduced manual verification efforts.\nD: LLM-generated transformations lack inherent interpretability, demanding supplementary checks for reliability.\nExplanation: The text explicitly states that LLM-based transformations \u201cdepend on the model\u2019s transparency and traceability\u201d and may require \u201cexplainability tools, or human-in-the-loop validation.\u201d\nCorrect Answer: D\n</output_format>",
    "question": "In the context of LLM-assisted benchmarks, what is the primary justification for employing additional validation mechanisms beyond purely interpretable transformations?",
    "choices": {
      "A": "Rule-based transformations inherently guarantee data correctness.",
      "B": "LLM-based transformations are always transparent and easily traceable.",
      "C": "The scale of transformed data necessitates reduced manual verification efforts.",
      "D": "LLM-generated transformations lack inherent interpretability, demanding supplementary checks for reliability."
    },
    "answer": "D",
    "explanation": "The text explicitly states that LLM-based transformations \u201cdepend on the model\u2019s transparency and traceability\u201d and may require \u201cexplainability tools, or human-in-the-loop validation.\u201d",
    "question_token_count": 25,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 12
  },
  {
    "context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n",
    "topic": "Explain the difference between external and internal diversity as defined in the provided text.",
    "response": "`<document_analysis>`\nThe text defines two types of diversity: external and internal. External diversity refers to the variation between the transformed dataset and the original dataset. Internal diversity refers to the differences between two different transformation trials. The formulas provided represent the expected value of a diversity function (\u0398) measuring the difference between datasets. The diversity function (\u0398) itself is not defined, but examples are given as N-gram metrics or reference-based metrics like BLEU scores. The question requires a nuanced understanding of how these two types of diversity relate to each other and the overall goal of dataset transformation. The difficulty lies in understanding the subtle distinction between comparing a transformed dataset *to* the original versus comparing *different* transformations of the same data.\n`</document_analysis>`\n\n`<output_format>`\nQuestion: Considering the definitions of external and internal diversity presented, how does the calculation of internal diversity fundamentally differ from the calculation of external diversity regarding the datasets being compared?\nA: Internal diversity measures the similarity between two distinct datasets, while external diversity measures the difference between a transformed dataset and the original dataset.\nB: Internal diversity calculates the average difference between all possible pairs of transformation trials, whereas external diversity calculates the difference between a single transformed dataset and the seed dataset.\nC: External diversity relies on N-gram metrics, while internal diversity utilizes BLEU scores, irrespective of the datasets involved.\nD: Both external and internal diversity calculations are identical, differing only in the specific diversity function (\u0398) employed.\nExplanation: The correct answer (B) highlights the key distinction: internal diversity involves comparing *different* transformations of the same dataset, whereas external diversity compares a single transformed dataset to the original. Options A and C are partially correct but miss the core difference. Option D is definitively incorrect.\nCorrect Answer: B\n</output_format>`",
    "question": "Considering the definitions of external and internal diversity presented, how does the calculation of internal diversity fundamentally differ from the calculation of external diversity regarding the datasets being compared?",
    "choices": {
      "A": "Internal diversity measures the similarity between two distinct datasets, while external diversity measures the difference between a transformed dataset and the original dataset.",
      "B": "Internal diversity calculates the average difference between all possible pairs of transformation trials, whereas external diversity calculates the difference between a single transformed dataset and the seed dataset.",
      "C": "External diversity relies on N-gram metrics, while internal diversity utilizes BLEU scores, irrespective of the datasets involved.",
      "D": "Both external and internal diversity calculations are identical, differing only in the specific diversity function (\u0398) employed."
    },
    "answer": "B",
    "explanation": "The correct answer (B) highlights the key distinction: internal diversity involves comparing *different* transformations of the same dataset, whereas external diversity compares a single transformed dataset to the original. Options A and C are partially correct but miss the core difference. Option D is definitively incorrect.",
    "question_token_count": 32,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 25
  },
  {
    "context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n",
    "topic": "Describe the challenges associated with scaling the difficulty of graph-based reasoning tasks, as exemplified by the use of the Traveling Salesman Problem (TSP) in NPHardEval.",
    "response": "<document_analysis>\nThe provided context discusses the challenges of scaling the difficulty of graph-based reasoning tasks, specifically using the Traveling Salesman Problem (TSP) as an example. It highlights how frameworks like DyVal and NPHardEval generate graphs of varying sizes to control difficulty and evaluate LLMs. The core challenge lies in the inherent complexity of TSP, which increases rapidly with the number of nodes and edges, making it computationally expensive to generate and evaluate large graphs. The context implicitly acknowledges that simply increasing the size of the graph isn\u2019t a reliable way to scale difficulty due to the exponential growth of the problem\u2019s complexity. It also mentions the use of puzzles like Knights and Knaves which further increases the difficulty.\n\nStrategic Complexity Calibration: 8/10 \u2013 The topic itself is inherently complex. The difficulty lies not just in the problem itself, but in the methodology of scaling it, which is a significant challenge.\n\nIntentional Question Planning: The question should probe the understanding of how the inherent complexity of TSP impacts the scalability of graph-based reasoning evaluations and the practical limitations of using graph size as a primary difficulty metric.\n</document_analysis>\n<output_format>\nQuestion: How does the exponential growth of the Traveling Salesman Problem (TSP) complicate the scaling of graph-based reasoning tasks within frameworks like NPHardEval?\nA: Increasing the number of nodes and edges in the graph proportionally increases the computational cost of evaluation, providing a straightforward and reliable method for controlling difficulty.\nB: The inherent complexity of TSP means that simply increasing graph size doesn't reliably correlate with difficulty, as the solution space grows exponentially.\nC: Graph size is the primary metric used by NPHardEval to determine the difficulty of a problem, ensuring consistent and reproducible results.\nD: Smaller graphs are inherently easier to solve than larger graphs, regardless of the problem being represented.\nExplanation: The correct answer (B) accurately reflects the central challenge described in the context \u2013 the exponential growth of TSP makes simply increasing graph size an insufficient method for controlling difficulty. The other options present misconceptions about the relationship between graph size and problem complexity.\nCorrect Answer: B\n</output_format>",
    "question": "How does the exponential growth of the Traveling Salesman Problem (TSP) complicate the scaling of graph-based reasoning tasks within frameworks like NPHardEval?",
    "choices": {
      "A": "Increasing the number of nodes and edges in the graph proportionally increases the computational cost of evaluation, providing a straightforward and reliable method for controlling difficulty.",
      "B": "The inherent complexity of TSP means that simply increasing graph size doesn't reliably correlate with difficulty, as the solution space grows exponentially.",
      "C": "Graph size is the primary metric used by NPHardEval to determine the difficulty of a problem, ensuring consistent and reproducible results.",
      "D": "Smaller graphs are inherently easier to solve than larger graphs, regardless of the problem being represented."
    },
    "answer": "B",
    "explanation": "The correct answer (B) accurately reflects the central challenge described in the context \u2013 the exponential growth of TSP makes simply increasing graph size an insufficient method for controlling difficulty. The other options present misconceptions about the relationship between graph size and problem complexity.",
    "question_token_count": 32,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 25
  },
  {
    "context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n",
    "topic": "What are the primary challenges associated with verifying the correctness of transformations generated within dynamic benchmarking frameworks?",
    "response": "<document_analysis>\nThe context discusses the challenges of verifying transformations generated within dynamic benchmarking frameworks, specifically highlighting the difficulty associated with large volumes of transformed data and the need for interpretability. It outlines four categories of dynamic benchmarks \u2013 temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches \u2013 and emphasizes the varying degrees of interpretability associated with each. The core issue revolves around ensuring the correctness of transformations, particularly those generated by LLMs, necessitating methods like explainability tools or human validation. The text does not provide a specific technical challenge related to verification, but rather a high-level overview of the problem and its categorization.\n\nStrategic Complexity Calibration: 7/10 \u2013 The topic is relevant and presents a nuanced challenge, requiring an understanding of benchmarking methodologies and the limitations of LLM-generated data. However, it doesn't delve into highly specific technical details.\n</document_analysis>\n\n<output_format>\nQuestion: Within the context of dynamic benchmarking, what is the primary impediment to verifying the correctness of transformations generated by LLM-based methods, and what strategy is suggested to mitigate this challenge?\nA: The inherent transparency of LLMs eliminates the need for verification strategies.\nB: The reliance on LLM-generated data necessitates the use of explainability tools and human-in-the-loop validation.\nC: Temporal cutoff benchmarks inherently guarantee transformation correctness.\nD: Rule-based transformations are always more reliable than LLM-based transformations.\nExplanation: The context states, \"In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\" Therefore, option B accurately reflects the suggested strategy.\nCorrect Answer: B\n</output_format>",
    "question": "Within the context of dynamic benchmarking, what is the primary impediment to verifying the correctness of transformations generated by LLM-based methods, and what strategy is suggested to mitigate this challenge?",
    "choices": {
      "A": "The inherent transparency of LLMs eliminates the need for verification strategies.",
      "B": "The reliance on LLM-generated data necessitates the use of explainability tools and human-in-the-loop validation.",
      "C": "Temporal cutoff benchmarks inherently guarantee transformation correctness.",
      "D": "Rule-based transformations are always more reliable than LLM-based transformations."
    },
    "answer": "B",
    "explanation": "The context states, \"In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\" Therefore, option B accurately reflects the suggested strategy.",
    "question_token_count": 36,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 14
  },
  {
    "context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n",
    "topic": "CONSTAT is a specific post-hoc detection method that compares model performance across benchmarks to identify contamination.",
    "response": "`<document_analysis>`\nThe text describes post-hoc detection methods for identifying data contamination in machine learning models. It highlights the limitations of label protection and introduces various techniques like n-gram matching, embedding-based similarity, and analyzing model behavior under different conditions. A specific method, CONSTAT, is mentioned as comparing model performance across benchmarks. The core challenge lies in accurately detecting contamination without introducing false negatives or relying on centralized evaluation systems. The text also points to research investigating memorization, partial completions, and preference for original over paraphrased test cases as potential contamination indicators. The difficulty lies in the nuanced understanding of how these methods work and their limitations.\n\nStrategic Complexity Calibration: 8/10 \u2013 Requires understanding of multiple detection techniques and their underlying principles. The question needs to probe beyond a simple recall of the method\u2019s description.\n\n</document_analysis>`\n\n`<output_format>`\nQuestion: CONSTAT\u2019s methodology centers on a comparative analysis of model performance. What fundamental assumption underlies CONSTAT\u2019s effectiveness as a contamination detection method, as suggested by the text?\nA: CONSTAT relies solely on exact n-gram matching to identify contaminated data.\nB: CONSTAT\u2019s efficacy is predicated on the assumption that model performance consistently degrades across diverse benchmarks when contamination is present.\nC: CONSTAT\u2019s success hinges on identifying instances where a model exhibits preferential behavior towards original test cases compared to paraphrased ones.\nD: CONSTAT\u2019s effectiveness is determined by analyzing model responses to masked inputs to detect memorization.\nExplanation: The correct answer (B) reflects the core principle of CONSTAT \u2013 comparing performance *across* benchmarks. The text states that CONSTAT \u201cdetects contamination by comparing model performance across benchmarks.\u201d The other options represent alternative contamination detection techniques described in the context, not the fundamental principle of CONSTAT itself.\nCorrect Answer: B\n</output_format>`",
    "question": "CONSTAT\u2019s methodology centers on a comparative analysis of model performance. What fundamental assumption underlies CONSTAT\u2019s effectiveness as a contamination detection method, as suggested by the text?",
    "choices": {
      "A": "CONSTAT relies solely on exact n-gram matching to identify contaminated data.",
      "B": "CONSTAT\u2019s efficacy is predicated on the assumption that model performance consistently degrades across diverse benchmarks when contamination is present.",
      "C": "CONSTAT\u2019s success hinges on identifying instances where a model exhibits preferential behavior towards original test cases compared to paraphrased ones.",
      "D": "CONSTAT\u2019s effectiveness is determined by analyzing model responses to masked inputs to detect memorization."
    },
    "answer": "B",
    "explanation": "The correct answer (B) reflects the core principle of CONSTAT \u2013 comparing performance *across* benchmarks. The text states that CONSTAT \u201cdetects contamination by comparing model performance across benchmarks.\u201d The other options represent alternative contamination detection techniques described in the context, not the fundamental principle of CONSTAT itself.",
    "question_token_count": 35,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 7,
    "question_difficulty_score": 6,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 21
  },
  {
    "context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n",
    "topic": "What are the potential vulnerabilities associated with encryption methods, specifically concerning key management and computational costs?",
    "response": "`<document_analysis>`\nThe context discusses encryption methods and label protection as techniques to prevent data leakage and maintain evaluation integrity. It highlights vulnerabilities related to key management and computational costs associated with these methods. The core concern revolves around the potential compromise of encryption and the overhead introduced by the encryption process itself. The text doesn't offer specific details about the nature of these vulnerabilities, but establishes the need for careful consideration of these aspects. The difficulty level is moderate, requiring an understanding of the trade-offs between security and efficiency.\n\n</document_analysis>\n`<output_format>`\nQuestion: What is a primary operational vulnerability introduced by the implementation of encryption techniques for data protection, beyond simple key compromise?\nA: The increased reliance on specialized hardware for secure computation.\nB: The elevated computational costs associated with encryption and decryption processes.\nC: The potential for introducing subtle biases in the data due to the encryption algorithm itself.\nD: The limitations of public key cryptography in preventing adversarial attacks.\nExplanation: The text explicitly states, \u201cthey introduce extra computational overheads.\u201d Option B directly reflects this statement. Options A, C, and D introduce complexities not directly addressed in the provided context.\nCorrect Answer: B</output_format>",
    "question": "What is a primary operational vulnerability introduced by the implementation of encryption techniques for data protection, beyond simple key compromise?",
    "choices": {
      "A": "The increased reliance on specialized hardware for secure computation.",
      "B": "The elevated computational costs associated with encryption and decryption processes.",
      "C": "The potential for introducing subtle biases in the data due to the encryption algorithm itself.",
      "D": "The limitations of public key cryptography in preventing adversarial attacks."
    },
    "answer": "B",
    "explanation": "The text explicitly states, \u201cthey introduce extra computational overheads.\u201d Option B directly reflects this statement. Options A, C, and D introduce complexities not directly addressed in the provided context.",
    "question_token_count": 23,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 8,
    "avg_answer_token_count": 13
  },
  {
    "context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n",
    "topic": "The source of data used by LiveAoPSBench for creating benchmarks focused on mathematical problem-solving.",
    "response": "<document_analysis>\nThe context describes various benchmarks used to evaluate Large Language Models (LLMs) and their strategies for mitigating data contamination. It highlights the specific data sources utilized by each benchmark: math competitions, newly emerged knowledge, arXiv papers, online coding problems, and math problems from the Art of Problem Solving (AoPS) forum. The core focus is on the temporal cutoff date and how benchmarks leverage data collected *after* this date to assess model performance accurately.\n\nStrategic Complexity Calibration: 8/10 \u2013 This requires an understanding of benchmark design, data contamination, and the specific challenges of evaluating LLMs in a dynamic knowledge landscape. It's not a simple factual recall but requires considering the rationale behind choosing a particular data source.\n\nIntentional Question Planning: The question should probe the reasoning behind selecting the AoPS forum as a data source, considering the nature of AoPS problems and the potential implications for benchmark design.\n</document_analysis>\n\n<output_format>\nQuestion: Why did LiveAoPSBench prioritize collecting math problems from the Art of Problem Solving (AoPS) forum as its data source, rather than, for example, a collection of past math competition problems?\nA: Because AoPS problems tend to be more consistently formatted and easier for LLMs to parse.\nB: Because the AoPS forum represents a continuous stream of problems reflecting current mathematical challenges and student engagement.\nC: Because competition problems are inherently more reliable indicators of mathematical understanding than forum discussions.\nD: Because AoPS problems are exclusively focused on topics covered in standardized exams.\nExplanation: The context states \u201cLiveAoPSBench (Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\u201d  Option B accurately reflects the continuous, current nature of the AoPS forum\u2019s problem stream, which is a key differentiator.\nCorrect Answer: B\n</output_format>",
    "question": "Why did LiveAoPSBench prioritize collecting math problems from the Art of Problem Solving (AoPS) forum as its data source, rather than, for example, a collection of past math competition problems?",
    "choices": {
      "A": "Because AoPS problems tend to be more consistently formatted and easier for LLMs to parse.",
      "B": "Because the AoPS forum represents a continuous stream of problems reflecting current mathematical challenges and student engagement.",
      "C": "Because competition problems are inherently more reliable indicators of mathematical understanding than forum discussions.",
      "D": "Because AoPS problems are exclusively focused on topics covered in standardized exams."
    },
    "answer": "B",
    "explanation": "The context states \u201cLiveAoPSBench (Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\u201d  Option B accurately reflects the continuous, current nature of the AoPS forum\u2019s problem stream, which is a key differentiator.",
    "question_token_count": 41,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 7,
    "avg_answer_token_count": 17
  },
  {
    "context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n",
    "topic": "Specialized benchmarks, including ControlBench, FRAMES, and GPQA Diamond, target specific technical and long-context challenges within LLM capabilities.",
    "response": "`<document_analysis>`\nThe provided context details various benchmarks used to assess Large Language Model (LLM) capabilities. It categorizes these benchmarks into \u201cMath\u201d and \u201cKnowledge\u201d categories, providing examples of datasets and challenges within each. Notably, it highlights more recent benchmarks like ControlBench, FRAMES, and GPQA Diamond as specifically targeting technical and long-context challenges. The text also references evaluations like AlpacaEval and ArenaHard. The overall goal of these benchmarks is to rigorously evaluate LLMs across different dimensions of performance. The difficulty level is high, requiring a deep understanding of the LLM evaluation landscape. </document_analysis>\n\n`<output_format>`\nQuestion: ControlBench, FRAMES, and GPQA Diamond represent a shift in LLM benchmarking, primarily focusing on what specific type of challenge?\nA: Evaluating factual recall across diverse domains.\nB: Assessing mathematical problem-solving abilities.\nC: Targeting technical and long-context capabilities.\nD: Measuring the model\u2019s ability to generate creative text formats.\nExplanation: The text explicitly states that ControlBench, FRAMES, and GPQA Diamond \u201ctarget technical and long-context challenges.\u201d\nCorrect Answer: C </output_format>",
    "question": "ControlBench, FRAMES, and GPQA Diamond represent a shift in LLM benchmarking, primarily focusing on what specific type of challenge?",
    "choices": {
      "A": "Evaluating factual recall across diverse domains.",
      "B": "Assessing mathematical problem-solving abilities.",
      "C": "Targeting technical and long-context capabilities.",
      "D": "Measuring the model\u2019s ability to generate creative text formats."
    },
    "answer": "C",
    "explanation": "The text explicitly states that ControlBench, FRAMES, and GPQA Diamond \u201ctarget technical and long-context challenges.\u201d",
    "question_token_count": 27,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 9
  },
  {
    "context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n",
    "topic": "Explain the significance of the dataset (\ud835\udcb3) within the context of a static benchmark.",
    "response": "`<document_analysis>`\nThe provided text describes static benchmarks used to evaluate LLM performance across various tasks. It introduces the dataset \ud835\udcb3 (X) as a core component of these benchmarks, alongside expected outputs (Y) and a scoring function (S). The text emphasizes that \ud835\udcb3 represents the seed dataset \u2013 the initial set of prompts used for evaluation. The significance of \ud835\udcb3 lies in its role as the foundation upon which the entire benchmark is built, providing the input stimuli for the LLM to respond to and for the scoring function to assess.\n\nStrategic Complexity Calibration: 8/10 \u2013 The concept is relatively straightforward, but requires understanding the role of a \"seed dataset\" in a benchmark system and its implications for evaluating model performance. The difficulty lies in appreciating the foundational nature of \ud835\udcb3.\n\nThought Process: I want to create a question that probes the expert\u2019s understanding of the dataset\u2019s role beyond simply being \u201cthe input.\u201d I want to assess if they grasp its impact on the benchmark\u2019s overall validity and reliability. The question should require them to think about how changes to \ud835\udcb3 would affect the benchmark results.\n</document_analysis>\n`<output_format>`\nQuestion: Within the framework of static benchmarking, what is the primary consequence of altering the dataset (\ud835\udcb3) used as the seed, and how does this alteration impact the reliability of the benchmark results?\nA: Altering \ud835\udcb3 will have no effect on the benchmark results, as the scoring function (\ud835\udcae) is independent of the input data.\nB: Modifying \ud835\udcb3 introduces systematic biases into the benchmark, consistently favoring models that perform well on those specific prompts.\nC: Changing \ud835\udcb3 fundamentally alters the benchmark, rendering previous results invalid and necessitating a complete re-evaluation of model performance.\nD: Altering \ud835\udcb3 only affects the scoring function (\ud835\udcae), but not the evaluation of the LLM's ability to perform the tasks defined in the benchmark.\nExplanation: The text states that \ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae) and that \ud835\udcb3 is the \u201cseed dataset.\u201d Changing the seed dataset directly impacts the benchmark, as it provides the initial set of prompts. Option C most accurately reflects this impact \u2013 a different seed dataset leads to different results, invalidating previous evaluations.\nCorrect Answer: C\n</output_format>`",
    "question": "Within the framework of static benchmarking, what is the primary consequence of altering the dataset (\ud835\udcb3) used as the seed, and how does this alteration impact the reliability of the benchmark results?",
    "choices": {
      "A": "Altering \ud835\udcb3 will have no effect on the benchmark results, as the scoring function (\ud835\udcae) is independent of the input data.",
      "B": "Modifying \ud835\udcb3 introduces systematic biases into the benchmark, consistently favoring models that perform well on those specific prompts.",
      "C": "Changing \ud835\udcb3 fundamentally alters the benchmark, rendering previous results invalid and necessitating a complete re-evaluation of model performance.",
      "D": "Altering \ud835\udcb3 only affects the scoring function (\ud835\udcae), but not the evaluation of the LLM's ability to perform the tasks defined in the benchmark."
    },
    "answer": "C",
    "explanation": "The text states that \ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae) and that \ud835\udcb3 is the \u201cseed dataset.\u201d Changing the seed dataset directly impacts the benchmark, as it provides the initial set of prompts. Option C most accurately reflects this impact \u2013 a different seed dataset leads to different results, invalidating previous evaluations.",
    "question_token_count": 39,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 7,
    "avg_answer_token_count": 27
  },
  {
    "context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n",
    "topic": "What are some potential strategies for mitigating the risk of data contamination during LLM training?",
    "response": "`<document_analysis>`\nThe context details the issue of \u201cdata contamination\u201d in Large Language Model (LLM) training, differentiating between \u201cexact contamination\u201d (duplicate data points) and \u201csyntactic contamination\u201d (transformed data points). The core problem is that evaluating LLM performance becomes unreliable if the evaluation data was inadvertently included in the training data. The analysis provides a formal definition of the contamination types. The text is highly technical and focuses on a specific problem in LLM development. The difficulty level is high, requiring a solid understanding of machine learning concepts.\n\nI will formulate a question that probes the nuances of syntactic contamination and requires the expert to articulate a strategy beyond simply identifying duplicates.\n</document_analysis>\n`<output_format>`\nQuestion: Considering the risk of syntactic contamination, what is the most effective approach to mitigate the potential for a test example to be re-identified in the training data through subtle transformations, without resorting to exhaustive manual inspection?\nA: Implement strict data sanitization protocols that remove all punctuation and whitespace from the training data.\nB: Employ a combination of synonym substitution and morphological variations during training to artificially broaden the training data\u2019s lexical diversity.\nC: Utilize a novel metric that quantifies the likelihood of a test example being syntactically equivalent to a training example, prioritizing examples with lower scores for inclusion in training.\nD: Rely on automated tools to detect verbatim copies of test examples within the training data, coupled with a human review process for ambiguous cases.\nExplanation: The correct answer is C. Syntactic contamination requires recognizing transformations, not just exact matches. Option C directly addresses this by proposing a metric to quantify the likelihood of syntactic equivalence, which is a more sophisticated approach. Options A and B are irrelevant and would not address the core problem. Option D is a reasonable partial solution but lacks the proactive, analytical element needed to truly mitigate the risk.\nCorrect Answer: C\n</output_format>`",
    "question": "Considering the risk of syntactic contamination, what is the most effective approach to mitigate the potential for a test example to be re-identified in the training data through subtle transformations, without resorting to exhaustive manual inspection?",
    "choices": {
      "A": "Implement strict data sanitization protocols that remove all punctuation and whitespace from the training data.",
      "B": "Employ a combination of synonym substitution and morphological variations during training to artificially broaden the training data\u2019s lexical diversity.",
      "C": "Utilize a novel metric that quantifies the likelihood of a test example being syntactically equivalent to a training example, prioritizing examples with lower scores for inclusion in training.",
      "D": "Rely on automated tools to detect verbatim copies of test examples within the training data, coupled with a human review process for ambiguous cases."
    },
    "answer": "C",
    "explanation": "The correct answer is C. Syntactic contamination requires recognizing transformations, not just exact matches. Option C directly addresses this by proposing a metric to quantify the likelihood of syntactic equivalence, which is a more sophisticated approach. Options A and B are irrelevant and would not address the core problem. Option D is a reasonable partial solution but lacks the proactive, analytical element needed to truly mitigate the risk.",
    "question_token_count": 43,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 26
  },
  {
    "context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n",
    "topic": "How is the \"Correctness\" of a dynamic benchmark defined and quantified according to the proposed evaluation criteria?",
    "response": "<document_analysis>\nThe context defines \"Correctness\" in a dynamic benchmark as the alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured by a scoring function. It explicitly states that an oracle function (\ud835\udca2\u2062(\u22c5)) is used to provide the ground truth. The key challenge lies in understanding how this alignment is quantified and what factors contribute to a high \"Correctness\" score. The equation presented is a simplified representation, and the deeper question is about the practical implementation and nuances of this evaluation. The difficulty is not in recalling a specific fact but in applying the concept to a realistic scenario and recognizing the implicit assumptions.\n</document_analysis>\n<output_format>\nQuestion: According to the text, how is the \"Correctness\" of a dynamic benchmark mathematically defined and what role does the oracle function (\ud835\udca2\u2062(\u22c5)) play in its quantification?\nA: Correctness is determined solely by the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) applied to the transformed dataset\u2019s outputs.\nB: Correctness is defined as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, determined by the oracle function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) and measured by the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ).\nC: Correctness is a subjective assessment based on the annotator\u2019s judgment of the transformed dataset\u2019s outputs.\nD: Correctness is irrelevant to the evaluation of dynamic benchmarks.\nExplanation: The correct answer (B) directly reflects the mathematical definition provided in the text, highlighting the role of the oracle function and the scoring function in measuring alignment.\nCorrect Answer: B\n</output_format>",
    "question": "According to the text, how is the \"Correctness\" of a dynamic benchmark mathematically defined and what role does the oracle function (\ud835\udca2\u2062(\u22c5)) play in its quantification?",
    "choices": {
      "A": "Correctness is determined solely by the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) applied to the transformed dataset\u2019s outputs.",
      "B": "Correctness is defined as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, determined by the oracle function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) and measured by the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ).",
      "C": "Correctness is a subjective assessment based on the annotator\u2019s judgment of the transformed dataset\u2019s outputs.",
      "D": "Correctness is irrelevant to the evaluation of dynamic benchmarks."
    },
    "answer": "B",
    "explanation": "The correct answer (B) directly reflects the mathematical definition provided in the text, highlighting the role of the oracle function and the scoring function in measuring alignment.",
    "question_token_count": 38,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 38
  },
  {
    "context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n",
    "topic": "Define \"exact contamination\" within the context of LLM evaluation data, citing the precise criteria for its occurrence.",
    "response": "`<document_analysis>`\nThe context defines \"exact contamination\" as the presence of an identical data point in both the training and evaluation datasets. It highlights verbatim examples, code snippets, and documentation leaks as common causes. The document also introduces \"syntactic contamination\" as a related, though distinct, phenomenon. The core difficulty lies in precisely identifying these duplicates, considering potential syntactic transformations. The question needs to probe the expert\u2019s understanding of the strict criteria for this type of contamination, going beyond a simple definition. The difficulty level is high because it requires recognizing subtle overlaps that might not be immediately apparent.\n</document_analysis>\n`<output_format>`\nQuestion: What constitutes \u201cexact contamination\u201d within the framework of LLM evaluation data, as precisely defined in the provided text?\nA: Any instance where the evaluation dataset contains data points that are semantically similar to those in the training dataset.\nB: The presence of identical data points in both the training and evaluation datasets, irrespective of syntactic variations.\nC: Instances where the evaluation dataset contains data points that are paraphrased versions of those in the training dataset.\nD: The inclusion of code snippets from benchmark implementations within the training dataset.\nExplanation: The text explicitly states that \u201cexact contamination occurs when there is any exact duplicate in the benchmark dataset.\u201d Option B correctly captures this precise definition. Options A, C, and D describe related but distinct forms of contamination (semantic similarity, paraphrasing, and code inclusion, respectively).\nCorrect Answer: B</output_format>",
    "question": "What constitutes \u201cexact contamination\u201d within the framework of LLM evaluation data, as precisely defined in the provided text?",
    "choices": {
      "A": "Any instance where the evaluation dataset contains data points that are semantically similar to those in the training dataset.",
      "B": "The presence of identical data points in both the training and evaluation datasets, irrespective of syntactic variations.",
      "C": "Instances where the evaluation dataset contains data points that are paraphrased versions of those in the training dataset.",
      "D": "The inclusion of code snippets from benchmark implementations within the training dataset."
    },
    "answer": "B",
    "explanation": "The text explicitly states that \u201cexact contamination occurs when there is any exact duplicate in the benchmark dataset.\u201d Option B correctly captures this precise definition. Options A, C, and D describe related but distinct forms of contamination (semantic similarity, paraphrasing, and code inclusion, respectively).",
    "question_token_count": 23,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 4,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 19
  },
  {
    "context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n",
    "topic": "StructEval\u2019s utilization of knowledge graphs to expand upon concepts within existing benchmarks using LLMs.",
    "response": "<document_analysis>\nThe context describes several methods for expanding upon concepts in existing benchmarks using LLMs. StructEval is highlighted as a method that utilizes knowledge graphs. The key challenge identified is the potential for contamination when using LLMs to rewrite benchmark samples. The document details Auto-Dataset, ITD, and VarBench as related approaches. The difficulty level of the original benchmark is maintained during the rewriting process. The passage focuses on the methodology and associated risks, rather than providing a detailed explanation of the techniques involved.\n\nStrategic Complexity Calibration: 8/10 \u2013 The question needs to probe the nuanced understanding of how knowledge graphs are integrated into the expansion process, considering potential contamination risks.\n</document_analysis>\n<output_format>\nQuestion: How does StructEval\u2019s utilization of knowledge graphs specifically mitigate the risk of in-distribution contamination during benchmark expansion, as highlighted in the context?\nA: By generating entirely new benchmark samples unrelated to the original.\nB: By identifying and removing problematic samples before LLM processing.\nC: By developing extended questions that build upon existing concepts within the original benchmark.\nD: By prompting the LLM to prioritize stylistic similarity over factual accuracy.\nExplanation: The correct answer is C. The context explicitly states that StructEval \u201cexpands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\u201d The other options misrepresent the method or its purpose.\nCorrect Answer: C\n</output_format>",
    "question": "How does StructEval\u2019s utilization of knowledge graphs specifically mitigate the risk of in-distribution contamination during benchmark expansion, as highlighted in the context?",
    "choices": {
      "A": "By generating entirely new benchmark samples unrelated to the original.",
      "B": "By identifying and removing problematic samples before LLM processing.",
      "C": "By developing extended questions that build upon existing concepts within the original benchmark.",
      "D": "By prompting the LLM to prioritize stylistic similarity over factual accuracy."
    },
    "answer": "C",
    "explanation": "The correct answer is C. The context explicitly states that StructEval \u201cexpands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\u201d The other options misrepresent the method or its purpose.",
    "question_token_count": 28,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 13
  },
  {
    "context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n",
    "topic": "The role of dynamic question generation in enhancing the effectiveness of benchmark evaluations, as demonstrated by TreeEval and KIEval.",
    "response": "`<document_analysis>`\nThe context describes different approaches to evaluating LLMs, specifically focusing on dynamic question generation. It highlights two main categories: Interactive Evaluation (using LLM interviewers) and Multi-Agent Evaluation. Key techniques discussed are TreeEval (generating follow-up questions based on topic and response) and KIEval (generating follow-up questions based on the evaluated model's response). Furthermore, the context introduces Benchmark Self-Evolving which utilizes a multi-agent framework to expand static benchmarks and Benchmark Generation utilizing BENCHAGENTS which employs a multi-agent framework for automated benchmark creation. The core concept being explored is the shift from static, pre-defined benchmarks to dynamic ones that adapt and evolve based on the model\u2019s performance.\n\nThe difficulty level should be high, prompting reflection on the strategic advantages of dynamic evaluation over static approaches. The question should assess understanding beyond simply recalling the names of the techniques.\n\n</document_analysis>`\n\n`<output_format>`\nQuestion: Considering the design principles of TreeEval and KIEval, what fundamental limitation of traditional static benchmark evaluations does dynamic question generation demonstrably address?\nA: Static benchmarks lack the ability to account for nuanced variations in model performance across different contexts.\nB: Static benchmarks inherently fail to capture the iterative nature of human reasoning and problem-solving.\nC: Static benchmarks provide an insufficient mechanism for adapting evaluation difficulty based on model proficiency.\nD: Static benchmarks are incapable of simulating the feedback loop present in human-LLM interactions.\nExplanation: The correct answer is C. TreeEval and KIEval both operate on the principle of iteratively refining questions based on the model's previous responses, directly addressing the limitation of static benchmarks which present a fixed set of questions regardless of the model\u2019s understanding.\nCorrect Answer: C\n</output_format>`",
    "question": "Considering the design principles of TreeEval and KIEval, what fundamental limitation of traditional static benchmark evaluations does dynamic question generation demonstrably address?",
    "choices": {
      "A": "Static benchmarks lack the ability to account for nuanced variations in model performance across different contexts.",
      "B": "Static benchmarks inherently fail to capture the iterative nature of human reasoning and problem-solving.",
      "C": "Static benchmarks provide an insufficient mechanism for adapting evaluation difficulty based on model proficiency.",
      "D": "Static benchmarks are incapable of simulating the feedback loop present in human-LLM interactions."
    },
    "answer": "C",
    "explanation": "The correct answer is C. TreeEval and KIEval both operate on the principle of iteratively refining questions based on the model's previous responses, directly addressing the limitation of static benchmarks which present a fixed set of questions regardless of the model\u2019s understanding.",
    "question_token_count": 30,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 17
  },
  {
    "context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n",
    "topic": "Dynamic LLM benchmarking methods face challenges regarding reliability and reproducibility.",
    "response": "<document_analysis>\n**Thoughtful Content Examination:** The text discusses the limitations of current LLM benchmarking methods, specifically highlighting the challenges of reliability and reproducibility, particularly with dynamic approaches. It acknowledges the evolving nature of the field and suggests future research directions.\n\n**Concept Exploration:** The core issue revolves around contamination \u2013 how easily benchmark results can be skewed by data present in the models themselves. Static methods are reliable but susceptible to growing datasets, while dynamic methods struggle with reproducibility. The limitations section points to a need for more robust and validated evaluation criteria.\n\n**Strategic Complexity Calibration:** Difficulty: 8/10. The question requires understanding the nuanced trade-offs between different benchmarking approaches and anticipating future research needs.\n\n**Intentional Question Planning:** The question aims to probe the expert\u2019s understanding of the fundamental challenges preventing reliable dynamic benchmarking, requiring them to synthesize information from both sections of the text.\n</document_analysis>\n\n<output_format>\nQuestion: Considering the identified limitations regarding dynamic LLM benchmarking, what represents the most significant obstacle to achieving reproducible results within this methodology?\nA: The inherent computational cost of evaluating dynamic benchmarks.\nB: The difficulty in designing benchmarks that accurately reflect real-world usage scenarios.\nC: The potential for contamination due to model training data, exacerbated by expanding datasets.\nD: The lack of standardized evaluation metrics for dynamic benchmarking techniques.\nExplanation: The correct answer is C. The text explicitly states that dynamic approaches \u201cface challenges in reliability and reproducibility\u201d and that static methods \u201cbecome more vulnerable to contamination as training datasets grow.\u201d Options A, B, and D represent secondary concerns discussed in the text.\nCorrect Answer: C\n</output_format>",
    "question": "Considering the identified limitations regarding dynamic LLM benchmarking, what represents the most significant obstacle to achieving reproducible results within this methodology?",
    "choices": {
      "A": "The inherent computational cost of evaluating dynamic benchmarks.",
      "B": "The difficulty in designing benchmarks that accurately reflect real-world usage scenarios.",
      "C": "The potential for contamination due to model training data, exacerbated by expanding datasets.",
      "D": "The lack of standardized evaluation metrics for dynamic benchmarking techniques."
    },
    "answer": "C",
    "explanation": "The correct answer is C. The text explicitly states that dynamic approaches \u201cface challenges in reliability and reproducibility\u201d and that static methods \u201cbecome more vulnerable to contamination as training datasets grow.\u201d Options A, B, and D represent secondary concerns discussed in the text.",
    "question_token_count": 24,
    "answer_correctness_score": 8,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 13
  },
  {
    "context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n",
    "topic": "The significance of benchmarks like ARC and OpenBookQA in challenging language models to integrate background knowledge with logical reasoning.",
    "response": "`<document_analysis>`\nThe provided text discusses various benchmarks used to evaluate language models across different capabilities \u2013 coding, instruction following, and reasoning. It highlights benchmarks like HumanEval, MBPP, IFEval, and others. The core focus is on assessing how well models integrate background knowledge with logical reasoning, specifically mentioning ARC and OpenBookQA as challenging datasets in this area. The difficulty level is high, requiring a sophisticated understanding of the benchmarks and their purpose. The strategic complexity is high as it probes the nuances of knowledge integration. I will generate a question that requires the expert to demonstrate an understanding of the core challenge presented by ARC and OpenBookQA, going beyond a simple recall of their names. </document_analysis>\n\n`<output_format>`\nQuestion: In the context of language model evaluation, what fundamental limitation does the ARC and OpenBookQA benchmark specifically target, and how does this differ from traditional reasoning benchmarks?\nA: They primarily assess a model\u2019s ability to generate creative text formats, similar to those evaluated by the HellaSwag dataset.\nB: They evaluate a model\u2019s capacity for zero-shot learning, testing its performance on tasks it has not been explicitly trained on.\nC: They challenge models to integrate external, often implicit, background knowledge with logical inference, demanding a more comprehensive understanding than simply applying learned patterns.\nD: They focus on evaluating a model\u2019s adherence to stylistic constraints within a given prompt, mirroring the objectives of the IFEval benchmark.\nExplanation: The correct answer is C. ARC and OpenBookQA are designed to test a model\u2019s ability to use external knowledge (background knowledge) alongside logical reasoning to answer questions. This goes beyond simply recognizing patterns or applying learned associations, as demonstrated by the other answer choices.\nCorrect Answer: C\n</output_format>`",
    "question": "In the context of language model evaluation, what fundamental limitation does the ARC and OpenBookQA benchmark specifically target, and how does this differ from traditional reasoning benchmarks?",
    "choices": {
      "A": "They primarily assess a model\u2019s ability to generate creative text formats, similar to those evaluated by the HellaSwag dataset.",
      "B": "They evaluate a model\u2019s capacity for zero-shot learning, testing its performance on tasks it has not been explicitly trained on.",
      "C": "They challenge models to integrate external, often implicit, background knowledge with logical inference, demanding a more comprehensive understanding than simply applying learned patterns.",
      "D": "They focus on evaluating a model\u2019s adherence to stylistic constraints within a given prompt, mirroring the objectives of the IFEval benchmark."
    },
    "answer": "C",
    "explanation": "The correct answer is C. ARC and OpenBookQA are designed to test a model\u2019s ability to use external knowledge (background knowledge) alongside logical reasoning to answer questions. This goes beyond simply recognizing patterns or applying learned associations, as demonstrated by the other answer choices.",
    "question_token_count": 33,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 7,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 26
  },
  {
    "context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n",
    "topic": "Knowledge benchmarks evaluate an LLM's internal factual knowledge, utilizing datasets like NaturalQuestions and TriviaQA, alongside more comprehensive assessments such as MMLU and BBH.",
    "response": "<document_analysis>\nThe context describes various benchmarks used to evaluate Large Language Models (LLMs), specifically focusing on their mathematical and general knowledge capabilities. It outlines datasets like GSM8K and MATH for math problems, and NaturalQuestions and TriviaQA for knowledge retrieval. More comprehensive benchmarks include MMLU, BBH, and AGI Eval. Recent advancements like MMLU-Redux and MMLU-Pro are also mentioned, alongside specialized benchmarks such as ControlBench, FRAMES, and GPQA Diamond. The overall theme is the increasing sophistication and diversity of benchmarks used to assess LLM performance.\n\nThe difficulty level is high, aiming to assess a deep understanding of benchmark methodologies and their evolution. The question should probe beyond simply recalling benchmark names and instead require an understanding of their purpose and relative complexity. </document_analysis>\n\n<output_format>\nQuestion: Considering the progression of knowledge benchmarks, what fundamental distinction underlies the shift from datasets like NaturalQuestions and TriviaQA to benchmarks such as ControlBench and GPQA Diamond?\nA: The former rely on simple factual recall, while the latter prioritize complex reasoning and multi-step problem-solving.\nB: NaturalQuestions and TriviaQA assess purely linguistic understanding, whereas ControlBench and GPQA Diamond focus exclusively on mathematical proficiency.\nC: The earlier datasets utilized a closed-domain approach, whereas the newer benchmarks represent an expansion into open-domain and specialized technical challenges.\nD: There is no fundamental distinction; all benchmarks ultimately measure the same core capabilities of an LLM.\nExplanation: The correct answer (C) highlights the crucial shift from closed-domain knowledge retrieval to open-domain benchmarks incorporating increasingly complex and specialized tasks. The earlier datasets were primarily focused on retrieving known facts, whereas the newer benchmarks demand more sophisticated reasoning and problem-solving skills.\nCorrect Answer: C\n</output_format>",
    "question": "Considering the progression of knowledge benchmarks, what fundamental distinction underlies the shift from datasets like NaturalQuestions and TriviaQA to benchmarks such as ControlBench and GPQA Diamond?",
    "choices": {
      "A": "The former rely on simple factual recall, while the latter prioritize complex reasoning and multi-step problem-solving.",
      "B": "NaturalQuestions and TriviaQA assess purely linguistic understanding, whereas ControlBench and GPQA Diamond focus exclusively on mathematical proficiency.",
      "C": "The earlier datasets utilized a closed-domain approach, whereas the newer benchmarks represent an expansion into open-domain and specialized technical challenges.",
      "D": "There is no fundamental distinction; all benchmarks ultimately measure the same core capabilities of an LLM."
    },
    "answer": "C",
    "explanation": "The correct answer (C) highlights the crucial shift from closed-domain knowledge retrieval to open-domain benchmarks incorporating increasingly complex and specialized tasks. The earlier datasets were primarily focused on retrieving known facts, whereas the newer benchmarks demand more sophisticated reasoning and problem-solving skills.",
    "question_token_count": 35,
    "answer_correctness_score": 8,
    "explanation_validity_score": 8,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 22
  },
  {
    "context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n",
    "topic": "The potential consequences of using data contaminated with information available to the model during training.",
    "response": "<document_analysis>\nThe context discusses methods to prevent data contamination in Large Language Models (LLMs) by using data collected after the model\u2019s knowledge cutoff date. Several benchmarks, such as LiveBench, AntiLeak-Bench, AcademicEval, LiveCodeBench, LiveAoPSBench, and Forecastbench, are presented as examples of this approach. The core issue being addressed is that information available to the model during training can inadvertently influence its responses, leading to inaccurate or misleading outputs. The benchmarks utilize various strategies \u2013 tracking recent events, querying for novel knowledge, and focusing on newly published materials \u2013 to mitigate this contamination.\n\nStrategic Complexity Calibration: 8/10 \u2013 This requires understanding the nuances of data contamination and the various strategies used to combat it. It\u2019s not a simple factual recall but requires applying the concepts to evaluate the benchmarks.\n\n</document_analysis>\n<output_format>\nQuestion: Which of the following best describes the primary motivation behind benchmarks like AntiLeak-Bench and Forecastbench?\nA: To assess the model\u2019s performance on historical data.\nB: To evaluate the model\u2019s ability to generate creative content.\nC: To identify questions about knowledge that emerged *after* the model\u2019s training cutoff.\nD: To measure the model\u2019s accuracy in predicting future events.\nExplanation: The context explicitly states that AntiLeak-Bench generates queries about \u201cnewly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date\u201d and Forecastbench \u201cupdates new forecasting questions on a daily basis from different data sources.\u201d\nCorrect Answer: C\n</output_format>",
    "question": "Which of the following best describes the primary motivation behind benchmarks like AntiLeak-Bench and Forecastbench?",
    "choices": {
      "A": "To assess the model\u2019s performance on historical data.",
      "B": "To evaluate the model\u2019s ability to generate creative content.",
      "C": "To identify questions about knowledge that emerged *after* the model\u2019s training cutoff.",
      "D": "To measure the model\u2019s accuracy in predicting future events."
    },
    "answer": "C",
    "explanation": "The context explicitly states that AntiLeak-Bench generates queries about \u201cnewly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date\u201d and Forecastbench \u201cupdates new forecasting questions on a daily basis from different data sources.\u201d",
    "question_token_count": 22,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 13
  },
  {
    "context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "The proposed criteria for evaluating dynamic benchmarks, highlighting the current shortcomings of existing approaches.",
    "response": "<document_analysis>\nThe provided text discusses the limitations of current LLM benchmarking, specifically highlighting the problem of data contamination due to LLMs being trained on data scraped from the internet. It then introduces dynamic benchmarking as a solution, outlining methods like continuous dataset updates and data regeneration. A key gap identified is the lack of established criteria for evaluating dynamic benchmarks themselves. The text\u2019s complexity lies in its nuanced discussion of these evolving benchmarking techniques and the challenges associated with ensuring their reliability.\n\n**Thoughtful Content Examination:** The core issue is not simply the existence of data contamination but the difficulty in *evaluating* whether a benchmark is truly free from it. The text identifies a crucial, previously unaddressed, challenge \u2013 the need for criteria to assess the quality of dynamic benchmarks.\n\n**Concept Exploration:** The text subtly highlights a meta-problem: How do we evaluate methods designed to *prevent* a problem? This is a classic example of a feedback loop, where the solution (dynamic benchmarking) requires a new, independent evaluation process. The difficulty lies in designing that evaluation process without introducing bias or further contamination. The proposed criteria represent a significant shift in focus.\n\n**Strategic Complexity Calibration:** 8/10 \u2013 This requires an understanding of machine learning, LLMs, benchmarking methodologies, and a critical awareness of potential biases and vulnerabilities within those systems. It goes beyond simple recall and demands an ability to analyze the design principles of evaluation methods.\n\n**Intentional Question Planning:** The question will probe the expert's understanding of the inherent difficulties in evaluating dynamic benchmarks and their ability to articulate the necessary characteristics of such a rigorous assessment.\n</document_analysis>\n\n<output_format>\nQuestion: Considering the inherent challenges of data contamination and the reliance on publicly available data for LLM training, what fundamental criteria, beyond mere dataset freshness, would be essential for definitively assessing the integrity and reliability of a dynamic benchmark?\nA:  Establishing a rigorous, independently-verified gold standard dataset, completely separate from any LLM training data, to serve as a reference for contamination detection.\nB: Implementing continuous, automated data provenance tracking mechanisms to definitively trace the origin of all benchmark data points.\nC: Defining a multi-faceted evaluation framework incorporating statistical analysis, adversarial testing, and human expert review to identify subtle forms of data leakage.\nD:  Prioritizing benchmark datasets that are exclusively generated through synthetic data generation techniques, eliminating any possibility of real-world data contamination.\nExplanation: The correct answer (C) acknowledges the complexity of the issue and necessitates a comprehensive approach. Data provenance tracking (B) is difficult to implement effectively, synthetic data (D) may not accurately reflect real-world performance, and a simple gold standard (A) is unrealistic given the scale of LLM training.\nCorrect Answer: C\n</output_format>",
    "question": "Considering the inherent challenges of data contamination and the reliance on publicly available data for LLM training, what fundamental criteria, beyond mere dataset freshness, would be essential for definitively assessing the integrity and reliability of a dynamic benchmark?",
    "choices": {
      "A": "Establishing a rigorous, independently-verified gold standard dataset, completely separate from any LLM training data, to serve as a reference for contamination detection.",
      "B": "Implementing continuous, automated data provenance tracking mechanisms to definitively trace the origin of all benchmark data points.",
      "C": "Defining a multi-faceted evaluation framework incorporating statistical analysis, adversarial testing, and human expert review to identify subtle forms of data leakage.",
      "D": "Prioritizing benchmark datasets that are exclusively generated through synthetic data generation techniques, eliminating any possibility of real-world data contamination."
    },
    "answer": "C",
    "explanation": "The correct answer (C) acknowledges the complexity of the issue and necessitates a comprehensive approach. Data provenance tracking (B) is difficult to implement effectively, synthetic data (D) may not accurately reflect real-world performance, and a simple gold standard (A) is unrealistic given the scale of LLM training.",
    "question_token_count": 44,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 9,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 26
  },
  {
    "context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n",
    "topic": "Compare and contrast the methodologies of S3Eval and DyVal in evaluating LLM reasoning capabilities.",
    "response": "<document_analysis>\nThe context presents a comparison of three methodologies \u2013 S3Eval, DyVal, and NPHardEval \u2013 for evaluating LLM reasoning. S3Eval focuses on SQL query execution on random tables, while DyVal utilizes randomly generated directed acyclic graphs (DAGs) transformed into natural language descriptions. NPHardEval assesses performance on well-known P and NP problems, specifically the Traveling Salesman Problem (TSP) and Knights and Knaves puzzles. A key distinction lies in the data structures used: tables versus graphs, and the methods for converting graphs into solvable natural language prompts. The core difference appears to be in the type of reasoning tested \u2013 SQL logic versus graph traversal and logical deduction.\n\nDifficulty Rating: 8/10 \u2013 The comparison requires understanding of different reasoning paradigms and the specific techniques employed by each method. It\u2019s not immediately obvious, requiring careful consideration of the data structures and evaluation criteria.\n\n</document_analysis>\n<output_format>\nQuestion: What fundamental difference underlies the data structures employed by S3Eval and DyVal in their evaluation of LLM reasoning?\nA: S3Eval utilizes DAGs, while DyVal employs randomly generated SQL tables.\nB: S3Eval focuses on SQL queries, whereas DyVal assesses the value of a root node in a graph.\nC: S3Eval evaluates reasoning on tables, while DyVal evaluates reasoning on directed acyclic graphs.\nD: S3Eval uses natural language prompts, while DyVal relies on rule-based conversion.\nExplanation: The context states that S3Eval assesses LLM accuracy in executing SQL queries on random tables, and DyVal evaluates LLMs using randomly generated DAGs. Therefore, the key difference is the data structure \u2013 tables versus graphs.\nCorrect Answer: C\n</output_format>",
    "question": "What fundamental difference underlies the data structures employed by S3Eval and DyVal in their evaluation of LLM reasoning?",
    "choices": {
      "A": "S3Eval utilizes DAGs, while DyVal employs randomly generated SQL tables.",
      "B": "S3Eval focuses on SQL queries, whereas DyVal assesses the value of a root node in a graph.",
      "C": "S3Eval evaluates reasoning on tables, while DyVal evaluates reasoning on directed acyclic graphs.",
      "D": "S3Eval uses natural language prompts, while DyVal relies on rule-based conversion."
    },
    "answer": "C",
    "explanation": "The context states that S3Eval assesses LLM accuracy in executing SQL queries on random tables, and DyVal evaluates LLMs using randomly generated DAGs. Therefore, the key difference is the data structure \u2013 tables versus graphs.",
    "question_token_count": 24,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 19
  },
  {
    "context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n",
    "topic": "The primary risk addressed by using canary strings in LLM training is the potential for data contamination stemming from the inclusion of static benchmark datasets.",
    "response": "```\n<document_analysis>\nThe context describes \"canary strings\" as a method to detect data contamination in LLM training. The core idea is to embed unique tokens within the training data. If a model generates output containing these tokens, it suggests memorization of training data rather than true generalization. The text highlights a key limitation: this method is ineffective if a developer intentionally leaks benchmark data to inflate performance metrics. The difficulty lies in the subtle dependence on the trainer\u2019s awareness and responsiveness to these markers.\n\nStrategic Complexity Calibration: 8/10 \u2013 This requires understanding the nuances of LLM training, the potential for data leakage, and the specific mechanism of canary strings. It\u2019s not a straightforward factual question.\n</document_analysis>\n\n<output_format>\nQuestion: What is the primary limitation of utilizing canary strings as a method for mitigating data contamination in LLM training, as described in the text?\nA: Canary strings are easily detectable by all LLMs, regardless of training data.\nB: Their effectiveness relies solely on the inclusion of unique tokens within the dataset.\nC: They are rendered ineffective if a developer deliberately introduces benchmark data to manipulate model performance.\nD: Canary strings provide no indication of whether a model is generalizing or memorizing.\nExplanation: The text explicitly states, \u201cIf a developer aims to leak benchmarking data to boost scores, this method will not work.\u201d\nCorrect Answer: C\n</output_format>\n```",
    "question": "What is the primary limitation of utilizing canary strings as a method for mitigating data contamination in LLM training, as described in the text?",
    "choices": {
      "A": "Canary strings are easily detectable by all LLMs, regardless of training data.",
      "B": "Their effectiveness relies solely on the inclusion of unique tokens within the dataset.",
      "C": "They are rendered ineffective if a developer deliberately introduces benchmark data to manipulate model performance.",
      "D": "Canary strings provide no indication of whether a model is generalizing or memorizing."
    },
    "answer": "C",
    "explanation": "The text explicitly states, \u201cIf a developer aims to leak benchmarking data to boost scores, this method will not work.\u201d",
    "question_token_count": 28,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 7,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 16
  },
  {
    "context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "What is the concept of data contamination in the context of LLM benchmarking, and why is it a concern?",
    "response": "`<document_analysis>`\nThe context discusses the challenges of static benchmarks in LLM evaluation due to the rapid evolution of models and continuous training on all available data. It introduces the concept of \"data contamination\" as a significant concern, leading to the development of contamination detectors and dynamic benchmarks. The core idea is that as LLMs are trained on more and more data, they can inadvertently memorize specific examples from the benchmark datasets, leading to artificially inflated performance scores that don't reflect genuine generalization ability. The document highlights the need for methods to identify and mitigate this contamination.\n\nThe difficulty level of this topic is high, requiring a nuanced understanding of LLM training dynamics and the potential for memorization. The question should probe beyond a simple definition to explore the implications of contamination and the strategies to address it.\n\nI plan to create a challenging question that forces the expert to consider the long-term consequences of using static benchmarks and the potential for misleading performance evaluations.\n</document_analysis>\n`<output_format>`\nQuestion: In the context of LLM benchmarking, how does data contamination fundamentally undermine the reliability of performance metrics, and what is the primary mechanism driving this phenomenon?\nA: Data contamination primarily affects the computational efficiency of LLMs, leading to slower inference times.\nB: Data contamination arises from the stochastic nature of the training process, resulting in inconsistent model behavior.\nC: Data contamination occurs when LLMs memorize specific examples from benchmark datasets, leading to inflated performance metrics that do not represent true generalization ability.\nD: Data contamination is a negligible issue in modern LLMs due to their inherent ability to generalize across diverse data distributions.\nExplanation: The correct answer is C. The text explicitly states that \u201cunchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\u201d This refers to the models memorizing examples from the benchmarks, which is the core of the problem.\nCorrect Answer: C\n</output_format>`",
    "question": "In the context of LLM benchmarking, how does data contamination fundamentally undermine the reliability of performance metrics, and what is the primary mechanism driving this phenomenon?",
    "choices": {
      "A": "Data contamination primarily affects the computational efficiency of LLMs, leading to slower inference times.",
      "B": "Data contamination arises from the stochastic nature of the training process, resulting in inconsistent model behavior.",
      "C": "Data contamination occurs when LLMs memorize specific examples from benchmark datasets, leading to inflated performance metrics that do not represent true generalization ability.",
      "D": "Data contamination is a negligible issue in modern LLMs due to their inherent ability to generalize across diverse data distributions."
    },
    "answer": "C",
    "explanation": "The correct answer is C. The text explicitly states that \u201cunchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\u201d This refers to the models memorizing examples from the benchmarks, which is the core of the problem.",
    "question_token_count": 30,
    "answer_correctness_score": 10,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 8,
    "avg_answer_token_count": 21
  },
  {
    "context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n",
    "topic": "Static benchmarks can inadvertently perpetuate biases due to reliance on outdated or biased data sources.",
    "response": "<document_analysis>\nThe context highlights a critical issue with static benchmarks: their potential to reinforce biases stemming from outdated or biased data. The core problem is the reliance on potentially flawed historical data. The passage emphasizes the need for careful construction of benchmarks and raises concerns about transparency and potential misuse. It also introduces a contrasting perspective \u2013 dynamic benchmarks \u2013 while acknowledging their own ethical challenges (privacy, security, misuse). The overarching theme is the responsibility of benchmark developers to prioritize fairness, accountability, and privacy.\n\nStrategic Complexity Calibration: 8/10 \u2013 This requires an understanding of how bias can be embedded in data and how that can manifest in benchmark results. It\u2019s not a simple recall question; it demands critical thinking about the system\u2019s vulnerabilities.\n\n</document_analysis>\n<output_format>\nQuestion: In the context of LLM evaluation benchmarks, what is the primary mechanism by which static benchmarks can perpetuate bias?\nA: Dynamic benchmarks inherently introduce bias due to their reliance on continuously updated data.\nB: Static benchmarks are inherently more accurate than dynamic benchmarks.\nC: Static benchmarks can inadvertently perpetuate biases through reliance on outdated or biased data sources.\nD: The use of static benchmarks is entirely immune to bias.\nExplanation: The correct answer is C. The passage explicitly states that static benchmarks \"can inadvertently perpetuate biases\u2026 through reliance on outdated or biased data sources.\" Options A, B, and D are factually incorrect based on the provided text.\nCorrect Answer: C\n</output_format>",
    "question": "In the context of LLM evaluation benchmarks, what is the primary mechanism by which static benchmarks can perpetuate bias?",
    "choices": {
      "A": "Dynamic benchmarks inherently introduce bias due to their reliance on continuously updated data.",
      "B": "Static benchmarks are inherently more accurate than dynamic benchmarks.",
      "C": "Static benchmarks can inadvertently perpetuate biases through reliance on outdated or biased data sources.",
      "D": "The use of static benchmarks is entirely immune to bias."
    },
    "answer": "C",
    "explanation": "The correct answer is C. The passage explicitly states that static benchmarks \"can inadvertently perpetuate biases\u2026 through reliance on outdated or biased data sources.\" Options A, B, and D are factually incorrect based on the provided text.",
    "question_token_count": 23,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 13
  },
  {
    "context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n",
    "topic": "Data privacy and commercial concerns contribute to the complexity of addressing data contamination issues in LLM benchmarking.",
    "response": "`<document_analysis>`\nThe text discusses the challenges of data contamination in LLM benchmarking, specifically highlighting the increasing probability of contamination with larger training datasets and the limitations of static benchmarks. It also introduces dynamic benchmarks as a potential solution but acknowledges a lack of standardized evaluation criteria for them. The core argument centers around the evolving nature of benchmarks due to the scale of training data and the need for more robust evaluation methods. The document emphasizes both the problems with existing approaches and the need for future research to address these issues. The difficulty level is high due to the nuanced discussion of benchmark evolution and the lack of readily available solutions. Strategic complexity calibration: 8/10.\n`</document_analysis>`\n\n<output_format>\nQuestion: Considering the observed increase in contamination probability proportional to the size of the training dataset, what fundamental limitation does this suggest regarding the applicability of traditional static benchmarks for evaluating LLMs trained on contemporary, web-scale corpora?\nA: Dynamic benchmarks inherently provide a more accurate assessment of model performance due to their ability to adapt to evolving data distributions.\nB: The increasing contamination rate necessitates a shift towards exclusively human-annotated datasets, eliminating the need for automated benchmarking.\nC: Static benchmarks are fundamentally unreliable for evaluating models trained on large datasets, requiring a complete redesign of the evaluation process.\nD: The contamination rate is negligible and does not significantly impact the validity of static benchmarks.\nExplanation: The correct answer is C. The text explicitly states \u201cOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}.\u201d This directly indicates that static benchmarks are becoming less effective as training data grows.\nCorrect Answer: C\n</output_format>",
    "question": "Considering the observed increase in contamination probability proportional to the size of the training dataset, what fundamental limitation does this suggest regarding the applicability of traditional static benchmarks for evaluating LLMs trained on contemporary, web-scale corpora?",
    "choices": {
      "A": "Dynamic benchmarks inherently provide a more accurate assessment of model performance due to their ability to adapt to evolving data distributions.",
      "B": "The increasing contamination rate necessitates a shift towards exclusively human-annotated datasets, eliminating the need for automated benchmarking.",
      "C": "Static benchmarks are fundamentally unreliable for evaluating models trained on large datasets, requiring a complete redesign of the evaluation process.",
      "D": "The contamination rate is negligible and does not significantly impact the validity of static benchmarks."
    },
    "answer": "C",
    "explanation": "The correct answer is C. The text explicitly states \u201cOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%",
    "question_token_count": 43,
    "answer_correctness_score": 10,
    "explanation_validity_score": 8,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 8,
    "avg_answer_token_count": 21
  },
  {
    "context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n",
    "topic": "The challenges associated with using temporal cutoffs for LLM evaluation and the risk of data contamination.",
    "response": "<document_analysis>\nThe context discusses limitations of using temporal cutoffs for LLM evaluation, specifically the risk of data contamination due to the reuse of problems in future competitions. It also introduces alternative methods for generating test cases, including rule-based generation, template-based generation, and the MMLU-CF approach. The core challenge highlighted is the potential for artificially inflating an LLM's performance by exposing it to data that was previously used in evaluation.  The difficulty level is high, requiring an understanding of benchmarking methodologies and the potential pitfalls of relying on competition data.\n\n</document_analysis>\n<output_format>\nQuestion: Considering the documented risk of data contamination associated with temporal cutoffs in LLM evaluation, what fundamental methodological flaw underlies the practice of utilizing recent competition results for benchmarking purposes?\nA: Temporal cutoffs ensure the most current and relevant data is used for evaluation.\nB: Reliance on competition data introduces a systematic bias, as problems and solutions are inevitably recycled across multiple evaluation cycles.\nC: Temporal cutoffs eliminate the need for human involvement in the evaluation process.\nD: Recent competition results always accurately reflect an LLM\u2019s true capabilities.\nExplanation: The text explicitly states that using recent information from competitions can lead to data contamination because these problems are likely to be reused. Option B directly addresses this concern.\nCorrect Answer: B\n</output_format>",
    "question": "Considering the documented risk of data contamination associated with temporal cutoffs in LLM evaluation, what fundamental methodological flaw underlies the practice of utilizing recent competition results for benchmarking purposes?",
    "choices": {
      "A": "Temporal cutoffs ensure the most current and relevant data is used for evaluation.",
      "B": "Reliance on competition data introduces a systematic bias, as problems and solutions are inevitably recycled across multiple evaluation cycles.",
      "C": "Temporal cutoffs eliminate the need for human involvement in the evaluation process.",
      "D": "Recent competition results always accurately reflect an LLM\u2019s true capabilities."
    },
    "answer": "B",
    "explanation": "The text explicitly states that using recent information from competitions can lead to data contamination because these problems are likely to be reused. Option B directly addresses this concern.",
    "question_token_count": 34,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 16
  },
  {
    "context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n",
    "topic": "What is the significance of a higher score on the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) in the context of evaluating dynamic benchmarks?",
    "response": "<document_analysis>\nThe provided text discusses evaluating dynamic benchmarks using a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). The core idea is that a higher score on this function indicates better correctness \u2013 a closer alignment between the benchmark\u2019s transformed dataset and its ground truth. The scoring function is implicitly linked to an oracle (\ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 )) that provides the ground truth. The question needs to probe the expert\u2019s understanding of what constitutes a \u2018higher\u2019 score and its implications for benchmark quality. The difficulty is moderate, requiring an understanding of the relationship between a scoring function and the concept of correctness.\n\n</document_analysis>\n<output_format>\nQuestion: In the context of evaluating dynamic benchmarks, what is the primary significance of achieving a higher score on the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5)?\nA: It indicates a lower level of alignment between the benchmark's output and the ground truth.\nB: It signifies a greater degree of transformation applied to the original data.\nC: It reflects a stronger correlation between the transformed dataset and the ground truth, suggesting higher correctness.\nD: It demonstrates the use of a more complex oracle function.\nExplanation: The text explicitly states that \u201cA higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\u201d Therefore, option C is the correct answer.\nCorrect Answer: C\n</output_format>",
    "question": "In the context of evaluating dynamic benchmarks, what is the primary significance of achieving a higher score on the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5)?",
    "choices": {
      "A": "It indicates a lower level of alignment between the benchmark's output and the ground truth.",
      "B": "It signifies a greater degree of transformation applied to the original data.",
      "C": "It reflects a stronger correlation between the transformed dataset and the ground truth, suggesting higher correctness.",
      "D": "It demonstrates the use of a more complex oracle function."
    },
    "answer": "C",
    "explanation": "The text explicitly states that \u201cA higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\u201d Therefore, option C is the correct answer.",
    "question_token_count": 46,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 15
  },
  {
    "context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n",
    "topic": "The distinction between syntactic transformations considered \u201ccontamination\u201d and a model\u2019s genuine reasoning capability during inference warrants critical examination.",
    "response": "<document_analysis>\nThe context discusses the concept of \"contamination\" in LLM benchmarking \u2013 specifically, how rephrasing training data (adding prefixes) can be considered contamination rather than genuine reasoning. It highlights the significance of identifying and mitigating this contamination to avoid overestimating a model\u2019s capabilities and to ensure the validity of benchmarks. The core argument revolves around distinguishing between memorization and true reasoning, and the potential for contamination to skew evaluations. The difficulty lies in discerning whether an LLM\u2019s response is based on genuine understanding or simply recalling a previously seen phrase.\n\nStrategic Complexity Calibration: 8/10 \u2013 The topic is nuanced and requires understanding of both LLM architecture and benchmarking methodologies. It\u2019s not immediately obvious why syntactic transformations are considered contamination.\n\n</document_analysis>\n<output_format>\nQuestion:  Considering the argument presented, what is the primary justification for classifying syntactic transformations (like adding prefixes) as \u201ccontamination\u201d during LLM benchmarking, rather than evidence of genuine reasoning?\nA: Syntactic transformations inherently improve a model\u2019s ability to generalize to unseen data.\nB:  Such transformations demonstrate a model\u2019s capacity to effectively utilize semantic relationships within the input.\nC:  They represent a form of memorization, blurring the line between recalling learned patterns and exhibiting independent inference.\nD: Syntactic transformations always indicate a deeper understanding of the underlying linguistic structure.\nExplanation: The context explicitly states that \u201csuch syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference.\u201d\nCorrect Answer: C\n</output_format>",
    "question": "Considering the argument presented, what is the primary justification for classifying syntactic transformations (like adding prefixes) as \u201ccontamination\u201d during LLM benchmarking, rather than evidence of genuine reasoning?",
    "choices": {
      "A": "Syntactic transformations inherently improve a model\u2019s ability to generalize to unseen data.",
      "B": "Such transformations demonstrate a model\u2019s capacity to effectively utilize semantic relationships within the input.",
      "C": "They represent a form of memorization, blurring the line between recalling learned patterns and exhibiting independent inference.",
      "D": "Syntactic transformations always indicate a deeper understanding of the underlying linguistic structure."
    },
    "answer": "C",
    "explanation": "The context explicitly states that \u201csuch syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference.\u201d",
    "question_token_count": 36,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 7,
    "question_difficulty_score": 6,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 18
  },
  {
    "context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n",
    "topic": "Explain the significance of independently transformed versions of a benchmark dataset in relation to quantifying collision.",
    "response": "<document_analysis>\nThe context describes the concept of \"collision\" in dynamic benchmarking \u2013 a measure of overlap between independently transformed versions of a benchmark dataset. It also introduces \u201cRepeat Trials\u201d as a metric to assess the benchmark\u2019s ability to generate novel variations. The core idea is to mitigate the risk of LLMs being trained on benchmark data, thus preserving the benchmark\u2019s utility. The analysis focuses on the *significance* of these transformed versions in quantifying collision and its implications for benchmark effectiveness. The difficulty level is high, requiring an understanding of benchmarking methodologies and potential data contamination issues. Strategic complexity calibration: 8/10.\n</document_analysis>\n<output_format>\nQuestion: In the context of dynamic benchmarking, what is the primary significance of independently transformed versions of a benchmark dataset?\nA: They provide a direct measure of the LLM\u2019s inherent capabilities.\nB: They quantify the collision rate, indicating potential data contamination and limiting the benchmark\u2019s ability to generate diverse test cases.\nC: They are used solely to increase the size of the benchmark dataset.\nD: They serve as a control group for comparing against traditional static benchmarks.\nExplanation: The text explicitly states that collision measures the percentage of overlap between transformed datasets, highlighting the risk of contamination.\nCorrect Answer: B\n</output_format>",
    "question": "In the context of dynamic benchmarking, what is the primary significance of independently transformed versions of a benchmark dataset?",
    "choices": {
      "A": "They provide a direct measure of the LLM\u2019s inherent capabilities.",
      "B": "They quantify the collision rate, indicating potential data contamination and limiting the benchmark\u2019s ability to generate diverse test cases.",
      "C": "They are used solely to increase the size of the benchmark dataset.",
      "D": "They serve as a control group for comparing against traditional static benchmarks."
    },
    "answer": "B",
    "explanation": "The text explicitly states that collision measures the percentage of overlap between transformed datasets, highlighting the risk of contamination.",
    "question_token_count": 22,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 16
  },
  {
    "context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Describe the proposed solutions to mitigate data contamination risks, specifically focusing on dynamic benchmarks.",
    "response": "`<document_analysis>`\nThe context discusses the challenges of static benchmarks in evaluating LLMs due to the rapid evolution of models and the potential for data contamination. It introduces dynamic benchmarks as a proposed solution to address this problem. The core issue is that as LLMs are continuously trained on new data, existing benchmarks can become trivial, and models may inadvertently learn to \u201ccheat\u201d by memorizing solutions rather than truly understanding the underlying concepts. The text highlights the need for contamination detectors and dynamic benchmarks. The difficulty level is moderate, requiring an understanding of the dynamic nature of LLM training and the potential for benchmark obsolescence.\n\nThe strategic complexity is moderate as it requires an understanding of the core problem of data contamination and the proposed solution.\n\nI will focus on eliciting a nuanced understanding of the core challenge presented \u2013 the potential for contamination \u2013 and the implications of using dynamic benchmarks. I will aim for a question that probes beyond a simple definition and requires the respondent to consider the practical challenges of implementing such a system.\n`</document_analysis>`\n`<output_format>`\nQuestion: Considering the potential for data contamination in LLM benchmarking, what is the primary operational challenge inherent in deploying dynamic benchmarks compared to static ones?\nA: Ensuring the dynamic benchmark tasks remain sufficiently diverse to prevent overfitting by the model.\nB: Maintaining the computational resources required to generate and execute a continuously updating set of benchmark tasks.\nC: Guaranteeing the dynamic benchmark tasks accurately reflect the real-world distribution of data the model will encounter after deployment.\nD: Avoiding the introduction of bias into the benchmark through the selection criteria for dynamically generated tasks.\nExplanation: The correct answer is (C). The core problem of dynamic benchmarks is ensuring they accurately represent the data the model will encounter in the real world. Static benchmarks are less susceptible to this because they are fixed.\nCorrect Answer: C\n</output_format>`",
    "question": "Considering the potential for data contamination in LLM benchmarking, what is the primary operational challenge inherent in deploying dynamic benchmarks compared to static ones?",
    "choices": {
      "A": "Ensuring the dynamic benchmark tasks remain sufficiently diverse to prevent overfitting by the model.",
      "B": "Maintaining the computational resources required to generate and execute a continuously updating set of benchmark tasks.",
      "C": "Guaranteeing the dynamic benchmark tasks accurately reflect the real-world distribution of data the model will encounter after deployment.",
      "D": "Avoiding the introduction of bias into the benchmark through the selection criteria for dynamically generated tasks."
    },
    "answer": "C",
    "explanation": "The correct answer is (C). The core problem of dynamic benchmarks is ensuring they accurately represent the data the model will encounter in the real world. Static benchmarks are less susceptible to this because they are fixed.",
    "question_token_count": 27,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 5,
    "avg_answer_token_count": 19
  },
  {
    "context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n",
    "topic": "The potential impact of in-distribution contamination on LLM performance and generalization.",
    "response": "<document_analysis>\nThe context discusses various methods for mitigating the risk of \u201cin-distribution contamination\u201d during LLM training \u2013 essentially, the risk that the training data itself contains examples that are too similar to the test data, leading to artificially inflated performance metrics. Several techniques are presented, including Auto-Dataset, StructEval, ITD, and VarBench, each employing LLMs to generate new samples or detect/correct contamination. The core issue being addressed is that relying on existing benchmarks, even when rewritten by LLMs, can introduce bias and undermine the ability to accurately assess an LLM\u2019s true generalization capabilities. The difficulty lies in distinguishing between genuine improvement and simply memorizing or adapting to the training data.\n\nStrategic Complexity Calibration: 8/10 \u2013 This requires a nuanced understanding of LLM training methodologies and the potential pitfalls of benchmark reliance. It\u2019s not a simple factual question, but one requiring synthesis and critical thinking.\n</document_analysis>\n<output_format>\nQuestion:  Considering the methods outlined to address in-distribution contamination, what is the primary limitation of relying on LLM-rewritten benchmarks for evaluating LLM generalization?\nA: LLMs consistently produce lower performance on rewritten benchmarks compared to the original data.\nB: The use of LLMs introduces a bias towards replicating the stylistic features of the original benchmark samples.\nC: The inherent risk of in-distribution contamination remains, even when LLMs are employed to generate or modify benchmark data.\nD: LLM-generated samples always exhibit lower cognitive difficulty than the original benchmark questions.\nExplanation: The context explicitly states that \u201cpublicly available rule-generated data may increase the risk of in-distribution contamination.\u201d  The methods described (Auto-Dataset, StructEval, ITD, VarBench) are attempts to *mitigate* this risk, suggesting it\u2019s a persistent concern.\nCorrect Answer: C\n</output_format>",
    "question": "Considering the methods outlined to address in-distribution contamination, what is the primary limitation of relying on LLM-rewritten benchmarks for evaluating LLM generalization?",
    "choices": {
      "A": "LLMs consistently produce lower performance on rewritten benchmarks compared to the original data.",
      "B": "The use of LLMs introduces a bias towards replicating the stylistic features of the original benchmark samples.",
      "C": "The inherent risk of in-distribution contamination remains, even when LLMs are employed to generate or modify benchmark data.",
      "D": "LLM-generated samples always exhibit lower cognitive difficulty than the original benchmark questions."
    },
    "answer": "C",
    "explanation": "The context explicitly states that \u201cpublicly available rule-generated data may increase the risk of in-distribution contamination.\u201d  The methods described (Auto-Dataset, StructEval, ITD, VarBench) are attempts to *mitigate* this risk, suggesting it\u2019s a persistent concern.",
    "question_token_count": 28,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 19
  },
  {
    "context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n",
    "topic": "How does the definition of scalability \u2013 the ratio of transformed dataset size to transformation cost \u2013 address the potential issues with smaller benchmark datasets?",
    "response": "<document_analysis>\nThe context describes \"scalability\" in dynamic benchmarking as the ratio of transformed dataset size to transformation cost. The key point is that smaller datasets introduce statistical errors, necessitating larger datasets with minimized costs. The document provides a mathematical definition of scalability. The challenge lies in understanding how this concept specifically addresses the limitations of smaller benchmark datasets. The question should probe the expert's understanding of *why* scalability is crucial when using smaller datasets.\n\nDifficulty Rating: 8/10\n</document_analysis>\n\n<output_format>\nQuestion: How does the defined scalability metric directly mitigate the potential for biased results when evaluating dynamic benchmarking methods using smaller benchmark datasets?\nA: It prioritizes generating datasets with a higher proportion of unique data points, regardless of cost.\nB: It focuses on minimizing the transformation cost, assuming a larger dataset size inherently reduces statistical error.\nC: It balances dataset size and transformation cost to maximize the amount of data generated per unit of resource expenditure, addressing statistical uncertainty introduced by smaller datasets.\nD: It solely emphasizes the computational efficiency of the transformation process, irrespective of the dataset size.\nExplanation: The correct answer (C) directly addresses the core issue of smaller datasets causing statistical error. Scalability, as defined, is about generating *more* data *efficiently*, which is essential for reducing the impact of small dataset size on statistical reliability.\nCorrect Answer: C\n</output_format>",
    "question": "How does the defined scalability metric directly mitigate the potential for biased results when evaluating dynamic benchmarking methods using smaller benchmark datasets?",
    "choices": {
      "A": "It prioritizes generating datasets with a higher proportion of unique data points, regardless of cost.",
      "B": "It focuses on minimizing the transformation cost, assuming a larger dataset size inherently reduces statistical error.",
      "C": "It balances dataset size and transformation cost to maximize the amount of data generated per unit of resource expenditure, addressing statistical uncertainty introduced by smaller datasets.",
      "D": "It solely emphasizes the computational efficiency of the transformation process, irrespective of the dataset size."
    },
    "answer": "C",
    "explanation": "The correct answer (C) directly addresses the core issue of smaller datasets causing statistical error. Scalability, as defined, is about generating *more* data *efficiently*, which is essential for reducing the impact of small dataset size on statistical reliability.",
    "question_token_count": 24,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 2,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 21
  },
  {
    "context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n",
    "topic": "How might the function \u0398(\ud835\udc9fi, \ud835\udc9fj) be implemented to calculate diversity between two datasets, according to the document?",
    "response": "`<document_analysis>`\nThe context defines the function \u0398(\ud835\udc9fi, \ud835\udc9fj) as a measure of diversity between two datasets, \ud835\udc9fi and \ud835\udc9fj. It highlights two components: external diversity (measuring variation from a seed dataset) and internal diversity (measuring differences between transformation trials). The context provides equations and mentions examples like N-gram metrics and BLEU scores as potential implementations of \u0398(\ud835\udc9fi, \ud835\udc9fj). The goal is to understand how this function is implemented to calculate diversity.\n\nStrategic Complexity Calibration: 8/10 \u2013 The question requires understanding of a function and its application within a specific domain (diversity calculation), demanding more than a simple factual recall. It also requires recognizing that the function itself (\u0398) can take different forms (N-gram metrics, BLEU scores).\n\nThought Process: The question aims to assess the reader's ability to grasp the conceptual role of the function \u0398(\ud835\udc9fi, \ud835\udc9fj) and to consider possible implementations without needing to know the specific details of those implementations.\n</document_analysis>`\n\n`<output_format>`\nQuestion: Considering the provided definitions, how would the function \u0398(\ud835\udc9fi, \ud835\udc9fj) most accurately be described in terms of its core computational purpose within the context of dataset diversity evaluation?\nA: \u0398(\ud835\udc9fi, \ud835\udc9fj) calculates the average Euclidean distance between individual data points in datasets \ud835\udc9fi and \ud835\udc9fj.\nB: \u0398(\ud835\udc9fi, \ud835\udc9fj) quantifies the similarity score between datasets \ud835\udc9fi and \ud835\udc9fj based on N-gram overlap.\nC: \u0398(\ud835\udc9fi, \ud835\udc9fj) measures the difference in the distribution of feature values across datasets \ud835\udc9fi and \ud835\udc9fj.\nD: \u0398(\ud835\udc9fi, \ud835\udc9fj) computes the cosine similarity between the transformed representations of datasets \ud835\udc9fi and \ud835\udc9fj.\nExplanation: The context states that \u0398(\ud835\udc9fi, \ud835\udc9fj) is a function that \u201cmeasures the diversity between two datasets\u201d and provides examples like N-gram metrics and BLEU scores.  Option C most accurately reflects this by describing a measure of distributional difference, aligning with the concept of diversity. Options A, B, and D represent other specific similarity or distance metrics, not the general function \u0398(\ud835\udc9fi, \ud835\udc9fj) as defined in the text.\nCorrect Answer: C\n</output_format>`",
    "question": "Considering the provided definitions, how would the function \u0398(\ud835\udc9fi, \ud835\udc9fj) most accurately be described in terms of its core computational purpose within the context of dataset diversity evaluation?",
    "choices": {
      "A": "\u0398(\ud835\udc9fi, \ud835\udc9fj) calculates the average Euclidean distance between individual data points in datasets \ud835\udc9fi and \ud835\udc9fj.",
      "B": "\u0398(\ud835\udc9fi, \ud835\udc9fj) quantifies the similarity score between datasets \ud835\udc9fi and \ud835\udc9fj based on N-gram overlap.",
      "C": "\u0398(\ud835\udc9fi, \ud835\udc9fj) measures the difference in the distribution of feature values across datasets \ud835\udc9fi and \ud835\udc9fj.",
      "D": "\u0398(\ud835\udc9fi, \ud835\udc9fj) computes the cosine similarity between the transformed representations of datasets \ud835\udc9fi and \ud835\udc9fj."
    },
    "answer": "C",
    "explanation": "The context states that \u0398(\ud835\udc9fi, \ud835\udc9fj) is a function that \u201cmeasures the diversity between two datasets\u201d and provides examples like N-gram metrics and BLEU scores.  Option C most accurately reflects this by describing a measure of distributional difference, aligning with the concept of diversity. Options A, B, and D represent other specific similarity or distance metrics, not the general function \u0398(\ud835\udc9fi, \ud835\udc9fj) as defined in the text.",
    "question_token_count": 39,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 30
  },
  {
    "context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n",
    "topic": "Retrieval-based detection methods for contamination are hampered by the scale and complexity of LLM training corpora, making complete exclusion of evaluation data challenging.",
    "response": "`<document_analysis>`\nThe core challenge presented is the difficulty of completely excluding evaluation data from LLM training due to the scale and complexity of the training corpora. The text highlights the risk of overlap between training and evaluation sets, compounded by fine-tuning on human-annotated or synthetic datasets that resemble evaluation tasks. The proprietary nature of training data adds another layer of complexity. The question needs to probe the expert\u2019s understanding of *why* complete exclusion is so difficult, going beyond simply stating the problem. It should also touch upon the implications of this challenge for evaluating LLM performance and the need for robust benchmarks. The difficulty level should be high, requiring an understanding of the technical hurdles involved.\n</document_analysis>\n`<output_format>`\nQuestion: Considering the scale and diversity of LLM training datasets, what is the primary technical obstacle preventing the complete removal of evaluation data during model development?\nA: The inherent randomness of the web scraping process.\nB: The limited computational resources available for data curation.\nC: The difficulty of accurately identifying and removing duplicate data points across diverse sources.\nD: The proprietary nature of training data, which restricts access and verification.\nExplanation: The text explicitly states that the \u201csheer scale and complexity of training corpora\u201d makes complete exclusion of evaluation data challenging. While other factors may contribute, this is the primary technical hurdle identified.\nCorrect Answer: C </output_format>",
    "question": "Considering the scale and diversity of LLM training datasets, what is the primary technical obstacle preventing the complete removal of evaluation data during model development?",
    "choices": {
      "A": "The inherent randomness of the web scraping process.",
      "B": "The limited computational resources available for data curation.",
      "C": "The difficulty of accurately identifying and removing duplicate data points across diverse sources.",
      "D": "The proprietary nature of training data, which restricts access and verification."
    },
    "answer": "C",
    "explanation": "The text explicitly states that the \u201csheer scale and complexity of training corpora\u201d makes complete exclusion of evaluation data challenging. While other factors may contribute, this is the primary technical hurdle identified.",
    "question_token_count": 28,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 12
  },
  {
    "context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n",
    "topic": "The consequences of misleading benchmark results, influenced by data contamination, extend to model comparisons, deployment, and policy-making.",
    "response": "```\n<document_analysis>\nThe context highlights the critical issue of data contamination in LLM benchmarking and its far-reaching consequences. It distinguishes between different types of contamination (syntactic) and emphasizes the significance of addressing this problem due to its potential to distort model evaluations and impact crucial decisions. The core argument is that contaminated benchmarks lead to an overestimation of model capabilities, undermining their validity and potentially influencing research, deployment, and policy. The difficulty in distinguishing between memorization and reasoning in LLMs related to syntactic contamination is also noted.\n\nStrategic Complexity Calibration: 8/10 \u2013 This requires an understanding of LLM evaluation methodologies, potential biases, and the impact of data quality on research outcomes. It\u2019s not a simple factual recall but demands critical analysis.\n\n</document_analysis>\n<output_format>\nQuestion: Beyond simple rephrasing, what fundamental methodological challenge does the context identify regarding the classification of \u201ccontamination\u201d in LLM benchmarking, specifically concerning syntactic transformations?\nA: Syntactic transformations are inherently unreliable indicators of a model's reasoning abilities.\nB: Distinguishing between a model\u2019s ability to recall memorized information and genuine reasoning during inference is exceptionally difficult, regardless of the transformation method.\nC: Syntactic contamination is easily detectable through statistical analysis of the transformed data.\nD: All NLP applications prioritize syntactic information over semantic understanding.\nExplanation: The text explicitly states, \u201cThere is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference.\u201d\nCorrect Answer: B\n</output_format>\n```",
    "question": "Beyond simple rephrasing, what fundamental methodological challenge does the context identify regarding the classification of \u201ccontamination\u201d in LLM benchmarking, specifically concerning syntactic transformations?",
    "choices": {
      "A": "Syntactic transformations are inherently unreliable indicators of a model's reasoning abilities.",
      "B": "Distinguishing between a model\u2019s ability to recall memorized information and genuine reasoning during inference is exceptionally difficult, regardless of the transformation method.",
      "C": "Syntactic contamination is easily detectable through statistical analysis of the transformed data.",
      "D": "All NLP applications prioritize syntactic information over semantic understanding."
    },
    "answer": "B",
    "explanation": "The text explicitly states, \u201cThere is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference.\u201d",
    "question_token_count": 33,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 17
  },
  {
    "context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n",
    "topic": "How do the cited benchmarks (GLUE, SuperGLUE, HumanEval) exemplify the application of label protection, and what is the intended consequence of withholding test labels?",
    "response": "`<document_analysis>`\nThe context discusses label protection techniques used in benchmarks like GLUE, SuperGLUE, and HumanEval. It highlights the core principle of withholding test labels to prevent models from memorizing answers during training and compromising evaluation integrity. The text also acknowledges limitations of these methods, such as reliance on secure key management and computational overheads. The overarching purpose of label protection is to ensure a fair and reliable assessment of model performance. The difficulty lies in understanding the nuanced impact of this technique beyond simply preventing memorization.\n\nStrategic Complexity Calibration: 8/10 \u2013 This requires understanding the broader implications of withholding labels and the potential strategies models might employ to circumvent this protection. </document_analysis>\n\n<output_format>\nQuestion: How does the deliberate withholding of test labels in benchmarks like GLUE, SuperGLUE, and HumanEval fundamentally alter the evaluation process, and what is the primary rationale for this alteration?\nA: It simplifies the evaluation process by reducing the complexity of the test data.\nB: It mitigates the risk of model overfitting by preventing the model from memorizing answers during training.\nC: It allows for faster model development by reducing the time required to generate test datasets.\nD: It guarantees a more accurate representation of a model\u2019s generalization capabilities.\nExplanation: The correct answer is B. The text explicitly states that the key advantage of label protection is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\nCorrect Answer: B\n</output_format>",
    "question": "How does the deliberate withholding of test labels in benchmarks like GLUE, SuperGLUE, and HumanEval fundamentally alter the evaluation process, and what is the primary rationale for this alteration?",
    "choices": {
      "A": "It simplifies the evaluation process by reducing the complexity of the test data.",
      "B": "It mitigates the risk of model overfitting by preventing the model from memorizing answers during training.",
      "C": "It allows for faster model development by reducing the time required to generate test datasets.",
      "D": "It guarantees a more accurate representation of a model\u2019s generalization capabilities."
    },
    "answer": "B",
    "explanation": "The correct answer is B. The text explicitly states that the key advantage of label protection is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.",
    "question_token_count": 38,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 16
  }
]