[
  {
    "context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n",
    "topic": "Reflect on potential biases introduced by static benchmarks\u2019 seed datasets and propose strategies for mitigation and diversification.",
    "response": "<document_analysis>\nThe context describes how static benchmarks are defined by seed datasets D=(X,Y,S(\u00b7)) and highlights potential limitations such as domain overrepresentation in inputs X and misalignments in scoring functions S. The question will probe strategies to mitigate both sampling bias in X and calibration issues in S, requiring understanding of dataset construction and evaluation calibration. Difficulty is rated 8/10: it requires synthesis of multiple mitigation strategies.\n</document_analysis>\n\n<output_format>\nQuestion: Within a static benchmark defined by the seed dataset D=(X,Y,S), which combination of interventions most directly mitigates input-domain overrepresentation in X while simultaneously reducing misalignment risks in the scoring function S?\nA: Apply genre-stratified sampling to rebalance X and integrate continuous human-in-the-loop calibration for S\nB: Generate adversarial paraphrases of outputs in Y and apply black-box test-time augmentation to X\nC: Augment Y using automated paraphrasing and adopt a fixed-threshold rule for S\nD: Expand X with randomly crawled web data and replace S with cross-model agreement metrics\nExplanation: Option A specifically rebalances input coverage via stratified sampling and improves scoring alignment through ongoing human calibration, directly addressing biases in both X and S.\nCorrect Answer: A\n</output_format>",
    "question": "Within a static benchmark defined by the seed dataset D=(X,Y,S), which combination of interventions most directly mitigates input-domain overrepresentation in X while simultaneously reducing misalignment risks in the scoring function S?",
    "choices": {
      "A": "Apply genre-stratified sampling to rebalance X and integrate continuous human-in-the-loop calibration for S",
      "B": "Generate adversarial paraphrases of outputs in Y and apply black-box test-time augmentation to X",
      "C": "Augment Y using automated paraphrasing and adopt a fixed-threshold rule for S",
      "D": "Expand X with randomly crawled web data and replace S with cross-model agreement metrics"
    },
    "answer": "A",
    "explanation": "Option A specifically rebalances input coverage via stratified sampling and improves scoring alignment through ongoing human calibration, directly addressing biases in both X and S.",
    "question_token_count": 41,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 9,
    "question_groundedness_score": 6,
    "avg_answer_token_count": 18
  },
  {
    "context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n",
    "topic": "Describe normalization techniques that ensure diversity scores are comparable across datasets of different sizes.",
    "response": "<document_analysis>\nThe context defines external and internal diversity metrics as expectations of a pairwise diversity function \u0398 over datasets of potentially varying sizes but does not prescribe how to normalize \u0398 so scores remain comparable when |D| differs. A deep engagement question will probe normalization choices (e.g., dividing by max, arithmetic mean, geometric mean of dataset sizes, or fixed-size sampling) and ask which yields true scale invariance. This requires understanding of how normalization impacts pairwise metrics, a subtlety even experts can overlook.\n</document_analysis>\n\n<output_format>\nQuestion: To make the pairwise diversity score \u0398(D_i,D_j) truly comparable across datasets of different sizes |D_i| and |D_j|, which normalization strategy preserves symmetry and scale invariance?\nA: Divide \u0398(D_i,D_j) by max(|D_i|,|D_j|)\nB: Divide \u0398(D_i,D_j) by the arithmetic mean (|D_i|+|D_j|)/2\nC: Divide \u0398(D_i,D_j) by the geometric mean \u221a(|D_i|\u00b7|D_j|)\nD: Forego normalization and instead sample both datasets down to a fixed size k\nExplanation: Dividing by the geometric mean of the dataset sizes yields a symmetric, scale\u2011invariant normalization, ensuring the score does not bias toward larger or smaller datasets.\nCorrect Answer: C\n</output_format>",
    "question": "To make the pairwise diversity score \u0398(D_i,D_j) truly comparable across datasets of different sizes |D_i| and |D_j|, which normalization strategy preserves symmetry and scale invariance?",
    "choices": {
      "A": "Divide \u0398(D_i,D_j) by max(|D_i|,|D_j|)",
      "B": "Divide \u0398(D_i,D_j) by the arithmetic mean (|D_i|+|D_j|)/2",
      "C": "Divide \u0398(D_i,D_j) by the geometric mean \u221a(|D_i|\u00b7|D_j|)",
      "D": "Forego normalization and instead sample both datasets down to a fixed size k"
    },
    "answer": "C",
    "explanation": "Dividing by the geometric mean of the dataset sizes yields a symmetric, scale\u2011invariant normalization, ensuring the score does not bias toward larger or smaller datasets.",
    "question_token_count": 38,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 8,
    "question_groundedness_score": 4,
    "avg_answer_token_count": 19
  },
  {
    "context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n",
    "topic": "Reflect on the implications of training data contamination for the validity and reliability of LLM benchmarks in research and industry settings.",
    "response": "<document_analysis>\nThe context discusses how large language models (LLMs) are pre-trained on massive, diverse, often proprietary datasets and subsequently fine-tuned on human-annotated or synthetic data. These practices greatly increase the risk of overlap between evaluation benchmarks and training data, leading to inflated performance metrics and undermining reliable, fair comparisons. Key themes include the difficulty of fully excluding evaluation data given the scale and opacity of training corpora, the limited efficacy of retrieval-based contamination detection, and the challenge posed by proprietary datasets that prevent external verification.  \nI will generate a question that probes an expert\u2019s understanding of how proprietary training data exacerbates contamination risks and compromises benchmark validity and comparability.\nDifficulty: 8/10\n</document_analysis>\n\n<output_format>\nQuestion: Which aspect of LLM benchmarking is most undermined by the proprietary and opaque nature of their training data?\nA: The reproducibility of benchmark workloads across different hardware platforms  \nB: The fairness of cross\u2010model comparisons due to unverifiable overlaps with evaluation sets  \nC: The interpretability of internal attention mechanisms during inference  \nD: The scalability of retrieval\u2010based detection methods across languages  \nExplanation: Proprietary training data opacity prevents external checks for evaluation\u2010set overlap, directly compromising fair, reliable comparisons.  \nCorrect Answer: B\n</output_format>",
    "question": "Which aspect of LLM benchmarking is most undermined by the proprietary and opaque nature of their training data?",
    "choices": {
      "A": "The reproducibility of benchmark workloads across different hardware platforms",
      "B": "The fairness of cross\u2010model comparisons due to unverifiable overlaps with evaluation sets",
      "C": "The interpretability of internal attention mechanisms during inference",
      "D": "The scalability of retrieval\u2010based detection methods across languages"
    },
    "answer": "B",
    "explanation": "Proprietary training data opacity prevents external checks for evaluation\u2010set overlap, directly compromising fair, reliable comparisons.",
    "question_token_count": 21,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 11
  },
  {
    "context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Proposed evaluation criteria for dynamic benchmarks and the rationale behind each criterion.",
    "response": "<document_analysis>\nThe context introduces the shift from static to dynamic benchmarks for LLM evaluation, emphasizing that static benchmarks\u2014once publicly released\u2014inevitably become part of LLM training corpora, leading to data contamination. Techniques like data encryption and post-hoc contamination detection help but cannot fully prevent models from encountering test data during training. This inherent limitation of static methods underpins the rationale for dynamic benchmarking strategies such as continuously updating datasets based on model training timestamps or regenerating benchmarks. The critical insight is that static approaches cannot guarantee benchmarks remain unseen by models that scrape Internet data, necessitating dynamic solutions.\n</document_analysis>\n\n<output_format>\nQuestion: Which fundamental limitation of static LLM benchmarks remains unaddressed by measures like data encryption and post-hoc contamination detection, thereby driving the adoption of dynamic benchmarking approaches?\nA: Static benchmarks cannot accommodate evolving downstream tasks with new metrics\nB: Once publicly released, benchmarks cannot be guaranteed unseen by models trained on Internet-scraped data\nC: They demand prohibitively expensive manual annotation efforts for each evaluation\nD: Static benchmarks inherently produce human-biased test items that skew assessments\nExplanation: Static benchmarks, once exposed online, risk being included in LLM training data despite encryption or detection, so they cannot ensure test items remain unseen, motivating dynamic updates.\nCorrect Answer: B\n</output_format>",
    "question": "Which fundamental limitation of static LLM benchmarks remains unaddressed by measures like data encryption and post-hoc contamination detection, thereby driving the adoption of dynamic benchmarking approaches?",
    "choices": {
      "A": "Static benchmarks cannot accommodate evolving downstream tasks with new metrics",
      "B": "Once publicly released, benchmarks cannot be guaranteed unseen by models trained on Internet-scraped data",
      "C": "They demand prohibitively expensive manual annotation efforts for each evaluation",
      "D": "Static benchmarks inherently produce human-biased test items that skew assessments"
    },
    "answer": "B",
    "explanation": "Static benchmarks, once exposed online, risk being included in LLM training data despite encryption or detection, so they cannot ensure test items remain unseen, motivating dynamic updates.",
    "question_token_count": 32,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 13
  },
  {
    "context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n",
    "topic": "The rationale for dynamic benchmarks and how they address shortcomings of static evaluation methods",
    "response": "<document_analysis>\nThe context discusses the shortcomings of static benchmarks\u2014specifically their rising contamination risk as training data grows, formalized by Pr_contam \u221d |D_train|\u22c5|D_test|\u207b\u00b9\u2014and the promise of dynamic benchmarks to combat this via real\u2010time or hybrid generation. It also highlights that despite these advantages, dynamic benchmarks introduce challenges like balancing correctness with scalability and often neglect complexity control. This interplay between contamination risk, dynamic generation methods, and remaining limitations forms the basis for a deep, multi\u2010layered question probing both theoretical and practical understanding.\n</document_analysis>\n\n<output_format>\nQuestion: How does the relationship Pr_contam \u221d |D_train|\u22c5|D_test|\u207b\u00b9 motivate the move toward dynamic LLM benchmarks, and what key drawback of these dynamic benchmarks is emphasized by the authors?\nA: It shows that contamination risk increases with larger training data, motivating continuous regeneration of test items; dynamic benchmarks, however, often neglect complexity control.\nB: It shows that contamination risk decreases as training data grows, motivating enlargement of static test sets; static benchmarks, however, often neglect privacy requirements.\nC: It indicates dynamic benchmarks face scalability issues, motivating a return to static evaluation; dynamic benchmarks, however, suffer from over\u2010reliance on post\u2010hoc contamination detection.\nD: It reveals that static benchmarks ensure transparency, motivating dynamic benchmarks; dynamic benchmarks, however, depend entirely on human annotation for reliability.\nExplanation: The contamination formula implies that static tests become increasingly polluted with larger training corpora, spurring dynamic generation of fresh evaluation items; yet these dynamic benchmarks frequently overlook managing task complexity, leading to inefficiencies.\nCorrect Answer: A\n</output_format>",
    "question": "How does the relationship Pr_contam \u221d |D_train|\u22c5|D_test|\u207b\u00b9 motivate the move toward dynamic LLM benchmarks, and what key drawback of these dynamic benchmarks is emphasized by the authors?",
    "choices": {
      "A": "It shows that contamination risk increases with larger training data, motivating continuous regeneration of test items; dynamic benchmarks, however, often neglect complexity control.",
      "B": "It shows that contamination risk decreases as training data grows, motivating enlargement of static test sets; static benchmarks, however, often neglect privacy requirements.",
      "C": "It indicates dynamic benchmarks face scalability issues, motivating a return to static evaluation; dynamic benchmarks, however, suffer from over\u2010reliance on post\u2010hoc contamination detection.",
      "D": "It reveals that static benchmarks ensure transparency, motivating dynamic benchmarks; dynamic benchmarks, however, depend entirely on human annotation for reliability."
    },
    "answer": "A",
    "explanation": "The contamination formula implies that static tests become increasingly polluted with larger training corpora, spurring dynamic generation of fresh evaluation items; yet these dynamic benchmarks frequently overlook managing task complexity, leading to inefficiencies.",
    "question_token_count": 43,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 29
  },
  {
    "context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n",
    "topic": "Direct overlap detection via n\u2011gram matching at token level and its limitations",
    "response": "<document_analysis>\nThe context discusses limitations of exact n-gram matching for detecting train\u2013test overlaps, specifically that exact matching often yields false negatives when text is paraphrased or slightly altered. It then mentions embedding-based similarity methods as a more robust alternative. The key conceptual tension is between lexical exactness (leading to missed overlaps) and semantic generalization via embeddings. This motivates a question probing the nuanced limitation of token-level matching and why embeddings mitigate it. Difficulty is high given readers must connect false negatives, paraphrasing issues, and semantic similarity concepts.\n</document_analysis>\n\n<output_format>\nQuestion: What fundamental limitation of direct token-level n\u2011gram matching in post\u2011hoc overlap detection most motivates the shift to embedding\u2011based similarity methods?\nA: It fails to capture semantic equivalence in paraphrased or lexically modified text, causing false negatives.\nB: It overemphasizes common function-word sequences, leading to excessive false positives.\nC: It scales poorly with large corpora due to combinatorial growth of n\u2011grams.\nD: It requires extensive hyperparameter tuning for each dataset\u2019s optimal n\u2011gram length.\nExplanation: Exact n\u2011gram matching misses semantically equivalent but lexically altered passages, resulting in false negatives; embedding\u2011based methods detect semantic similarity to address this.\nCorrect Answer: A\n</output_format>",
    "question": "What fundamental limitation of direct token-level n\u2011gram matching in post\u2011hoc overlap detection most motivates the shift to embedding\u2011based similarity methods?",
    "choices": {
      "A": "It fails to capture semantic equivalence in paraphrased or lexically modified text, causing false negatives.",
      "B": "It overemphasizes common function-word sequences, leading to excessive false positives.",
      "C": "It scales poorly with large corpora due to combinatorial growth of n\u2011grams.",
      "D": "It requires extensive hyperparameter tuning for each dataset\u2019s optimal n\u2011gram length."
    },
    "answer": "A",
    "explanation": "Exact n\u2011gram matching misses semantically equivalent but lexically altered passages, resulting in false negatives; embedding\u2011based methods detect semantic similarity to address this.",
    "question_token_count": 29,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 18
  },
  {
    "context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n",
    "topic": "Analyze how the expectation operator and index ranges (i=1 to N, i\u2260j) functionally contribute to the computation of external and internal diversity.",
    "response": "<document_analysis>\nThe context defines two diversity measures using expectation operators over index ranges. External diversity averages \u0398(D_i, D) over i=1\u2026N. Internal diversity averages \u0398(D_i, D_j) over all i,j from 1 to N with i\u2260j. The key functional role of the index ranges is to determine the total number of comparisons included in each expectation\u2014external uses N comparisons, internal uses all ordered pairs excluding self-comparisons, totaling N(N\u22121). This affects normalization and the interpretation of the averaged diversity metric. Distractors can involve common misconceptions: counting unordered pairs (N(N\u22121)/2), including self-pairs (N^2), or linear counts (N\u22121). The question tests understanding of how index constraints shape the normalization of the expectation.\nDifficulty: 6/10\n</document_analysis>\n\n<output_format>\nQuestion: In the internal diversity metric defined as E_{i,j=1, i\u2260j}^N \u0398(D_i, D_j), the expectation operator averages over how many distinct ordered dataset comparisons?\nA: N(N\u22121)\n/2\nB: N(N\u22121)\nC: N^2\nD: N\u22121\nExplanation: The double summation over i and j (i\u2260j) includes all ordered pairs except self-comparisons, yielding N(N\u22121) terms.\nCorrect Answer: B\n</output_format>",
    "question": "In the internal diversity metric defined as E_{i,j=1, i\u2260j}^N \u0398(D_i, D_j), the expectation operator averages over how many distinct ordered dataset comparisons?",
    "choices": {
      "A": "N(N\u22121)",
      "B": "N(N\u22121)",
      "C": "N^2",
      "D": "N\u22121"
    },
    "answer": "B",
    "explanation": "The double summation over i and j (i\u2260j) includes all ordered pairs except self-comparisons, yielding N(N\u22121) terms.",
    "question_token_count": 39,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 5
  },
  {
    "context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n",
    "topic": "Critically assess the limitations of current coding benchmarks in measuring a model\u2019s long-term maintainability and software design skills.",
    "response": "<document_analysis>\nThe context outlines various coding benchmarks (HumanEval, MBPP, SWE-Bench, Codeforces, Aider) focused on functional code synthesis, debugging, and dynamic problem solving. However, none explicitly measure long-term maintainability, software architecture quality, modular cohesion, coupling, or iterative refactoring skills. This absence is the critical content gap needed for a probing question.\nDifficulty: 8/10 \u2014 requires expert-level insight into benchmarking and software engineering quality attributes.\n</document_analysis>\n\n<output_format>\nQuestion: Which specific limitation of benchmarks such as HumanEval and MBPP most directly impedes evaluating a model\u2019s ability to generate maintainable code with robust software design principles?\nA: They do not measure algorithmic correctness under time constraints.\nB: They lack metrics for assessing modular cohesion and low coupling across evolving requirements.\nC: They fail to test code execution performance in production-like environments.\nD: They do not enforce adherence to language-specific syntactic conventions.\nExplanation: Current benchmarks focus on correctness and debugging but omit evaluation of modular cohesion, coupling, and other architectural qualities key to maintainability.\nCorrect Answer: B\n</output_format>",
    "question": "Which specific limitation of benchmarks such as HumanEval and MBPP most directly impedes evaluating a model\u2019s ability to generate maintainable code with robust software design principles?",
    "choices": {
      "A": "They do not measure algorithmic correctness under time constraints.",
      "B": "They lack metrics for assessing modular cohesion and low coupling across evolving requirements.",
      "C": "They fail to test code execution performance in production-like environments.",
      "D": "They do not enforce adherence to language-specific syntactic conventions."
    },
    "answer": "B",
    "explanation": "Current benchmarks focus on correctness and debugging but omit evaluation of modular cohesion, coupling, and other architectural qualities key to maintainability.",
    "question_token_count": 33,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 7,
    "avg_answer_token_count": 12
  },
  {
    "context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n",
    "topic": "Critique the suitability of BLEU as a diversity metric and identify potential pitfalls when using reference\u2011based measures.",
    "response": "<document_analysis>\nThe context introduces external and internal diversity metrics for transformed datasets, both relying on a generic similarity/diversity function \u0398(\u00b7,\u00b7). It specifically notes that \u0398 can be instantiated with standard n\u2011gram overlap measures or reference\u2011based metrics such as BLEU. This raises a nuanced critique: BLEU is fundamentally a similarity score against fixed references and thus may penalize novel, diverse transformations\u2014contrary to the intention of a diversity measure. Potential pitfalls include BLEU\u2019s bias towards conservative outputs, insensitivity to true semantic variation, and misinterpretation when used as a diversity metric.\nDifficulty rating: 8/10\nPlanned question: probe why BLEU is unsuitable for diversity measurement by contrasting its reference\u2011based similarity focus with the goal of maximizing variation.\n</document_analysis>\n\n<output_format>\nQuestion: Why is BLEU considered a problematic choice for measuring the diversity of transformed datasets?\nA: Because BLEU prioritizes overlap with reference texts, discouraging novel or varied transformations.\nB: Because BLEU\u2019s n\u2011gram overlap inherently conflates semantic depth with lexical variation.\nC: Because BLEU applies a brevity penalty that skews assessments when comparing dataset pairs of different sizes.\nD: Because BLEU is symmetric and thus cannot distinguish directionality when comparing two datasets.\nExplanation: BLEU is a reference\u2011based similarity metric; it rewards closeness to the reference and thus underestimates diversity by penalizing novel variations.\nCorrect Answer: A\n</output_format>",
    "question": "Why is BLEU considered a problematic choice for measuring the diversity of transformed datasets?",
    "choices": {
      "A": "Because BLEU prioritizes overlap with reference texts, discouraging novel or varied transformations.",
      "B": "Because BLEU\u2019s n\u2011gram overlap inherently conflates semantic depth with lexical variation.",
      "C": "Because BLEU applies a brevity penalty that skews assessments when comparing dataset pairs of different sizes.",
      "D": "Because BLEU is symmetric and thus cannot distinguish directionality when comparing two datasets."
    },
    "answer": "A",
    "explanation": "BLEU is a reference\u2011based similarity metric; it rewards closeness to the reference and thus underestimates diversity by penalizing novel variations.",
    "question_token_count": 17,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 6,
    "avg_answer_token_count": 18
  },
  {
    "context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n",
    "topic": "Comparative considerations between table-based and graph-based evaluation frameworks in measuring different dimensions of LLM reasoning ability.",
    "response": "<document_analysis>\nThe context contrasts table-based evaluation (S3Eval) where LLMs execute SQL queries on synthetic tables (deterministic, polynomial-time relational operations) with graph-based frameworks (DyVal, NPHardEval, Xie et al.) that transform random graphs into NL descriptions to probe DAG inference or NP-hard combinatorial optimization (exponential search). The key comparative dimension lies in computational complexity (polynomial vs NP-hard) and reasoning type (deterministic relational algebra vs heuristic search/graph inference). A high-difficulty question should probe this complexity and reasoning contrast.\n</document_analysis>\n\n<output_format>\nQuestion: Which statement best characterizes the fundamental computational complexity and reasoning difference between S3Eval and NPHardEval in evaluating large language models?\nA: S3Eval evaluates deterministic relational algebra tasks solvable in polynomial time, whereas NPHardEval challenges LLMs with NP-hard combinatorial optimization requiring implicit heuristic search through exponential spaces.\nB: Both S3Eval and NPHardEval require LLMs to translate structured inputs into natural language, but S3Eval focuses on directed acyclic graphs while NPHardEval focuses on SQL joins.\nC: S3Eval assesses graph traversal algorithms over random DAGs, whereas NPHardEval measures the accuracy of SQL query execution on large synthetic tables.\nD: S3Eval tests generative planning within conceptual graphs, while NPHardEval evaluates deductive theorem proving over relational schemas.\nExplanation: S3Eval benchmarks SQL query execution (polynomial-time relational operations), while NPHardEval uses NP-hard problems like TSP on random graphs, imposing exponential search and heuristic reasoning demands.\nCorrect Answer: A\n</output_format>",
    "question": "Which statement best characterizes the fundamental computational complexity and reasoning difference between S3Eval and NPHardEval in evaluating large language models?",
    "choices": {
      "A": "S3Eval evaluates deterministic relational algebra tasks solvable in polynomial time, whereas NPHardEval challenges LLMs with NP-hard combinatorial optimization requiring implicit heuristic search through exponential spaces.",
      "B": "Both S3Eval and NPHardEval require LLMs to translate structured inputs into natural language, but S3Eval focuses on directed acyclic graphs while NPHardEval focuses on SQL joins.",
      "C": "S3Eval assesses graph traversal algorithms over random DAGs, whereas NPHardEval measures the accuracy of SQL query execution on large synthetic tables.",
      "D": "S3Eval tests generative planning within conceptual graphs, while NPHardEval evaluates deductive theorem proving over relational schemas."
    },
    "answer": "A",
    "explanation": "S3Eval benchmarks SQL query execution (polynomial-time relational operations), while NPHardEval uses NP-hard problems like TSP on random graphs, imposing exponential search and heuristic reasoning demands.",
    "question_token_count": 27,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 32
  },
  {
    "context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n",
    "topic": "Define collision probability in the context of rule-based benchmark generation and assess its significance for dataset uniqueness.",
    "response": "<document_analysis>\nThe context outlines \"rule-based generation\" methods that synthesize test cases via predefined rules and emphasizes an \"extremely low collision probability.\" Collision probability here refers to the chance that two independently generated instances will be identical. Its significance lies in ensuring high uniqueness and minimal duplicates within the dataset, which supports robust evaluation and reduces contamination. Irrelevant details (e.g., template examples) are noted but the core concept is collision probability and its impact on dataset diversity.\nDifficulty rating: 7/10 \u2014 requires understanding of probabilistic diversity measures in benchmark creation and their practical implications for LLM evaluation.\n</document_analysis>\n\n<output_format>\nQuestion: Which statement best captures the definition of collision probability in rule-based benchmark generation and explains why an extremely low value is essential for dataset uniqueness?\nA: It is the likelihood that a generated instance matches an existing human-created benchmark, and a low value prevents overlap with prior datasets.\nB: It is the chance that two independently generated instances are identical, and an extremely low value ensures minimal duplicates and maximizes uniqueness.\nC: It quantifies how often rule applications produce semantically equivalent but syntactically varied instances, and a low value indicates limited semantic diversity.\nD: It measures the probability of conflicting rule interactions causing generation to fail, and a low value reflects a robust rule set.\nExplanation: Collision probability refers specifically to the chance of identical duplicates; an extremely low rate means very few repeats, thus high uniqueness.\nCorrect Answer: B\n</output_format>",
    "question": "Which statement best captures the definition of collision probability in rule-based benchmark generation and explains why an extremely low value is essential for dataset uniqueness?",
    "choices": {
      "A": "It is the likelihood that a generated instance matches an existing human-created benchmark, and a low value prevents overlap with prior datasets.",
      "B": "It is the chance that two independently generated instances are identical, and an extremely low value ensures minimal duplicates and maximizes uniqueness.",
      "C": "It quantifies how often rule applications produce semantically equivalent but syntactically varied instances, and a low value indicates limited semantic diversity.",
      "D": "It measures the probability of conflicting rule interactions causing generation to fail, and a low value reflects a robust rule set."
    },
    "answer": "B",
    "explanation": "Collision probability refers specifically to the chance of identical duplicates; an extremely low rate means very few repeats, thus high uniqueness.",
    "question_token_count": 28,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 5,
    "avg_answer_token_count": 25
  },
  {
    "context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n",
    "topic": "Formulate guidelines for responsible data usage in benchmark creation, balancing openness with privacy and ethical constraints.",
    "response": "<document_analysis>\nThe context discusses the ethical trade-offs in creating LLM benchmarks, contrasting static benchmarks (risking perpetuated bias from outdated data) with dynamic benchmarks (raising privacy and security issues due to ongoing data collection). It emphasizes transparency, fairness, accountability, and privacy, and warns against misuse such as biased evaluation or inflated performance claims. A question that probes an expert\u2019s ability to integrate these principles into concrete guideline recommendations is appropriate. Difficulty is rated 8/10, as it demands synthesis of multiple ethical concerns into a coherent set of practices.\n</document_analysis>\n\n<output_format>\nQuestion: Which combination of practices most comprehensively aligns with ethical guidelines for responsible data usage in developing AI benchmarks, effectively balancing openness, fairness, privacy, and accountability?\nA: Collect data from public sources, anonymize it, publish raw datasets openly, and perform internal reviews.\nB: Update benchmarks dynamically using raw user logs, provide full transparency of methods online, and rely on community feedback for bias mitigation.\nC: Use static, curated public datasets with known provenance, apply anonymization, conduct periodic third\u2011party audits, and document methodology publicly.\nD: Obtain informed consent for new data, apply differential privacy techniques before integration, maintain transparent benchmarking protocols, and enforce independent oversight.\nExplanation: Option D integrates informed consent, differential privacy, methodological transparency, and independent accountability, addressing privacy, fairness, transparency, and misuse risks simultaneously.\nCorrect Answer: D\n</output_format>",
    "question": "Which combination of practices most comprehensively aligns with ethical guidelines for responsible data usage in developing AI benchmarks, effectively balancing openness, fairness, privacy, and accountability?",
    "choices": {
      "A": "Collect data from public sources, anonymize it, publish raw datasets openly, and perform internal reviews.",
      "B": "Update benchmarks dynamically using raw user logs, provide full transparency of methods online, and rely on community feedback for bias mitigation.",
      "C": "Use static, curated public datasets with known provenance, apply anonymization, conduct periodic third\u2011party audits, and document methodology publicly.",
      "D": "Obtain informed consent for new data, apply differential privacy techniques before integration, maintain transparent benchmarking protocols, and enforce independent oversight."
    },
    "answer": "D",
    "explanation": "Option D integrates informed consent, differential privacy, methodological transparency, and independent accountability, addressing privacy, fairness, transparency, and misuse risks simultaneously.",
    "question_token_count": 32,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 6,
    "avg_answer_token_count": 24
  },
  {
    "context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n",
    "topic": "Role and implementation of the oracle function G(\u00b7) as the source of ground truth in correctness evaluation.",
    "response": "<document_analysis>\nThe context describes the role of the oracle function G(\u00b7) as the definitive source of ground truth in measuring correctness of dynamic benchmarks. It notes that correctness is quantified as the expected alignment between transformed outputs and G(\u00b7)\u2019s ground truth, using a scoring function S(\u00b7). The key nuance is that G(\u00b7) must be objective and accurate; any bias or mislabeling directly impacts the correctness score. This insight enables a question probing how different flaws in G(\u00b7) affect the correctness evaluation, especially distinguishing systematic bias (which inflates scores) from random noise or omissions (which affect variance or coverage but not systematic inflation).\nDifficulty rating: 8/10\u2014requires understanding of bias vs. variance in ground-truth oracles and their impact on benchmark metrics.\n</document_analysis>\n\n<output_format>\nQuestion: Which flaw in the oracle function G(\u00b7) can systematically inflate the expected alignment score between transformed outputs and ground truth?\nA: Occasional omission of ground truth labels for rare categories.\nB: Random annotation noise introduced uniformly across examples.\nC: Systematic mislabeling that aligns erroneous oracle outputs with the model's predictions.\nD: Variability in oracle outputs due to non-deterministic decision processes.\nExplanation: Systematic mislabeling biases the reference labels toward the model's predictions, leading to inflated correctness scores; random noise or omissions affect variance or coverage but do not systematically inflate scores.\nCorrect Answer: C\n</output_format>",
    "question": "Which flaw in the oracle function G(\u00b7) can systematically inflate the expected alignment score between transformed outputs and ground truth?",
    "choices": {
      "A": "Occasional omission of ground truth labels for rare categories.",
      "B": "Random annotation noise introduced uniformly across examples.",
      "C": "Systematic mislabeling that aligns erroneous oracle outputs with the model's predictions.",
      "D": "Variability in oracle outputs due to non-deterministic decision processes."
    },
    "answer": "C",
    "explanation": "Systematic mislabeling biases the reference labels toward the model's predictions, leading to inflated correctness scores; random noise or omissions affect variance or coverage but do not systematically inflate scores.",
    "question_token_count": 24,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 2,
    "question_difficulty_score": 9,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 12
  },
  {
    "context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n",
    "topic": "Analyze the theoretical justification for employing a temporal cutoff in LLM evaluation to prevent data contamination and discuss its implications for model validity.",
    "response": "<document_analysis>\nThe provided context defines the concept of a temporal cutoff in LLM evaluation: using post-knowledge-cutoff data to build benchmarks that avoid models having seen the answers during training. It cites multiple examples (LiveBench, AntiLeak-Bench, etc.) illustrating continuous dataset updates to ensure new content. The core theoretical justification is preventing data contamination\u2014overlap between training and evaluation\u2014which directly impacts the validity of performance estimates by isolating true generalization from memorized responses. This yields a foundation to probe an expert\u2019s understanding of internal validity, memorization versus inference, and the implications of temporal dataset curation. Given these nuances, a high-difficulty (8/10) question can center on distinguishing the specific validity advantage that temporal cutoffs impart.\n\n</document_analysis>\n\n<output_format>\nQuestion: What fundamental theoretical benefit does employing a temporal cutoff for constructing evaluation benchmarks provide when assessing large language models?\nA: It guarantees that evaluation items were absent from the model\u2019s training data, thereby isolating and measuring true generalization rather than memorization.\nB: It continuously adapts the evaluation to real-world concept drift, thus primarily testing the model\u2019s adaptability to emerging topics.\nC: It minimizes distribution mismatch by selecting tasks from the same domains as pretraining but occurring at later time points.\nD: It ensures evaluation focuses on novel content only, thereby eliminating any reliance on both pretraining and fine-tuning knowledge.\nExplanation: Using a temporal cutoff excludes any overlap with the model\u2019s pretraining set, ensuring performance reflects genuine generalization instead of recall of memorized examples.\nCorrect Answer: A\n</output_format>",
    "question": "What fundamental theoretical benefit does employing a temporal cutoff for constructing evaluation benchmarks provide when assessing large language models?",
    "choices": {
      "A": "It guarantees that evaluation items were absent from the model\u2019s training data, thereby isolating and measuring true generalization rather than memorization.",
      "B": "It continuously adapts the evaluation to real-world concept drift, thus primarily testing the model\u2019s adaptability to emerging topics.",
      "C": "It minimizes distribution mismatch by selecting tasks from the same domains as pretraining but occurring at later time points.",
      "D": "It ensures evaluation focuses on novel content only, thereby eliminating any reliance on both pretraining and fine-tuning knowledge."
    },
    "answer": "A",
    "explanation": "Using a temporal cutoff excludes any overlap with the model\u2019s pretraining set, ensuring performance reflects genuine generalization instead of recall of memorized examples.",
    "question_token_count": 21,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 24
  },
  {
    "context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n",
    "topic": "Evaluate different scoring function designs S(\u00b7), contrasting exact-match metrics with semantic similarity and human-in-the-loop assessments.",
    "response": "<document_analysis>\nThe context defines static benchmarks as triplets D=(X,Y,S(\u00b7)), where S evaluates LLM outputs against ground-truth Y, and lists task categories (math, coding, reasoning, etc.). It implies that different S designs (exact-match vs. semantic similarity vs. human-in-the-loop) carry distinct trade-offs: exact-match is rigid, semantic similarity handles paraphrases but may introduce false positives, and human assessments capture nuance but at cost and lower consistency. This informs a question on the primary risk when substituting semantic\u2010similarity metrics for exact\u2010match scoring.\n</document_analysis>\n\n<output_format>\nQuestion: In static benchmarks formalized by D=(X,Y,S(\u00b7)), what is the primary risk when replacing an exact\u2010match scoring function with a semantic similarity\u2013based metric?\nA: Under-crediting correct outputs that use unusual synonyms  \nB: Over-crediting responses that are lexically or semantically misleading  \nC: Drastically increasing evaluation speed at the expense of ground-truth references  \nD: Ensuring perfect alignment with human judgments, thereby reducing objectivity  \nExplanation: Semantic similarity metrics can match outputs that appear close in embedding space yet contain critical factual or logical errors, leading to false positives.  \nCorrect Answer: B\n</output_format>",
    "question": "In static benchmarks formalized by D=(X,Y,S(\u00b7)), what is the primary risk when replacing an exact\u2010match scoring function with a semantic similarity\u2013based metric?",
    "choices": {
      "A": "Under-crediting correct outputs that use unusual synonyms",
      "B": "Over-crediting responses that are lexically or semantically misleading",
      "C": "Drastically increasing evaluation speed at the expense of ground-truth references",
      "D": "Ensuring perfect alignment with human judgments, thereby reducing objectivity"
    },
    "answer": "B",
    "explanation": "Semantic similarity metrics can match outputs that appear close in embedding space yet contain critical factual or logical errors, leading to false positives.",
    "question_token_count": 35,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 4,
    "avg_answer_token_count": 13
  },
  {
    "context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n",
    "topic": "Embedding\u2011based similarity approaches for robust identification of overlapping data samples",
    "response": "<document_analysis>\nThe context discusses post-hoc detection of dataset contamination, contrasting brittle exact n-gram matching (leading to false negatives) with more robust embedding-based similarity methods and improved mapping metrics. Embedding-based methods capture semantic overlap, while improved mapping metrics refine how text units are aligned. Key nuance: embeddings handle semantic variation, mapping metrics handle structural alignment. This distinction can be framed as a challenging question for experts.\n</document_analysis>\n\n<output_format>\nQuestion: In post\u2011hoc detection of overlapping training and test samples, embedding\u2011based similarity methods mitigate false negatives from semantic variation. What specific benefit do improved mapping metrics add beyond embedding approaches?\nA: They refine alignment of text units across multiple granularities (e.g., tokens, phrases) to detect partial overlaps.\nB: They enforce exact token-level matches to eliminate all false positives.\nC: They establish a centralized evaluation protocol for transparent benchmarking.\nD: They significantly reduce the computational cost of overlap detection on large corpora.\nExplanation: Improved mapping metrics focus on aligning text segments at varied granularities to catch overlaps that embeddings alone might miss, such as partial or hierarchical matches.\nCorrect Answer: A\n</output_format>",
    "question": "In post\u2011hoc detection of overlapping training and test samples, embedding\u2011based similarity methods mitigate false negatives from semantic variation. What specific benefit do improved mapping metrics add beyond embedding approaches?",
    "choices": {
      "A": "They refine alignment of text units across multiple granularities (e.g., tokens, phrases) to detect partial overlaps.",
      "B": "They enforce exact token-level matches to eliminate all false positives.",
      "C": "They establish a centralized evaluation protocol for transparent benchmarking.",
      "D": "They significantly reduce the computational cost of overlap detection on large corpora."
    },
    "answer": "A",
    "explanation": "Improved mapping metrics focus on aligning text segments at varied granularities to catch overlaps that embeddings alone might miss, such as partial or hierarchical matches.",
    "question_token_count": 37,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 15
  },
  {
    "context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n",
    "topic": "The role and effectiveness of human-in-the-loop feedback within multi-agent systems for ensuring benchmark quality, diversity, and scalability.",
    "response": "<document_analysis>\nThe context contrasts interactive evaluations with multi-agent evaluation frameworks such as Benchmark Self-Evolving and BENCHAGENTS, highlighting that BENCHAGENTS divides benchmark creation into planning, generation, verification, and evaluation phases. It specifically notes that human-in-the-loop feedback, integrated into this coordinated multi-agent pipeline, is key to producing benchmarks that are scalable, diverse, and high-quality. The critical insight is understanding how human feedback interplays with each specialized agent phase to ensure both variety and rigor. For a domain expert, the most challenging connection is identifying which phase most heavily relies on human oversight to simultaneously enforce content validity (quality) and representational breadth (diversity), and explaining why that phase is pivotal.\nDifficulty: 9\n</document_analysis>\n\n<output_format>\nQuestion: In a multi-agent benchmark creation framework that splits processes into planning, generation, verification, and evaluation, at which phase does human-in-the-loop feedback most critically intervene to ensure both the diversity and the high quality of the final benchmarks?\nA: Planning, where humans guide topic selection to broaden the scope.\nB: Generation, where humans refine prompts to steer content creativity.\nC: Verification, where humans filter and correct agent outputs to enforce validity and representational balance.\nD: Evaluation, where humans assess the finished items against performance criteria.\nExplanation: Human feedback during verification directly identifies and removes low-quality or biased items while ensuring coverage across topics, providing the primary mechanism for balancing benchmark diversity and quality. \nCorrect Answer: C\n</output_format>",
    "question": "In a multi-agent benchmark creation framework that splits processes into planning, generation, verification, and evaluation, at which phase does human-in-the-loop feedback most critically intervene to ensure both the diversity and the high quality of the final benchmarks?",
    "choices": {
      "A": "Planning, where humans guide topic selection to broaden the scope.",
      "B": "Generation, where humans refine prompts to steer content creativity.",
      "C": "Verification, where humans filter and correct agent outputs to enforce validity and representational balance.",
      "D": "Evaluation, where humans assess the finished items against performance criteria."
    },
    "answer": "C",
    "explanation": "Human feedback during verification directly identifies and removes low-quality or biased items while ensuring coverage across topics, providing the primary mechanism for balancing benchmark diversity and quality.",
    "question_token_count": 47,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 14
  },
  {
    "context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Dynamic benchmarking via timestamp-based continuous dataset updates to align evaluation data with model training periods.",
    "response": "<document_analysis>\nThe context introduces dynamic benchmarking as a response to contamination in static LLM evaluations, specifically highlighting a timestamp-based method that continuously updates benchmark datasets to exclude any content that predates the model\u2019s training period. The key underlying assumption is that content publication timestamps reliably indicate whether the model could have ingested that data during training. This assumption is subtle yet critical: any discrepancy between publication and crawler indexing/access could reintroduce contamination. No irrelevance detected; all content supports a technical question on timestamp-based dynamic benchmarking.\nDifficulty: 8/10\nQuestion planned to probe the expert\u2019s understanding of the foundational assumption and the main vulnerability of timestamp-driven dataset updates.\n</document_analysis>\n\n<output_format>\nQuestion: In dynamic benchmarking approaches that continuously update evaluation datasets based on content publication timestamps to align them with a model\u2019s training cut-off, what critical assumption underpins their effectiveness in preventing data contamination?\nA: The publication timestamp accurately indicates when web crawlers first had access to the content.\nB: Large language models strictly enforce exclusion of any data published before the cut-off.\nC: Content edits or backdating of timestamps never occur in online sources.\nD: All benchmark datasets uniformly include reliable timestamp metadata.\nExplanation: These methods assume that the publication timestamp reliably marks the earliest point at which the model\u2019s training crawlers could access the data; any inaccuracy there risks contamination.\nCorrect Answer: A\n</output_format>",
    "question": "In dynamic benchmarking approaches that continuously update evaluation datasets based on content publication timestamps to align them with a model\u2019s training cut-off, what critical assumption underpins their effectiveness in preventing data contamination?",
    "choices": {
      "A": "The publication timestamp accurately indicates when web crawlers first had access to the content.",
      "B": "Large language models strictly enforce exclusion of any data published before the cut-off.",
      "C": "Content edits or backdating of timestamps never occur in online sources.",
      "D": "All benchmark datasets uniformly include reliable timestamp metadata."
    },
    "answer": "A",
    "explanation": "These methods assume that the publication timestamp reliably marks the earliest point at which the model\u2019s training crawlers could access the data; any inaccuracy there risks contamination.",
    "question_token_count": 38,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 14
  },
  {
    "context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n",
    "topic": "Reflect on how scenario\u2011specific definitions of Cost(\u00b7) influence comparative evaluations of dynamic benchmarking approaches.",
    "response": "<document_analysis>\nThoughtful Content Examination:\nThe context defines scalability for dynamic benchmarking as the expected ratio of generated dataset size to original size normalized by a cost function, Cost(\u00b7). It emphasizes that Cost(\u00b7) can represent various scenario\u2011specific measures\u2014monetary cost, time, manual effort\u2014and that changing this definition alters the per\u2011unit\u2011cost output measure.\n\nConcept Exploration:\nIf Cost(\u00b7) shifts from one dimension (e.g., monetary) to another (e.g., manual effort), methods optimized for different resource trade\u2011offs will be evaluated differently. For instance, an automation\u2011heavy transformation may incur high compute expenses but low human effort, whereas a manual labeling approach is low in monetary fees but high in labor hours. Changing the cost lens inverts their comparative scalability.\n\nStrategic Complexity Calibration:\nDifficulty rated 8: requires synthesis of metric definition, understanding of how cost denominator interacts with numerator, and insight into comparative method evaluation under varying cost scenarios.\n\nIntentional Question Planning:\nThe question contrasts two representative approaches (automation vs manual) under different cost definitions to probe deep understanding of how choice of Cost(\u00b7) affects benchmarking comparisons.\n</document_analysis>\n\n<output_format>\nQuestion: Given two dynamic benchmarking methods that produce equal dataset expansions\u2014one automation\u2011heavy with high compute expenses but minimal human intervention, and the other reliant on manual labeling with low compute costs but extensive human effort\u2014how does defining Cost(\u00b7) as manual\u2011effort hours rather than monetary expense affect their relative scalability scores?\nA: The automation\u2011heavy method\u2019s scalability score increases relative to the manual method.\nB: Both methods retain the same scalability ranking because dataset sizes are equal.\nC: The manual\u2011labeling method\u2019s scalability score increases relative to the automation method.\nD: Neither method\u2019s scalability is defined when Cost only measures a single resource.\nExplanation: Using manual\u2011effort hours as Cost favors low\u2011effort automation, boosting its per\u2011unit\u2011cost output and thus its scalability score over manual labeling.\nCorrect Answer: A\n</output_format>",
    "question": "Given two dynamic benchmarking methods that produce equal dataset expansions\u2014one automation\u2011heavy with high compute expenses but minimal human intervention, and the other reliant on manual labeling with low compute costs but extensive human effort\u2014how does defining Cost(\u00b7) as manual\u2011effort hours rather than monetary expense affect their relative scalability scores?",
    "choices": {
      "A": "The automation\u2011heavy method\u2019s scalability score increases relative to the manual method.",
      "B": "Both methods retain the same scalability ranking because dataset sizes are equal.",
      "C": "The manual\u2011labeling method\u2019s scalability score increases relative to the automation method.",
      "D": "Neither method\u2019s scalability is defined when Cost only measures a single resource."
    },
    "answer": "A",
    "explanation": "Using manual\u2011effort hours as Cost favors low\u2011effort automation, boosting its per\u2011unit\u2011cost output and thus its scalability score over manual labeling.",
    "question_token_count": 63,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 15
  },
  {
    "context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n",
    "topic": "Critically assess the interpretation of scalability as the proportion of data generated per unit cost.",
    "response": "<document_analysis>\nThe context defines scalability in dynamic benchmarking as the expected size of transformed datasets relative to the original dataset, normalized by cost. It highlights a common interpretation\u2014proportion of data generated per unit cost\u2014but this simplification overlooks deeper concerns: the metric treats all data points equally regardless of their novelty or utility, ignores dataset diversity and distributional validity, and may misrepresent fixed versus variable costs. \nBy focusing solely on volume per cost, the interpretation fails to capture qualitative aspects (like statistical robustness and transformation complexity), which are critical for expert benchmarking. \nDifficulty rating: 8/10 (requires nuanced understanding of benchmarking metrics and their limitations).\n</document_analysis>\n\n<output_format>\nQuestion: Which of the following most precisely describes a critical limitation of interpreting scalability solely as the proportion of data generated per unit cost?\nA: It conflates incremental throughput with overall throughput, neglecting that larger original datasets may disproportionately inflate the metric without reflecting true generative efficiency.\nB: It treats all generated data as equally valuable, failing to account for statistical relevance, diversity, or diminishing informational returns of additional data points.\nC: It assumes cost scales linearly with dataset size, overlooking fixed setup costs that can dominate in small-scale transformations and distort the metric.\nD: It aggregates cost and output across heterogeneous transformations, masking worst-case scenarios and variability in individual transformation efficiency.\nExplanation: Option B identifies the core flaw: the metric measures quantity per cost but ignores data utility, diversity, and diminishing informational value.\nCorrect Answer: B\n</output_format>",
    "question": "Which of the following most precisely describes a critical limitation of interpreting scalability solely as the proportion of data generated per unit cost?",
    "choices": {
      "A": "It conflates incremental throughput with overall throughput, neglecting that larger original datasets may disproportionately inflate the metric without reflecting true generative efficiency.",
      "B": "It treats all generated data as equally valuable, failing to account for statistical relevance, diversity, or diminishing informational returns of additional data points.",
      "C": "It assumes cost scales linearly with dataset size, overlooking fixed setup costs that can dominate in small-scale transformations and distort the metric.",
      "D": "It aggregates cost and output across heterogeneous transformations, masking worst-case scenarios and variability in individual transformation efficiency."
    },
    "answer": "B",
    "explanation": "Option B identifies the core flaw: the metric measures quantity per cost but ignores data utility, diversity, and diminishing informational value.",
    "question_token_count": 25,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 25
  },
  {
    "context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Propose potential strategies for implementing specific transformation functions T_t to generate dynamic benchmarks and evaluate their effects on transparency and faithfulness.",
    "response": "<document_analysis>\nThe context outlines the concept of dynamic benchmarking: defining a static dataset D and transformation functions T\u209c that generate time\u2010varying evaluation datasets D\u209c = T\u209c(D). The goal is to prevent data contamination while ensuring transparent and faithful assessments of LLMs. Key themes include the invertibility of transformations (for transparency), semantic preservation (for faithfulness), and resistance to memorization. To craft a challenging question, we must probe understanding of how specific transformation strategies trade off transparency and faithfulness, requiring knowledge of invertible mappings, paraphrasing limitations, translation artifacts, and lexical perturbations. Difficulty is calibrated at 8/10 to engage domain experts in critical evaluation of transformation design.\n</document_analysis>\n\n<output_format>\nQuestion: Which transformation function strategy for dynamic benchmarking best ensures exact traceability (highest transparency) of every data example while also preserving semantic integrity (faithfulness) and hindering simple memorization of the original dataset?\nA: Token\u2011level invertible permutation using a secret shuffle key.\nB: Stochastic paraphrasing through a large language model API.\nC: Round\u2011trip translation across multiple intermediate languages.\nD: Random synonym substitution guided solely by a lexical database.\nExplanation: An invertible token\u2011level permutation allows each example to be exactly reversed (maximal transparency), retains the same tokens in a rearranged order (perfect semantic faithfulness), and obscures the original instance to prevent direct memorization.\nCorrect Answer: A\n</output_format>",
    "question": "Which transformation function strategy for dynamic benchmarking best ensures exact traceability (highest transparency) of every data example while also preserving semantic integrity (faithfulness) and hindering simple memorization of the original dataset?",
    "choices": {
      "A": "Token\u2011level invertible permutation using a secret shuffle key.",
      "B": "Stochastic paraphrasing through a large language model API.",
      "C": "Round\u2011trip translation across multiple intermediate languages.",
      "D": "Random synonym substitution guided solely by a lexical database."
    },
    "answer": "A",
    "explanation": "An invertible token\u2011level permutation allows each example to be exactly reversed (maximal transparency), retains the same tokens in a rearranged order (perfect semantic faithfulness), and obscures the original instance to prevent direct memorization.",
    "question_token_count": 41,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 9,
    "question_groundedness_score": 6,
    "avg_answer_token_count": 11
  },
  {
    "context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n",
    "topic": "Evaluate the significance of internal diversity in preventing redundancy across multiple augmented datasets.",
    "response": "<document_analysis>\nThe context defines two complementary diversity metrics for transformed datasets: external diversity, which measures the average divergence of each augmented dataset from the seed dataset, and internal diversity, which measures the average pairwise divergence among all augmented datasets themselves. A high internal diversity indicates that different augmentation trials produce distinct outputs, directly controlling for redundancy that external diversity alone cannot detect. This distinction is critical for designing augmentation pipelines that yield complementary, non-redundant data. Difficulty level: 8/10.\n</document_analysis>\n\n<output_format>\nQuestion: Why is internal diversity, defined as the expectation of \u0398(D_i, D_j) over all i\u2260j, essential for preventing redundant transformations across multiple augmented datasets, and how does it complement external diversity?\nA: By averaging pairwise diversity scores across all dataset pairs, it directly quantifies overlap among augmentations that external diversity, focused solely on deviations from the seed dataset, would overlook.\nB: By computing only the maximum diversity score among pairs, it ensures at least one highly distinct augmentation regardless of overall redundancy.\nC: By measuring global variance of feature embeddings within each dataset, it captures intra-dataset complexity rather than inter-dataset redundancy.\nD: By comparing each augmented dataset to a validation set, it prioritizes generalization over inter-augmentation distinctiveness.\nExplanation: Internal diversity averages pairwise distances between augmented sets to detect overlap among them, addressing redundancy that external diversity (which compares only to the original data) cannot identify.\nCorrect Answer: A\n</output_format>",
    "question": "Why is internal diversity, defined as the expectation of \u0398(D_i, D_j) over all i\u2260j, essential for preventing redundant transformations across multiple augmented datasets, and how does it complement external diversity?",
    "choices": {
      "A": "By averaging pairwise diversity scores across all dataset pairs, it directly quantifies overlap among augmentations that external diversity, focused solely on deviations from the seed dataset, would overlook.",
      "B": "By computing only the maximum diversity score among pairs, it ensures at least one highly distinct augmentation regardless of overall redundancy.",
      "C": "By measuring global variance of feature embeddings within each dataset, it captures intra-dataset complexity rather than inter-dataset redundancy.",
      "D": "By comparing each augmented dataset to a validation set, it prioritizes generalization over inter-augmentation distinctiveness."
    },
    "answer": "A",
    "explanation": "Internal diversity averages pairwise distances between augmented sets to detect overlap among them, addressing redundancy that external diversity (which compares only to the original data) cannot identify.",
    "question_token_count": 42,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 26
  },
  {
    "context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n",
    "topic": "Discuss the methodological criteria for detecting and quantifying contamination in LLM evaluation datasets.",
    "response": "<document_analysis>\nThe given context highlights syntactic contamination\u2014test data derived by rephrasing training examples\u2014and argues for treating such transformations as contamination because they preserve surface patterns that models can memorize, conflating recall with reasoning. This nuance is critical for ensuring benchmark validity and requires careful methodological consideration. The context does not provide specific detection algorithms but emphasizes the importance of excluding any dataset overlap at the syntactic level. A challenging question would probe why syntactic rephrasings must be classified as contamination to avoid overestimating model reasoning capabilities.\n</document_analysis>\n\n<output_format>\nQuestion: In LLM benchmark design, why is it crucial to classify syntactically rephrased training examples as contamination rather than legitimate test cases?\nA: Because they maintain surface-level patterns, allowing models to exploit memorized cues rather than perform genuine reasoning.\nB: Because they systematically increase task complexity, making benchmarks overly challenging.\nC: Because they alter semantic content, leading models to face entirely new concepts.\nD: Because they demand specialized tokenization, skewing performance metrics.\nExplanation: Syntactic rephrasings preserve surface patterns that enable models to rely on memorized cues, conflating recall with reasoning and invalidating true capability assessment.\nCorrect Answer: A\n</output_format>",
    "question": "In LLM benchmark design, why is it crucial to classify syntactically rephrased training examples as contamination rather than legitimate test cases?",
    "choices": {
      "A": "Because they maintain surface-level patterns, allowing models to exploit memorized cues rather than perform genuine reasoning.",
      "B": "Because they systematically increase task complexity, making benchmarks overly challenging.",
      "C": "Because they alter semantic content, leading models to face entirely new concepts.",
      "D": "Because they demand specialized tokenization, skewing performance metrics."
    },
    "answer": "A",
    "explanation": "Syntactic rephrasings preserve surface patterns that enable models to rely on memorized cues, conflating recall with reasoning and invalidating true capability assessment.",
    "question_token_count": 29,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 15
  },
  {
    "context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n",
    "topic": "Mathematical formulation of exact contamination\u2014identifying when a data point d appears in both \ud835\udc9f_train and \ud835\udc9f_test",
    "response": "<document_analysis>\nThe context defines two types of data contamination: exact contamination and syntactic contamination. Exact contamination is formally any data point d that appears verbatim in both the training set \ud835\udc9f_train and test set \ud835\udc9f_test. This can be expressed in set-theoretic or logical notation. To challenge domain experts, the question will probe the precise logical quantifier formulation distinguishing \u201cat least one shared element\u201d from stronger or weaker conditions (e.g., \u2200 vs. \u2203, conjunction vs. implication). This tests deep understanding of formal definitions and common logical pitfalls.\nDifficulty: 8/10\n</document_analysis>\n\n<output_format>\nQuestion: Which logical expression precisely captures the condition for exact contamination, meaning that there is at least one data point shared verbatim between the training set D_train and the test set D_test?\nA: \u2203 d (d \u2208 D_train \u2227 d \u2208 D_test)\nB: \u2200 d (d \u2208 D_train \u2192 d \u2208 D_test)\nC: \u2200 d (d \u2208 D_test \u2192 d \u2208 D_train)\nD: \u2203 d (d \u2208 D_train \u222a D_test)\nExplanation: Exact contamination requires at least one element that belongs simultaneously to D_train and D_test, which is expressed by \u2203 d (d \u2208 D_train \u2227 d \u2208 D_test).\nCorrect Answer: A\n</output_format>",
    "question": "Which logical expression precisely captures the condition for exact contamination, meaning that there is at least one data point shared verbatim between the training set D_train and the test set D_test?",
    "choices": {
      "A": "\u2203 d (d \u2208 D_train \u2227 d \u2208 D_test)",
      "B": "\u2200 d (d \u2208 D_train \u2192 d \u2208 D_test)",
      "C": "\u2200 d (d \u2208 D_test \u2192 d \u2208 D_train)",
      "D": "\u2203 d (d \u2208 D_train \u222a D_test)"
    },
    "answer": "A",
    "explanation": "Exact contamination requires at least one element that belongs simultaneously to D_train and D_test, which is expressed by \u2203 d (d \u2208 D_train \u2227 d \u2208 D_test).",
    "question_token_count": 37,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 8,
    "avg_answer_token_count": 14
  },
  {
    "context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n",
    "topic": "Comparative interpretation of Collision Rate versus Repeat Trials in assessing benchmark robustness.",
    "response": "<document_analysis>\nThe context introduces two metrics for evaluating dynamic benchmarks: Collision Rate (percentage overlap between two independently transformed datasets, indicating contamination risk) and Repeat Trials (expected number of trials to regenerate an existing transformation, reflecting transformation diversity). The key nuance is that Collision Rate is a pairwise overlap measure, while Repeat Trials captures global diversity akin to expected waiting time before sampling a previous item. The question should probe the difference in what each metric fundamentally quantifies\u2014specifically, which better estimates the size of the transformation space\u2014and why, requiring understanding of concepts like expected waiting time in stochastic sampling.\nDifficulty rating: 7/10\n</document_analysis>\n\n<output_format>\nQuestion: Which metric more directly quantifies the size of a dynamic benchmark\u2019s transformation space and why?\nA: Collision Rate, because it inversely reflects transformation diversity.\nB: Collision Rate, because it measures overlap between any two trials.\nC: Repeat Trials, because it is analogous to the expected number of draws before re\u2011sampling a previously seen transformation.\nD: Repeat Trials, because it reports the average pairwise overlap percentage across trials.\nExplanation: Repeat Trials measures the expected number of transformations needed to reproduce an existing dataset, directly relating to the total number of unique variants (like waiting time in a coupon collector scenario).\nCorrect Answer: C\n</output_format>",
    "question": "Which metric more directly quantifies the size of a dynamic benchmark\u2019s transformation space and why?",
    "choices": {
      "A": "Collision Rate, because it inversely reflects transformation diversity.",
      "B": "Collision Rate, because it measures overlap between any two trials.",
      "C": "Repeat Trials, because it is analogous to the expected number of draws before re\u2011sampling a previously seen transformation.",
      "D": "Repeat Trials, because it reports the average pairwise overlap percentage across trials."
    },
    "answer": "C",
    "explanation": "Repeat Trials measures the expected number of transformations needed to reproduce an existing dataset, directly relating to the total number of unique variants (like waiting time in a coupon collector scenario).",
    "question_token_count": 19,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 2,
    "question_difficulty_score": 8,
    "question_groundedness_score": 4,
    "avg_answer_token_count": 15
  },
  {
    "context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n",
    "topic": "Emerging research directions and open challenges in mitigating data contamination through dynamic benchmarking.",
    "response": "<document_analysis>\nThe context is an abstract discussing the evolution from static to dynamic benchmarking to mitigate data contamination in LLMs. Key points include: static benchmarks have inherent limitations (e.g., staleness and fixed data snapshots); a critical gap is the absence of standardized criteria for evaluating dynamic benchmarks; and the authors propose optimal design principles to address this gap. This content is coherent and educationally valuable for probing expert understanding of why dynamic benchmarking emerged and what challenge remains. A high\u2010difficulty question will require synthesizing both the motivation for dynamic benchmarking and its unresolved obstacle.\n</document_analysis>\n\n<output_format>\nQuestion: Which critical insight underlies the shift from static to dynamic benchmarking for large language models, and what key challenge does it reveal remains unresolved?\nA: It remedies staleness of static benchmarks but reveals a lack of standardized criteria for evaluating dynamic benchmarks.\nB: It reduces computational overhead but reveals insufficient data privacy mechanisms.\nC: It enhances benchmark interpretability but reveals a deficit in human annotation quality.\nD: It increases data diversity but reveals an absence of open\u2010source repository maintenance.\nExplanation: Dynamic benchmarking overcomes the temporal staleness and fixed\u2010data limitations of static benchmarks, yet currently lacks standardized evaluation criteria.\nCorrect Answer: A\n</output_format>",
    "question": "Which critical insight underlies the shift from static to dynamic benchmarking for large language models, and what key challenge does it reveal remains unresolved?",
    "choices": {
      "A": "It remedies staleness of static benchmarks but reveals a lack of standardized criteria for evaluating dynamic benchmarks.",
      "B": "It reduces computational overhead but reveals insufficient data privacy mechanisms.",
      "C": "It enhances benchmark interpretability but reveals a deficit in human annotation quality.",
      "D": "It increases data diversity but reveals an absence of open\u2010source repository maintenance."
    },
    "answer": "A",
    "explanation": "Dynamic benchmarking overcomes the temporal staleness and fixed\u2010data limitations of static benchmarks, yet currently lacks standardized evaluation criteria.",
    "question_token_count": 28,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 16
  },
  {
    "context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n",
    "topic": "Describe the GSM-Symbolic template-based approach to generating dynamic math benchmarks and analyze its strengths and weaknesses.",
    "response": "<document_analysis>\nThe context describes the GSM-Symbolic template-based generation method: it uses query templates with placeholder variables to dynamically generate diverse math problems. Its main strength is automated diversity and reduced manual effort in creating unique benchmarks; its weakness stems from reliance on a finite, human-designed set of templates, which can limit problem novelty and depth and may require oversight to prevent invalid instances. Other methods mentioned do not share this exact trade\u2011off. I will craft a question that asks the test-taker to identify the correct pairing of GSM-Symbolic\u2019s core advantage and primary limitation, with plausible distractors reflecting other generation approaches or misconceptions.\n</document_analysis>\n\n<output_format>\nQuestion: Which pairing accurately captures the primary advantage and principal drawback of GSM-Symbolic\u2019s template-based generation for dynamic math benchmarks?\nA: Advantage: rapid creation of diverse math problems via query templates with placeholders; Drawback: diversity confined by the finite scope of hand\u2011crafted templates\nB: Advantage: fully eliminates any human involvement in benchmark updates; Drawback: high risk of re\u2011using competition problems leading to data contamination\nC: Advantage: ensures correctness of all generated problems through built\u2011in verification; Drawback: restricted exclusively to multiple\u2011choice question formats\nD: Advantage: guarantees zero instance collisions; Drawback: mandates extensive manual rule engineering to define each problem structure\nExplanation: GSM-Symbolic automates diverse instance generation via template variables (a strength) but is limited by the range of its predefined templates (a weakness).\nCorrect Answer: A\n</output_format>",
    "question": "Which pairing accurately captures the primary advantage and principal drawback of GSM-Symbolic\u2019s template-based generation for dynamic math benchmarks?",
    "choices": {
      "A": "Advantage: rapid creation of diverse math problems via query templates with placeholders; Drawback: diversity confined by the finite scope of hand\u2011crafted templates",
      "B": "Advantage: fully eliminates any human involvement in benchmark updates; Drawback: high risk of re\u2011using competition problems leading to data contamination",
      "C": "Advantage: ensures correctness of all generated problems through built\u2011in verification; Drawback: restricted exclusively to multiple\u2011choice question formats",
      "D": "Advantage: guarantees zero instance collisions; Drawback: mandates extensive manual rule engineering to define each problem structure"
    },
    "answer": "A",
    "explanation": "GSM-Symbolic automates diverse instance generation via template variables (a strength) but is limited by the range of its predefined templates (a weakness).",
    "question_token_count": 25,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 5,
    "avg_answer_token_count": 27
  },
  {
    "context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n",
    "topic": "Assess how the introduction of live data streams (e.g., LiveAoPSBench) impacts reproducibility and longitudinal comparability of model evaluations.",
    "response": "<document_analysis>\nThe context outlines how live benchmarks (e.g., LiveAoPSBench) continuously add new items to prevent data contamination beyond a model\u2019s knowledge cutoff. While this ensures up-to-date evaluations, it also dynamically changes the test set over time. This dynamic nature can undermine reproducibility\u2014since the exact set of items at a given time may not be recoverable without versioning\u2014and hampers longitudinal comparability, as performance measured on different, evolving question pools cannot be directly compared. The core trade\u2011off is between contamination\u2011free, current evaluation and the ability to replicate and track performance trends without rigorous dataset archiving/versioning.\n</document_analysis>\n\n<output_format>\nQuestion: Which of the following best describes the primary methodological challenge introduced by live streaming benchmarks such as LiveAoPSBench with respect to reproducibility and longitudinal comparability of model assessments?\nA: They enhance reproducibility by sourcing from a single continuously updated repository, but make longitudinal comparisons difficult due to shifts in question distribution.\nB: They require explicit version control or archiving of the continually evolving problem set to ensure that results can be replicated and performance trends meaningfully compared over time.\nC: They do not affect reproducibility or longitudinal comparability since every evaluation draws questions from the same API endpoint, regardless of updates.\nD: They prevent data contamination yet still guarantee seamless reproducibility and comparability by retaining all previous problems in public archives.\nExplanation: Continuous updates without proper versioning or archival mean the exact test set at any point cannot be recovered, and evolving questions prevent straightforward comparison of model performance across time.\nCorrect Answer: B\n</output_format>",
    "question": "Which of the following best describes the primary methodological challenge introduced by live streaming benchmarks such as LiveAoPSBench with respect to reproducibility and longitudinal comparability of model assessments?",
    "choices": {
      "A": "They enhance reproducibility by sourcing from a single continuously updated repository, but make longitudinal comparisons difficult due to shifts in question distribution.",
      "B": "They require explicit version control or archiving of the continually evolving problem set to ensure that results can be replicated and performance trends meaningfully compared over time.",
      "C": "They do not affect reproducibility or longitudinal comparability since every evaluation draws questions from the same API endpoint, regardless of updates.",
      "D": "They prevent data contamination yet still guarantee seamless reproducibility and comparability by retaining all previous problems in public archives."
    },
    "answer": "B",
    "explanation": "Continuous updates without proper versioning or archival mean the exact test set at any point cannot be recovered, and evolving questions prevent straightforward comparison of model performance across time.",
    "question_token_count": 34,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 25
  },
  {
    "context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n",
    "topic": "The rule-based conversion techniques used in DyVal to translate graph structures into natural language descriptions for LLM evaluation.",
    "response": "<document_analysis>\nThe context describes DyVal\u2019s methodology: constructing directed acyclic graphs with controlled node/edge counts to vary difficulty, then using a rule-based conversion to turn each DAG into a natural language description that the LLM must interpret to compute the root value. Key nuance: the conversion must systematically and unambiguously reflect each graph element (nodes and directed edges) so that difficulty scales linearly with graph size and the LLM can reconstruct the underlying structure from text. This insight underpins a question about how the rule-based conversion ties description complexity to graph complexity.\n</document_analysis>\n\n<output_format>\nQuestion: In DyVal\u2019s evaluation framework, how does the rule-based conversion of a DAG into natural language ensure that the complexity of the text prompt scales directly with the number of graph edges?\nA: By generating an explicit relational clause for each directed edge, so every edge corresponds to a distinct sentence segment.\nB: By summarizing groups of edges into higher-level narrative paragraphs to maintain a constant text length.\nC: By selectively omitting redundant edges in the description when the graph exceeds a threshold size.\nD: By encoding only the root-to-leaf paths and ignoring side branches to simplify the prompt.\nExplanation: The rule-based conversion creates a separate descriptive clause for each directed edge, thus making the prompt length and complexity directly proportional to the edge count.\nCorrect Answer: A\n</output_format>",
    "question": "In DyVal\u2019s evaluation framework, how does the rule-based conversion of a DAG into natural language ensure that the complexity of the text prompt scales directly with the number of graph edges?",
    "choices": {
      "A": "By generating an explicit relational clause for each directed edge, so every edge corresponds to a distinct sentence segment.",
      "B": "By summarizing groups of edges into higher-level narrative paragraphs to maintain a constant text length.",
      "C": "By selectively omitting redundant edges in the description when the graph exceeds a threshold size.",
      "D": "By encoding only the root-to-leaf paths and ignoring side branches to simplify the prompt."
    },
    "answer": "A",
    "explanation": "The rule-based conversion creates a separate descriptive clause for each directed edge, thus making the prompt length and complexity directly proportional to the edge count.",
    "question_token_count": 37,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 4,
    "avg_answer_token_count": 19
  },
  {
    "context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n",
    "topic": "Analyze the importance of input prompt selection (X) in creating a representative seed dataset for static benchmarking.",
    "response": "<document_analysis>\nThe context outlines static benchmarking as a triplet D = (X, Y, S(.)), where X is the set of input prompts. It emphasizes that benchmarks cover diverse tasks and that X anchors the evaluation by defining which problems the model is tested on. No irrelevant or boilerplate text is present. A question probing the role of X in ensuring comprehensive assessment is appropriate.\n</document_analysis>\n\n<output_format>\nQuestion: Within the static benchmarking framework D = (X, Y, S(\u00b7)), why is the careful selection of the input prompt set X most critical for creating a representative seed dataset?\nA: It directly determines the diversity of tasks and model capabilities that the benchmark will exercise.\nB: It fixes the scoring function\u2019s sensitivity to minor output variations.\nC: It establishes the ground-truth outputs Y before any evaluation.\nD: It dictates the architecture and training regimen of the evaluated model.\nExplanation: The choice of X defines which tasks and difficulty levels the model faces, ensuring the benchmark covers a broad and representative range of capabilities.\nCorrect Answer: A\n</output_format>",
    "question": "Within the static benchmarking framework D = (X, Y, S(\u00b7)), why is the careful selection of the input prompt set X most critical for creating a representative seed dataset?",
    "choices": {
      "A": "It directly determines the diversity of tasks and model capabilities that the benchmark will exercise.",
      "B": "It fixes the scoring function\u2019s sensitivity to minor output variations.",
      "C": "It establishes the ground-truth outputs Y before any evaluation.",
      "D": "It dictates the architecture and training regimen of the evaluated model."
    },
    "answer": "A",
    "explanation": "The choice of X defines which tasks and difficulty levels the model faces, ensuring the benchmark covers a broad and representative range of capabilities.",
    "question_token_count": 36,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 14
  },
  {
    "context": "###### Safety\nSafety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content. Datasets such as RealToxicityPrompts\u00a0(Gehman et\u00a0al., 2020) and ToxiGen\u00a0(Hartvigsen et\u00a0al., 2022) assess resilience against producing harmful outputs.\n\nBy providing a controlled environment to measure these aspects, safety benchmarks play a critical role in guiding the development of models that are not only powerful outputs but also responsible and trustworthy for real-world applications.\n\n\n###### Language\nLanguage benchmarks assess the LLMs\u2019 proficiency in specific languages.\n\nGLUE\u00a0(Wang, 2018) and SuperGLUE\u00a0(Wang et\u00a0al., 2019) cover tasks from sentiment analysis to language inference, while CLUE\u00a0(Xu et\u00a0al., 2020) targets Chinese language.\n\nTypo-fixing\u00a0(Suzgun et\u00a0al., 2022) is also widely used.\n\n\n###### Reading Comprehension\nReading comprehension tasks test a model\u2019s ability to extract and infer information from text. Benchmarks like SQuAD\u00a0(Rajpurkar et\u00a0al., 2018), QuAC\u00a0(Choi et\u00a0al., 2018), and BoolQ\u00a0(Clark et\u00a0al., 2019) challenge models to understand passages and draw logical conclusions.\n",
    "topic": "Design objectives and task coverage in GLUE versus SuperGLUE benchmarks",
    "response": "<document_analysis>\nThe context categorizes benchmarks into safety, language, and reading comprehension. GLUE and SuperGLUE are placed under language benchmarks, covering sentiment analysis to language inference. Safety benchmarks (e.g., RealToxicityPrompts, ToxiGen) evaluate non-toxic output generation, while CLUE specifically targets Chinese. Reading comprehension benchmarks (SQuAD, QuAC, BoolQ) assess passage understanding. This mapping allows creating a question linking a GLUE-strong model to further evaluations in safety and Chinese language proficiency.\nDifficulty rating: 7/10 (requires cross-referencing benchmark categories and specific dataset purposes)\n</document_analysis>\n\n<output_format>\nQuestion: A language model achieves high scores on GLUE and SuperGLUE. To specifically test its resistance to generating harmful content and separately assess its Chinese language capabilities, which pair of benchmarks should be applied?\nA: QuAC and CLUE\nB: RealToxicityPrompts and CLUE\nC: ToxiGen and BoolQ\nD: Typo-fixing and SQuAD\nExplanation: RealToxicityPrompts measures safety against harmful outputs, and CLUE targets Chinese language proficiency.\nCorrect Answer: B\n</output_format>",
    "question": "A language model achieves high scores on GLUE and SuperGLUE. To specifically test its resistance to generating harmful content and separately assess its Chinese language capabilities, which pair of benchmarks should be applied?",
    "choices": {
      "A": "QuAC and CLUE",
      "B": "RealToxicityPrompts and CLUE",
      "C": "ToxiGen and BoolQ",
      "D": "Typo-fixing and SQuAD"
    },
    "answer": "B",
    "explanation": "RealToxicityPrompts measures safety against harmful outputs, and CLUE targets Chinese language proficiency.",
    "question_token_count": 40,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 8
  },
  {
    "context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n",
    "topic": "Comparative analysis of encryption versus label protection strategies, focusing on their respective strengths, limitations, and deployment trade\u2010offs",
    "response": "<document_analysis>\nThe context contrasts encryption methods (public key encryption with \u201cNo Derivatives\u201d licensing, confidential computing, secure multi-party computation) and their reliance on key management and computational overhead, against label protection approaches (withholding test answers) used in GLUE, SuperGLUE, and HumanEval. The nuanced technical theme is that decontamination (pattern\u2010based filtering) can be bypassed by paraphrasing, whereas encryption renders raw test data inaccessible\u2014but encryption itself is vulnerable if keys are compromised. A high\u2010difficulty question will probe this subtle trade\u2010off, requiring understanding of how decontamination fails, how encryption succeeds, and the residual risk tied to key management.\nDifficulty rating: 8/10\n</document_analysis>\n\n<output_format>\nQuestion: How do minor text variations undermine decontamination methods, and what fundamental vulnerability remains in encryption\u2010based test data protection?\nA: Decontamination relies on pattern matching which fails on paraphrases; encryption denies plaintext but depends on robust key management.\nB: Decontamination is semantically blind making paraphrasing effective; encryption\u2019s confidentiality hinges on public key secrecy.\nC: Decontamination falters at inference attacks; encryption cannot enforce license restrictions.\nD: Decontamination fails with token\u2011level obfuscation; encryption protects only labels.\nExplanation: Pattern\u2010based decontamination misses paraphrased samples, while encryption prevents any plaintext access but still requires secure handling of private keys.\nCorrect Answer: A\n</output_format>",
    "question": "How do minor text variations undermine decontamination methods, and what fundamental vulnerability remains in encryption\u2010based test data protection?",
    "choices": {
      "A": "Decontamination relies on pattern matching which fails on paraphrases; encryption denies plaintext but depends on robust key management.",
      "B": "Decontamination is semantically blind making paraphrasing effective; encryption\u2019s confidentiality hinges on public key secrecy.",
      "C": "Decontamination falters at inference attacks; encryption cannot enforce license restrictions.",
      "D": "Decontamination fails with token\u2011level obfuscation; encryption protects only labels."
    },
    "answer": "A",
    "explanation": "Pattern\u2010based decontamination misses paraphrased samples, while encryption prevents any plaintext access but still requires secure handling of private keys.",
    "question_token_count": 25,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 4,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 20
  },
  {
    "context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n",
    "topic": "Formal characterization of syntactic contamination via the transformation function \u2131_syntactic",
    "response": "<document_analysis>\nThe context defines exact versus syntactic data contamination, formalizing syntactic contamination via a transformation function \u2131_syntactic encompassing operations that preserve lexical meaning\u2014punctuation normalization, whitespace modification, synonym substitution, morphological variations, and syntactic paraphrasing. A high\u2010difficulty question will test subtle understanding of what preserves versus what alters lexical meaning within \u2131_syntactic. I propose an MCQ distinguishing hypernym substitution (which alters lexical specificity) from true syntactic operations.\nDifficulty: 7/10\n</document_analysis>\n\n<output_format>\nQuestion: Which transformation falls outside the formally defined \u2131_syntactic for syntactic contamination, given that \u2131_syntactic includes operations like punctuation/whitespace normalization, synonym substitution, morphological variation, and syntactic paraphrasing, all preserving lexical meaning?\nA: Substituting a word with its synonym of equivalent meaning.\nB: Reordering independent adverbial phrases within a sentence.\nC: Replacing a domain-specific term with its more general category (a hypernym).\nD: Uniformly normalizing punctuation marks and extra spaces.\nExplanation: Hypernym substitution (C) changes the lexical specificity and thus alters lexical meaning, whereas the other options are exactly the types of transformations allowed by \u2131_syntactic.\nCorrect Answer: C\n</output_format>",
    "question": "Which transformation falls outside the formally defined \u2131_syntactic for syntactic contamination, given that \u2131_syntactic includes operations like punctuation/whitespace normalization, synonym substitution, morphological variation, and syntactic paraphrasing, all preserving lexical meaning?",
    "choices": {
      "A": "Substituting a word with its synonym of equivalent meaning.",
      "B": "Reordering independent adverbial phrases within a sentence.",
      "C": "Replacing a domain-specific term with its more general category (a hypernym).",
      "D": "Uniformly normalizing punctuation marks and extra spaces."
    },
    "answer": "C",
    "explanation": "Hypernym substitution (C) changes the lexical specificity and thus alters lexical meaning, whereas the other options are exactly the types of transformations allowed by \u2131_syntactic.",
    "question_token_count": 50,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 12
  },
  {
    "context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n",
    "topic": "Design principles and evaluation criteria of temporal cutoff\u2013based dynamic benchmarks for newly released information.",
    "response": "<document_analysis>\n1. Identified central concepts:\n   - Interpretability challenge in dynamic benchmarks: rule-based vs LLM-assisted transformations, need for explainability or human-in-the-loop.\n   - Four dynamic benchmark construction types: temporal cutoff, rule-based, LLM-based, hybrid.\n2. Explored nuance:\n   - Trade\u2011offs between generation of novel evaluation data and interpretability.\n   - Temporal cutoff avoids transformation but only uses raw new data; rule\u2011based is interpretable but limited; LLM-based is powerful but opaque; hybrid aims to combine strengths.\n3. Difficulty calibration: Rated 8/10, requiring integration of interpretability principle with benchmark categorization.\n4. Intent: Prompt deep reflection on how to achieve both novelty and interpretability in benchmark design, synthesizing both sections.\n</document_analysis>\n\n<output_format>\nQuestion: Which dynamic benchmark construction approach most effectively balances the creation of novel evaluation items with high interpretability of its transformations, thereby minimizing manual validation overhead?\nA: Temporal cutoff, since it uses raw newly released data without any transformation.\nB: Rule-based generation, because handcrafted rules produce novel items with transparent transformations.\nC: LLM-based generation, as it leverages strong generative capabilities while inherently offering traceability.\nD: Hybrid approach, by combining interpretable rule-based steps with LLM-driven data generation.\nExplanation: The hybrid approach merges the transparency of rule-based transformations with the novelty delivered by LLM generation, reducing the need for extensive manual verification.\nCorrect Answer: D\n</output_format>",
    "question": "Which dynamic benchmark construction approach most effectively balances the creation of novel evaluation items with high interpretability of its transformations, thereby minimizing manual validation overhead?",
    "choices": {
      "A": "Temporal cutoff, since it uses raw newly released data without any transformation.",
      "B": "Rule-based generation, because handcrafted rules produce novel items with transparent transformations.",
      "C": "LLM-based generation, as it leverages strong generative capabilities while inherently offering traceability.",
      "D": "Hybrid approach, by combining interpretable rule-based steps with LLM-driven data generation."
    },
    "answer": "D",
    "explanation": "The hybrid approach merges the transparency of rule-based transformations with the novelty delivered by LLM generation, reducing the need for extensive manual verification.",
    "question_token_count": 29,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 4,
    "question_difficulty_score": 8,
    "question_groundedness_score": 6,
    "avg_answer_token_count": 16
  },
  {
    "context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n",
    "topic": "Consequences of low correctness scores on the reliability and credibility of dynamic benchmarks for LLM evaluation.",
    "response": "<document_analysis>\nThe context defines \u201cCorrectness\u201d as an evaluation criterion for dynamic benchmarks, stressing that low correctness undermines trustworthiness by providing a false sense of reliability and leads to misleading evaluations. Key relationships: correctness score measures alignment with an oracle ground truth via scoring function, and low scores imply unreliable benchmarking. This yields a straightforward inference about consequences on reliability and credibility. Difficulty level: 7/10, requiring understanding of evaluation metrics and their impact on trust in benchmarks.\n</document_analysis>\n\n<output_format>\nQuestion: What is the primary consequence of a low correctness score in a dynamic benchmark for LLM evaluation?\nA: Artificially inflating perceived model performance by hiding errors\nB: Compromising the benchmark\u2019s reliability by inducing a false sense of trust\nC: Expanding dataset coverage through uncontrolled transformations\nD: Streamlining evaluation pipelines by reducing reliance on human annotations\nExplanation: A low correctness score means the benchmark outputs do not reliably align with ground truth, leading users to trust misleading evaluations.\nCorrect Answer: B\n</output_format>",
    "question": "What is the primary consequence of a low correctness score in a dynamic benchmark for LLM evaluation?",
    "choices": {
      "A": "Artificially inflating perceived model performance by hiding errors",
      "B": "Compromising the benchmark\u2019s reliability by inducing a false sense of trust",
      "C": "Expanding dataset coverage through uncontrolled transformations",
      "D": "Streamlining evaluation pipelines by reducing reliance on human annotations"
    },
    "answer": "B",
    "explanation": "A low correctness score means the benchmark outputs do not reliably align with ground truth, leading users to trust misleading evaluations.",
    "question_token_count": 19,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 9,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 11
  },
  {
    "context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n",
    "topic": "Critically assess the trade-offs between maintaining essential knowledge fidelity and introducing sufficient novelty in LLM-based benchmark rewrites.",
    "response": "<document_analysis>\nThe context discusses the inherent tension in LLM-based benchmark rewrites: retaining original sample fidelity (risking contamination and low diversity) versus introducing novelty (expanding diversity but potentially altering core knowledge alignment and question difficulty). It cites specific systems addressing either side: ITD uses contamination detection to preserve difficulty, Auto-Dataset balances fidelity and cognitive-level novelty, StructEval extends via knowledge graphs, and VarBench swaps variables for variety. The core trade-off is between knowledge fidelity (and its contamination/diversity limits) and novelty (with potential fidelity/difficulty drift).\nDifficulty rating: 9/10.\nIntent: Prompt assessment of the fundamental trade-off by asking which statement best synthesizes both sides.\n</document_analysis>\n\n<output_format>\nQuestion: Which statement best captures the fundamental trade-off in rewriting benchmarks with LLMs between preserving essential knowledge and introducing novelty?\nA: High fidelity ensures knowledge retention but yields minimal diversity and increases in-distribution contamination risk, whereas high novelty boosts diversity but can erode core knowledge alignment and shift cognitive difficulty.\nB: Fidelity and novelty objectives are independent in LLM rewrites, allowing simultaneous optimization without any mutual compromise.\nC: Leveraging knowledge graphs for extended questions inherently guarantees both complete knowledge preservation and maximal sample diversity.\nD: Contamination detection alone fully resolves the tension, enabling LLMs to produce novel yet perfectly faithful benchmark samples.\nExplanation: Option A accurately describes the intrinsic tension: fidelity risks contamination and low diversity, while novelty risks undermining essential knowledge and consistent difficulty.\nCorrect Answer: A\n</output_format>",
    "question": "Which statement best captures the fundamental trade-off in rewriting benchmarks with LLMs between preserving essential knowledge and introducing novelty?",
    "choices": {
      "A": "High fidelity ensures knowledge retention but yields minimal diversity and increases in-distribution contamination risk, whereas high novelty boosts diversity but can erode core knowledge alignment and shift cognitive difficulty.",
      "B": "Fidelity and novelty objectives are independent in LLM rewrites, allowing simultaneous optimization without any mutual compromise.",
      "C": "Leveraging knowledge graphs for extended questions inherently guarantees both complete knowledge preservation and maximal sample diversity.",
      "D": "Contamination detection alone fully resolves the tension, enabling LLMs to produce novel yet perfectly faithful benchmark samples."
    },
    "answer": "A",
    "explanation": "Option A accurately describes the intrinsic tension: fidelity risks contamination and low diversity, while novelty risks undermining essential knowledge and consistent difficulty.",
    "question_token_count": 23,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 24
  },
  {
    "context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n",
    "topic": "Contrast scalability with other dynamic benchmarking criteria, such as data fidelity or quality metrics.",
    "response": "<document_analysis>\nThe context defines scalability in dynamic benchmarking as the expected proportion of transformed dataset size to original size per unit cost. It contrasts with other criteria: data fidelity (preserving statistical properties) and quality metrics (semantic correctness, labeling accuracy). This yields a nuanced understanding of three distinct evaluation dimensions. Difficulty is calibrated at 8/10, requiring expert-level familiarity with dynamic benchmarking criteria and their mathematical characterizations. \n</document_analysis>\n\n<output_format>\nQuestion: Which statement best distinguishes scalability from data fidelity and quality metrics in dynamic benchmarking?\nA: Scalability quantifies the expected ratio of generated dataset size to cost, whereas data fidelity measures preservation of the original data distribution, and quality metrics assess semantic correctness or labeling accuracy.\nB: Scalability measures absolute dataset size irrespective of cost, data fidelity measures transformation cost trade-offs, and quality metrics evaluate dataset diversity.\nC: Scalability assesses statistical error rates introduced during data augmentation, fidelity quantifies cost efficiency, and quality metrics measure annotation consistency.\nD: Scalability, data fidelity, and quality metrics are simply different normalization schemes of the same underlying size-to-cost ratio.\nExplanation: Scalability uniquely captures data-generation efficiency per unit cost, while fidelity and quality address distributional match and correctness, respectively.\nCorrect Answer: A\n</output_format>",
    "question": "Which statement best distinguishes scalability from data fidelity and quality metrics in dynamic benchmarking?",
    "choices": {
      "A": "Scalability quantifies the expected ratio of generated dataset size to cost, whereas data fidelity measures preservation of the original data distribution, and quality metrics assess semantic correctness or labeling accuracy.",
      "B": "Scalability measures absolute dataset size irrespective of cost, data fidelity measures transformation cost trade-offs, and quality metrics evaluate dataset diversity.",
      "C": "Scalability assesses statistical error rates introduced during data augmentation, fidelity quantifies cost efficiency, and quality metrics measure annotation consistency.",
      "D": "Scalability, data fidelity, and quality metrics are simply different normalization schemes of the same underlying size-to-cost ratio."
    },
    "answer": "A",
    "explanation": "Scalability uniquely captures data-generation efficiency per unit cost, while fidelity and quality address distributional match and correctness, respectively.",
    "question_token_count": 16,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 4,
    "avg_answer_token_count": 27
  },
  {
    "context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n",
    "topic": "Examine the meanings of \u2225T\u1d62(D)\u2225 and \u2225D\u2225 within the scalability metric and their impact on benchmarking.",
    "response": "<document_analysis>\nThe context defines a scalability metric for dynamic benchmarking as an expectation over transformations. It specifies \u2225T_i(\ud835\udc9f)\u2225 as the size of the transformed dataset and \u2225\ud835\udc9f\u2225 as the original dataset size. The equation interprets scalability as proportion of data generated per unit cost. Key nuance: using the difference \u2225T_i(D)\u2225 \u2013 \u2225D\u2225 measures net new data created rather than absolute size, controlling for initial dataset volume. This underpins fair comparison across transformations. For a domain expert, probing why the metric focuses on net increase deepens understanding of its design choices and benchmarking implications.\n</document_analysis>\n\n<output_format>\nQuestion: In assessing scalability of dynamic benchmarks, why does the metric use the net increase (\u2225T\u1d62(D)\u2225 \u2013 \u2225D\u2225) rather than the absolute transformed dataset size \u2225T\u1d62(D)\u2225?\nA: To normalize against variations in transformation cost by accounting for original dataset processing overhead.\nB: To measure only the additional data generated by the transformation, enabling fair comparison of scalability across methods.\nC: To compensate for statistical errors by discounting the original dataset size in variance calculations.\nD: To enforce an upper bound on transformations so that benchmarks remain within predefined size limits.\nExplanation: The subtraction isolates the new data introduced by each transformation, ensuring the scalability metric reflects net generation per cost and is comparable across different baseline dataset sizes.\nCorrect Answer: B\n</output_format>",
    "question": "In assessing scalability of dynamic benchmarks, why does the metric use the net increase (\u2225T\u1d62(D)\u2225 \u2013 \u2225D\u2225) rather than the absolute transformed dataset size \u2225T\u1d62(D)\u2225?",
    "choices": {
      "A": "To normalize against variations in transformation cost by accounting for original dataset processing overhead.",
      "B": "To measure only the additional data generated by the transformation, enabling fair comparison of scalability across methods.",
      "C": "To compensate for statistical errors by discounting the original dataset size in variance calculations.",
      "D": "To enforce an upper bound on transformations so that benchmarks remain within predefined size limits."
    },
    "answer": "B",
    "explanation": "The subtraction isolates the new data introduced by each transformation, ensuring the scalability metric reflects net generation per cost and is comparable across different baseline dataset sizes.",
    "question_token_count": 44,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 17
  },
  {
    "context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n",
    "topic": "The mathematical relationship between dataset size, transformation strategy, and expected Collision Rate.",
    "response": "<document_analysis>\nThe context introduces \u201ccollision rate\u201d as the percentage overlap between two independently transformed versions of a dataset and motivates its use to assess novelty in dynamic benchmarks. Although no explicit formula is given, the simplest quantitative model assumes each transformation samples k items without replacement from an original dataset of size N. Under this model, the probability any given item in one sampled set also appears in the other is k/N, yielding an expected overlap fraction of k/N. This distinction between absolute overlap count (k^2/N) and fraction relative to the transform size is non\u2010trivial and easily miscomputed. Hence, a high\u2010difficulty question probes the derivation of the expected collision rate fraction, challenging experts to correctly map sampling probabilities to the collision metric.\n</document_analysis>\n\n<output_format>\nQuestion: In a dynamic benchmarking scenario where each transformation independently selects k examples without replacement from an original dataset of size N, what is the expected collision rate defined as the fraction of items in one transformed set that also appear in another?\nA: (k/N)\u00b2  \nB: k/N  \nC: k\u00b2/N  \nD: 1 \u2212 ((N\u2212k)/N)\u1d4f  \nExplanation: Each of the k items in the first sample has a k/N chance of being chosen in the second, so the expected overlap fraction is k/N.  \nCorrect Answer: B\n</output_format>",
    "question": "In a dynamic benchmarking scenario where each transformation independently selects k examples without replacement from an original dataset of size N, what is the expected collision rate defined as the fraction of items in one transformed set that also appear in another?",
    "choices": {
      "A": "(k/N)\u00b2",
      "B": "k/N",
      "C": "k\u00b2/N",
      "D": "1 \u2212 ((N\u2212k)/N)\u1d4f"
    },
    "answer": "B",
    "explanation": "Each of the k items in the first sample has a k/N chance of being chosen in the second, so the expected overlap fraction is k/N.",
    "question_token_count": 45,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 4,
    "avg_answer_token_count": 6
  },
  {
    "context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n",
    "topic": "Compare and contrast the objectives and evaluation methodologies of code synthesis benchmarks (HumanEval, MBPP) versus software engineering challenge benchmarks (SWE\u2011Bench).",
    "response": "<document_analysis>\n1. Thoughtful Content Examination:\n   - The context distinguishes between \u201ccoding benchmarks\u201d (HumanEval, MBPP) focused on code generation and debugging, and \u201csoftware engineering challenge benchmarks\u201d (SWE\u2011Bench) that address more advanced tasks.\n   - HumanEval and MBPP validate generated snippets against unit\u2011style input/output tests. SWE\u2011Bench by contrast implies end\u2011to\u2011end, higher\u2011complexity software tasks.\n2. Concept Exploration:\n   - Implicit assumptions: simpler function\u2010level tasks vs integrated software systems; unit testing vs multi\u2010criteria evaluation (integration, maintainability, specification compliance).\n   - Underlying theory: granularity of evaluation and the sophistication of benchmarks shape LLM capabilities differently.\n3. Strategic Complexity Calibration:\n   - Difficulty Level: 8/10. Requires synthesizing nuanced differences in benchmark objectives and methodologies beyond surface descriptions.\n4. Intentional Question Planning:\n   - Frame a comparison prompt that demands recognition of both the goal (objectives) and the evaluation approaches (unit tests vs holistic metrics).\n   - Provide four plausible alternatives, with subtle technical inaccuracies in distractors.\n   - Ensure answerable without quoting the context directly but relying on domain expertise.\n</document_analysis>\n\n<output_format>\nQuestion: Which statement most accurately contrasts the objectives and evaluation methods of HumanEval/MBPP with those of SWE\u2011Bench?\nA: HumanEval/MBPP assess isolated function\u2011level generation through predefined input\u2011output tests, whereas SWE\u2011Bench evaluates comprehensive software tasks for end\u2011to\u2011end functionality, maintainability, and specification compliance.\nB: HumanEval/MBPP emphasize debugging of existing snippets under strict execution correctness, while SWE\u2011Bench focuses solely on generating novel algorithms optimized for performance metrics without correctness checks.\nC: HumanEval/MBPP target high\u2011level system design validated by dynamic integration test suites, whereas SWE\u2011Bench concentrates on small coding exercises scored by a fixed set of unit tests.\nD: HumanEval/MBPP measure natural language to code translation without runtime validation, whereas SWE\u2011Bench tests code primarily for runtime efficiency in production\u2011scale workloads but ignores logical correctness.\nExplanation: Option A correctly contrasts function\u2011level code synthesis with unit\u2011test validation against SWE\u2011Bench\u2019s end\u2011to\u2011end software engineering tasks assessed on multiple criteria including integration, maintainability, and adherence to specifications.\nCorrect Answer: A\n</output_format>",
    "question": "Which statement most accurately contrasts the objectives and evaluation methods of HumanEval/MBPP with those of SWE\u2011Bench?",
    "choices": {
      "A": "HumanEval/MBPP assess isolated function\u2011level generation through predefined input\u2011output tests, whereas SWE\u2011Bench evaluates comprehensive software tasks for end\u2011to\u2011end functionality, maintainability, and specification compliance.",
      "B": "HumanEval/MBPP emphasize debugging of existing snippets under strict execution correctness, while SWE\u2011Bench focuses solely on generating novel algorithms optimized for performance metrics without correctness checks.",
      "C": "HumanEval/MBPP target high\u2011level system design validated by dynamic integration test suites, whereas SWE\u2011Bench concentrates on small coding exercises scored by a fixed set of unit tests.",
      "D": "HumanEval/MBPP measure natural language to code translation without runtime validation, whereas SWE\u2011Bench tests code primarily for runtime efficiency in production\u2011scale workloads but ignores logical correctness."
    },
    "answer": "A",
    "explanation": "Option A correctly contrasts function\u2011level code synthesis with unit\u2011test validation against SWE\u2011Bench\u2019s end\u2011to\u2011end software engineering tasks assessed on multiple criteria including integration, maintainability, and adherence to specifications.",
    "question_token_count": 24,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 37
  },
  {
    "context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Future challenges and research directions for sustaining robust, contamination-resistant LLM benchmarking frameworks.",
    "response": "<document_analysis>\nThe context outlines the evolution of LLM benchmarking from static, human\u2010curated datasets toward systems that both adapt over time and detect contamination from training corpora. It highlights two core challenges: as models improve, fixed tests become trivial (performance saturation), and models may have seen benchmark items during training (data contamination). Proposed remedies include dynamic benchmarks that refresh and diversify tasks, and contamination detectors that score and filter leaked examples. These insights directly suggest integrating adaptive evaluation pipelines with quantitative leakage assessment. The question probes understanding of how these dual mechanisms interact to sustain challenge and purity in LLM benchmarks, requiring synthesis of the two strategies.\n</document_analysis>\n\n<output_format>\nQuestion: Given the twin challenges of static benchmarks becoming trivial for advancing LLMs and test\u2010item leakage into training data, which architectural design most directly ensures that evaluations remain both challenging and uncontaminated?\nA: Periodic replacement of benchmark tasks by manually curated private datasets combined with cryptographic data audits.\nB: Adaptive benchmarks that automatically diversify prompts over time alongside probabilistic contamination scoring to filter leaked items.\nC: Enlargement of static test suites through synthetic input generation and rejecting outputs based on a fixed perplexity threshold.\nD: Ensemble evaluation using heterogeneous static tasks paired with adversarial filtering but without real-time updates.\nExplanation: Option B integrates dynamic prompt diversification (to maintain difficulty) with contamination detectors (to quantify and exclude leaked examples), directly addressing both key challenges.\nCorrect Answer: B\n</output_format>",
    "question": "Given the twin challenges of static benchmarks becoming trivial for advancing LLMs and test\u2010item leakage into training data, which architectural design most directly ensures that evaluations remain both challenging and uncontaminated?",
    "choices": {
      "A": "Periodic replacement of benchmark tasks by manually curated private datasets combined with cryptographic data audits.",
      "B": "Adaptive benchmarks that automatically diversify prompts over time alongside probabilistic contamination scoring to filter leaked items.",
      "C": "Enlargement of static test suites through synthetic input generation and rejecting outputs based on a fixed perplexity threshold.",
      "D": "Ensemble evaluation using heterogeneous static tasks paired with adversarial filtering but without real-time updates."
    },
    "answer": "B",
    "explanation": "Option B integrates dynamic prompt diversification (to maintain difficulty) with contamination detectors (to quantify and exclude leaked examples), directly addressing both key challenges.",
    "question_token_count": 40,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 19
  },
  {
    "context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Future research directions and open challenges in designing and standardizing dynamic benchmarks for LLMs.",
    "response": "<document_analysis>\nThe provided context introduces static versus dynamic benchmarking for LLMs, highlights methods to reduce data contamination (timestamps updating, data regeneration, encryption, post-hoc detection), and emphasizes a key survey contribution: proposing evaluation criteria for dynamic benchmarks\u2014and revealing that existing dynamic approaches fail to meet these criteria. The gap\u2014the absence of a standardized set of criteria for assessing dynamic benchmarks\u2014represents a vital open challenge and future research direction. This central unmet requirement forms the basis for a probing, expert-level question.\n</document_analysis>\n\n<output_format>\nQuestion: Which critical requirement for next\u2010generation dynamic LLM benchmarks, essential to rigorously guard against data contamination, has been identified as unfulfilled by current approaches and thus demands formal standardization?\nA: Real\u2010time dataset updates aligned exactly with model training timestamps  \nB: A unified framework of evaluation criteria ensuring both contamination resistance and benchmark validity  \nC: Automated regeneration of benchmark tasks from heterogeneous sources to prevent pre\u2010exposure  \nD: Encryption schemes integrated into dynamic benchmark pipelines to block pretraining leakage  \nExplanation: Although timestamp updates (A), task regeneration (C), and encryption (D) have been explored, the survey highlights that no existing work defines or satisfies a unified set of evaluation criteria for dynamic benchmarks\u2014making (B) the pivotal unaddressed requirement.  \nCorrect Answer: B\n</output_format>",
    "question": "Which critical requirement for next\u2010generation dynamic LLM benchmarks, essential to rigorously guard against data contamination, has been identified as unfulfilled by current approaches and thus demands formal standardization?",
    "choices": {
      "A": "Real\u2010time dataset updates aligned exactly with model training timestamps",
      "B": "A unified framework of evaluation criteria ensuring both contamination resistance and benchmark validity",
      "C": "Automated regeneration of benchmark tasks from heterogeneous sources to prevent pre\u2010exposure",
      "D": "Encryption schemes integrated into dynamic benchmark pipelines to block pretraining leakage"
    },
    "answer": "B",
    "explanation": "Although timestamp updates (A), task regeneration (C), and encryption (D) have been explored, the survey highlights that no existing work defines or satisfies a unified set of evaluation criteria for dynamic benchmarks\u2014making (B) the pivotal unaddressed requirement.",
    "question_token_count": 36,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 13
  },
  {
    "context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n",
    "topic": "Trade-offs between manual validation costs and the level of interpretability in dynamic benchmark data creation.",
    "response": "<document_analysis>\nThe context discusses interpretability in dynamic benchmarking transformations and categorizes generation approaches (temporal cutoff, rule-based, LLM-based, hybrid). It highlights how rule-based transformations are inherently interpretable and low in manual validation cost, while LLM-assisted require extra explainability tools or human-in-loop checks. Hybrid approaches blend both to balance transparency and efficiency. Thus the content is educationally meaningful and supports a question probing how hybrid methods leverage rule-based elements to reduce manual validation overhead.\n</document_analysis>\n\n<output_format>\nQuestion: In dynamic benchmark creation, how do hybrid generation approaches optimally balance transformation interpretability with reduced manual validation costs compared to pure LLM\u2011based or pure rule\u2011based methods?\nA: They use temporal cutoff sampling to avoid transformations entirely.\nB: They apply model\u2011agnostic explainability audits on LLM outputs without any rule scaffolding.\nC: They integrate rule\u2011based templates to structure LLM output, ensuring transparent mappings.\nD: They defer all human validation to the final evaluation stage, eliminating intermediate checks.\nExplanation: Hybrid methods combine rule\u2011based scaffolding with LLM generation, leveraging the inherent transparency of rules to guide model outputs and thus lower manual validation needs.\nCorrect Answer: C\n</output_format>",
    "question": "In dynamic benchmark creation, how do hybrid generation approaches optimally balance transformation interpretability with reduced manual validation costs compared to pure LLM\u2011based or pure rule\u2011based methods?",
    "choices": {
      "A": "They use temporal cutoff sampling to avoid transformations entirely.",
      "B": "They apply model\u2011agnostic explainability audits on LLM outputs without any rule scaffolding.",
      "C": "They integrate rule\u2011based templates to structure LLM output, ensuring transparent mappings.",
      "D": "They defer all human validation to the final evaluation stage, eliminating intermediate checks."
    },
    "answer": "C",
    "explanation": "Hybrid methods combine rule\u2011based scaffolding with LLM generation, leveraging the inherent transparency of rules to guide model outputs and thus lower manual validation needs.",
    "question_token_count": 34,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 8,
    "avg_answer_token_count": 15
  },
  {
    "context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n",
    "topic": "Preference testing between original and paraphrased test cases as an indicator of data leakage",
    "response": "<document_analysis>\nThe context describes post-hoc detection methods for data contamination, highlighting that exact n\u2011gram matching often yields false negatives and motivates more robust behavioral analyses\u2014specifically, comparing model performance on original versus paraphrased test cases to reveal memorization beyond direct overlap. The question will probe the expert\u2019s grasp of why preference testing is superior to naive overlap detection, asking for the core methodological advantage. Difficulty is calibrated at 8/10 given the need to understand trade\u2011offs between lexical matching and behavioral checks.\n</document_analysis>\n\n<output_format>\nQuestion: Which advantage does using preference testing between original and paraphrased test cases offer compared to relying solely on n\u2011gram overlap for detecting data leakage?\nA: It exposes memorization of surface\u2011form patterns by measuring performance on lexically altered yet semantically equivalent inputs.\nB: It completely removes false negatives by converting all paraphrases back to their original text.\nC: It enhances detection speed by using direct string comparisons without additional computations.\nD: It guarantees zero false positives by focusing only on exact matches.\nExplanation: Preference testing uncovers memorization beyond exact matches by observing performance drops on paraphrases while semantics remain constant.\nCorrect Answer: A\n</output_format>",
    "question": "Which advantage does using preference testing between original and paraphrased test cases offer compared to relying solely on n\u2011gram overlap for detecting data leakage?",
    "choices": {
      "A": "It exposes memorization of surface\u2011form patterns by measuring performance on lexically altered yet semantically equivalent inputs.",
      "B": "It completely removes false negatives by converting all paraphrases back to their original text.",
      "C": "It enhances detection speed by using direct string comparisons without additional computations.",
      "D": "It guarantees zero false positives by focusing only on exact matches."
    },
    "answer": "A",
    "explanation": "Preference testing uncovers memorization beyond exact matches by observing performance drops on paraphrases while semantics remain constant.",
    "question_token_count": 30,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 16
  },
  {
    "context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n",
    "topic": "A critical comparison of static benchmarks versus dynamically evolving benchmarks generated by multi-agent frameworks in terms of adaptability and coverage.",
    "response": "<document_analysis>\nThe context contrasts static benchmarks with dynamic, multi-agent generated benchmarks in terms of adaptability (i.e., the ability to evolve and pose novel, follow-up or more varied questions) and coverage (i.e., breadth and depth across subtopics). Key aspects include the multi-agent pipeline\u2014planning, generation, verification, evaluation\u2014and its human-in-the-loop feedback that yields scalable, diverse, high-quality benchmarks. The question probes which mechanism within self\u2011evolving multi-agent frameworks directly underpins their superior adaptability and coverage over one\u2011time static benchmarks. Distractors reference plausible but incorrect mechanisms (paraphrasing, manual editing, predefined fixed tiers).\n</document_analysis>\n\n<output_format>\nQuestion: What unique capability of multi-agent self\u2011evolving benchmark frameworks most directly enhances both adaptability and expansive coverage compared to static benchmarks?\nA: The ability to paraphrase existing static benchmark queries for improved clarity  \nB: Autonomously orchestrating iterative planning, generation, verification, and evaluation across specialized agents  \nC: Relying on manual, human-in-the-loop editing to refine each question\u2019s difficulty  \nD: Limiting question generation to predefined topics with fixed difficulty tiers  \nExplanation: Self\u2011evolving multi\u2011agent frameworks use a coordinated pipeline\u2014planning, generation, verification, evaluation\u2014handled by specialized agents (with human feedback) to continuously extend and diversify the benchmark, thus boosting adaptability and coverage.  \nCorrect Answer: B\n</output_format>",
    "question": "What unique capability of multi-agent self\u2011evolving benchmark frameworks most directly enhances both adaptability and expansive coverage compared to static benchmarks?",
    "choices": {
      "A": "The ability to paraphrase existing static benchmark queries for improved clarity",
      "B": "Autonomously orchestrating iterative planning, generation, verification, and evaluation across specialized agents",
      "C": "Relying on manual, human-in-the-loop editing to refine each question\u2019s difficulty",
      "D": "Limiting question generation to predefined topics with fixed difficulty tiers"
    },
    "answer": "B",
    "explanation": "Self\u2011evolving multi\u2011agent frameworks use a coordinated pipeline\u2014planning, generation, verification, evaluation\u2014handled by specialized agents (with human feedback) to continuously extend and diversify the benchmark, thus boosting adaptability and coverage.",
    "question_token_count": 26,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 15
  },
  {
    "context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n",
    "topic": "Propose a methodological framework for creating fair and reliable evaluation benchmarks that minimize the risk of contamination from large-scale training datasets.",
    "response": "<document_analysis>\nThe context highlights the difficulty of preventing evaluation-contamination in LLMs pre-trained on massive, proprietary corpora and fine-tuned on diverse annotated or synthetic datasets. Key themes include the limitations of retrieval-based detection, the opacity of training data, and the urgent need for benchmark design that enforces strict data provenance to ensure evaluation items were not seen during training. To challenge experts, the question will probe which methodological element most robustly guarantees uncontaminated benchmarks, offering several plausible yet subtly flawed alternatives.\n</document_analysis>\n\n<output_format>\nQuestion: Which methodological strategy most robustly ensures that an evaluation dataset remains uncontaminated by a large\u2010scale language model\u2019s proprietary training corpus?\nA: Apply retrieval\u2010based similarity filters against publicly known subsets of the model\u2019s training data and remove any matches.  \nB: Generate all evaluation items synthetically using a separate model never involved in pretraining and record strict provenance for each item.  \nC: Select evaluation items exclusively from publicly released documents dated after the model\u2019s declared training cutoff.  \nD: Use crowd-sourced human annotations on in-domain texts while seeding tasks with representative examples from the application domain.  \nExplanation: Synthetic creation with a separate model and rigorous provenance tracking prevents any overlap with the proprietary training set, whereas retrieval filters (A) are incomplete, post-cutoff selection (C) fails if the cutoff is uncertain, and crowd-sourcing on existing texts (D) risks hidden overlaps.  \nCorrect Answer: B\n</output_format>",
    "question": "Which methodological strategy most robustly ensures that an evaluation dataset remains uncontaminated by a large\u2010scale language model\u2019s proprietary training corpus?",
    "choices": {
      "A": "Apply retrieval\u2010based similarity filters against publicly known subsets of the model\u2019s training data and remove any matches.",
      "B": "Generate all evaluation items synthetically using a separate model never involved in pretraining and record strict provenance for each item.",
      "C": "Select evaluation items exclusively from publicly released documents dated after the model\u2019s declared training cutoff.",
      "D": "Use crowd-sourced human annotations on in-domain texts while seeding tasks with representative examples from the application domain."
    },
    "answer": "B",
    "explanation": "Synthetic creation with a separate model and rigorous provenance tracking prevents any overlap with the proprietary training set, whereas retrieval filters (A) are incomplete, post-cutoff selection (C) fails if the cutoff is uncertain, and crowd-sourcing on existing texts (D) risks hidden overlaps.",
    "question_token_count": 29,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 21
  },
  {
    "context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n",
    "topic": "How collision metrics inform decisions on the frequency and variety of benchmark regenerations.",
    "response": "<document_analysis>\nThe context describes two collision metrics\u2014Collision Rate (overlap percentage between two transformed benchmark versions) and Repeat Trials (expected number of trials to recreate an existing transformed dataset)\u2014and links them to decisions about ensuring novelty and diversity in dynamic benchmarks. High collision implies excessive overlap; low repeat trials implies easy regeneration of duplicates. These insights inform how often and how diversely to regenerate benchmarks. A nuanced question can ask how these metrics should guide adjustments to regeneration frequency and transformation variety.\n</document_analysis>\n\n<output_format>\nQuestion: If a dynamic benchmark\u2019s Collision Rate is high and its Repeat Trials metric is low, which adjustment to the regeneration strategy best ensures continued novelty and robustness?\nA: Decrease the frequency of regenerations and narrow the set of transformation methods.\nB: Increase the frequency of regenerations and expand the variety of transformation methods.\nC: Keep the current regeneration frequency but eliminate less-used transformation methods.\nD: Increase regeneration frequency while limiting the number of transformation variants.\nExplanation: A high Collision Rate indicates excessive overlap between transformed datasets, and a low Repeat Trials value means duplicates appear quickly, so both more frequent and more diverse transformations are required to maintain novel, robust benchmarks.\nCorrect Answer: B\n</output_format>",
    "question": "If a dynamic benchmark\u2019s Collision Rate is high and its Repeat Trials metric is low, which adjustment to the regeneration strategy best ensures continued novelty and robustness?",
    "choices": {
      "A": "Decrease the frequency of regenerations and narrow the set of transformation methods.",
      "B": "Increase the frequency of regenerations and expand the variety of transformation methods.",
      "C": "Keep the current regeneration frequency but eliminate less-used transformation methods.",
      "D": "Increase regeneration frequency while limiting the number of transformation variants."
    },
    "answer": "B",
    "explanation": "A high Collision Rate indicates excessive overlap between transformed datasets, and a low Repeat Trials value means duplicates appear quickly, so both more frequent and more diverse transformations are required to maintain novel, robust benchmarks.",
    "question_token_count": 31,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 13
  },
  {
    "context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n",
    "topic": "The problem of false negatives in exact matching and its influence on contamination metrics",
    "response": "<document_analysis>\nThe context outlines the challenge of false negatives in exact n-gram matching for contamination detection and describes complementary post-hoc methods. It highlights that embedding-based similarity and improved mapping metrics mitigate false negatives by capturing semantic and lexical overlaps beyond verbatim matches. It then discusses behavioral analyses\u2014e.g., masked-input memorization\u2014that detect contamination through model outputs rather than input overlap. A targeted question can probe the nuanced interplay between false negatives in exact matching and the use of masked-input memorization to reveal hidden contamination.\n</document_analysis>\n\n<output_format>\nQuestion: How does masked-input memorization analysis uncover contamination cases that exact n-gram overlap detection misses, thereby reducing false negatives?\nA: By evaluating if a model reconstructs original training text when central tokens are masked in paraphrased inputs, revealing semantic overlaps that token-level matching fails to catch.\nB: By computing embedding-distance thresholds on masked inputs to directly match paraphrased content with training data.\nC: By comparing a model\u2019s performance on original versus paraphrased test cases to infer contamination without textual overlap.\nD: By aligning token frequency distributions in masked inputs to detect rare vocabulary overlaps with training data.\nExplanation: Masked-input memorization checks whether a model, given paraphrased context with key tokens masked, still reproduces the exact training text\u2014a scenario exact n-gram matching overlooks.\nCorrect Answer: A\n</output_format>",
    "question": "How does masked-input memorization analysis uncover contamination cases that exact n-gram overlap detection misses, thereby reducing false negatives?",
    "choices": {
      "A": "By evaluating if a model reconstructs original training text when central tokens are masked in paraphrased inputs, revealing semantic overlaps that token-level matching fails to catch.",
      "B": "By computing embedding-distance thresholds on masked inputs to directly match paraphrased content with training data.",
      "C": "By comparing a model\u2019s performance on original versus paraphrased test cases to infer contamination without textual overlap.",
      "D": "By aligning token frequency distributions in masked inputs to detect rare vocabulary overlaps with training data."
    },
    "answer": "A",
    "explanation": "Masked-input memorization checks whether a model, given paraphrased context with key tokens masked, still reproduces the exact training text\u2014a scenario exact n-gram matching overlooks.",
    "question_token_count": 24,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 23
  },
  {
    "context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n",
    "topic": "Explore the ethical considerations and data privacy implications when continuously scraping newly published or user-generated content for benchmark creation.",
    "response": "<document_analysis>\nThe context describes various benchmarks (LiveBench, AntiLeak-Bench, AcademicEval, LiveCodeBench, LiveAoPSBench, Forecastbench) that continuously scrape or collect newly published or user-generated content to prevent data contamination and keep evaluations up to date. While the snippet does not explicitly discuss ethical or legal frameworks, it implies potential data privacy and ownership considerations when harvesting user-originated material from platforms like LeetCode or AoPS. A probing question on GDPR legal bases for processing scraped user-generated content aligns with the ethical and privacy focus, challenging domain experts to apply external regulatory knowledge to the context of LiveCodeBench.\n</document_analysis>\n\n<output_format>\nQuestion: When continuously scraping user\u2010generated coding problems from an online platform to build a benchmark (as in LiveCodeBench), under GDPR and assuming no explicit user consent, which legal basis most appropriately justifies processing this personal data?\nA: Performance of a contract to which the data subject is party  \nB: Explicit consent obtained from each user  \nC: Legitimate interest of the data controller, balanced against user rights  \nD: Processing necessary for tasks carried out in the public interest  \nExplanation: Under GDPR, when no user consent is obtained, \u201clegitimate interest\u201d can justify processing user\u2010generated content if the controller\u2019s interests do not override data subjects\u2019 rights and appropriate safeguards are applied.  \nCorrect Answer: C\n</output_format>",
    "question": "When continuously scraping user\u2010generated coding problems from an online platform to build a benchmark (as in LiveCodeBench), under GDPR and assuming no explicit user consent, which legal basis most appropriately justifies processing this personal data?",
    "choices": {
      "A": "Performance of a contract to which the data subject is party",
      "B": "Explicit consent obtained from each user",
      "C": "Legitimate interest of the data controller, balanced against user rights",
      "D": "Processing necessary for tasks carried out in the public interest"
    },
    "answer": "C",
    "explanation": "Under GDPR, when no user consent is obtained, \u201clegitimate interest\u201d can justify processing user\u2010generated content if the controller\u2019s interests do not override data subjects\u2019 rights and appropriate safeguards are applied.",
    "question_token_count": 45,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 1,
    "avg_answer_token_count": 11
  },
  {
    "context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n",
    "topic": "Analyze why high variance in complexity measurements across trials indicates instability in dynamic benchmarking methods.",
    "response": "<document_analysis>\nThe context discusses the need for stability in dynamic benchmark complexity measurements (\u03a8), defining stability via low variance across trials. It highlights that if variance is high, the method\u2019s complexity estimates fluctuate, making it impossible to disentangle performance drops due to genuine complexity increases from those due to data contamination. This relationship between variance and interpretability of performance changes is central. The question will probe a deep understanding of how variance undermines attribution of performance drops in dynamic benchmarking. Difficulty: 8/10.\n</document_analysis>\n\n<output_format>\nQuestion: In dynamic benchmarking, what fundamental problem arises when the complexity measurement \u03a8 shows high variance across trials, and why is this problematic for diagnosing performance drops in an LLM?\nA: It suggests inconsistency in complexity estimates, which can lead to misattributing performance declines to data contamination rather than actual increases in task complexity.\nB: It reveals that the chosen complexity metric is too general, causing the benchmark to overfit specific reasoning tasks.\nC: It indicates that transformed datasets have uniform complexity, preventing detection of subtle performance regressions.\nD: It confirms that the dynamic transformations have no effect on complexity, falsely attributing all performance changes to model training issues.\nExplanation: High variance in \u03a8 means complexity estimates are unstable, so one cannot reliably discern if performance drops stem from genuine complexity increases or contamination.\nCorrect Answer: A\n</output_format>",
    "question": "In dynamic benchmarking, what fundamental problem arises when the complexity measurement \u03a8 shows high variance across trials, and why is this problematic for diagnosing performance drops in an LLM?",
    "choices": {
      "A": "It suggests inconsistency in complexity estimates, which can lead to misattributing performance declines to data contamination rather than actual increases in task complexity.",
      "B": "It reveals that the chosen complexity metric is too general, causing the benchmark to overfit specific reasoning tasks.",
      "C": "It indicates that transformed datasets have uniform complexity, preventing detection of subtle performance regressions.",
      "D": "It confirms that the dynamic transformations have no effect on complexity, falsely attributing all performance changes to model training issues."
    },
    "answer": "A",
    "explanation": "High variance in \u03a8 means complexity estimates are unstable, so one cannot reliably discern if performance drops stem from genuine complexity increases or contamination.",
    "question_token_count": 33,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 23
  },
  {
    "context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n",
    "topic": "Compare and contrast the data collection methodologies of LiveBench and LiveCodeBench, focusing on domain-specific challenges and update cadences.",
    "response": "<document_analysis>\nThe context describes LiveBench and LiveCodeBench\u2019s methodologies: LiveBench gathers math competition questions from the past year, updating every few months; LiveCodeBench continuously harvests human-written coding problems from platforms like LeetCode. The domain-specific contrast lies in their data types\u2014symbolic math versus executable code\u2014and in their cadence-driven challenges. LiveCodeBench\u2019s continuous pipeline must handle code execution environments and test-case validation across languages, whereas LiveBench\u2019s slower updates center on normalizing diverse math problem formats. This distinction guides our high-difficulty comparative question focusing on domain-specific ingestion challenges tied to each cadence.\n</document_analysis>\n\n<output_format>\nQuestion: Which domain-specific challenge is uniquely critical for LiveCodeBench\u2019s continuous data ingestion pipeline compared to LiveBench\u2019s periodic updates?\nA: Continuous sandboxed code execution and test-case maintenance\nB: Periodic semantic normalization of diverse math contest statements\nC: Manual calibration of contest problem difficulty tiers\nD: Version control and citation updates of new research papers\nExplanation: LiveCodeBench must continuously fetch and validate executable coding problems\u2014requiring sandboxed runtime environments and test-case upkeep\u2014which is not a concern for LiveBench\u2019s math-focused, less frequent updates.\nCorrect Answer: A\n</output_format>",
    "question": "Which domain-specific challenge is uniquely critical for LiveCodeBench\u2019s continuous data ingestion pipeline compared to LiveBench\u2019s periodic updates?",
    "choices": {
      "A": "Continuous sandboxed code execution and test-case maintenance",
      "B": "Periodic semantic normalization of diverse math contest statements",
      "C": "Manual calibration of contest problem difficulty tiers",
      "D": "Version control and citation updates of new research papers"
    },
    "answer": "A",
    "explanation": "LiveCodeBench must continuously fetch and validate executable coding problems\u2014requiring sandboxed runtime environments and test-case upkeep\u2014which is not a concern for LiveBench\u2019s math-focused, less frequent updates.",
    "question_token_count": 25,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 9
  },
  {
    "context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n",
    "topic": "The rationale behind dynamic benchmarking in the presence of public availability and the risk of LLM training data contamination.",
    "response": "<document_analysis>\nThe context introduces dynamic benchmarking\u2019s collision concept to balance transparency with contamination risk. It defines two key metrics: Collision Rate (pairwise overlap percentage between two transformed datasets) and Repeat Trials (average number of transformations required to regenerate an existing transformed dataset). These metrics together measure a benchmark\u2019s ability to generate novel test cases even if publicly known. The nuanced tradeoff is that a low collision rate alone may not guarantee a large transformation space if repeat trials is small. Thus, comparing algorithms by both metrics reveals which offers stronger protection against LLM training data contamination. Difficulty: 8/10.\n</document_analysis>\n\n<output_format>\nQuestion: Two dynamic benchmarking algorithms, A and B, yield the following metrics: A has a 5% collision rate and requires 20 repeat trials to reproduce a prior transformation, while B has a 1% collision rate and requires only 3 repeat trials. Which algorithm provides stronger resilience against LLM training data contamination and why?\nA: Algorithm A, because its higher repeat trials indicate a larger transformation space and thus greater diversity despite a modestly higher collision rate.\nB: Algorithm B, because its lower collision rate ensures minimal overlap and therefore lower contamination risk, regardless of repeat trials.\nC: Algorithm A, because collision rate is the primary measure of contamination risk and it has the lower repeat trials advantage.\nD: Algorithm B, because requiring fewer repeat trials means transformations converge more quickly, ensuring consistency.\nExplanation: Repeat trials directly measures how many unique variations a dynamic benchmark can generate before reproducing previous data; a higher value implies greater novelty and better protection against contamination.\nCorrect Answer: A\n</output_format>",
    "question": "Two dynamic benchmarking algorithms, A and B, yield the following metrics: A has a 5% collision rate and requires 20 repeat trials to reproduce a prior transformation, while B has a 1% collision rate and requires only 3 repeat trials. Which algorithm provides stronger resilience against LLM training data contamination and why?",
    "choices": {
      "A": "Algorithm A, because its higher repeat trials indicate a larger transformation space and thus greater diversity despite a modestly higher collision rate.",
      "B": "Algorithm B, because its lower collision rate ensures minimal overlap and therefore lower contamination risk, regardless of repeat trials.",
      "C": "Algorithm A, because collision rate is the primary measure of contamination risk and it has the lower repeat trials advantage.",
      "D": "Algorithm B, because requiring fewer repeat trials means transformations converge more quickly, ensuring consistency."
    },
    "answer": "A",
    "explanation": "Repeat trials directly measures how many unique variations a dynamic benchmark can generate before reproducing previous data; a higher value implies greater novelty and better protection against contamination.",
    "question_token_count": 65,
    "answer_correctness_score": 8,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 22
  },
  {
    "context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Design, scope, and evaluation metrics of coding benchmarks assessing LLMs\u2019 code generation and comprehension (e.g., Chen et al. 2021; Austin et al. 2021; Jimenez et al. 2024).",
    "response": "<document_analysis>\nThe context discusses challenges in benchmarking LLMs on coding tasks, specifically highlighting that static benchmarks can become compromised over time due to models training on the same data (\u201cdata contamination\u201d) and may also grow too trivial as model capabilities advance. It then notes two complementary solutions: contamination detectors, which quantify overlap between benchmark items and a model\u2019s training corpus, and dynamic benchmarks, which are designed to evolve (e.g., by refreshing or rotating tasks) to prevent such overlap and maintain difficulty. The central nuance is distinguishing measurement of contamination risk versus actively mitigating it via dynamic test construction.\n</document_analysis>\n\n<output_format>\nQuestion: In the realm of coding benchmarks for large language models, how do dynamic benchmarks fundamentally differ from contamination detectors in addressing the shortcomings of static evaluations?\nA: Dynamic benchmarks periodically refresh test instances to prevent overlap with a model\u2019s training data, whereas contamination detectors quantify the extent of any existing training\u2013test data leakage.\nB: Dynamic benchmarks quantify contamination risk over time, while contamination detectors generate new evaluation tasks tailored to evolving model strengths.\nC: Dynamic benchmarks integrate instruction-following evaluations alongside coding tasks, whereas contamination detectors focus solely on measuring code-generation accuracy.\nD: Dynamic benchmarks rely on manual difficulty calibration by expert raters, while contamination detectors automatically adjust benchmark difficulty to match model performance.\nExplanation: Dynamic benchmarks actively refresh or rotate problems to avoid train/test overlap, whereas contamination detectors only measure how much of the evaluation set appears in the model\u2019s training data.\nCorrect Answer: A\n</output_format>",
    "question": "In the realm of coding benchmarks for large language models, how do dynamic benchmarks fundamentally differ from contamination detectors in addressing the shortcomings of static evaluations?",
    "choices": {
      "A": "Dynamic benchmarks periodically refresh test instances to prevent overlap with a model\u2019s training data, whereas contamination detectors quantify the extent of any existing training\u2013test data leakage.",
      "B": "Dynamic benchmarks quantify contamination risk over time, while contamination detectors generate new evaluation tasks tailored to evolving model strengths.",
      "C": "Dynamic benchmarks integrate instruction-following evaluations alongside coding tasks, whereas contamination detectors focus solely on measuring code-generation accuracy.",
      "D": "Dynamic benchmarks rely on manual difficulty calibration by expert raters, while contamination detectors automatically adjust benchmark difficulty to match model performance."
    },
    "answer": "A",
    "explanation": "Dynamic benchmarks actively refresh or rotate problems to avoid train/test overlap, whereas contamination detectors only measure how much of the evaluation set appears in the model\u2019s training data.",
    "question_token_count": 29,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 6,
    "avg_answer_token_count": 25
  },
  {
    "context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n",
    "topic": "Define and explain the scalability criterion in dynamic benchmarking, highlighting its objectives and significance.",
    "response": "<document_analysis>\nThe provided context defines the scalability criterion in dynamic benchmarking as the expectation over the transformation space of the ratio between the size of the generated (transformed) dataset and the cost of producing it. It emphasizes maximizing the dataset size while minimizing monetary, temporal, or manual costs. The key components are the numerator (additional data generated) and the cost function, and their interplay determines the overall metric. To challenge a domain expert, the question will require applying this definition to compare two cost models (linear vs. quadratic) and deduce which yields better scalability.\n</document_analysis>\n<output_format>\nQuestion: Given the scalability metric S = E_T[(||T(D)|| \u2212 ||D||) / Cost(T)], suppose every transformation doubles the original dataset (so ||T(D)|| \u2212 ||D|| = N for dataset size N). Method A uses Cost_A(T) = c\u00b7N (linear cost), while Method B uses Cost_B(T) = c\u00b7N^2 (quadratic cost), with the same constant c. Which method achieves higher scalability and why?\nA: Method A, because its linear cost produces a constant data\u2010per\u2010cost ratio of 1/c, which exceeds Method B\u2019s ratio of 1/(cN).\nB: Method B, because the quadratic cost function rewards larger transformations by amplifying the numerator.\nC: Both methods yield identical scalability since the numerator N and constant c appear in both ratios.\nD: Scalability cannot be determined without knowing the full distribution of transformations and their costs.\nExplanation: Method A\u2019s ratio is N/(cN)=1/c, whereas Method B\u2019s ratio is N/(cN^2)=1/(cN), which is smaller for N>1, so A has higher scalability.\nCorrect Answer: A\n</output_format>",
    "question": "Given the scalability metric S = E_T[(||T(D)|| \u2212 ||D||) / Cost(T)], suppose every transformation doubles the original dataset (so ||T(D)|| \u2212 ||D|| = N for dataset size N). Method A uses Cost_A(T) = c\u00b7N (linear cost), while Method B uses Cost_B(T) = c\u00b7N^2 (quadratic cost), with the same constant c. Which method achieves higher scalability and why?",
    "choices": {
      "A": "Method A, because its linear cost produces a constant data\u2010per\u2010cost ratio of 1/c, which exceeds Method B\u2019s ratio of 1/(cN).",
      "B": "Method B, because the quadratic cost function rewards larger transformations by amplifying the numerator.",
      "C": "Both methods yield identical scalability since the numerator N and constant c appear in both ratios.",
      "D": "Scalability cannot be determined without knowing the full distribution of transformations and their costs."
    },
    "answer": "A",
    "explanation": "Method A\u2019s ratio is N/(cN)=1/c, whereas Method B\u2019s ratio is N/(cN^2)=1/(cN), which is smaller for N>1, so A has higher scalability.",
    "question_token_count": 95,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 22
  },
  {
    "context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n",
    "topic": "Formulate potential extensions or hybrid benchmarks that combine coding, instruction following, and reasoning tasks to evaluate holistic model competence.",
    "response": "<document_analysis>\nThe context outlines separate benchmark categories\u2014coding (e.g., HumanEval, Codeforces), instruction following (e.g., IFEval, InfoBench), and reasoning (e.g., PIQA, ARC). A truly holistic evaluation would need to blend these competencies. The question should probe understanding of how to design such a hybrid benchmark by requiring knowledge of dynamic problem solving, interactive instructions, and commonsense/inferential reasoning. It must challenge experts to identify the critical characteristic that integrates these three aspects effectively.\n</document_analysis>\n\n<output_format>\nQuestion: Which feature is most essential in a hybrid benchmark designed to assess a model\u2019s ability to generate or debug code, follow complex instructions, and apply commonsense reasoning in a unified task?\nA: A static set of coding puzzles paired with multiple-choice reasoning questions.\nB: Step-by-step textual instructions that guide code editing without branching based on reasoning outcomes.\nC: An interactive environment where subsequent coding prompts and instructions adapt based on the model\u2019s reasoning about real-world scenarios.\nD: Isolated code-synthesis challenges with predefined correct outputs and no external context.\nExplanation: Only an adaptive, interactive environment can seamlessly integrate dynamic coding, instruction following, and reasoning by branching prompts based on the model\u2019s reasoning.\nCorrect Answer: C\n</output_format>",
    "question": "Which feature is most essential in a hybrid benchmark designed to assess a model\u2019s ability to generate or debug code, follow complex instructions, and apply commonsense reasoning in a unified task?",
    "choices": {
      "A": "A static set of coding puzzles paired with multiple-choice reasoning questions.",
      "B": "Step-by-step textual instructions that guide code editing without branching based on reasoning outcomes.",
      "C": "An interactive environment where subsequent coding prompts and instructions adapt based on the model\u2019s reasoning about real-world scenarios.",
      "D": "Isolated code-synthesis challenges with predefined correct outputs and no external context."
    },
    "answer": "C",
    "explanation": "Only an adaptive, interactive environment can seamlessly integrate dynamic coding, instruction following, and reasoning by branching prompts based on the model\u2019s reasoning.",
    "question_token_count": 37,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 2,
    "question_difficulty_score": 7,
    "question_groundedness_score": 2,
    "avg_answer_token_count": 17
  },
  {
    "context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n",
    "topic": "Examine how large-scale web-scraped corpora such as FineWeb contribute to evaluation data overlap and propose criteria to assess their contamination potential.",
    "response": "<document_analysis>\nThe context discusses the challenge of evaluation data contamination arising from large-scale web-scraped pre\u2010training corpora (like FineWeb) and the limitations of current detection methods due to scale, opacity, and fine\u2010tuning overlaps. Key points include (1) risk of direct overlap with benchmarks, (2) sheer scale and complexity hindering complete exclusion, (3) opacity of proprietary datasets, and (4) existence but insufficiency of retrieval\u2010based detection. A deep\u2010expert question can probe quantitative criteria that most directly capture overlap potential under these constraints.\n</document_analysis>\n\n<output_format>\nQuestion: When assessing the contamination potential of a massive web\u2010scraped pre\u2010training corpus (e.g., FineWeb) against proprietary evaluation benchmarks\u2014given the corpus\u2019s scale, complexity, and opaque provenance\u2014which quantitative criterion most directly estimates the likelihood of direct text overlap?\nA: Shannon entropy of domain distribution within the scraped dataset\nB: Proportion of synthetic fine\u2010tuning examples formatted like benchmark tasks\nC: Sample\u2010based MinHash similarity estimates between shingled subsets of the corpus and benchmark texts\nD: Transparency score of data provenance derived from archived metadata\nExplanation: MinHash similarity on shingled document subsets provides an approximate yet scalable measure of actual text overlap, directly capturing contamination risk; the other metrics gauge indirect or metadata\u2010level aspects.\nCorrect Answer: C\n</output_format>",
    "question": "When assessing the contamination potential of a massive web\u2010scraped pre\u2010training corpus (e.g., FineWeb) against proprietary evaluation benchmarks\u2014given the corpus\u2019s scale, complexity, and opaque provenance\u2014which quantitative criterion most directly estimates the likelihood of direct text overlap?",
    "choices": {
      "A": "Shannon entropy of domain distribution within the scraped dataset",
      "B": "Proportion of synthetic fine\u2010tuning examples formatted like benchmark tasks",
      "C": "Sample\u2010based MinHash similarity estimates between shingled subsets of the corpus and benchmark texts",
      "D": "Transparency score of data provenance derived from archived metadata"
    },
    "answer": "C",
    "explanation": "MinHash similarity on shingled document subsets provides an approximate yet scalable measure of actual text overlap, directly capturing contamination risk; the other metrics gauge indirect or metadata\u2010level aspects.",
    "question_token_count": 54,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 9,
    "question_groundedness_score": 4,
    "avg_answer_token_count": 13
  },
  {
    "context": "###### Safety\nSafety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content. Datasets such as RealToxicityPrompts\u00a0(Gehman et\u00a0al., 2020) and ToxiGen\u00a0(Hartvigsen et\u00a0al., 2022) assess resilience against producing harmful outputs.\n\nBy providing a controlled environment to measure these aspects, safety benchmarks play a critical role in guiding the development of models that are not only powerful outputs but also responsible and trustworthy for real-world applications.\n\n\n###### Language\nLanguage benchmarks assess the LLMs\u2019 proficiency in specific languages.\n\nGLUE\u00a0(Wang, 2018) and SuperGLUE\u00a0(Wang et\u00a0al., 2019) cover tasks from sentiment analysis to language inference, while CLUE\u00a0(Xu et\u00a0al., 2020) targets Chinese language.\n\nTypo-fixing\u00a0(Suzgun et\u00a0al., 2022) is also widely used.\n\n\n###### Reading Comprehension\nReading comprehension tasks test a model\u2019s ability to extract and infer information from text. Benchmarks like SQuAD\u00a0(Rajpurkar et\u00a0al., 2018), QuAC\u00a0(Choi et\u00a0al., 2018), and BoolQ\u00a0(Clark et\u00a0al., 2019) challenge models to understand passages and draw logical conclusions.\n",
    "topic": "Techniques for testing information extraction versus inference in reading comprehension",
    "response": "<document_analysis>\nI reviewed the context, which outlines three categories of benchmarks\u2014safety, language, and reading comprehension\u2014and lists specific datasets under each. For reading comprehension, it distinguishes between tasks requiring direct extraction of text spans (e.g., SQuAD, QuAC) versus those requiring logical inference to answer yes/no questions (e.g., BoolQ). I will formulate a question that probes the key methodological distinction between extraction-focused and inference-focused reading comprehension benchmarks. This question challenges the expert to identify which benchmark assesses implicit inference rather than surface-level span extraction.\n</document_analysis>\n\n<output_format>\nQuestion: Which reading comprehension benchmark is specifically designed to evaluate an LLM\u2019s ability to draw binary inferences from passages rather than merely extract answer spans?\nA: SQuAD\nB: QuAC\nC: BoolQ\nD: CLUE\nExplanation: BoolQ challenges models to answer yes/no questions requiring inference beyond explicit text spans, whereas SQuAD and QuAC focus on span extraction and CLUE targets Chinese language tasks.\nCorrect Answer: C\n</output_format>",
    "question": "Which reading comprehension benchmark is specifically designed to evaluate an LLM\u2019s ability to draw binary inferences from passages rather than merely extract answer spans?",
    "choices": {
      "A": "SQuAD",
      "B": "QuAC",
      "C": "BoolQ",
      "D": "CLUE"
    },
    "answer": "C",
    "explanation": "BoolQ challenges models to answer yes/no questions requiring inference beyond explicit text spans, whereas SQuAD and QuAC focus on span extraction and CLUE targets Chinese language tasks.",
    "question_token_count": 27,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 3
  },
  {
    "context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n",
    "topic": "The design principles and objective metrics of S3Eval\u2019s random SQL table generation and query execution in assessing LLM reasoning accuracy.",
    "response": "<document_analysis>\nThoughtful Content Examination:\nThe context briefly describes S3Eval\u2019s methodology: generating random SQL tables and assessing LLMs by measuring their accuracy in executing random SQL queries. This establishes two core design principles: random input generation and execution-based accuracy as an objective metric.\n\nConcept Exploration:\nThe randomization strategy minimizes dataset-specific bias and overfitting, while execution accuracy provides a clear, quantifiable measure of semantic reasoning beyond mere query syntax. Contrasting with static benchmarks, this approach enhances difficulty control and coverage of SQL constructs.\n\nStrategic Complexity Calibration:\nGiven the succinct context, the question focuses on synthesizing why randomness and execution accuracy create a robust evaluation. Difficulty rated 8/10: it probes understanding of evaluation methodology, bias prevention, and objective metrics in LLM benchmarking.\n\nIntentional Question Planning:\nThe question invites experts to reflect on the advantages of random input generation and execution-based evaluation over traditional static benchmarks, requiring deep comprehension of evaluation design and potential pitfalls in LLM assessment.\n</document_analysis>\n\n<output_format>\nQuestion: Why does S3Eval leverage random SQL table generation coupled with execution accuracy metrics to evaluate large language model reasoning more objectively than static, human-curated benchmarks?\nA: It prevents overfitting by ensuring test data is unknown to the model, providing unbiased difficulty variations and comprehensive coverage of SQL constructs.\nB: It allows human experts to manually label correct query results for each scenario, thereby improving metric precision.\nC: It focuses solely on testing SQL syntax correctness rather than semantic comprehension, thus simplifying the evaluation process.\nD: It reduces computational overhead by minimizing the total number of queries needed for a statistically reliable assessment.\nExplanation: Random table and query generation prevents dataset familiarity, ensuring unbiased difficulty and broad construct coverage, while execution accuracy directly measures semantic reasoning.\nCorrect Answer: A\n</output_format>",
    "question": "Why does S3Eval leverage random SQL table generation coupled with execution accuracy metrics to evaluate large language model reasoning more objectively than static, human-curated benchmarks?",
    "choices": {
      "A": "It prevents overfitting by ensuring test data is unknown to the model, providing unbiased difficulty variations and comprehensive coverage of SQL constructs.",
      "B": "It allows human experts to manually label correct query results for each scenario, thereby improving metric precision.",
      "C": "It focuses solely on testing SQL syntax correctness rather than semantic comprehension, thus simplifying the evaluation process.",
      "D": "It reduces computational overhead by minimizing the total number of queries needed for a statistically reliable assessment."
    },
    "answer": "A",
    "explanation": "Random table and query generation prevents dataset familiarity, ensuring unbiased difficulty and broad construct coverage, while execution accuracy directly measures semantic reasoning.",
    "question_token_count": 32,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 4,
    "avg_answer_token_count": 21
  },
  {
    "context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n",
    "topic": "Propose a set of evaluation metrics to measure sample diversity, difficulty retention, and contamination resistance in rewritten benchmarks.",
    "response": "<document_analysis>\nThe context outlines the challenges and existing methods for rewriting benchmark samples with LLMs, focusing on three key evaluation dimensions: sample diversity, difficulty retention, and contamination resistance. It mentions specific techniques (e.g., variable substitution, knowledge-graph guided expansion, use of a contamination detector) but does not specify concrete metrics. A domain expert would need to synthesize appropriate measures: \n1. For diversity: move beyond simple lexical measures to semantic/embedding-based assessments (e.g., cluster entropy). \n2. For difficulty retention: track model performance consistency (e.g., delta accuracy on benchmark tasks). \n3. For contamination resistance: quantify reduction in detected contaminated samples (e.g., drop in contamination-detector positive rate). \nThe planned question asks which composite metric set best aligns with these dimensions, testing deep understanding of metric design beyond surface similarity scores.\nDifficulty rating: 8/10 \u2013 requires integration of evaluation theory and practical contamination considerations.\n</document_analysis>\n\n<output_format>\nQuestion: Which combination of evaluation metrics most comprehensively assesses sample diversity, difficulty retention, and contamination resistance in a rewritten benchmark?\nA: Type\u2011token ratio for lexical variation, edit\u2011distance distribution against originals, and n\u2011gram overlap rate with training data\nB: Embedding\u2011based clustering entropy for semantic diversity, delta in model accuracy on original tasks for difficulty retention, and reduction in contamination\u2011detector positive rate\nC: BLEU score against original samples, change in language model perplexity, and overall dataset redundancy rate\nD: Shannon entropy over surface vocabulary, increase in human annotation time for difficulty proxy, and Jaccard similarity of answer sets\nExplanation: Option B pairs a semantic-diversity metric (cluster entropy), a performance-based difficulty measure (accuracy delta), and a direct contamination-resistance indicator (drop in detector positives), covering all three dimensions most effectively.\nCorrect Answer: B\n</output_format>",
    "question": "Which combination of evaluation metrics most comprehensively assesses sample diversity, difficulty retention, and contamination resistance in a rewritten benchmark?",
    "choices": {
      "A": "Type\u2011token ratio for lexical variation, edit\u2011distance distribution against originals, and n\u2011gram overlap rate with training data",
      "B": "Embedding\u2011based clustering entropy for semantic diversity, delta in model accuracy on original tasks for difficulty retention, and reduction in contamination\u2011detector positive rate",
      "C": "BLEU score against original samples, change in language model perplexity, and overall dataset redundancy rate",
      "D": "Shannon entropy over surface vocabulary, increase in human annotation time for difficulty proxy, and Jaccard similarity of answer sets"
    },
    "answer": "B",
    "explanation": "Option B pairs a semantic-diversity metric (cluster entropy), a performance-based difficulty measure (accuracy delta), and a direct contamination-resistance indicator (drop in detector positives), covering all three dimensions most effectively.",
    "question_token_count": 24,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 4,
    "question_difficulty_score": 8,
    "question_groundedness_score": 5,
    "avg_answer_token_count": 25
  },
  {
    "context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n",
    "topic": "Assess how contaminated benchmarks distort assessments of an LLM\u2019s generalization, robustness, and real\u2010world applicability.",
    "response": "<document_analysis>\nThe context discusses \u201csyntactic contamination\u201d in LLM benchmarks, where test examples are simply rephrased versions of training data (e.g., with added prefixes), making it difficult to distinguish true reasoning from memorized recall. It emphasizes that this type of contamination can artificially boost performance metrics, undermining the validity of evaluations of generalization, robustness, and real-world applicability. Key nuances include ongoing debates about whether such transformations truly constitute contamination and the specific risk that superficial syntactic similarity poses by retriggering memorized responses rather than testing novel inference. Given these subtleties, a challenging question can probe the precise mechanism by which syntactic contamination leads to inflated assessments of LLM capabilities.\nDifficulty: 8/10\n</document_analysis>\n\n<output_format>\nQuestion: Which mechanism best explains why syntactic contamination, such as rephrasing training instances with prefixes, can lead to inflated assessments of an LLM\u2019s inference abilities?\nA: It changes semantic content to test deeper understanding while preserving original syntax.\nB: It leverages superficial lexical or structural similarity to retrigger memorized responses.\nC: It introduces adversarial noise that enhances generalization through data augmentation.\nD: It prevents the model from exploiting memorized data by altering token frequency distributions.\nExplanation: Minor rephrasings maintain enough overlap to let the model recall memorized answers rather than perform novel reasoning.\nCorrect Answer: B\n</output_format>",
    "question": "Which mechanism best explains why syntactic contamination, such as rephrasing training instances with prefixes, can lead to inflated assessments of an LLM\u2019s inference abilities?",
    "choices": {
      "A": "It changes semantic content to test deeper understanding while preserving original syntax.",
      "B": "It leverages superficial lexical or structural similarity to retrigger memorized responses.",
      "C": "It introduces adversarial noise that enhances generalization through data augmentation.",
      "D": "It prevents the model from exploiting memorized data by altering token frequency distributions."
    },
    "answer": "B",
    "explanation": "Minor rephrasings maintain enough overlap to let the model recall memorized answers rather than perform novel reasoning.",
    "question_token_count": 32,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 14
  },
  {
    "context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n",
    "topic": "Reflect on the ethical implications of intentionally releasing or withholding benchmark data to influence LLM evaluation outcomes.",
    "response": "<document_analysis>\nThe context describes the use of canary strings in benchmarks to detect data contamination and notes that if developers intentionally leak benchmark data (to boost LLM scores), canary strings lose effectiveness. This raises the ethical issue of manipulating model evaluation by controlling benchmark data. Relevant themes include transparency, reproducibility, scientific integrity, and deceptive practices in LLM benchmarking. A high-difficulty question can probe the expert\u2019s understanding of how selective release or withholding of canary-embedded benchmarks undermines core ethical principles of research evaluation.\n</document_analysis>\n\n<output_format>\nQuestion: Which ethical concern arises most directly when benchmark datasets embed canary strings and model developers selectively withhold or leak these benchmarks to artificially inflate LLM performance?\nA: Erosion of scientific reproducibility\nB: Invasion of model user privacy\nC: Compromise of data confidentiality owed to participants\nD: Reduction in algorithmic efficiency\nExplanation: Selectively manipulating benchmark access undermines the ability of others to reproduce and validate evaluation results.\nCorrect Answer: A\n</output_format>",
    "question": "Which ethical concern arises most directly when benchmark datasets embed canary strings and model developers selectively withhold or leak these benchmarks to artificially inflate LLM performance?",
    "choices": {
      "A": "Erosion of scientific reproducibility",
      "B": "Invasion of model user privacy",
      "C": "Compromise of data confidentiality owed to participants",
      "D": "Reduction in algorithmic efficiency"
    },
    "answer": "A",
    "explanation": "Selectively manipulating benchmark access undermines the ability of others to reproduce and validate evaluation results.",
    "question_token_count": 30,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 4,
    "avg_answer_token_count": 6
  },
  {
    "context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n",
    "topic": "Reflect on the broader implications of using rewritten benchmarks for LLM training and performance evaluation in terms of fairness, robustness, and generalization.",
    "response": "<document_analysis>\nThe context outlines several LLM-driven benchmark rewriting methods and their distinct mechanisms: Auto-Dataset\u2019s style-preserving and cognitive-level variation, StructEval\u2019s concept expansion via knowledge graphs, ITD\u2019s use of a contamination detector with difficulty preservation, and VarBench\u2019s variable replacement. A deep comparison requires pinpointing which pipeline directly targets contamination while also assessing the residual fairness risk from LLM paraphrasing biases. The question probes both factual recall of the contamination mitigation step and critical insight into how LLM-driven rewriting can still perpetuate stylistic or dialectal biases, affecting fairness across linguistic groups. Difficulty is rated 8/10 due to the two-part reasoning: method identification and nuanced fairness implication.\n</document_analysis>\n\n<output_format>\nQuestion: When aiming to maximize both robustness against data contamination and the preservation of task difficulty, which rewriting framework incorporates a dedicated contamination filter, and why might its approach inadvertently undermine fairness across linguistic variants?\nA: Auto-Dataset, because it replicates cognitive levels but overlooks contamination risks.\nB: StructEval, since it expands concepts via knowledge graphs but ignores distribution overlap.\nC: ITD, as it employs a contamination detector to identify and rewrite problematic samples, yet may perpetuate systematic paraphrase biases affecting non-dominant dialects.\nD: VarBench, though it diversifies variables, lacks contamination mitigation and may skew difficulty balance.\nExplanation: ITD uniquely integrates a contamination detector to filter and rewrite samples without altering difficulty, but its paraphrasing can embed stylistic biases that harm fairness for underrepresented dialects.\nCorrect Answer: C\n</output_format>",
    "question": "When aiming to maximize both robustness against data contamination and the preservation of task difficulty, which rewriting framework incorporates a dedicated contamination filter, and why might its approach inadvertently undermine fairness across linguistic variants?",
    "choices": {
      "A": "Auto-Dataset, because it replicates cognitive levels but overlooks contamination risks.",
      "B": "StructEval, since it expands concepts via knowledge graphs but ignores distribution overlap.",
      "C": "ITD, as it employs a contamination detector to identify and rewrite problematic samples, yet may perpetuate systematic paraphrase biases affecting non-dominant dialects.",
      "D": "VarBench, though it diversifies variables, lacks contamination mitigation and may skew difficulty balance."
    },
    "answer": "C",
    "explanation": "ITD uniquely integrates a contamination detector to filter and rewrite samples without altering difficulty, but its paraphrasing can embed stylistic biases that harm fairness for underrepresented dialects.",
    "question_token_count": 38,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 1,
    "question_difficulty_score": 10,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 20
  },
  {
    "context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n",
    "topic": "Assess trade\u2011offs between maximizing external diversity and maintaining semantic consistency with the original dataset.",
    "response": "<document_analysis>\nThe context defines external diversity as the expected distance \u0398(D_i, D) between transformed datasets and the original, with \u0398 often instantiated by n\u2011gram or BLEU metrics. It implies a trade\u2011off: maximizing such diversity may distort semantic content despite high surface variation. Experts must understand that n\u2011gram\u2010based measures fail to capture meaning and thus can drive semantic drift, and they should know of embedding\u2011based metrics (e.g., BERTScore) that better preserve semantics. The question will probe this nuance and mitigation strategy.\n</document_analysis>\n<output_format>\nQuestion: When using BLEU as \u0398 to maximize external diversity, what unintended semantic trade\u2011off typically occurs, and which complementary metric most directly mitigates this issue?\nA: Over\u2010paraphrasing leading to loss of fluency; mitigate with perplexity.\nB: Token\u2010level swaps without semantic slack; mitigate with ROUGE.\nC: Semantic drift away from original meaning; mitigate with BERTScore.\nD: Reduced variation across trials; mitigate with Jaccard similarity.\nExplanation: BLEU optimizes n\u2011gram differences but neglects meaning, causing semantic drift; BERTScore uses contextual embeddings to better ensure semantic consistency.\nCorrect Answer: C\n</output_format>",
    "question": "When using BLEU as \u0398 to maximize external diversity, what unintended semantic trade\u2011off typically occurs, and which complementary metric most directly mitigates this issue?",
    "choices": {
      "A": "Over\u2010paraphrasing leading to loss of fluency; mitigate with perplexity.",
      "B": "Token\u2010level swaps without semantic slack; mitigate with ROUGE.",
      "C": "Semantic drift away from original meaning; mitigate with BERTScore.",
      "D": "Reduced variation across trials; mitigate with Jaccard similarity."
    },
    "answer": "C",
    "explanation": "BLEU optimizes n\u2011gram differences but neglects meaning, causing semantic drift; BERTScore uses contextual embeddings to better ensure semantic consistency.",
    "question_token_count": 32,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 4,
    "question_difficulty_score": 8,
    "question_groundedness_score": 6,
    "avg_answer_token_count": 14
  },
  {
    "context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n",
    "topic": "Design considerations and obstacles in establishing standardized dynamic evaluation frameworks for contamination assessment.",
    "response": "<document_analysis>\nThe conclusion emphasizes the promise of dynamic evaluation for contamination assessment but highlights core challenges in reliability and reproducibility when datasets are continually updated. The limitations note that proposed criteria are preliminary, lacking fine\u2010grained validation, and that rapid evolution of LLMs further complicates consistent benchmarking. Key design considerations therefore center on how to preserve reproducibility (i.e., exact replication of evaluation conditions) while allowing dynamic data freshness. A high\u2010difficulty question can probe the trade\u2010offs and specific mechanisms (e.g., version control of datasets) that address this tension.\n</document_analysis>\n\n<output_format>\nQuestion: When standardizing a dynamic evaluation framework for LLM contamination assessment, which design element most directly ensures both dataset freshness and exact reproducibility across successive benchmark runs?\nA: Automating real\u2010time label inference via active learning pipelines  \nB: Leveraging versioned data artifacts with immutable identifiers  \nC: Defining contamination criteria solely at a high conceptual level  \nD: Utilizing stochastic data augmentation to simulate novel contamination cases  \nExplanation: Versioned data artifacts with immutable identifiers allow continuous updates (freshness) while preserving fixed snapshots for reproducibility.  \nCorrect Answer: B\n</output_format>",
    "question": "When standardizing a dynamic evaluation framework for LLM contamination assessment, which design element most directly ensures both dataset freshness and exact reproducibility across successive benchmark runs?",
    "choices": {
      "A": "Automating real\u2010time label inference via active learning pipelines",
      "B": "Leveraging versioned data artifacts with immutable identifiers",
      "C": "Defining contamination criteria solely at a high conceptual level",
      "D": "Utilizing stochastic data augmentation to simulate novel contamination cases"
    },
    "answer": "B",
    "explanation": "Versioned data artifacts with immutable identifiers allow continuous updates (freshness) while preserving fixed snapshots for reproducibility.",
    "question_token_count": 30,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 9,
    "question_groundedness_score": 2,
    "avg_answer_token_count": 11
  },
  {
    "context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Interpret the notation (\u25cf, \u25d0, \u25cb) used in Table\u00a03 to assess the quality of existing dynamic benchmarks against summarized criteria.",
    "response": "<document_analysis>\nThe context defines dynamic benchmarking and references Table\u00a03, which evaluates existing dynamic benchmarks against summarized criteria using three symbols: \u25cf, \u25d0, and \u25cb. It explicitly states that \u25cf represents support, \u25d0 represents partial support, and \u25cb represents no support. This notation is critical for quickly assessing whether a benchmark fully, partially, or fails to meet each criterion. The content is coherent and meaningful for generating a factual multiple-choice question about interpreting these symbols.\n</document_analysis>\n\n<output_format>\nQuestion: In Table\u00a03\u2019s evaluation of dynamic benchmarks, what do the symbols \u25cf, \u25d0, and \u25cb denote with respect to each summarized criterion?\nA: Full support, partial support, and no support  \nB: No support, partial support, and full support  \nC: Mandatory requirement, optional feature, and prohibited element  \nD: High priority, medium priority, and low priority  \nExplanation: The text specifies that \u25cf indicates support, \u25d0 indicates partial support, and \u25cb indicates no support.  \nCorrect Answer: A\n</output_format>",
    "question": "In Table\u00a03\u2019s evaluation of dynamic benchmarks, what do the symbols \u25cf, \u25d0, and \u25cb denote with respect to each summarized criterion?",
    "choices": {
      "A": "Full support, partial support, and no support",
      "B": "No support, partial support, and full support",
      "C": "Mandatory requirement, optional feature, and prohibited element",
      "D": "High priority, medium priority, and low priority"
    },
    "answer": "A",
    "explanation": "The text specifies that \u25cf indicates support, \u25d0 indicates partial support, and \u25cb indicates no support.",
    "question_token_count": 30,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 9,
    "question_difficulty_score": 2,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 10
  },
  {
    "context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n",
    "topic": "Effects of continuously growing training datasets on the robustness of static contamination detection methods.",
    "response": "<document_analysis>\nThe context highlights that static benchmarking methods remain consistent but become increasingly vulnerable to data contamination as training datasets grow. This vulnerability stems from the fixed nature of static test sets: as training corpora expand, previously unseen evaluation examples may be included in the training data, undermining the validity of contamination detection. The key relationship is between dataset growth and loss of novelty in static test sets. For a domain expert question, probing the mechanism driving this increased vulnerability\u2014namely, fixed benchmarks losing representativeness due to overlap\u2014will challenge nuanced understanding of contamination dynamics.\n</document_analysis>\n\n<output_format>\nQuestion: What is the primary reason that static contamination detection methods for language model benchmarks become more vulnerable as training datasets continuously expand?\nA: Fixed evaluation sets increasingly overlap with the growing training data, reducing their novelty.\nB: Static methods lack standardized protocols, causing inconsistent contamination thresholds.\nC: Growing datasets force static methods to consume more computational resources, leading to scaling failures.\nD: Static approaches cannot adapt to new model architectures, making them obsolete over time.\nExplanation: As training corpora grow, previously unseen test examples in a fixed benchmark are more likely to appear in the training data, compromising the test\u2019s novelty and allowing contamination to go undetected.\nCorrect Answer: A\n</output_format>",
    "question": "What is the primary reason that static contamination detection methods for language model benchmarks become more vulnerable as training datasets continuously expand?",
    "choices": {
      "A": "Fixed evaluation sets increasingly overlap with the growing training data, reducing their novelty.",
      "B": "Static methods lack standardized protocols, causing inconsistent contamination thresholds.",
      "C": "Growing datasets force static methods to consume more computational resources, leading to scaling failures.",
      "D": "Static approaches cannot adapt to new model architectures, making them obsolete over time."
    },
    "answer": "A",
    "explanation": "As training corpora grow, previously unseen test examples in a fixed benchmark are more likely to appear in the training data, compromising the test\u2019s novelty and allowing contamination to go undetected.",
    "question_token_count": 24,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 15
  },
  {
    "context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n",
    "topic": "Evaluate strategies to detect and mitigate perpetuated biases in benchmark datasets originating from historically biased data sources.",
    "response": "<document_analysis>\nThe context discusses ethical challenges in LLM evaluation benchmarks, contrasting static benchmarks (which risk perpetuating outdated biases) with dynamic benchmarks (which raise privacy and security concerns). It emphasizes the need to design benchmarking frameworks that uphold fairness, accountability, and privacy, preventing harm or disadvantage to user groups. Key themes include bias mitigation, transparency, and responsible data updating.\nDifficulty: 7/10 \u2013 requires understanding of both bias mitigation and privacy-preserving techniques in dynamic evaluation pipelines.\nPlanned question: Ask which strategy best balances bias prevention, privacy, and accountability in benchmark design.\n</document_analysis>\n\n<output_format>\nQuestion: In constructing evaluation benchmarks for large language models that avoid perpetuating historical biases while also upholding privacy and accountability, which strategy most effectively addresses both concerns?\nA: Rely exclusively on static benchmarks with periodic manual curation to remove biased examples.\nB: Implement dynamic benchmarks with integrated differential privacy mechanisms and transparent governance of data provenance.\nC: Generate and use only synthetic datasets to eliminate any traces of historical bias.\nD: Update benchmarks dynamically without privacy safeguards but provide detailed performance reports to stakeholders.\nExplanation: Dynamic updates combined with differential privacy and transparent provenance directly mitigate bias perpetuation while protecting user data and ensuring accountability.\nCorrect Answer: B\n</output_format>",
    "question": "In constructing evaluation benchmarks for large language models that avoid perpetuating historical biases while also upholding privacy and accountability, which strategy most effectively addresses both concerns?",
    "choices": {
      "A": "Rely exclusively on static benchmarks with periodic manual curation to remove biased examples.",
      "B": "Implement dynamic benchmarks with integrated differential privacy mechanisms and transparent governance of data provenance.",
      "C": "Generate and use only synthetic datasets to eliminate any traces of historical bias.",
      "D": "Update benchmarks dynamically without privacy safeguards but provide detailed performance reports to stakeholders."
    },
    "answer": "B",
    "explanation": "Dynamic updates combined with differential privacy and transparent provenance directly mitigate bias perpetuation while protecting user data and ensuring accountability.",
    "question_token_count": 32,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 4,
    "avg_answer_token_count": 15
  },
  {
    "context": "###### Safety\nSafety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content. Datasets such as RealToxicityPrompts\u00a0(Gehman et\u00a0al., 2020) and ToxiGen\u00a0(Hartvigsen et\u00a0al., 2022) assess resilience against producing harmful outputs.\n\nBy providing a controlled environment to measure these aspects, safety benchmarks play a critical role in guiding the development of models that are not only powerful outputs but also responsible and trustworthy for real-world applications.\n\n\n###### Language\nLanguage benchmarks assess the LLMs\u2019 proficiency in specific languages.\n\nGLUE\u00a0(Wang, 2018) and SuperGLUE\u00a0(Wang et\u00a0al., 2019) cover tasks from sentiment analysis to language inference, while CLUE\u00a0(Xu et\u00a0al., 2020) targets Chinese language.\n\nTypo-fixing\u00a0(Suzgun et\u00a0al., 2022) is also widely used.\n\n\n###### Reading Comprehension\nReading comprehension tasks test a model\u2019s ability to extract and infer information from text. Benchmarks like SQuAD\u00a0(Rajpurkar et\u00a0al., 2018), QuAC\u00a0(Choi et\u00a0al., 2018), and BoolQ\u00a0(Clark et\u00a0al., 2019) challenge models to understand passages and draw logical conclusions.\n",
    "topic": "The scope and challenges of CLUE in evaluating Chinese language proficiency",
    "response": "<document_analysis>\nThe context distinguishes CLUE as a Chinese-language benchmark analogous to GLUE but highlights that language-specific benchmarks must address unique linguistic properties. For Chinese, challenges such as word segmentation, pro-drop pronouns, and absence of inflectional morphology are central. To probe expert understanding, the question will focus on identifying a phenomenon that CLUE must handle in NLI tasks but GLUE does not, namely the pro-drop characteristic of Chinese. This pushes beyond basic recall, requiring knowledge of Chinese grammar and its implications for inference benchmarks.\nDifficulty: 8/10\n</document_analysis>\n\n<output_format>\nQuestion: In designing natural language inference tasks for CLUE, which Chinese-specific linguistic phenomenon must be explicitly handled to avoid misinterpretation of entailment relations, but is not encountered in English-based GLUE tasks?\nA: Pro-drop leading to omitted subjects and objects in clauses\nB: Rich inflectional morphology encoding tense and aspect on verbs\nC: Obligatory grammatical gender agreement on adjectives\nD: Case marking of nouns with postpositional particles\nExplanation: Chinese frequently omits pronouns (pro-drop), requiring models to infer implicit subjects or objects, a challenge absent in English GLUE benchmarks.\nCorrect Answer: A\n</output_format>",
    "question": "In designing natural language inference tasks for CLUE, which Chinese-specific linguistic phenomenon must be explicitly handled to avoid misinterpretation of entailment relations, but is not encountered in English-based GLUE tasks?",
    "choices": {
      "A": "Pro-drop leading to omitted subjects and objects in clauses",
      "B": "Rich inflectional morphology encoding tense and aspect on verbs",
      "C": "Obligatory grammatical gender agreement on adjectives",
      "D": "Case marking of nouns with postpositional particles"
    },
    "answer": "A",
    "explanation": "Chinese frequently omits pronouns (pro-drop), requiring models to infer implicit subjects or objects, a challenge absent in English GLUE benchmarks.",
    "question_token_count": 41,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 4,
    "avg_answer_token_count": 10
  },
  {
    "context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n",
    "topic": "Jacovi et al. (2023)\u2019s approach to test\u2010data encryption using public\u2011key infrastructure combined with a \u201cNo Derivatives\u201d license to block automated crawling and reuse",
    "response": "<document_analysis>\nThe context describes Jacovi et al. (2023)\u2019s proposal to encrypt test data via public\u2010key encryption paired with a \u201cNo Derivatives\u201d license to block automated crawling and reuse, noting a key vulnerability: if the private key is exposed, data leakage occurs. TRUCE is introduced as a more robust alternative leveraging confidential computing and secure multi\u2010party computation (MPC) that prevents single\u2010point key compromise. The mention of MPC directly addresses the limitation of Jacovi et al.\u2019s reliance on a single private key. This semantic relationship between encryption\u2010based protection and MPC approaches provides fertile ground for a challenging question that probes understanding of threat models and cryptographic defenses.\n</document_analysis>\n\n<output_format>\nQuestion: Which feature of TRUCE\u2019s secure multi\u2011party computation approach most directly mitigates the private\u2011key exposure vulnerability inherent in using a single public\u2011key encryption scheme for test data protection?\nA: Continuous rotation of a single private key to reduce the window for compromise.\nB: Distribution of decryption capabilities across multiple parties so that no single compromised key reveals the entire test set.\nC: Automatic legal revocation of encryption rights upon detection of unauthorized access.\nD: Switch from public\u2011key to symmetric encryption to avoid public\u2011key management complexities.\nExplanation: Secure multi\u2011party computation distributes decryption shares among parties, eliminating a single private\u2011key point of failure.\nCorrect Answer: B\n</output_format>",
    "question": "Which feature of TRUCE\u2019s secure multi\u2011party computation approach most directly mitigates the private\u2011key exposure vulnerability inherent in using a single public\u2011key encryption scheme for test data protection?",
    "choices": {
      "A": "Continuous rotation of a single private key to reduce the window for compromise.",
      "B": "Distribution of decryption capabilities across multiple parties so that no single compromised key reveals the entire test set.",
      "C": "Automatic legal revocation of encryption rights upon detection of unauthorized access.",
      "D": "Switch from public\u2011key to symmetric encryption to avoid public\u2011key management complexities."
    },
    "answer": "B",
    "explanation": "Secure multi\u2011party computation distributes decryption shares among parties, eliminating a single private\u2011key point of failure.",
    "question_token_count": 38,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 4,
    "avg_answer_token_count": 16
  },
  {
    "context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n",
    "topic": "Computational overheads introduced by encryption and secure computation techniques, and strategies for balancing performance with security",
    "response": "<document_analysis>\nThe context discusses two main approaches to secure model evaluation data: encryption-based methods (including public\u2010key encryption, confidential computing, and secure multi\u2010party computation) and label protection (withholding true answers). Encryption-based approaches ensure input confidentiality but incur extra cryptographic overhead and require robust key management; label protection avoids such overheads by only hiding labels. The contrast between runtime/latency costs and security guarantees is the core theme. A deep question for experts would probe the specific performance-security trade-off that motivates choosing label protection over encryption-based schemes in high-throughput benchmarking.\n</document_analysis>\n\n<output_format>\nQuestion: In high-performance model benchmarking where test\u2010set confidentiality is required, why might a designer opt for withholding labels rather than encrypting test data under public keys and secure computation frameworks?\nA: Because public\u2010key encryption and secure computation introduce significant runtime latency from cryptographic operations, whereas label withholding does not.\nB: Because encrypted datasets consume far more memory than plaintext tests with hidden labels, making them impractical.\nC: Because encryption schemes cannot defend against minor prompt paraphrasing attacks, which label withholding inherently prevents.\nD: Because managing \u201cNo Derivatives\u201d licensing for encrypted data is more legally complex than hiding answer keys.\nExplanation: Encrypting and securely computing over test inputs adds decryption and protocol overhead, increasing latency, while simply withholding labels incurs no such cryptographic cost.\nCorrect Answer: A\n</output_format>",
    "question": "In high-performance model benchmarking where test\u2010set confidentiality is required, why might a designer opt for withholding labels rather than encrypting test data under public keys and secure computation frameworks?",
    "choices": {
      "A": "Because public\u2010key encryption and secure computation introduce significant runtime latency from cryptographic operations, whereas label withholding does not.",
      "B": "Because encrypted datasets consume far more memory than plaintext tests with hidden labels, making them impractical.",
      "C": "Because encryption schemes cannot defend against minor prompt paraphrasing attacks, which label withholding inherently prevents.",
      "D": "Because managing \u201cNo Derivatives\u201d licensing for encrypted data is more legally complex than hiding answer keys."
    },
    "answer": "A",
    "explanation": "Encrypting and securely computing over test inputs adds decryption and protocol overhead, increasing latency, while simply withholding labels incurs no such cryptographic cost.",
    "question_token_count": 36,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 20
  },
  {
    "context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n",
    "topic": "Design an experimental framework to test the robustness of a proposed complexity measurement function \u03a8(\u22c5) across a series of controlled dataset transformations.",
    "response": "<document_analysis>\nThe context discusses the notion of stability of a complexity measurement function \u03a8(\u22c5), defined in terms of the variance of \u03a8 across repeated trials under dynamic transformations. A robust \u03a8 should show low variance when the transformation does not alter inherent complexity, moderate variance when complexity systematically increases, and high variance when randomness dominates. This insight guides the design of an experiment with three controlled transformation types to probe \u03a8\u2019s sensitivity and noise robustness.\n</document_analysis>\n\n<output_format>\nQuestion: When evaluating the robustness of a complexity function \u03a8 across three controlled dataset transformations\u2014(P) semantic\u2011preserving paraphrase, (S) systematic composition of reasoning steps, and (D) stochastic token dropout\u2014which ordering of var(\u03a8) across P, S, and D indicates that \u03a8 is both insensitive to inconsequential changes and sensitive to true complexity shifts while flagging random noise?\nA: var(P) \u2248 0 < var(S) < var(D)\nB: var(S) < var(P) \u2248 0 < var(D)\nC: var(D) < var(S) < var(P) \u2248 0\nD: var(P) \u2248 0 < var(D) < var(S)\nExplanation: A robust \u03a8 yields near-zero variance under paraphrasing (no complexity change), moderate variance under systematic step composition (controlled complexity increase), and highest variance under random dropout (noise).\nCorrect Answer: A\n</output_format>",
    "question": "When evaluating the robustness of a complexity function \u03a8 across three controlled dataset transformations\u2014(P) semantic\u2011preserving paraphrase, (S) systematic composition of reasoning steps, and (D) stochastic token dropout\u2014which ordering of var(\u03a8) across P, S, and D indicates that \u03a8 is both insensitive to inconsequential changes and sensitive to true complexity shifts while flagging random noise?",
    "choices": {
      "A": "var(P) \u2248 0 < var(S) < var(D)",
      "B": "var(S) < var(P) \u2248 0 < var(D)",
      "C": "var(D) < var(S) < var(P) \u2248 0",
      "D": "var(P) \u2248 0 < var(D) < var(S)"
    },
    "answer": "A",
    "explanation": "A robust \u03a8 yields near-zero variance under paraphrasing (no complexity change), moderate variance under systematic step composition (controlled complexity increase), and highest variance under random dropout (noise).",
    "question_token_count": 79,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 9,
    "question_groundedness_score": 4,
    "avg_answer_token_count": 15
  },
  {
    "context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n",
    "topic": "Key management requirements and best practices in encryption\u2010based data protection, including challenges in distribution, rotation, and revocation of cryptographic keys",
    "response": "<document_analysis>\n- The context emphasizes encryption-based protection of evaluation data, noting reliance on strong key management and vulnerability if a private key is exposed.\n- It references asymmetric encryption using public/private keys and advanced methods like secure multi-party computation.\n- Central challenge: preventing a single point of failure where exposure of the private key compromises all protected data.\n- Implicit best practice: distribute trust (e.g., via threshold cryptography) to mitigate key compromise risk.\n- Difficulty calibration: 8/10, as threshold cryptography is an advanced key management technique not explicitly named but inferable.\n</document_analysis>\n\n<output_format>\nQuestion: Which key management strategy most effectively mitigates the risk posed by a single point of failure due to private\u2011key exposure in encryption\u2011based data protection?\nA: Implementing periodic key rotation combined with automated revocation lists  \nB: Enforcing a \u201cNo Derivatives\u201d license on the encrypted data  \nC: Splitting the private key across multiple parties using threshold cryptography  \nD: Storing the private key in a centralized hardware security module  \nExplanation: Threshold cryptography distributes key shares so that no single compromised component exposes the full private key, directly preventing a single point of failure.  \nCorrect Answer: C\n</output_format>",
    "question": "Which key management strategy most effectively mitigates the risk posed by a single point of failure due to private\u2011key exposure in encryption\u2011based data protection?",
    "choices": {
      "A": "Implementing periodic key rotation combined with automated revocation lists",
      "B": "Enforcing a \u201cNo Derivatives\u201d license on the encrypted data",
      "C": "Splitting the private key across multiple parties using threshold cryptography",
      "D": "Storing the private key in a centralized hardware security module"
    },
    "answer": "C",
    "explanation": "Threshold cryptography distributes key shares so that no single compromised component exposes the full private key, directly preventing a single point of failure.",
    "question_token_count": 31,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 6,
    "avg_answer_token_count": 12
  },
  {
    "context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n",
    "topic": "The impact of label protection on transparency and reproducibility in machine learning evaluation",
    "response": "<document_analysis>\nThe context highlights two key points: (1) label protection restricts transparency and external error analysis, forcing reliance on centralized evaluation and impairing reproducibility; (2) exact n\u2011gram matching for post\u2011hoc train\u2013test overlap detection yields false negatives, leading to the adoption of embedding\u2011based similarity methods. A sophisticated question can probe why embedding\u2011based techniques become indispensable in label\u2011protected settings, integrating both concepts. Difficulty is rated 8/10 for requiring understanding of privacy constraints and contamination detection subtleties.\n</document_analysis>\n\n<output_format>\nQuestion: Why are embedding-based similarity methods preferred over exact n\u2011gram matching for post\u2011hoc contamination detection when labels are protected in a centralized evaluation framework?\nA: They allow researchers to bypass the centralized evaluation server and compute labels locally.\nB: They capture semantic overlaps missed by exact matching, reducing false negatives when ground-truth labels are hidden.\nC: They provide transparent token-level error explanations without needing any overlap detection.\nD: They guarantee perfect contamination detection by eliminating all mapping uncertainties.\nExplanation: Embedding-based methods reduce false negatives by detecting semantic overlap that exact n\u2011gram matching misses, which is critical when labels are concealed in centralized evaluations.\nCorrect Answer: B\n</output_format>",
    "question": "Why are embedding-based similarity methods preferred over exact n\u2011gram matching for post\u2011hoc contamination detection when labels are protected in a centralized evaluation framework?",
    "choices": {
      "A": "They allow researchers to bypass the centralized evaluation server and compute labels locally.",
      "B": "They capture semantic overlaps missed by exact matching, reducing false negatives when ground-truth labels are hidden.",
      "C": "They provide transparent token-level error explanations without needing any overlap detection.",
      "D": "They guarantee perfect contamination detection by eliminating all mapping uncertainties."
    },
    "answer": "B",
    "explanation": "Embedding-based methods reduce false negatives by detecting semantic overlap that exact n\u2011gram matching misses, which is critical when labels are concealed in centralized evaluations.",
    "question_token_count": 30,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 15
  },
  {
    "context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n",
    "topic": "The concept of data contamination in large language model training corpora, its primary sources, and associated risks.",
    "response": "<document_analysis>\nThe context is an abstract discussing data contamination in LLM training, the shift from static to dynamic benchmarking, examination of methods enhancing static benchmarks, the critical unaddressed gap\u2014lack of standardized criteria for dynamic benchmarks\u2014and proposed design principles. No irrelevant or bogus information detected. This content supports generation of a deep, domain-specific question about benchmarking gaps in LLM evaluation. Difficulty is calibrated at 8/10, as the question requires synthesis and precise recall of the survey\u2019s key observation.\n</document_analysis>\n\n<output_format>\nQuestion: Which critical gap in transitioning from static to dynamic benchmarking for large language models does the survey identify as the primary motivation for proposing optimal design principles?\nA: The dynamic benchmarks\u2019 reliance on small, homogeneous datasets  \nB: The absence of standardized criteria for evaluating dynamic benchmarks  \nC: The lack of automated contamination-detection tooling within dynamic benchmarks  \nD: The persistent use of outdated static benchmarks alongside new dynamic ones  \nExplanation: The survey highlights that no standardized criteria exist for evaluating dynamic benchmarks, prompting the proposal of design principles.  \nCorrect Answer: B\n</output_format>",
    "question": "Which critical gap in transitioning from static to dynamic benchmarking for large language models does the survey identify as the primary motivation for proposing optimal design principles?",
    "choices": {
      "A": "The dynamic benchmarks\u2019 reliance on small, homogeneous datasets",
      "B": "The absence of standardized criteria for evaluating dynamic benchmarks",
      "C": "The lack of automated contamination-detection tooling within dynamic benchmarks",
      "D": "The persistent use of outdated static benchmarks alongside new dynamic ones"
    },
    "answer": "B",
    "explanation": "The survey highlights that no standardized criteria exist for evaluating dynamic benchmarks, prompting the proposal of design principles.",
    "question_token_count": 29,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 2,
    "question_difficulty_score": 6,
    "question_groundedness_score": 1,
    "avg_answer_token_count": 11
  },
  {
    "context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n",
    "topic": "Identify potential limitations or blind spots of temporal-cutoff benchmarks and propose methodological enhancements to address them.",
    "response": "<document_analysis>\nThe context describes how temporal-cutoff benchmarks collect post\u2013knowledge-cutoff data (e.g., LiveBench, AntiLeak-Bench, AcademicEval) to avoid training data contamination, but does not discuss how they test for model robustness to evolving language or domain-specific distributional shifts. The main limitation is that purely using new data avoids contamination without necessarily probing a model\u2019s ability to generalize across changes in terminology, style, or domain conventions. A suitable methodological enhancement is to design cross-temporal adaptation tasks that explicitly require models to map or align pre-cutoff knowledge to post-cutoff innovations.\n</document_analysis>\n\n<output_format>\nQuestion: Despite effectively preventing data contamination, temporal-cutoff benchmarks often overlook evaluating a model\u2019s adaptability to shifts in domain-specific language and conventions. Which methodological enhancement would most directly address this blind spot?\nA: Increasing the refresh frequency of benchmark data to continuously add new questions\nB: Incorporating retrospective analog tasks that juxtapose pre-cutoff concepts with their post-cutoff terminology to test domain adaptation\nC: Applying stricter semantic filters to remove queries that resemble pre-cutoff content\nD: Broadening the benchmark to cover multiple subject areas beyond the original domain\nExplanation: Only retrospective analog tasks explicitly require the model to align and adapt its pre-cutoff representations to new post-cutoff terms, directly evaluating robustness to distributional shifts.\nCorrect Answer: B\n</output_format>",
    "question": "Despite effectively preventing data contamination, temporal-cutoff benchmarks often overlook evaluating a model\u2019s adaptability to shifts in domain-specific language and conventions. Which methodological enhancement would most directly address this blind spot?",
    "choices": {
      "A": "Increasing the refresh frequency of benchmark data to continuously add new questions",
      "B": "Incorporating retrospective analog tasks that juxtapose pre-cutoff concepts with their post-cutoff terminology to test domain adaptation",
      "C": "Applying stricter semantic filters to remove queries that resemble pre-cutoff content",
      "D": "Broadening the benchmark to cover multiple subject areas beyond the original domain"
    },
    "answer": "B",
    "explanation": "Only retrospective analog tasks explicitly require the model to align and adapt its pre-cutoff representations to new post-cutoff terms, directly evaluating robustness to distributional shifts.",
    "question_token_count": 38,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 6,
    "avg_answer_token_count": 16
  },
  {
    "context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n",
    "topic": "Motivations for adopting dynamic benchmarks over static benchmarks to assess LLM capabilities while mitigating data contamination.",
    "response": "<document_analysis>\nThe context discusses dynamic benchmarks for LLM evaluation, emphasizing two conflicting goals: interpretability of data transformations (to reduce manual validation costs) and novelty of evaluation data (to minimize contamination). It categorizes dynamic benchmarks into temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches, noting that rule-based methods are inherently interpretable while LLM-based ones require additional explainability mechanisms. The key nuanced tension lies between transparency of rule-based transformations and the creative, novel data that LLM-based generation can produce. This tension and each method\u2019s contamination mitigation strategy form the basis for a high\u2010difficulty question.\n</document_analysis>\n\n<output_format>\nQuestion: Which contrasting dynamic benchmarking approaches most directly illustrate the trade\u2011off between interpretability of transformations and novelty of evaluation data, and how does each mitigate data contamination?\nA: Temporal cutoff gathers only new data past a cutoff date to avoid contamination but applies no transformations, sacrificing interpretability for freshness.\nB: Rule\u2011based generation uses hand\u2011crafted rules for transparent transformations to minimize manual validation costs but yields limited novel data, controlling contamination via deterministic rules.\nC: Rule\u2011based generation ensures full transformation transparency through predefined rules and thus low contamination risk, while LLM\u2011based generation maximizes novelty but relies on explainability tools to verify correctness and prevent contamination.\nD: Hybrid approaches combine temporal cutoff and LLM generations to balance freshness and novelty, inherently eliminating contamination without interpretability concerns.\nExplanation: Option C directly contrasts the inherent transparency and low contamination risk of rule\u2011based transformations with the novel data generation of LLM methods, which require extra explainability to guard against contamination.\nCorrect Answer: C\n</output_format>",
    "question": "Which contrasting dynamic benchmarking approaches most directly illustrate the trade\u2011off between interpretability of transformations and novelty of evaluation data, and how does each mitigate data contamination?",
    "choices": {
      "A": "Temporal cutoff gathers only new data past a cutoff date to avoid contamination but applies no transformations, sacrificing interpretability for freshness.",
      "B": "Rule\u2011based generation uses hand\u2011crafted rules for transparent transformations to minimize manual validation costs but yields limited novel data, controlling contamination via deterministic rules.",
      "C": "Rule\u2011based generation ensures full transformation transparency through predefined rules and thus low contamination risk, while LLM\u2011based generation maximizes novelty but relies on explainability tools to verify correctness and prevent contamination.",
      "D": "Hybrid approaches combine temporal cutoff and LLM generations to balance freshness and novelty, inherently eliminating contamination without interpretability concerns."
    },
    "answer": "C",
    "explanation": "Option C directly contrasts the inherent transparency and low contamination risk of rule\u2011based transformations with the novel data generation of LLM methods, which require extra explainability to guard against contamination.",
    "question_token_count": 32,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 29
  },
  {
    "context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n",
    "topic": "Explore case studies or historical examples where benchmark contamination led to misleading perceptions of LLM progress.",
    "response": "<document_analysis>\nThe context discusses two main aspects: syntactic contamination\u2014where test data is a rephrased version of training data with added prefixes\u2014and the significant risk it poses to the validity of LLM benchmarks by conflating memorization with genuine reasoning. Key themes include the challenge of distinguishing recall from inference and how contamination can inflate performance measures, misleading both research comparisons and policy decisions. There are no irrelevant or bogus elements in the excerpt.\n</document_analysis>\n\n<output_format>\nQuestion: Which option best captures why syntactic contamination undermines the validity of LLM benchmarks?\nA: It causes models to misalign semantic understanding, underestimating their true reasoning capabilities.\nB: It rephrases training examples with added prefixes, enabling recall of memorized data as if through reasoning, thus inflating performance metrics.\nC: It introduces novel syntactic constructs that LLMs have not encountered, artificially deflating their benchmark scores.\nD: It shifts the data distribution toward more complex structures, leading to overestimation of model robustness.\nExplanation: Syntactic contamination is defined as rephrasing training data with prefixes, which lets models rely on memorization rather than genuine inference, thereby inflating benchmark results.\nCorrect Answer: B\n</output_format>",
    "question": "Which option best captures why syntactic contamination undermines the validity of LLM benchmarks?",
    "choices": {
      "A": "It causes models to misalign semantic understanding, underestimating their true reasoning capabilities.",
      "B": "It rephrases training examples with added prefixes, enabling recall of memorized data as if through reasoning, thus inflating performance metrics.",
      "C": "It introduces novel syntactic constructs that LLMs have not encountered, artificially deflating their benchmark scores.",
      "D": "It shifts the data distribution toward more complex structures, leading to overestimation of model robustness."
    },
    "answer": "B",
    "explanation": "Syntactic contamination is defined as rephrasing training data with prefixes, which lets models rely on memorization rather than genuine inference, thereby inflating benchmark results.",
    "question_token_count": 16,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 21
  },
  {
    "context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n",
    "topic": "The specific mechanisms by which KIEval generates follow-up questions based on evaluated model responses and the implications for static benchmark enhancement.",
    "response": "<document_analysis>\nThe context describes three interactive evaluation methods: LLM-as-an-Interviewer (which paraphrases static queries), TreeEval (which branches into subtopics based on topic and prior responses), and KIEval (which uniquely generates follow-up questions directly from the evaluated model\u2019s answers to initial static benchmark questions). The key mechanism in KIEval\u2014response-driven follow-up generation\u2014enables dynamic extension of otherwise static benchmarks. This question challenges experts to distinguish KIEval\u2019s approach from paraphrasing, tree-based subtopic generation, and multi-agent frameworks, and to understand its implications for benchmark adaptability and scalability.\nDifficulty: 6/10\n</document_analysis>\n\n<output_format>\nQuestion: Which aspect of KIEval\u2019s methodology is primarily responsible for converting static benchmark items into a dynamically extendable evaluation process?\nA: Paraphrasing original benchmark questions to avoid redundancy\nB: Generating follow-up questions conditioned on the model\u2019s responses to initial benchmark items\nC: Creating hierarchical subtopics and questions solely based on a predefined topic tree\nD: Employing a multi-agent pipeline for planning, generation, verification, and evaluation\nExplanation: KIEval uniquely uses the model\u2019s own answers to generate subsequent questions, transforming fixed benchmarks into dynamic interactions.\nCorrect Answer: B\n</output_format>",
    "question": "Which aspect of KIEval\u2019s methodology is primarily responsible for converting static benchmark items into a dynamically extendable evaluation process?",
    "choices": {
      "A": "Paraphrasing original benchmark questions to avoid redundancy",
      "B": "Generating follow-up questions conditioned on the model\u2019s responses to initial benchmark items",
      "C": "Creating hierarchical subtopics and questions solely based on a predefined topic tree",
      "D": "Employing a multi-agent pipeline for planning, generation, verification, and evaluation"
    },
    "answer": "B",
    "explanation": "KIEval uniquely uses the model\u2019s own answers to generate subsequent questions, transforming fixed benchmarks into dynamic interactions.",
    "question_token_count": 25,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 14
  },
  {
    "context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n",
    "topic": "Formal definition of data contamination and its effect on the validity of LLM evaluation benchmarks",
    "response": "<document_analysis>\nThe context defines two types of data contamination for LLM benchmarks: exact (verbatim overlap) and syntactic (overlap after meaning-preserving transformations such as punctuation normalization, whitespace changes, synonym swaps, morphological variations, or paraphrasing). A strong question for experts will test the boundary of what transformations qualify as syntactic contamination. \nDifficulty: 7/10 \u2013 requires precise understanding of which modifications preserve the same lexical content within the formal F_syntactic set.\n</document_analysis>\n\n<output_format>\nQuestion: Which of the following modifications to a test example would NOT be classified as syntactic contamination under the formal definition of \u2131_syntactic?\nA: Normalizing punctuation (e.g., converting \u201cdon\u2019t\u201d to \u201cdo not\u201d)\nB: Adjusting whitespace (e.g., adding or removing spaces)\nC: Reordering clauses while preserving original meaning\nD: Translating the sentence into another language and back\nExplanation: \u2131_syntactic only includes meaning-preserving changes within the same language such as punctuation, whitespace, synonyms, morphological variations, or paraphrasing. Cross-lingual translation falls outside this scope.\nCorrect Answer: D\n</output_format>",
    "question": "Which of the following modifications to a test example would NOT be classified as syntactic contamination under the formal definition of \u2131_syntactic?",
    "choices": {
      "A": "Normalizing punctuation (e.g., converting \u201cdon\u2019t\u201d to \u201cdo not\u201d)",
      "B": "Adjusting whitespace (e.g., adding or removing spaces)",
      "C": "Reordering clauses while preserving original meaning",
      "D": "Translating the sentence into another language and back"
    },
    "answer": "D",
    "explanation": "\u2131_syntactic only includes meaning-preserving changes within the same language such as punctuation, whitespace, synonyms, morphological variations, or paraphrasing. Cross-lingual translation falls outside this scope.",
    "question_token_count": 28,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 12
  },
  {
    "context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "The importance of holistic benchmarks to evaluate general-purpose LLMs as they evolve into task solvers.",
    "response": "<document_analysis>\n1. Thoughtful Content Examination: The context emphasizes the necessity of holistic benchmarks for LLMs as they evolve into general-purpose task solvers, highlighting two major issues with static benchmarks: they become too easy as models improve and are prone to data contamination when models train on all available data. The text references the development of contamination detectors (to quantify leakage) and dynamic benchmarks (to maintain novelty and difficulty).  \n2. Concept Exploration: Key implicit assumption is that evaluation must adapt as model capabilities advance and data exposure increases. Dynamic benchmarks and contamination detectors serve complementary roles: one generates fresh unseen tasks, the other measures contamination risk.  \n3. Strategic Complexity Calibration: Rated difficulty 7/10. The question requires synthesis of both concepts and eliminates simplistic one-dimensional answers, demanding nuanced understanding of benchmark design strategies.  \n4. Intentional Question Planning: Pose a question that asks which combined strategy addresses both the ease-of-task and contamination challenges. Provide plausible but subtly incorrect alternatives that confuse aspects of benchmark maintenance (e.g., static updates, human review) with the dynamic-contamination framework.\n</document_analysis>\n\n<output_format>\nQuestion: As general-purpose language models continually improve and train on all available data, static benchmarks risk losing rigor and suffering from evaluation-data leakage. Which dual approach most comprehensively maintains benchmark difficulty while guarding against contaminated test data?\nA: Periodically updating static benchmarks and emphasizing instruction-following tasks  \nB: Employing contamination detectors to estimate leaked exposure and deploying dynamic benchmarks to introduce unseen tasks  \nC: Expanding the size of static benchmark pools and focusing on complex coding challenges  \nD: Relying on human raters for output review and manual redaction of leaked evaluation items  \nExplanation: Contamination detectors quantify the extent of data leakage, and dynamic benchmarks continually generate new, unseen tasks to sustain difficulty, jointly addressing both primary issues of static evaluations.  \nCorrect Answer: B\n</output_format>",
    "question": "As general-purpose language models continually improve and train on all available data, static benchmarks risk losing rigor and suffering from evaluation-data leakage. Which dual approach most comprehensively maintains benchmark difficulty while guarding against contaminated test data?",
    "choices": {
      "A": "Periodically updating static benchmarks and emphasizing instruction-following tasks",
      "B": "Employing contamination detectors to estimate leaked exposure and deploying dynamic benchmarks to introduce unseen tasks",
      "C": "Expanding the size of static benchmark pools and focusing on complex coding challenges",
      "D": "Relying on human raters for output review and manual redaction of leaked evaluation items"
    },
    "answer": "B",
    "explanation": "Contamination detectors quantify the extent of data leakage, and dynamic benchmarks continually generate new, unseen tasks to sustain difficulty, jointly addressing both primary issues of static evaluations.",
    "question_token_count": 44,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 7,
    "avg_answer_token_count": 15
  },
  {
    "context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n",
    "topic": "The specific evaluation workflow in DyVal of querying LLMs for the root-node value and interpreting performance metrics.",
    "response": "<document_analysis>\nThe context describes S3Eval and various graph-based evaluation frameworks, focusing on DyVal\u2019s workflow: \n1. Random generation of directed acyclic graphs (DAGs) with varying nodes and edges to control difficulty.\n2. Rule-based conversion of these DAGs into natural language.\n3. Querying the LLM for the value of the root node to assess reasoning.\n\nKey themes: how DyVal ensures inference over memorization via randomness, the conversion pipeline, and the root-node query metric. All content is relevant; no extraneous or promotional material is present. A high-difficulty question will probe the link between design choices (random DAG generation) and the measurement of inferential reasoning over memorization.\nDifficulty rating: 8/10.\n</document_analysis>\n\n<output_format>\nQuestion: Which component of the DyVal framework primarily prevents an LLM from relying on memorized graph patterns and instead compels it to perform genuine inferential reasoning?\nA: Random generation of directed acyclic graphs with variable node and edge counts\nB: Rule-based conversion of graph structures into natural language descriptions\nC: Direct querying of the model for the root-node value\nD: Systematic adjustment of edge-to-node ratios to modulate task difficulty\nExplanation: By constructing entirely new DAGs for each evaluation, the framework avoids any reuse of known patterns and forces the model to infer results on unseen structures.\nCorrect Answer: A\n</output_format>",
    "question": "Which component of the DyVal framework primarily prevents an LLM from relying on memorized graph patterns and instead compels it to perform genuine inferential reasoning?",
    "choices": {
      "A": "Random generation of directed acyclic graphs with variable node and edge counts",
      "B": "Rule-based conversion of graph structures into natural language descriptions",
      "C": "Direct querying of the model for the root-node value",
      "D": "Systematic adjustment of edge-to-node ratios to modulate task difficulty"
    },
    "answer": "A",
    "explanation": "By constructing entirely new DAGs for each evaluation, the framework avoids any reuse of known patterns and forces the model to infer results on unseen structures.",
    "question_token_count": 31,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 4,
    "question_difficulty_score": 8,
    "question_groundedness_score": 2,
    "avg_answer_token_count": 12
  },
  {
    "context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n",
    "topic": "The CONSTAT framework for contamination detection through cross\u2011benchmark performance comparison",
    "response": "<document_analysis>\nThe context discusses various post-hoc contamination detection methods: direct n\u2011gram overlap, embedding\u2011based similarity, behavior under masked inputs, completion patterns, paraphrase preference, and finally introduces CONSTAT as a \u201ccomparing model performance across benchmarks\u201d approach. The key novelty is cross\u2011benchmark performance comparison to infer contamination. All other methods detect contamination within a single benchmark or via feature matching, whereas CONSTAT infers contamination indirectly through relative performance anomalies. This distinction guides question focus.\nDifficulty rating: 8/10\n</document_analysis>\n\n<output_format>\nQuestion: The CONSTAT framework for contamination detection is based on comparing model performance across multiple benchmarks. Which core assumption underlies this approach?\nA: Contamination will produce high lexical overlap, detectable through n\u2011gram matching between training and test data.\nB: A model will consistently outperform on benchmarks whose test sets overlap with its training data, relative to structurally analogous but uncontaminated benchmarks.\nC: Contamination primarily manifests as elevated embedding similarity between training and test samples, identifiable by cosine\u2010similarity thresholds.\nD: Memorization due to contamination is revealed by an increased ability to complete masked inputs from the test set.\nExplanation: CONSTAT detects contamination by observing relative performance gains on benchmarks with overlapping data versus similar non\u2011overlapping ones, distinguishing it from direct overlap, embedding, or masked\u2011input methods.\nCorrect Answer: B\n</output_format>",
    "question": "The CONSTAT framework for contamination detection is based on comparing model performance across multiple benchmarks. Which core assumption underlies this approach?",
    "choices": {
      "A": "Contamination will produce high lexical overlap, detectable through n\u2011gram matching between training and test data.",
      "B": "A model will consistently outperform on benchmarks whose test sets overlap with its training data, relative to structurally analogous but uncontaminated benchmarks.",
      "C": "Contamination primarily manifests as elevated embedding similarity between training and test samples, identifiable by cosine\u2010similarity thresholds.",
      "D": "Memorization due to contamination is revealed by an increased ability to complete masked inputs from the test set."
    },
    "answer": "B",
    "explanation": "CONSTAT detects contamination by observing relative performance gains on benchmarks with overlapping data versus similar non\u2011overlapping ones, distinguishing it from direct overlap, embedding, or masked\u2011input methods.",
    "question_token_count": 26,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 23
  },
  {
    "context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n",
    "topic": "Formulate the stability of complexity in dynamic benchmarks using a complexity measurement function \u03a8(\u22c5) and interpret the meaning of variance in this context.",
    "response": "<document_analysis>\nThe context discusses the challenge of quantifying complexity stability in dynamic benchmarks. It introduces a complexity measurement function \u03a8(\u22c5) and defines the stability as the variance of \u03a8 across multiple transformation trials. A high variance signals instability in complexity and undermines the ability to distinguish between performance drops caused by data contamination versus increased task difficulty. This excerpt is coherent and contains a definable formalism suitable for probing expert understanding of the variance metric\u2019s interpretive role. The core is: stability = Var[\u03a8(T_i(seed))]; high variance = unstable complexity => performance changes may be due to complexity shifts. \nDifficulty calibration: 7/10 \u2013 requires understanding of statistical variance in benchmarking context and its diagnostic implications.\n</document_analysis>\n\n<output_format>\nQuestion: Stability in dynamic benchmark complexity is quantified by S = Var_{i}[\u03a8(T_i(D))], where \u03a8 measures complexity of each transformed trial T_i(D). If S is observed to be high, which of the following best describes the correct interpretation when diagnosing an LLM\u2019s performance drop on the transformed datasets?\nA: The transformations maintain consistent complexity, so any performance degradation is most likely due to data contamination rather than task difficulty.\nB: The transformed datasets exhibit large fluctuations in complexity, making it difficult to attribute performance changes solely to contamination because the tasks themselves vary in difficulty.\nC: The average complexity has increased relative to the seed dataset, indicating that the performance drop is primarily caused by higher inherent task difficulty in all transformations.\nD: \u03a8 is systematically overestimating complexity across all trials, implying the need to recalibrate \u03a8 before diagnosing the cause of performance changes.\nExplanation: A high variance S directly implies that the complexity of transformed datasets is unstable (varies widely), so one cannot blame performance drops solely on contamination since task difficulty itself fluctuates.\nCorrect Answer: B\n</output_format>",
    "question": "Stability in dynamic benchmark complexity is quantified by S = Var_{i}[\u03a8(T_i(D))], where \u03a8 measures complexity of each transformed trial T_i(D). If S is observed to be high, which of the following best describes the correct interpretation when diagnosing an LLM\u2019s performance drop on the transformed datasets?",
    "choices": {
      "A": "The transformations maintain consistent complexity, so any performance degradation is most likely due to data contamination rather than task difficulty.",
      "B": "The transformed datasets exhibit large fluctuations in complexity, making it difficult to attribute performance changes solely to contamination because the tasks themselves vary in difficulty.",
      "C": "The average complexity has increased relative to the seed dataset, indicating that the performance drop is primarily caused by higher inherent task difficulty in all transformations.",
      "D": "\u03a8 is systematically overestimating complexity across all trials, implying the need to recalibrate \u03a8 before diagnosing the cause of performance changes."
    },
    "answer": "B",
    "explanation": "A high variance S directly implies that the complexity of transformed datasets is unstable (varies widely), so one cannot blame performance drops solely on contamination since task difficulty itself fluctuates.",
    "question_token_count": 63,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 26
  },
  {
    "context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Analyze the trade-offs between finite and infinite timestamp settings in dynamic benchmarking frameworks and their impact on evaluation fidelity.",
    "response": "<document_analysis>\n1. Thoughtful Content Examination:\n   - The context defines a dynamic benchmark as a dataset D and transformation function T(\u00b7), producing a sequence of datasets D\u209c for t=1\u2026N, where N may be finite or infinite.\n   - It highlights the goal of avoiding data contamination via repeated transformations but does not elaborate on the trade-offs of choosing N.\n2. Concept Exploration:\n   - Implicitly, finite N limits the number of dataset transformations, capping novelty and potential drift; infinite N allows unbounded evolution but risks distributional drift, resource overhead, and loss of fidelity.\n   - Key considerations include evaluation fidelity, dataset representativeness, contamination risk, and computational feasibility.\n3. Strategic Complexity Calibration:\n   - Difficulty Level: 8/10. The question requires understanding beyond mere definitions\u2014evaluating nuanced impacts of infinite vs finite timestamps on benchmarking integrity.\n4. Intentional Question Planning:\n   - Aim: Probe the expert\u2019s deep insight into how infinite sequence strategies can paradoxically degrade evaluation fidelity through distributional drift.\n   - Ensure that incorrect options are plausible yet subtly flawed, making the correct option non-obvious.\n</document_analysis>\n\n<output_format>\nQuestion: Although infinite timestamps in a dynamic benchmarking framework enable continuous introduction of novel evaluation data, what primary drawback does this strategy introduce compared to a finite timestamp approach?\nA: It guarantees elimination of data contamination through perpetual transformations.\nB: It causes accumulation of distributional drift away from the target population.\nC: It reduces computational overhead by limiting dataset size.\nD: It ensures convergence back to the original static benchmark distribution.\nExplanation: With infinitely many transformations, the dataset can gradually diverge from the intended distribution, undermining evaluation fidelity.\nCorrect Answer: B\n</output_format>",
    "question": "Although infinite timestamps in a dynamic benchmarking framework enable continuous introduction of novel evaluation data, what primary drawback does this strategy introduce compared to a finite timestamp approach?",
    "choices": {
      "A": "It guarantees elimination of data contamination through perpetual transformations.",
      "B": "It causes accumulation of distributional drift away from the target population.",
      "C": "It reduces computational overhead by limiting dataset size.",
      "D": "It ensures convergence back to the original static benchmark distribution."
    },
    "answer": "B",
    "explanation": "With infinitely many transformations, the dataset can gradually diverge from the intended distribution, undermining evaluation fidelity.",
    "question_token_count": 31,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 4,
    "question_difficulty_score": 8,
    "question_groundedness_score": 6,
    "avg_answer_token_count": 11
  },
  {
    "context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "The motivations for transitioning from static to dynamic benchmarking frameworks to reduce contamination bias.",
    "response": "<document_analysis>\nThe provided context outlines the evolution from static to dynamic benchmarking in LLM evaluation, driven primarily by the need to mitigate data contamination when benchmarks are publicly released and models scrape Internet data. It details two main dynamic strategies: \n1) Continuously updating datasets according to model training timestamps to exclude tasks that may have been ingested during training, and \n2) Regenerating benchmark instances by reconstructing or paraphrasing original tasks to prevent direct overlap with training data while preserving task semantics.\n\nThese nuanced methods respond to the static approach\u2019s inability to prevent contamination and ensure more reliable assessments. Designing a question around matching each dynamic strategy to its contamination-mitigation mechanism will probe an expert\u2019s deep understanding of these innovations. The question difficulty is calibrated at 8/10, targeting domain experts familiar with LLM benchmarking frameworks.\n</document_analysis>\n\n<output_format>\nQuestion: Dynamic benchmarking for LLMs often employs two principal strategies to reduce training-data contamination bias. Which of the following correctly pairs each strategy with its core mechanism?\nA: Timestamp-based updates use encryption to obscure benchmark data; data regeneration filters instances using sliding time windows.\nB: Timestamp-based updates filter out benchmark items released before a model\u2019s training cut-off; data regeneration creates novel instances by altering surface forms while preserving original semantics.\nC: Timestamp-based updates rely on manual curation of human-crafted tasks; data regeneration focuses on updating evaluation metrics dynamically.\nD: Timestamp-based updates enforce semantic invariance across versions; data regeneration randomizes content to maximize diversity.\nExplanation: Timestamp-based updating excludes any benchmark items released prior to the model\u2019s training date, while data regeneration mitigates contamination by creating semantically equivalent yet freshly surfaced examples.\nCorrect Answer: B\n</output_format>",
    "question": "Dynamic benchmarking for LLMs often employs two principal strategies to reduce training-data contamination bias. Which of the following correctly pairs each strategy with its core mechanism?",
    "choices": {
      "A": "Timestamp-based updates use encryption to obscure benchmark data; data regeneration filters instances using sliding time windows.",
      "B": "Timestamp-based updates filter out benchmark items released before a model\u2019s training cut-off; data regeneration creates novel instances by altering surface forms while preserving original semantics.",
      "C": "Timestamp-based updates rely on manual curation of human-crafted tasks; data regeneration focuses on updating evaluation metrics dynamically.",
      "D": "Timestamp-based updates enforce semantic invariance across versions; data regeneration randomizes content to maximize diversity."
    },
    "answer": "B",
    "explanation": "Timestamp-based updating excludes any benchmark items released prior to the model\u2019s training date, while data regeneration mitigates contamination by creating semantically equivalent yet freshly surfaced examples.",
    "question_token_count": 31,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 23
  },
  {
    "context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n",
    "topic": "Use of partial completion analysis to reveal model overfitting on contaminated examples",
    "response": "<document_analysis>\nThoughtful Content Examination:\nThe provided context outlines post-hoc methods for detecting data contamination in large models. It contrasts direct overlap techniques (n-gram matching, embedding-based similarity) with behavioral analyses, including masked-input memorization, partial completions, and preference for original vs paraphrased cases. Partial completion analysis probes how well a model completes truncated prompts, revealing overfitting when a model memorizes training sequences.\n\nConcept Exploration:\nEmbedding-based similarity methods identify near-duplicate examples but don\u2019t directly measure model behavior. Partial completion analysis instead extracts a behavioral signal\u2014models overfit contaminated examples show a disproportionate accuracy boost when given only partial prompts of memorized sequences. This nuance highlights contamination even without explicit textual overlap detection.\n\nStrategic Complexity Calibration:\nRated difficulty at 7/10. The question requires understanding nuanced differences between detection approaches and linking specific behavioral signals to the right method.\n\nIntentional Question Planning:\nThe question asks which unique signal partial completion analysis provides that embedding-based overlap detection does not inherently capture, forcing experts to distinguish between content-overlap metrics and behavioral probes. This invites deeper reflection on methodological trade\u2011offs in contamination detection.\n\n</document_analysis>\n\n<output_format>\nQuestion: In post-hoc contamination detection, what unique signal does partial completion analysis extract that embedding-based overlap methods do not inherently capture?\nA: The stability of model output probabilities for tail tokens across semantically equivalent inputs  \nB: The disproportionate improvement in completion accuracy when prompts coincide with memorized training sequences  \nC: The embedding distance threshold distinguishing training/test overlap  \nD: The change in performance between original versus paraphrased test cases  \nExplanation: Partial completion analysis uniquely measures how model accuracy jumps for truncated prompts of memorized sequences, a behavioral signal not given by embedding overlap metrics.  \nCorrect Answer: B\n</output_format>",
    "question": "In post-hoc contamination detection, what unique signal does partial completion analysis extract that embedding-based overlap methods do not inherently capture?",
    "choices": {
      "A": "The stability of model output probabilities for tail tokens across semantically equivalent inputs",
      "B": "The disproportionate improvement in completion accuracy when prompts coincide with memorized training sequences",
      "C": "The embedding distance threshold distinguishing training/test overlap",
      "D": "The change in performance between original versus paraphrased test cases"
    },
    "answer": "B",
    "explanation": "Partial completion analysis uniquely measures how model accuracy jumps for truncated prompts of memorized sequences, a behavioral signal not given by embedding overlap metrics.",
    "question_token_count": 25,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 4,
    "avg_answer_token_count": 13
  },
  {
    "context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n",
    "topic": "Analyze how smaller dataset sizes introduce statistical errors and how scalability quantifies the mitigation of these errors.",
    "response": "<document_analysis>\nThe context defines a scalability metric for dynamic benchmarks as the expected ratio of transformed dataset size to original size, normalized by the cost of transformation. Key elements: \u2225T_i(D)\u2225/\u2225D\u2225 captures dataset expansion; Cost(T_i) in denominator captures resource expenditure; expectation over all transformations captures average efficiency. A challenging question can involve computing and comparing these ratios under specific numeric scenarios to probe deep understanding of the trade-off between dataset growth and cost efficiency.\n</document_analysis>\n\n<output_format>\nQuestion: Given the scalability score S = E[||T(D)||/||D|| \u00f7 Cost(T)], suppose ||D||=N. Transformation T1 produces ||T1(D)||=3N at a cost of 1.5 units, while T2 produces ||T2(D)||=2N at a cost of 0.5 units. Which transformation yields a higher per-unit-cost contribution to S?\nA: T1, because it expands the data by a larger absolute factor  \nB: T2, because it achieves a higher data\u2010per\u2010cost ratio  \nC: Both yield the same contribution, since 3/1.5 = 2 and 2/0.5 = 4 balance out in expectation  \nD: Cannot be determined without knowing the full transformation distribution  \nExplanation: Compute each ratio: T1 gives (3N/N)/1.5 = 2; T2 gives (2N/N)/0.5 = 4, so T2 has the higher per\u2010unit\u2010cost contribution.  \nCorrect Answer: B\n</output_format>",
    "question": "Given the scalability score S = E[||T(D)||/||D|| \u00f7 Cost(T)], suppose ||D||=N. Transformation T1 produces ||T1(D)||=3N at a cost of 1.5 units, while T2 produces ||T2(D)||=2N at a cost of 0.5 units. Which transformation yields a higher per-unit-cost contribution to S?",
    "choices": {
      "A": "T1, because it expands the data by a larger absolute factor",
      "B": "T2, because it achieves a higher data\u2010per\u2010cost ratio",
      "C": "Both yield the same contribution, since 3/1.5 = 2 and 2/0.5 = 4 balance out in expectation",
      "D": "Cannot be determined without knowing the full transformation distribution"
    },
    "answer": "B",
    "explanation": "Compute each ratio: T1 gives (3N/N)/1.5 = 2; T2 gives (2N/N)/0.5 = 4, so T2 has the higher per\u2010unit\u2010cost contribution.",
    "question_token_count": 84,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 17
  },
  {
    "context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n",
    "topic": "Requirements for refinement, real-world validation, and scalability of the proposed dynamic benchmarking criteria.",
    "response": "<document_analysis>\nThe context outlines the need for further refinement and real-world validation of dynamic benchmarking criteria and emphasizes challenges in reliability, reproducibility, and scalability. It calls for standardized dynamic evaluation and practical mitigation tools. Irrelevant elements (e.g., boilerplate \u201cLimitations\u201d heading) are disregarded. Key themes: iterative refinement, pilot real-world testing, automation for large-scale deployment, and community standards.\n</document_analysis>\n\n<output_format>\nQuestion: Which sequence of actions most directly fulfills the survey\u2019s identified needs for refining, validating in real environments, and scaling dynamic benchmarking criteria?\nA: Propose high-level conceptual frameworks, gather theoretical community feedback, then deploy updated static benchmarks globally.\nB: Refine criteria to incorporate emerging methodologies, conduct pilot studies in real-world settings to test reliability, and implement automated updating mechanisms for large-scale use.\nC: Focus on exhaustive fine-grained technical specifications before any deployment, postpone public evaluation until full coverage is achieved, and rely solely on manual dataset curation.\nD: Emphasize creating comparable static benchmarks, apply ad-hoc contamination controls during evaluation, and avoid establishing standardized protocols.\nExplanation: Option B directly addresses the three phases\u2014refinement, real-world validation, and scalability\u2014by iterating on criteria, piloting them in practice, and automating updates for broader deployment.\nCorrect Answer: B\n</output_format>",
    "question": "Which sequence of actions most directly fulfills the survey\u2019s identified needs for refining, validating in real environments, and scaling dynamic benchmarking criteria?",
    "choices": {
      "A": "Propose high-level conceptual frameworks, gather theoretical community feedback, then deploy updated static benchmarks globally.",
      "B": "Refine criteria to incorporate emerging methodologies, conduct pilot studies in real-world settings to test reliability, and implement automated updating mechanisms for large-scale use.",
      "C": "Focus on exhaustive fine-grained technical specifications before any deployment, postpone public evaluation until full coverage is achieved, and rely solely on manual dataset curation.",
      "D": "Emphasize creating comparable static benchmarks, apply ad-hoc contamination controls during evaluation, and avoid establishing standardized protocols."
    },
    "answer": "B",
    "explanation": "Option B directly addresses the three phases\u2014refinement, real-world validation, and scalability\u2014by iterating on criteria, piloting them in practice, and automating updates for broader deployment.",
    "question_token_count": 28,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 4,
    "question_difficulty_score": 8,
    "question_groundedness_score": 3,
    "avg_answer_token_count": 26
  },
  {
    "context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n",
    "topic": "Integration strategies and complexity management in hybrid dynamic benchmark construction approaches.",
    "response": "<document_analysis>\nThe context discusses interpretability challenges in dynamic benchmarking, contrasting rule-based (inherently interpretable) and LLM-assisted transformations (requiring explainability tools or human-in-the-loop). It also categorizes dynamic benchmarks by construction: temporal cutoff, rule-based, LLM-based, and hybrid. For hybrid approaches, effective integration strategies must balance transparency (interpretability) with the complexity introduced by large-scale, LLM-generated transformations. The question will probe understanding of how to architect a hybrid pipeline that maintains interpretability while containing complexity.\nDifficulty: 8/10\n</document_analysis>\n<output_format>\nQuestion: In a hybrid dynamic benchmark construction pipeline, which integration strategy most effectively balances interpretability and controls the complexity introduced by LLM-assisted transformations?\nA: Layering rule-based scaffolding both before and after the LLM generation step, supplemented with explainability modules to trace transformation decisions.\nB: Relying exclusively on human-in-the-loop validation of raw LLM outputs without predefined transformation rules.\nC: Constraining the LLM\u2019s domain solely through temporal cutoff data, eliminating the need for rule-based interpretability.\nD: Post-processing LLM-generated items by filtering them against a static benchmark corpus without integrating rules or explainability tools.\nExplanation: Only option A embeds interpretability directly in the hybrid workflow by combining rule-based scaffolds with tools that trace LLM decisions, thereby controlling complexity and ensuring correctness.\nCorrect Answer: A\n</output_format>",
    "question": "In a hybrid dynamic benchmark construction pipeline, which integration strategy most effectively balances interpretability and controls the complexity introduced by LLM-assisted transformations?",
    "choices": {
      "A": "Layering rule-based scaffolding both before and after the LLM generation step, supplemented with explainability modules to trace transformation decisions.",
      "B": "Relying exclusively on human-in-the-loop validation of raw LLM outputs without predefined transformation rules.",
      "C": "Constraining the LLM\u2019s domain solely through temporal cutoff data, eliminating the need for rule-based interpretability.",
      "D": "Post-processing LLM-generated items by filtering them against a static benchmark corpus without integrating rules or explainability tools."
    },
    "answer": "A",
    "explanation": "Only option A embeds interpretability directly in the hybrid workflow by combining rule-based scaffolds with tools that trace LLM decisions, thereby controlling complexity and ensuring correctness.",
    "question_token_count": 27,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 4,
    "question_difficulty_score": 8,
    "question_groundedness_score": 6,
    "avg_answer_token_count": 22
  },
  {
    "context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Critically analyze why assumptions about higher memorization or lower perplexity for contaminated instances might not hold across different LLM architectures and tasks.",
    "response": "<document_analysis>\nThe context discusses limitations of post-hoc contamination detection, specifically noting that assumptions such as \u201chigher memorization\u201d or \u201clower perplexity\u201d for contaminated instances are not universally valid across different LLM architectures and tasks. It then introduces dynamic benchmarking, which uses transformation functions T\u209c to continually modify the evaluation dataset and avoid contamination. A meaningful, challenging question probes the expert\u2019s understanding of why perplexity-based detection fails generally and what aspect of model/task variability undermines these assumptions. We will focus on the core insight: heterogeneity in memorization and perplexity behaviors across models and tasks.\nDifficulty: 8/10\n</document_analysis>\n\n<output_format>\nQuestion: Which factor most fundamentally explains why expecting consistently lower perplexity or higher memorization for contaminated examples fails across various LLMs and tasks?\nA: Legal and privacy restrictions that prevent full access to the training data  \nB: Model- and task-dependent variability in how each LLM memorizes and computes perplexity  \nC: The static nature of seed datasets in traditional benchmarks  \nD: The qualitative scoring system (\u25cf\u25cf, \u25d0\u25d0, \u25cb\u25cb) used to evaluate dynamic benchmarks  \n\nExplanation: Different architectures and task objectives lead to distinct memorization dynamics and perplexity responses, so contamination cannot be reliably detected by a single perplexity/memorization assumption.  \nCorrect Answer: B\n</output_format>",
    "question": "Which factor most fundamentally explains why expecting consistently lower perplexity or higher memorization for contaminated examples fails across various LLMs and tasks?",
    "choices": {
      "A": "Legal and privacy restrictions that prevent full access to the training data",
      "B": "Model- and task-dependent variability in how each LLM memorizes and computes perplexity",
      "C": "The static nature of seed datasets in traditional benchmarks",
      "D": "The qualitative scoring system (\u25cf\u25cf, \u25d0\u25d0, \u25cb\u25cb) used to evaluate dynamic benchmarks"
    },
    "answer": "B",
    "explanation": "Different architectures and task objectives lead to distinct memorization dynamics and perplexity responses, so contamination cannot be reliably detected by a single perplexity/memorization assumption.",
    "question_token_count": 27,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 1,
    "avg_answer_token_count": 15
  },
  {
    "context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n",
    "topic": "Key reliability and reproducibility challenges inherent to dynamic benchmarking of large language models.",
    "response": "<document_analysis>\nThe context outlines that dynamic benchmarking reduces data contamination but faces challenges in reliability and reproducibility due to evolving evaluation inputs and environmental dependencies. Key issues include unversioned, real-time data streams and lack of standardized archival processes. Static methods avoid this but suffer from contamination. The analysis focuses on the paradox of contamination mitigation versus reproducibility. Difficulty rated 8/10: requires deep understanding of benchmarking trade\u2011offs in LLM evaluation.\n</document_analysis>\n\n<output_format>\nQuestion: Which dynamic benchmarking practice mitigates static data contamination yet most directly undermines reproducibility by introducing untracked variability in evaluation inputs?\nA: Sampling live, user\u2011generated queries in real time without archiving their state  \nB: Freezing dataset snapshots before each evaluation run  \nC: Employing a fixed random seed for all model generations  \nD: Version\u2011controlling evaluation scripts and metrics  \nExplanation: Real\u2011time sampling of user queries ensures novel, uncontaminated inputs but, without archiving snapshots, the exact test set changes across runs, impairing reproducibility.  \nCorrect Answer: A\n</output_format>",
    "question": "Which dynamic benchmarking practice mitigates static data contamination yet most directly undermines reproducibility by introducing untracked variability in evaluation inputs?",
    "choices": {
      "A": "Sampling live, user\u2011generated queries in real time without archiving their state",
      "B": "Freezing dataset snapshots before each evaluation run",
      "C": "Employing a fixed random seed for all model generations",
      "D": "Version\u2011controlling evaluation scripts and metrics"
    },
    "answer": "A",
    "explanation": "Real\u2011time sampling of user queries ensures novel, uncontaminated inputs but, without archiving snapshots, the exact test set changes across runs, impairing reproducibility.",
    "question_token_count": 25,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 9,
    "question_groundedness_score": 4,
    "avg_answer_token_count": 11
  },
  {
    "context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n",
    "topic": "Design an experimental framework to study the effect of varying external vs. internal diversity levels on downstream model performance.",
    "response": "<document_analysis>\nThe context defines two quantitative diversity metrics\u2014external diversity (\u0398 between each transformed dataset D_i and the seed D) and internal diversity (\u0398 between pairs of transformed datasets D_i, D_j)\u2014and highlights the need to measure both with functions like n\u2011gram overlap or BLEU. A deep experimental design challenge is how to independently vary one metric while holding the other constant. Crafting a question about the protocol to decouple these effects probes a domain expert\u2019s understanding of sampling constraints, metric control, and experimental rigor. Difficulty is rated 8/10 due to subtlety in maintaining fixed external diversity while altering internal diversity.\n</document_analysis>\n\n<output_format>\nQuestion: In an experimental framework aiming to isolate internal diversity effects on downstream model performance, which protocol most effectively varies \u0398(D_i, D_j) across multiple transformations while keeping \u0398(D_i, D) constant for all i?\nA: Apply rejection sampling to select transformation trials D_i such that each \u0398(D_i, D) falls within a narrow target range, then choose trials with maximally distinct pairwise \u0398(D_i, D_j).\nB: Partition the seed dataset into N disjoint subsets and apply the same transformation process to each, assuming this equalizes \u0398(D_i, D) but randomizes \u0398(D_i, D_j).\nC: Use different transformation algorithms for each D_i to maximize pairwise diversity, without controlling their similarity to the seed.\nD: Uniformly vary the transformation intensity parameter (e.g., n\u2011gram perturbation rate) across trials, affecting both \u0398(D_i, D) and \u0398(D_i, D_j) simultaneously.\nExplanation: Option A explicitly enforces identical external diversity via rejection sampling while selecting for varying internal diversity, thus decoupling the two metrics.\nCorrect Answer: A\n</output_format>",
    "question": "In an experimental framework aiming to isolate internal diversity effects on downstream model performance, which protocol most effectively varies \u0398(D_i, D_j) across multiple transformations while keeping \u0398(D_i, D) constant for all i?",
    "choices": {
      "A": "Apply rejection sampling to select transformation trials D_i such that each \u0398(D_i, D) falls within a narrow target range, then choose trials with maximally distinct pairwise \u0398(D_i, D_j).",
      "B": "Partition the seed dataset into N disjoint subsets and apply the same transformation process to each, assuming this equalizes \u0398(D_i, D) but randomizes \u0398(D_i, D_j).",
      "C": "Use different transformation algorithms for each D_i to maximize pairwise diversity, without controlling their similarity to the seed.",
      "D": "Uniformly vary the transformation intensity parameter (e.g., n\u2011gram perturbation rate) across trials, affecting both \u0398(D_i, D) and \u0398(D_i, D_j) simultaneously."
    },
    "answer": "A",
    "explanation": "Option A explicitly enforces identical external diversity via rejection sampling while selecting for varying internal diversity, thus decoupling the two metrics.",
    "question_token_count": 44,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 35
  },
  {
    "context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n",
    "topic": "Critically evaluate the role of human-annotated versus synthetic fine-tuning datasets in exacerbating contamination, including comparative risks and detection challenges.",
    "response": "<document_analysis>\nThe context discusses how LLMs are pre-trained on vast, scraped corpora and then fine-tuned on either human-annotated or synthetic datasets, both of which can inadvertently resemble evaluation tasks and thereby heighten contamination risks. It further notes that retrieval-based detection methods struggle at scale and that proprietary training data opacity compounds the problem. Key differentiators between human-annotated and synthetic datasets include provenance transparency and detectability of overlaps. Synthetic data\u2019s machine-generated nature makes its provenance opaque and its overlap with evaluation items harder to detect via retrieval methods. \nDifficulty: 8/10\nPlanned Question: Probe the core comparative risk factor\u2014opaque provenance of synthetic data\u2014that escalates contamination and impedes retrieval-based detection relative to human-annotated data.\n</document_analysis>\n\n<output_format>\nQuestion: In large language model fine-tuning, what key characteristic of synthetic datasets, compared to human-annotated ones, most significantly amplifies the risk of contaminating evaluation benchmarks and eludes retrieval-based detection?\nA: Uniform annotation standards that produce overly consistent examples  \nB: Opaque provenance that can inadvertently include reprints of evaluation-like content  \nC: Smaller dataset sizes that hinder effective sampling for contamination screening  \nD: Explicit human oversight that obfuscates machine-origin patterns  \nExplanation: Synthetic datasets are machine-generated with unclear provenance and may unknowingly replicate or paraphrase evaluation-like examples, making retrieval-based overlap detection far less effective than with human-annotated data.  \nCorrect Answer: B\n</output_format>",
    "question": "In large language model fine-tuning, what key characteristic of synthetic datasets, compared to human-annotated ones, most significantly amplifies the risk of contaminating evaluation benchmarks and eludes retrieval-based detection?",
    "choices": {
      "A": "Uniform annotation standards that produce overly consistent examples",
      "B": "Opaque provenance that can inadvertently include reprints of evaluation-like content",
      "C": "Smaller dataset sizes that hinder effective sampling for contamination screening",
      "D": "Explicit human oversight that obfuscates machine-origin patterns"
    },
    "answer": "B",
    "explanation": "Synthetic datasets are machine-generated with unclear provenance and may unknowingly replicate or paraphrase evaluation-like examples, making retrieval-based overlap detection far less effective than with human-annotated data.",
    "question_token_count": 41,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 7,
    "question_difficulty_score": 9,
    "question_groundedness_score": 8,
    "avg_answer_token_count": 11
  },
  {
    "context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n",
    "topic": "How centralized evaluation systems impede detailed error analysis due to protected labels",
    "response": "<document_analysis>\nThe context highlights that protected labels within centralized evaluation systems restrict researchers\u2019 access to ground-truth, forcing reliance on opaque, aggregated performance scores and inhibiting detailed error analysis and reproducibility. This limitation is the central idea: without label transparency, per-instance diagnostics, confusion matrices, and error slices cannot be independently computed. Irrelevant details (e.g., post-hoc detection methods) are secondary here, so the question focuses on the impact of label protection on error analysis.\nDifficulty calibration: 8/10, as it requires understanding of evaluation pipeline transparency issues and their implications on error diagnostics.\nThe planned question asks which mechanism specifically impedes detailed error analysis due to label protection in centralized evaluation.\n</document_analysis>\n\n<output_format>\nQuestion: Which primary consequence of label protection in centralized evaluation systems most directly prevents detailed error analysis and reproducibility?\nA: Inability to access true test labels, blocking per-instance error categorization.\nB: Automatic adjustment of scoring thresholds, introducing non-deterministic performance drift.\nC: Exclusive use of composite evaluation metrics, obscuring specific failure modes.\nD: Rate limits on API queries, restricting the number of test examples evaluated.\nExplanation: Hiding true labels stops researchers from computing per-example error breakdowns or confusion matrices, directly blocking detailed analysis and reproducibility.\nCorrect Answer: A\n</output_format>",
    "question": "Which primary consequence of label protection in centralized evaluation systems most directly prevents detailed error analysis and reproducibility?",
    "choices": {
      "A": "Inability to access true test labels, blocking per-instance error categorization.",
      "B": "Automatic adjustment of scoring thresholds, introducing non-deterministic performance drift.",
      "C": "Exclusive use of composite evaluation metrics, obscuring specific failure modes.",
      "D": "Rate limits on API queries, restricting the number of test examples evaluated."
    },
    "answer": "A",
    "explanation": "Hiding true labels stops researchers from computing per-example error breakdowns or confusion matrices, directly blocking detailed analysis and reproducibility.",
    "question_token_count": 20,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 14
  },
  {
    "context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n",
    "topic": "The impact of high Collision Rate on the validity and reliability of benchmarking LLM capabilities.",
    "response": "<document_analysis>\nThe context introduces dynamic benchmarking for LLMs and identifies \u201cCollision Rate\u201d as the percentage of overlap between two independently transformed versions of a benchmark dataset, used to gauge contamination risk. It also defines \u201cRepeat Trials\u201d as the expected number of transformations needed to reproduce an existing transformation. A key underlying assumption is that these transformations are independent; if they are correlated, the measured Collision Rate will underestimate actual overlap. This subtlety is implicit but critical for expert understanding.\nDifficulty: 8/10 \u2013 requires recognizing statistical assumptions underlying the metric and the impact of their violation.\n</document_analysis>\n\n<output_format>\nQuestion: In the context of dynamic benchmarking\u2019s Collision Rate metric, what fundamental assumption about dataset transformations must hold to ensure accurate estimation of contamination risk, and what specific breach of this assumption would most directly cause the Collision Rate to underestimate actual data overlap?\nA: Transformations must be independent; if they share patterns or heuristics, the Collision Rate will underestimate true overlap.\nB: Dataset difficulty must remain constant; if difficulty varies across transforms, the Collision Rate will underestimate contamination risk.\nC: Data distributions must be identical across trials; if distributions shift, the Collision Rate will underestimate actual overlap.\nD: Transformations must preserve all information content; if they discard features, the Collision Rate will underestimate the risk of contamination.\nExplanation: The Collision Rate presumes independent transformations; any correlation (shared patterns) inflates overlap beyond the measured rate, leading to underestimation.\nCorrect Answer: A\n</output_format>",
    "question": "In the context of dynamic benchmarking\u2019s Collision Rate metric, what fundamental assumption about dataset transformations must hold to ensure accurate estimation of contamination risk, and what specific breach of this assumption would most directly cause the Collision Rate to underestimate actual data overlap?",
    "choices": {
      "A": "Transformations must be independent; if they share patterns or heuristics, the Collision Rate will underestimate true overlap.",
      "B": "Dataset difficulty must remain constant; if difficulty varies across transforms, the Collision Rate will underestimate contamination risk.",
      "C": "Data distributions must be identical across trials; if distributions shift, the Collision Rate will underestimate actual overlap.",
      "D": "Transformations must preserve all information content; if they discard features, the Collision Rate will underestimate the risk of contamination."
    },
    "answer": "A",
    "explanation": "The Collision Rate presumes independent transformations; any correlation (shared patterns) inflates overlap beyond the measured rate, leading to underestimation.",
    "question_token_count": 48,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 4,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 22
  },
  {
    "context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n",
    "topic": "Discuss the role of verification in live benchmarks, reasons it is often neglected, and the potential impact on benchmark validity.",
    "response": "<document_analysis>\nThe context emphasizes that live benchmarks for LLMs often rely on temporal cutoffs and human-driven data collection but frequently skip the step of verification. This oversight permits reused or contaminated problems to remain in evaluation sets, which can artificially inflate model performance and undermine the benchmark\u2019s validity. The central idea is the critical role of verification in ensuring novel, untainted test items. A question that challenges experts should probe the nuanced consequence of neglecting verification\u2014specifically, how it compromises the integrity of evaluation metrics by introducing contamination rather than touching on more superficial or unrelated issues like template mechanics or human cost.\nDifficulty: 9/10\n</document_analysis>\n\n<output_format>\nQuestion: In live benchmarks for large language models that use temporal cutoffs and human-driven data collection but omit verification, what is the primary threat to the benchmark\u2019s validity?\nA: Artificial performance inflation from previously seen or contaminated problems  \nB: Systematic generation bias introduced by randomizing template variables  \nC: Unsustainable human labor demands that reduce benchmark scalability  \nD: Increased collision probability in rule-based test case synthesis  \nExplanation: Without verification, reused competition problems or contaminated samples remain in the test set, causing models to leverage prior exposure and thus artificially inflate evaluation scores.  \nCorrect Answer: A\n</output_format>",
    "question": "In live benchmarks for large language models that use temporal cutoffs and human-driven data collection but omit verification, what is the primary threat to the benchmark\u2019s validity?",
    "choices": {
      "A": "Artificial performance inflation from previously seen or contaminated problems",
      "B": "Systematic generation bias introduced by randomizing template variables",
      "C": "Unsustainable human labor demands that reduce benchmark scalability",
      "D": "Increased collision probability in rule-based test case synthesis"
    },
    "answer": "A",
    "explanation": "Without verification, reused competition problems or contaminated samples remain in the test set, causing models to leverage prior exposure and thus artificially inflate evaluation scores.",
    "question_token_count": 33,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 1,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 10
  },
  {
    "context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n",
    "topic": "Reflect on the ethical and scientific responsibilities of researchers in reporting and addressing contamination in NLP evaluations.",
    "response": "<document_analysis>\nThe context outlines two critical themes: the subtle nature of syntactic data contamination in NLP benchmarks and the broader ethical and scientific implications of ignoring such contamination. It emphasizes the challenge of distinguishing memorization from genuine reasoning, the risk of inflated performance metrics, and the necessity for transparency and mitigation to preserve benchmark validity, robustness, and real-world applicability. A high-difficulty question should synthesize these ideas, asking about researchers\u2019 dual responsibility to both disclose and address contamination to uphold scientific integrity and ethical standards.\n</document_analysis>\n\n<output_format>\nQuestion: In ensuring ethical and scientific integrity when evaluating large language models, which action best aligns with researchers\u2019 responsibilities regarding data contamination in NLP benchmarks?\nA: Incorporate diverse real-world datasets into evaluations without needing to disclose prior overlap, as long as performance improves.\nB: Eliminate and publicly disclose any contaminated test items to maintain transparency, preserve benchmark validity, and prevent misleading conclusions about model capabilities.\nC: Assume that minor contamination effects average out across large-scale benchmarks, focusing instead on overall performance gains.\nD: Rely primarily on syntactic transformations of existing data, since these always provide novel evaluation challenges without requiring contamination checks.\nExplanation: Option B captures the dual responsibility to both remove and disclose contamination, ensuring transparent, valid, and reliable assessments of model generalization and preventing inflated performance claims.\nCorrect Answer: B\n</output_format>",
    "question": "In ensuring ethical and scientific integrity when evaluating large language models, which action best aligns with researchers\u2019 responsibilities regarding data contamination in NLP benchmarks?",
    "choices": {
      "A": "Incorporate diverse real-world datasets into evaluations without needing to disclose prior overlap, as long as performance improves.",
      "B": "Eliminate and publicly disclose any contaminated test items to maintain transparency, preserve benchmark validity, and prevent misleading conclusions about model capabilities.",
      "C": "Assume that minor contamination effects average out across large-scale benchmarks, focusing instead on overall performance gains.",
      "D": "Rely primarily on syntactic transformations of existing data, since these always provide novel evaluation challenges without requiring contamination checks."
    },
    "answer": "B",
    "explanation": "Option B captures the dual responsibility to both remove and disclose contamination, ensuring transparent, valid, and reliable assessments of model generalization and preventing inflated performance claims.",
    "question_token_count": 28,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 4,
    "avg_answer_token_count": 23
  },
  {
    "context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n",
    "topic": "Explain the limitations of using pre-defined rules for sample generation and how they contribute to reduced diversity and increased in-distribution contamination.",
    "response": "<document_analysis>\nThe context highlights that using pre-defined rule-based methods for generating samples constrains the variety of examples and, when these rule-generated datasets are publicly available, they risk overlapping with the target distribution\u2014thereby introducing in-distribution contamination during model training. This key limitation contrasts with LLM-based rewriting techniques that aim for higher diversity and controlled contamination detection. To challenge domain experts, the question will probe their understanding of both the diversity constraint and contamination risk inherent in pre-defined rule generation, requiring them to distinguish this approach from alternative LLM-driven methods.\n</document_analysis>\n\n<output_format>\nQuestion: Which of the following most accurately describes how pre-defined rule-based sample generation methods undermine the quality of training data for large language models?\nA: They constrain the linguistic and conceptual variety of generated examples and, due to the public availability of rule-generated datasets, can inadvertently duplicate target-distribution content, increasing in-distribution contamination.\nB: They automatically verify each new sample against existing benchmarks to ensure uniqueness, thereby minimizing contamination but sometimes reducing overall dataset size.\nC: They employ LLM-driven paraphrasing of benchmark items to boost cognitive diversity, though this often leads to inconsistent difficulty levels.\nD: They randomly replace variables within benchmark questions to maximize novelty, which can introduce semantic incoherence but significantly increases diversity.\nExplanation: Pre-defined rules inherently limit sample diversity and, when the resulting data is publicly released, risk replicating original distribution instances, thus increasing in-distribution contamination.\nCorrect Answer: A\n</output_format>",
    "question": "Which of the following most accurately describes how pre-defined rule-based sample generation methods undermine the quality of training data for large language models?",
    "choices": {
      "A": "They constrain the linguistic and conceptual variety of generated examples and, due to the public availability of rule-generated datasets, can inadvertently duplicate target-distribution content, increasing in-distribution contamination.",
      "B": "They automatically verify each new sample against existing benchmarks to ensure uniqueness, thereby minimizing contamination but sometimes reducing overall dataset size.",
      "C": "They employ LLM-driven paraphrasing of benchmark items to boost cognitive diversity, though this often leads to inconsistent difficulty levels.",
      "D": "They randomly replace variables within benchmark questions to maximize novelty, which can introduce semantic incoherence but significantly increases diversity."
    },
    "answer": "A",
    "explanation": "Pre-defined rules inherently limit sample diversity and, when the resulting data is publicly released, risk replicating original distribution instances, thus increasing in-distribution contamination.",
    "question_token_count": 27,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 26
  },
  {
    "context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n",
    "topic": "Comparative strengths and weaknesses of direct overlap versus behavior\u2011based post\u2011hoc detection methods",
    "response": "<document_analysis>\nThe context outlines two main categories of post-hoc contamination detection: direct overlap methods (n\u2011gram matching at token/word level, embedding\u2011based similarity, improved mapping metrics) and behavior\u2011based methods (masked input memorization, partial completions, preference for original vs paraphrased, cross\u2011benchmark performance comparisons like CONSTAT).  \nKey nuances:\n- Direct overlap: precise but prone to false negatives on paraphrases; embedding\u2011based similarity reduces some false negatives but still tied to representational thresholds.\n- Behavior\u2011based: evaluates model outputs under manipulations (masking, paraphrasing, partial inputs), capable of revealing memorization beyond literal overlap.\nThis suggests combining semantic matching with behavioral probing for comprehensive detection.  \nDifficulty: 8/10 (requires synthesis of methods and their complementary strengths).\n</document_analysis>\n\n<output_format>\nQuestion: For comprehensive post\u2011hoc detection of data contamination\u2014capturing both semantically altered (paraphrased) overlaps and hidden memorization patterns\u2014which combination of methods is most effective?\nA: Token\u2011level n\u2011gram matching coupled with partial completion probes  \nB: Embedding\u2011based similarity detection paired with masked\u2011input memorization tests  \nC: Enhanced mapping metrics together with cross\u2011benchmark performance comparisons  \nD: Preference tests between original and paraphrased inputs combined with token\u2011level n\u2011gram matching  \nExplanation: Embedding\u2011based techniques detect semantic (paraphrased) overlaps missed by exact matching, while masked\u2011input memorization tests reveal memorization regardless of any lexical or semantic overlap.  \nCorrect Answer: B\n</output_format>",
    "question": "For comprehensive post\u2011hoc detection of data contamination\u2014capturing both semantically altered (paraphrased) overlaps and hidden memorization patterns\u2014which combination of methods is most effective?",
    "choices": {
      "A": "Token\u2011level n\u2011gram matching coupled with partial completion probes",
      "B": "Embedding\u2011based similarity detection paired with masked\u2011input memorization tests",
      "C": "Enhanced mapping metrics together with cross\u2011benchmark performance comparisons",
      "D": "Preference tests between original and paraphrased inputs combined with token\u2011level n\u2011gram matching"
    },
    "answer": "B",
    "explanation": "Embedding\u2011based techniques detect semantic (paraphrased) overlaps missed by exact matching, while masked\u2011input memorization tests reveal memorization regardless of any lexical or semantic overlap.",
    "question_token_count": 37,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 9,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 14
  },
  {
    "context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n",
    "topic": "Potential broader applications of dynamic benchmarking beyond contamination risk reduction.",
    "response": "<document_analysis>\nThe context outlines the shift from static to dynamic benchmarking for reducing data contamination in LLMs, highlights the lack of standard evaluation criteria, and proposes optimal design principles for dynamic benchmarks. Central ideas include the iterative, continuously updated nature of dynamic benchmarks and their potential for real-time, evolving evaluation beyond just contamination checks. For experts, probing how these core principles extend to other domains\u2014robustness, fairness, drift detection\u2014invites deep reflection. Difficulty is rated 8/10, since recognizing which evaluation tasks inherently require dynamic updates versus static assessments demands nuanced understanding.\n</document_analysis>\n\n<output_format>\nQuestion: Which of the following is NOT a plausible broader application of dynamic benchmarking, given its core design principles of continuously evolving evaluation sets?\nA: Continuously assessing adversarial robustness against newly emerging attack strategies  \nB: Iterative fairness monitoring to track biases under shifting societal norms  \nC: Real-time detection of performance drift when adapting an LLM to new specialized domains  \nD: A one-time static evaluation of model capabilities prior to deployment  \nExplanation: Dynamic benchmarking relies on ongoing updates to test instances to capture evolving challenges; a single static evaluation does not leverage its continuously adaptive nature.  \nCorrect Answer: D\n</output_format>",
    "question": "Which of the following is NOT a plausible broader application of dynamic benchmarking, given its core design principles of continuously evolving evaluation sets?",
    "choices": {
      "A": "Continuously assessing adversarial robustness against newly emerging attack strategies",
      "B": "Iterative fairness monitoring to track biases under shifting societal norms",
      "C": "Real-time detection of performance drift when adapting an LLM to new specialized domains",
      "D": "A one-time static evaluation of model capabilities prior to deployment"
    },
    "answer": "D",
    "explanation": "Dynamic benchmarking relies on ongoing updates to test instances to capture evolving challenges; a single static evaluation does not leverage its continuously adaptive nature.",
    "question_token_count": 26,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 4,
    "question_difficulty_score": 8,
    "question_groundedness_score": 6,
    "avg_answer_token_count": 12
  }
]