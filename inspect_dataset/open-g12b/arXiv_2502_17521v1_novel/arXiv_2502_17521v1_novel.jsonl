[
  {
    "context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n",
    "topic": "Explain the role of the diversity function, \u0398(\u22c5), in calculating both external and internal diversity, and provide examples of metrics that could be used for this function.",
    "response": "<document_analysis>\nThe context discusses diversity in transformed datasets, differentiating between external and internal diversity. It provides mathematical formulations for calculating these diversities, using a function \u0398(\u22c5) to measure the difference between datasets. The document explicitly mentions N-gram metrics and reference-based metrics (like BLEU scores) as examples of functions that could be used for \u0398(\u22c5). The difficulty is rated 8/10 because it requires understanding of the mathematical notation, the concept of dataset diversity, and the application of specific metrics. The question will probe the understanding of \u0398(\u22c5)'s role and elicit examples, requiring synthesis of the information provided. No irrelevant information was present.\n</document_analysis>\n\n<output_format>\nQuestion: Considering the provided definitions of external and internal dataset diversity, how does the choice of the diversity function, \u0398(\u22c5), influence the quantification of these diversity measures, and what are the implications of selecting metrics like BLEU scores versus N-gram metrics for this function?\nExplanation: The question probes the expert's understanding of the function's role in quantifying diversity, forcing them to consider the implications of different metric choices and their impact on the calculated diversity values.\nCorrect Answer: \u0398(\u22c5) quantifies the dissimilarity between datasets; its choice directly impacts the diversity score, with BLEU scores emphasizing reference alignment while N-gram metrics focus on lexical variation.\n</output_format>",
    "question": "Considering the provided definitions of external and internal dataset diversity, how does the choice of the diversity function, \u0398(\u22c5), influence the quantification of these diversity measures, and what are the implications of selecting metrics like BLEU scores versus N-gram metrics for this function?",
    "answer": "\u0398(\u22c5) quantifies the dissimilarity between datasets; its choice directly impacts the diversity score, with BLEU scores emphasizing reference alignment while N-gram metrics focus on lexical variation.",
    "explanation": "The question probes the expert's understanding of the function's role in quantifying diversity, forcing them to consider the implications of different metric choices and their impact on the calculated diversity values.",
    "question_token_count": 53,
    "answer_correctness_score": 9,
    "explanation_validity_score": 8,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 37
  },
  {
    "context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n",
    "topic": "Explain the distinction between external and internal diversity in the context of transformed datasets, providing a clear explanation of what each measures.",
    "response": "<document_analysis>\nThe provided text segment focuses on diversity metrics within transformed datasets. It defines and differentiates between external and internal diversity, providing mathematical formulations for each. The formulas utilize expected values (\ud835\udd3c) and a diversity function \u0398(\u22c5) to quantify the differences between datasets. The text also suggests potential implementations of \u0398(\u22c5) such as N-gram metrics or BLEU scores.\n\nDifficulty: 8/10. The question requires understanding of the mathematical notation and the conceptual difference between comparing a transformed dataset to the original (external) versus comparing two different transformations (internal). A domain expert should be able to articulate this clearly, but the mathematical notation might pose a challenge.\n\nIntentional Question Planning: The question aims to assess the expert's ability to translate the formal definitions into a clear, intuitive explanation. It probes understanding of the purpose and implications of each diversity metric.\n\nNo irrelevant information was detected in the context.\n</document_analysis>\n\n<output_format>\nQuestion: How do the metrics for external and internal diversity differ in their application to evaluating transformed datasets?\nExplanation: External diversity assesses the difference between a transformed dataset and the original seed dataset, while internal diversity evaluates the differences between two independently generated transformed datasets.\nCorrect Answer: One compares transformations to the original, the other compares transformations to each other.\n</output_format>",
    "question": "How do the metrics for external and internal diversity differ in their application to evaluating transformed datasets?",
    "answer": "One compares transformations to the original, the other compares transformations to each other.",
    "explanation": "External diversity assesses the difference between a transformed dataset and the original seed dataset, while internal diversity evaluates the differences between two independently generated transformed datasets.",
    "question_token_count": 19,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 4,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 16
  },
  {
    "context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n",
    "topic": "The significance of analyzing model behavior under various conditions, such as masked inputs and partial completions, in post-hoc data contamination detection.",
    "response": "<document_analysis>\nThe provided context discusses data contamination in large language models and the methods used to detect it. It specifically highlights \"post-hoc detection\" as a mitigation strategy. The section delves into various techniques within post-hoc detection, including n-gram matching, embedding-based similarity, and analyzing model behavior under different conditions. The latter category, which is the focus of the question topic, includes methods like masked inputs, partial completions, and preference for original test cases. The analysis indicates that these behavioral analyses are employed to identify memorization and other signs of contamination beyond simple overlap detection. The difficulty level is rated 8/10, as it requires understanding of the nuances of data contamination detection and the rationale behind employing these specific behavioral analyses. The question will probe the domain expert's understanding of why these techniques are necessary and what they reveal about potential contamination. There is no irrelevant or bogus information present.\n</document_analysis>\n\n<output_format>\nQuestion: Why do post-hoc data contamination detection methods analyze model behavior under conditions like masked inputs and partial completions?\nExplanation: Detecting contamination requires methods beyond simple overlap detection, as models can memorize data without exact matches. Analyzing behavior under specific conditions reveals such memorization patterns.\nCorrect Answer: Memorization patterns\n</output_format>",
    "question": "Why do post-hoc data contamination detection methods analyze model behavior under conditions like masked inputs and partial completions?",
    "answer": "Memorization patterns",
    "explanation": "Detecting contamination requires methods beyond simple overlap detection, as models can memorize data without exact matches. Analyzing behavior under specific conditions reveals such memorization patterns.",
    "question_token_count": 22,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 5
  },
  {
    "context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "[ Topic Text ]",
    "response": "<document_analysis>\n1. **Thoughtful Content Examination:** The context details the evolution of LLM benchmarking, highlighting the critical issue of data contamination and the shift from static to dynamic methods. It identifies a gap in the literature \u2013 the lack of a systematic survey and evaluation criteria for dynamic benchmarking. The paper aims to address this gap by surveying existing methods and proposing evaluation criteria.\n\n2. **Concept Exploration:** The text implicitly assumes a deep understanding of machine learning principles, particularly the separation of training and test sets. It also touches upon the practical challenges of tracing training data for large LLMs due to privacy and commercial concerns. The discussion of data encryption and post-hoc contamination detection reveals an understanding of mitigation strategies. The evolution from static to dynamic benchmarking indicates an awareness of the limitations of traditional approaches.\n\n3. **Strategic Complexity Calibration:** Difficulty: 8. The question will require the integration of several concepts from the text \u2013 data contamination, static vs. dynamic benchmarking, and the paper's contribution. A domain expert should be able to answer, but it won't be immediately obvious.\n\n4. **Intentional Question Planning:** The question will focus on the core argument of the paper: the lack of evaluation criteria for dynamic benchmarks. It aims to probe the expert's understanding of why this is a significant issue and how the paper attempts to address it. The question will be designed to elicit a nuanced response that goes beyond a simple restatement of the paper's goals.\n</document_analysis>\n<output_format>\nQuestion: Considering the inherent challenges in tracing training data for Large Language Models (LLMs), how does the absence of standardized evaluation criteria for dynamic benchmarks exacerbate the risk of flawed intelligence assessment compared to static benchmarks, and why is this a novel concern?\nExplanation: This question requires the expert to connect the difficulty in data provenance with the need for robust evaluation of dynamic benchmarks, recognizing that the evolving nature of dynamic benchmarks necessitates a specific evaluation framework not previously addressed. It probes the expert\u2019s understanding of the limitations of static benchmarks in the face of data contamination and why dynamic benchmarks, despite their potential, require careful scrutiny.\nCorrect Answer: Dynamic benchmarks, while intended to mitigate data contamination, lack standardized evaluation, making it difficult to assess their effectiveness and potentially leading to flawed intelligence assessments due to unvalidated methodologies. This is novel because prior reviews focused on contamination detection, not the benchmarks themselves.\n</output_format>",
    "question": "Considering the inherent challenges in tracing training data for Large Language Models (LLMs), how does the absence of standardized evaluation criteria for dynamic benchmarks exacerbate the risk of flawed intelligence assessment compared to static benchmarks, and why is this a novel concern?",
    "answer": "Dynamic benchmarks, while intended to mitigate data contamination, lack standardized evaluation, making it difficult to assess their effectiveness and potentially leading to flawed intelligence assessments due to unvalidated methodologies. This is novel because prior reviews focused on contamination detection, not the benchmarks themselves.",
    "explanation": "This question requires the expert to connect the difficulty in data provenance with the need for robust evaluation of dynamic benchmarks, recognizing that the evolving nature of dynamic benchmarks necessitates a specific evaluation framework not previously addressed. It probes the expert\u2019s understanding of the limitations of static benchmarks in the face of data contamination and why dynamic benchmarks, despite their potential, require careful scrutiny.",
    "question_token_count": 49,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 51
  },
  {
    "context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n",
    "topic": "Analyze the importance of standardized dynamic evaluation and practical mitigation tools for data contamination in LLMs, and discuss the feasibility of achieving these goals.",
    "response": "`<document_analysis>`\nThe provided context discusses data contamination in LLM benchmarking, differentiating between static and dynamic approaches. It identifies limitations in both, highlighting the vulnerability of static methods to growing training datasets and the challenges of reliability and reproducibility in dynamic methods. The conclusion emphasizes the need for standardized dynamic evaluation and practical mitigation tools. The \"Limitations\" section acknowledges the rapidly evolving nature of the field, potential gaps in coverage, the need for refinement of dynamic benchmarking criteria, and a lack of detailed technical implementation guidelines.\n\nThe question topic asks for an analysis of the importance of standardized dynamic evaluation and practical mitigation tools, and a discussion of the feasibility of achieving these goals. This requires synthesizing information from both the conclusion and the limitations section, and making inferences about the broader challenges of LLM benchmarking. The difficulty is rated 8 due to the need for nuanced understanding of the tradeoffs between static and dynamic methods, and the realistic assessment of the challenges in implementing effective solutions. The question is designed to encourage reflection on the practical implications of the survey\u2019s findings and the future direction of research in this area.\n\nthought_process: The question requires integrating the call for standardized dynamic evaluation and mitigation tools from the conclusion with the limitations section, which outlines the challenges in achieving these goals. The question is designed to be challenging by asking for a feasibility discussion, which necessitates critical thinking about the current state of the field and the potential roadblocks. The answer needs to be concise, focusing on the core issues.\n\n</document_analysis>`\n\n<output_format>\nQuestion: Considering the identified limitations in current LLM benchmarking practices, what are the primary obstacles to implementing standardized dynamic evaluation and practical mitigation tools for data contamination, and how might these obstacles be realistically addressed?\nExplanation: The question probes the feasibility of the recommended solutions (standardized dynamic evaluation and mitigation tools) by directly asking about obstacles and potential solutions, requiring synthesis of the conclusion's recommendations and the limitations section's caveats. It requires the expert to assess the practical challenges beyond the stated needs.\nCorrect Answer: Rapid evolution, reliability concerns, and lack of detailed implementation guidelines.\n</output_format>",
    "question": "Considering the identified limitations in current LLM benchmarking practices, what are the primary obstacles to implementing standardized dynamic evaluation and practical mitigation tools for data contamination, and how might these obstacles be realistically addressed?",
    "answer": "Rapid evolution, reliability concerns, and lack of detailed implementation guidelines.",
    "explanation": "The question probes the feasibility of the recommended solutions (standardized dynamic evaluation and mitigation tools) by directly asking about obstacles and potential solutions, requiring synthesis of the conclusion's recommendations and the limitations section's caveats. It requires the expert to assess the practical challenges beyond the stated needs.",
    "question_token_count": 39,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 14
  },
  {
    "context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n",
    "topic": "Describe the strategy used by MMLU-CF to create novel multiple-choice questions, specifically addressing the manipulation of answer choices and the random replacement of incorrect options.",
    "response": "`<document_analysis>`\nThe provided context discusses limitations of LLM evaluation and rule-based test case generation techniques. Specifically, it details how MMLU-CF generates novel samples. The analysis identifies that the core strategy involves shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\" The difficulty rating is 8, as it requires careful reading and understanding of a specific detail within a broader discussion of LLM evaluation methods. The question is designed to assess understanding of a particular technique, requiring the expert to recall a specific detail and articulate it concisely. No irrelevant information was present and the content is of high educational value.\n`</document_analysis>`\n\n<output_format>\nQuestion: How does MMLU-CF generate new multiple-choice questions?\nExplanation: MMLU-CF generates new questions by manipulating existing multiple-choice questions.\nCorrect Answer: Shuffling answer choices and replacing incorrect options with \"None of the other choices.\"\n</output_format>",
    "question": "How does MMLU-CF generate new multiple-choice questions?",
    "answer": "Shuffling answer choices and replacing incorrect options with \"None of the other choices.\"",
    "explanation": "MMLU-CF generates new questions by manipulating existing multiple-choice questions.",
    "question_token_count": 14,
    "answer_correctness_score": 10,
    "explanation_validity_score": 8,
    "question_clarity_score": 9,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 18
  },
  {
    "context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n",
    "topic": "Discuss the significance of using competitive platforms like Codeforces and datasets like Aider in probing dynamic problem-solving capabilities of language models.",
    "response": "<document_analysis>\nThe provided context discusses various benchmarks used to evaluate language models across different capabilities: coding, instruction following, and reasoning. The coding section highlights HumanEval, MBPP, and SWE-Bench, then introduces Codeforces and Aider as competitive platforms and datasets, respectively, for probing dynamic problem-solving. The instruction following section details IFEval, InfoBench, and C-Eval. Finally, the reasoning section covers PIQA, SIQA, HellaSwag, WinoGrande, ARC, OpenBookQA, CommonsenseQA, and C-SimpleQA.\n\nThe question topic focuses specifically on the significance of Codeforces and Aider within the coding benchmark landscape. The context explicitly states that these tools \"further probe dynamic problem solving.\" The difficulty is rated 7/10 because it requires understanding the role of competitive platforms and datasets in evaluating dynamic problem-solving, which isn't explicitly defined but can be inferred.\n\nThe question will be formulated to elicit a response that demonstrates an understanding of the differences between static benchmarks (like HumanEval and MBPP) and the more dynamic nature of competitive coding environments. The goal is to assess whether the respondent grasps why these platforms are valuable additions to the evaluation suite.\n</document_analysis>\n<output_format>\nQuestion: How does the inclusion of competitive coding environments like Codeforces and datasets such as Aider enhance the evaluation of language models compared to static code generation benchmarks?\nExplanation: The inclusion of dynamic environments like Codeforces and Aider allows for a more realistic assessment of a model\u2019s coding abilities, as these platforms require adaptation and problem-solving in response to changing conditions.\nCorrect Answer: Dynamic problem solving.\n</output_format>",
    "question": "How does the inclusion of competitive coding environments like Codeforces and datasets such as Aider enhance the evaluation of language models compared to static code generation benchmarks?",
    "answer": "Dynamic problem solving.",
    "explanation": "The inclusion of dynamic environments like Codeforces and Aider allows for a more realistic assessment of a model\u2019s coding abilities, as these platforms require adaptation and problem-solving in response to changing conditions.",
    "question_token_count": 31,
    "answer_correctness_score": 8,
    "explanation_validity_score": 8,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 5
  },
  {
    "context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n",
    "topic": "The implications of label protection on the transparency and independent verification of large language model performance metrics.",
    "response": "<document_analysis>\nThe provided context discusses limitations in evaluating large language models (LLMs), specifically focusing on label protection and post-hoc detection methods for data contamination. The central argument revolves around how label protection, while intended to safeguard data, inadvertently hinders transparency and independent verification. It then details various post-hoc detection techniques used to mitigate data contamination.\n\nThe text is highly relevant and provides a clear basis for generating a challenging question. The difficulty rating is 8/10, as it requires understanding the nuanced implications of label protection and its impact on the broader evaluation ecosystem.\n\nThe question will focus on the consequences of relying on centralized evaluation systems due to label protection, pushing the expert to consider the broader implications beyond just error analysis. The question will be designed to require a synthesis of understanding and a critical perspective on the tradeoffs involved. There is no irrelevant information to exclude.\n</document_analysis>\n\n<output_format>\nQuestion: How does the reliance on centralized evaluation systems, necessitated by label protection strategies, fundamentally alter the landscape of LLM performance assessment, and what are the potential long-term consequences for the field's ability to establish reliable and reproducible benchmarks?\nExplanation: The text states that label protection forces reliance on centralized systems, impeding error analysis and reproducibility. This question asks the expert to expand on the broader consequences of this shift in evaluation practices.\nCorrect Answer: Reduced independent validation.\n</output_format>",
    "question": "How does the reliance on centralized evaluation systems, necessitated by label protection strategies, fundamentally alter the landscape of LLM performance assessment, and what are the potential long-term consequences for the field's ability to establish reliable and reproducible benchmarks?",
    "answer": "Reduced independent validation.",
    "explanation": "The text states that label protection forces reliance on centralized systems, impeding error analysis and reproducibility. This question asks the expert to expand on the broader consequences of this shift in evaluation practices.",
    "question_token_count": 46,
    "answer_correctness_score": 8,
    "explanation_validity_score": 8,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 5
  },
  {
    "context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n",
    "topic": "Define and explain the concept of syntactic contamination as presented in the text, detailing the process of rephrasing training data with a prefix string.",
    "response": "<document_analysis>\n1. **Thoughtful Content Examination:** The context discusses data contamination in LLM benchmarking, specifically focusing on syntactic contamination. It describes how syntactic contamination occurs through rephrasing training data with a prefix string and highlights the debate surrounding whether this constitutes true contamination. The significance of addressing contamination is emphasized to ensure accurate assessment of LLM capabilities.\n\n2. **Concept Exploration:** The text implicitly acknowledges the challenge of differentiating between memorization and reasoning in LLMs when syntactic contamination is present. It also suggests that some NLP applications rely heavily on syntactic information, justifying the consideration of syntactic transformations as contamination.\n\n3. **Strategic Complexity Calibration:** Difficulty: 8. The question requires not only defining syntactic contamination but also explaining the specific process of rephrasing with a prefix string. It also necessitates understanding the ongoing debate and the reasoning behind classifying it as contamination.\n\n4. **Intentional Question Planning:** The question is designed to assess the expert's understanding of the definition and process of syntactic contamination, as well as the rationale behind its classification as contamination despite the ongoing debate. It requires a comprehensive understanding of the nuances presented in the text.\n</document_analysis>\n\n<output_format>\nQuestion: How does the text define syntactic contamination, and what methodology is used to generate it?\nExplanation: The text defines syntactic contamination as a scenario where test data is derived from training data by rephrasing it with a prefix string. This process is explicitly described as a method for creating contaminated data.\nCorrect Answer: Rephrasing with a prefix string.\n</output_format>",
    "question": "How does the text define syntactic contamination, and what methodology is used to generate it?",
    "answer": "Rephrasing with a prefix string.",
    "explanation": "The text defines syntactic contamination as a scenario where test data is derived from training data by rephrasing it with a prefix string. This process is explicitly described as a method for creating contaminated data.",
    "question_token_count": 18,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 9,
    "question_difficulty_score": 5,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 10
  },
  {
    "context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n",
    "topic": "Summarize the future research directions for LLM benchmarking, emphasizing the need for standardized criteria and other potential avenues for improvement.",
    "response": "<document_analysis>\nThe provided text discusses data contamination in LLM benchmarking and explores various approaches to address it, including hybrid generation and dynamic benchmarks. The core focus of the question topic aligns with the \"Future Directions\" section of the text. The difficulty is rated 8/10 because it requires synthesizing information from a specific section and articulating a nuanced understanding of the challenges and potential improvements. The text explicitly states the need for standardized criteria for dynamic benchmarks and suggests that future efforts should focus on this area. Other avenues for improvement include addressing the balance between correctness and scalability in dynamic benchmarks and incorporating complexity control to improve evaluation efficiency. Irrelevant information, such as specific benchmark names or equations, are excluded as they are not central to the question topic.\n</document_analysis>\n\n<output_format>\nQuestion: What is the primary area of focus for future research in LLM benchmarking, according to the text, and what other improvements are suggested?\nExplanation: The text specifically mentions the lack of standardized criteria for dynamic benchmarks as a key area needing attention, while also highlighting the need to balance correctness with scalability and incorporate complexity control.\nCorrect Answer: Standardized criteria for dynamic benchmarks.\n</output_format>",
    "question": "What is the primary area of focus for future research in LLM benchmarking, according to the text, and what other improvements are suggested?",
    "answer": "Standardized criteria for dynamic benchmarks.",
    "explanation": "The text specifically mentions the lack of standardized criteria for dynamic benchmarks as a key area needing attention, while also highlighting the need to balance correctness with scalability and incorporate complexity control.",
    "question_token_count": 27,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 9,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 8
  },
  {
    "context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n",
    "topic": "Explain how Mathador-LM generates evaluation queries, focusing on its utilization of Mathador game rules and the variation of input numbers.",
    "response": "<document_analysis>\nThe provided text discusses limitations in LLM evaluation and methods for generating test cases, specifically rule-based generation, including template-based approaches. The text highlights the human effort required in collection processes, the risk of data contamination, and the often-overlooked aspect of verification. Within rule-based generation, it details three template-based methods: GSM-Symbolic, Mathador-LM, and MMLU-CF. The question topic specifically targets Mathador-LM. The difficulty is rated 8/10 because it requires extracting specific details about a particular method from a larger discussion and synthesizing that information concisely. The text directly describes the method, making it answerable without external knowledge. No irrelevant information is present.\n</document_analysis>\n<output_format>\nQuestion: How does Mathador-LM generate evaluation queries?\nExplanation: The text details Mathador-LM's method as adhering to the rules of Mathador games and varying input numbers to create queries.\nCorrect Answer: Adhering to Mathador game rules and varying input numbers.\n</output_format>",
    "question": "How does Mathador-LM generate evaluation queries?",
    "answer": "Adhering to Mathador game rules and varying input numbers.",
    "explanation": "The text details Mathador-LM's method as adhering to the rules of Mathador games and varying input numbers to create queries.",
    "question_token_count": 11,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 9,
    "question_difficulty_score": 5,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 14
  },
  {
    "context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n",
    "topic": "Explain the rationale behind utilizing multi-agent systems for LLM evaluation, and how Benchmark Self-Evolving exemplifies this strategy.",
    "response": "```xml\n<document_analysis>\n1. **Thoughtful Content Examination:** The context describes two categories of LLM evaluation: Interactive Evaluation and Multi-Agent Evaluation. Interactive Evaluation utilizes LLMs to conduct interviews or generate follow-up questions. Multi-Agent Evaluation leverages multiple LLM agents to dynamically construct benchmarks. Benchmark Self-Evolving and BENCHAGENTS are examples of this latter approach.\n\n2. **Concept Exploration:** The core concept is that multi-agent systems can overcome limitations of static benchmarks by dynamically generating and evolving benchmarks. This allows for more scalable, diverse, and high-quality evaluations. The human-in-the-loop feedback mentioned in the context suggests a hybrid approach that combines the strengths of automated generation with human oversight.\n\n3. **Strategic Complexity Calibration:** Difficulty: 8. This question requires understanding the motivation behind multi-agent systems and the specific implementation details of Benchmark Self-Evolving. It demands synthesis of information across multiple sentences and an understanding of the broader trend in LLM evaluation.\n\n4. **Intentional Question Planning:** The question will probe the understanding of why multi-agent systems are employed and how Benchmark Self-Evolving specifically utilizes this approach. The question is designed to assess if the user grasps the benefits of dynamic benchmark generation and the role of agents in that process.\n</document_analysis>\n\n<output_format>\nQuestion: How do multi-agent systems contribute to LLM evaluation, and what is the specific role of Benchmark Self-Evolving within this framework?\nExplanation: The passage highlights the success of multi-agent systems and how they are utilized to create dynamic benchmarks that are scalable, diverse, and high-quality. Benchmark Self-Evolving specifically demonstrates this by employing a multi-agent framework to dynamically extend existing benchmarks.\nCorrect Answer: Dynamic benchmark generation\n</output_format>\n```",
    "question": "How do multi-agent systems contribute to LLM evaluation, and what is the specific role of Benchmark Self-Evolving within this framework?",
    "answer": "Dynamic benchmark generation",
    "explanation": "The passage highlights the success of multi-agent systems and how they are utilized to create dynamic benchmarks that are scalable, diverse, and high-quality. Benchmark Self-Evolving specifically demonstrates this by employing a multi-agent framework to dynamically extend existing benchmarks.",
    "question_token_count": 26,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 4
  },
  {
    "context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n",
    "topic": "The range of tasks that static benchmarks are utilized to assess in large language models, such as math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension.",
    "response": "`<document_analysis>`\nThe provided context describes static benchmarks used to evaluate large language models (LLMs). It outlines the components of a static benchmark (seed dataset, input prompts, expected outputs, and a scoring function). The context specifically mentions a diverse range of tasks assessed by these benchmarks, including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. The difficulty is rated 7/10 as it requires understanding of the formal definition of a static benchmark and its components, rather than just recalling a list of tasks.\n\nThe question will focus on the relationship between the formal definition of a static benchmark and the breadth of tasks it aims to evaluate, probing understanding beyond a simple list. The question will be designed to assess whether the expert can infer the purpose and scope of static benchmarks given their formal definition. The question is designed to be challenging by requiring the expert to consider the implications of the scoring function within the benchmark's overall purpose.\n\n</document_analysis>`\n\n`<output_format>`\nQuestion: How does the inclusion of a scoring function, \ud835\udcae\u2062(\u22c5), within the formal definition of a static benchmark, \ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.)), directly enable the evaluation of diverse, complex tasks such as commonsense reasoning and program synthesis, beyond simply assessing the correctness of factual recall?\nExplanation: The scoring function's ability to evaluate the quality of an LLM's output against expected outputs (\ud835\udcb4) allows for nuanced assessments beyond simple factual correctness. It can incorporate criteria for reasoning, coherence, and other aspects of complex tasks.\nCorrect Answer: nuanced evaluation criteria",
    "question": "How does the inclusion of a scoring function, \ud835\udcae\u2062(\u22c5), within the formal definition of a static benchmark, \ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.)), directly enable the evaluation of diverse, complex tasks such as commonsense reasoning and program synthesis, beyond simply assessing the correctness of factual recall?",
    "answer": "nuanced evaluation criteria",
    "explanation": "The scoring function's ability to evaluate the quality of an LLM's output against expected outputs (\ud835\udcb4) allows for nuanced assessments beyond simple factual correctness. It can incorporate criteria for reasoning, coherence, and other aspects of complex tasks.",
    "question_token_count": 66,
    "answer_correctness_score": 8,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 5
  },
  {
    "context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n",
    "topic": "What are the key differences in the approaches used by NaturalQuestions, TriviaQA, MMLU, BBH, AGI Eval, MMLU-Redux, and MMLU-Pro to evaluate an LLM's internal knowledge, and how do these benchmarks contribute to a comprehensive assessment of an LLM\u2019s knowledge capabilities?",
    "response": "`<document_analysis>\nThe provided context details benchmarks used to evaluate Large Language Models (LLMs), specifically focusing on \"Math\" and \"Knowledge\" domains. The \"Knowledge\" section lists several datasets and briefly describes their focus. My analysis indicates that the question topic requires a synthesis of information from multiple parts of the text, demanding an understanding of the nuances of each benchmark's purpose. The difficulty level is rated as 8/10, as it necessitates a comparative understanding of different datasets and their contribution to overall LLM knowledge assessment. There is no irrelevant information present.\n</document_analysis>\n\n<output_format>\nQuestion: How do benchmarks like ControlBench, FRAMES, and GPQA Diamond extend the evaluation of LLMs beyond traditional knowledge benchmarks, and what specific capabilities do they target?\nExplanation: These benchmarks target technical and long-context challenges, as well as open-domain evaluations.\nCorrect Answer: Technical and long-context challenges.\n</output_format>",
    "question": "How do benchmarks like ControlBench, FRAMES, and GPQA Diamond extend the evaluation of LLMs beyond traditional knowledge benchmarks, and what specific capabilities do they target?",
    "answer": "Technical and long-context challenges.",
    "explanation": "These benchmarks target technical and long-context challenges, as well as open-domain evaluations.",
    "question_token_count": 34,
    "answer_correctness_score": 10,
    "explanation_validity_score": 8,
    "question_clarity_score": 7,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 7
  },
  {
    "context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Describe the purpose of the transformation function T(\u22c5) in the context of dynamic benchmarking and how it addresses the problem of data contamination.",
    "response": "<document_analysis>\nThe provided context discusses limitations of post-hot detection methods and introduces dynamic benchmarking as a solution to address issues with static benchmarking. The core of dynamic benchmarking lies in the transformation function T(\u22c5). The text explicitly states that T(\u22c5) \"modifies the dataset during benchmarking to avoid possible data contamination.\" This is a crucial point, as data contamination (overlap between the training data and the benchmark dataset) is a significant problem in evaluating LLMs. The dynamic dataset is created by repeatedly applying this transformation function to the original dataset, ensuring that the evaluation data at each timestamp is different, thereby mitigating contamination. The difficulty is rated 8/10 because it requires understanding the concept of data contamination and how dynamic benchmarking actively combats it. The question encourages a deep engagement with the text and requires the synthesis of multiple pieces of information to construct a comprehensive answer. No irrelevant information was present.\n</document_analysis>\n<output_format>\nQuestion: How does dynamic benchmarking attempt to mitigate the problem of data contamination, and what role does the transformation function play in this process?\nExplanation: The transformation function is critical to dynamic benchmarking's ability to avoid data contamination. By modifying the static benchmark dataset over time, it creates new evaluation datasets that are less likely to contain data the LLM has already seen during training.\nCorrect Answer: Modifies the dataset to avoid contamination.\n</output_format>",
    "question": "How does dynamic benchmarking attempt to mitigate the problem of data contamination, and what role does the transformation function play in this process?",
    "answer": "Modifies the dataset to avoid contamination.",
    "explanation": "The transformation function is critical to dynamic benchmarking's ability to avoid data contamination. By modifying the static benchmark dataset over time, it creates new evaluation datasets that are less likely to contain data the LLM has already seen during training.",
    "question_token_count": 26,
    "answer_correctness_score": 8,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 9
  },
  {
    "context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n",
    "topic": "Analyze the trade-offs between manual validation and automated verification in the context of dynamic benchmarking transformations, considering the volume of data generated.",
    "response": "<document_analysis>\nThe context discusses dynamic benchmarking for LLM evaluation, focusing on the interpretability of transformations and categorizing different construction processes. The central theme revolves around the challenges of verifying transformations, particularly with the large volumes of data generated.\n\nThe key ideas are: 1) Manual verification is costly and challenging due to data volume; 2) Interpretability reduces the need for manual validation; 3) Rule-based transformations are inherently interpretable, while LLM-assisted transformations require additional mechanisms (explainability tools, human-in-the-loop validation) to ensure reliability; and 4) Dynamic benchmarks can be categorized into temporal cutoff, rule-based, LLM-based, and hybrid approaches.\n\nThe implicit assumption is that transformation correctness is crucial for reliable LLM evaluation. The potential application is designing dynamic benchmarks that balance data generation with verification cost.\n\nDifficulty: 8/10. The question requires synthesizing information about data volume, validation methods, and the trade-offs associated with different transformation approaches. It's not a direct recall question, but rather requires analytical reasoning.\n\nThe question will invite deeper understanding by prompting reflection on the practical implications of large-scale data verification and the challenges of ensuring reliability with LLM-assisted transformations. The goal is to assess the ability to reason about the trade-offs involved in benchmark design.\n</document_analysis>\n\n<output_format>\nQuestion: How does the scale of data generated in dynamic benchmarking influence the selection of transformation verification strategies, and what are the implications for relying on LLMs to generate these transformations?\nExplanation: The context highlights the challenge of manual verification due to data volume, and the need for interpretable transformations to reduce this burden. LLM-assisted transformations, while powerful, require additional verification mechanisms due to their lack of inherent transparency.\nCorrect Answer: Increased verification complexity.\n</output_format>",
    "question": "How does the scale of data generated in dynamic benchmarking influence the selection of transformation verification strategies, and what are the implications for relying on LLMs to generate these transformations?",
    "answer": "Increased verification complexity.",
    "explanation": "The context highlights the challenge of manual verification due to data volume, and the need for interpretable transformations to reduce this burden. LLM-assisted transformations, while powerful, require additional verification mechanisms due to their lack of inherent transparency.",
    "question_token_count": 34,
    "answer_correctness_score": 8,
    "explanation_validity_score": 8,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 5
  },
  {
    "context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n",
    "topic": "Analyze the approach of VarBench (Qian et al., 2024) in generating new samples by prompting LLMs to identify and replace variables within existing benchmarks, considering the potential benefits and drawbacks of this technique.",
    "response": "<document_analysis>\nThe provided context discusses various methods for leveraging Large Language Models (LLMs) to generate new datasets or augment existing benchmarks, particularly addressing the issue of contamination. The text highlights limitations associated with rule-defined data generation and the risk of in-distribution contamination. It then details specific approaches like Auto-Dataset, StructEval, ITD, and VarBench.\n\nVarBench's approach, specifically prompting LLMs to identify and replace variables in benchmark samples, is of particular interest for question generation. The difficulty of this question lies in requiring the user to not only understand the technique but also to critically evaluate its potential advantages and disadvantages within the broader context of dataset contamination and LLM-generated data.\n\nDifficulty: 8/10. This requires a nuanced understanding of LLM dataset generation techniques and the challenges of contamination.\n\nThe question will probe the respondent's ability to synthesize information about VarBench's method and assess its implications. The answer needs to be concise but demonstrate an understanding of the core benefits and potential pitfalls of variable replacement.\n</document_analysis>\n\n<output_format>\nQuestion: How does the variable replacement strategy employed by VarBench (Qian et al., 2024) mitigate the risks associated with benchmark contamination, and what potential drawbacks might arise from this approach?\nExplanation: VarBench's method addresses contamination by generating new samples from existing ones, reducing reliance on potentially over-represented data. However, simply replacing variables might not fully capture the complexities of the original problem, potentially leading to a reduction in sample quality or introducing new biases.\nCorrect Answer: Reduced reliance on over-represented data, potential reduction in sample quality.\n</output_format>",
    "question": "How does the variable replacement strategy employed by VarBench (Qian et al., 2024) mitigate the risks associated with benchmark contamination, and what potential drawbacks might arise from this approach?",
    "answer": "Reduced reliance on over-represented data, potential reduction in sample quality.",
    "explanation": "VarBench's method addresses contamination by generating new samples from existing ones, reducing reliance on potentially over-represented data. However, simply replacing variables might not fully capture the complexities of the original problem, potentially leading to a reduction in sample quality or introducing new biases.",
    "question_token_count": 39,
    "answer_correctness_score": 8,
    "explanation_validity_score": 8,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 15
  },
  {
    "context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n",
    "topic": "Describe the core concept of interactive evaluation in LLMs and how methods like LLM-as-an-Interviewer, TreeEval, and KIEval contribute to this approach.",
    "response": "`<document_analysis>\nThe context describes two categories of LLM evaluation: Interactive Evaluation and Multi-Agent Evaluation. The Interactive Evaluation section details methods that utilize multi-round interactions to assess LLMs, drawing inspiration from human interviews. Specific examples include LLM-as-an-Interviewer, TreeEval, and KIEval, each employing different strategies for generating follow-up questions and evaluating responses. The Multi-Agent Evaluation section introduces the use of multi-agent systems for dynamic benchmark creation, citing Benchmark Self-Evolving and BENCHAGENTS as examples. BENCHAGENTS\u2019 process, involving planning, generation, verification, and evaluation handled by specialized agents, is highlighted. The document does not contain irrelevant information such as advertisements or disclaimers. The difficulty level is rated as 7/10, requiring synthesis of information across multiple methods within the interactive evaluation category. The question will probe understanding of the overarching concept and the distinct approaches of the mentioned methods.\n</document_analysis>`\n\n`<output_format>\nQuestion: How do interactive evaluation methods leverage multi-round interactions to assess LLMs, and what differentiates approaches like LLM-as-an-Interviewer, TreeEval, and KIEval?\nExplanation: This question requires the respondent to understand the fundamental principle of interactive evaluation\u2014the use of dynamic, multi-turn conversations\u2014and then to articulate the specific mechanisms by which three different methods implement this principle.\nCorrect Answer: Multi-round interactions mirroring human interviews, with methods differing in question generation strategies based on prior responses.\n</output_format>`",
    "question": "How do interactive evaluation methods leverage multi-round interactions to assess LLMs, and what differentiates approaches like LLM-as-an-Interviewer, TreeEval, and KIEval?",
    "answer": "Multi-round interactions mirroring human interviews, with methods differing in question generation strategies based on prior responses.",
    "explanation": "This question requires the respondent to understand the fundamental principle of interactive evaluation\u2014the use of dynamic, multi-turn conversations\u2014and then to articulate the specific mechanisms by which three different methods implement this principle.",
    "question_token_count": 36,
    "answer_correctness_score": 9,
    "explanation_validity_score": 8,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 21
  },
  {
    "context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n",
    "topic": "The increasing concern of data contamination in Large Language Models (LLMs) and its impact on the evolution of benchmarking practices.",
    "response": "`<document_analysis>\n1. **Thoughtful Content Examination:** The context discusses the growing problem of data contamination in LLMs, stemming from their training on massive internet datasets. It details the shift from static to dynamic benchmarking as a response to this issue. The paper analyzes existing methods, identifies limitations, and proposes design principles for dynamic benchmarking, highlighting a lack of standardized evaluation criteria. It also mentions a GitHub repository for collecting benchmarking methods.\n\n2. **Concept Exploration:** The core concept revolves around the tension between the desire for comprehensive training data for LLMs and the risk of including evaluation data in that training set. This leads to a 'contamination' problem, where LLMs essentially memorize answers rather than demonstrating true understanding. The shift to dynamic benchmarking reflects an attempt to address this by changing the evaluation data during the assessment process. The lack of standardized criteria for dynamic benchmarks is a significant research gap.\n\n3. **Strategic Complexity Calibration:** Difficulty: 8/10. This question requires understanding the nuances of data contamination, the motivation for dynamic benchmarking, and the specific research gap identified in the paper. It demands a grasp of the practical implications of each concept, not just a superficial understanding.\n\n4. **Intentional Question Planning:** The question is designed to probe whether the expert understands the *why* behind the shift to dynamic benchmarking and the limitations of current approaches. It aims to assess their ability to synthesize the core arguments of the paper and extrapolate to the broader implications for LLM evaluation.\n</document_analysis>\n\n<output_format>\nQuestion: Why does the evolution of LLM benchmarking necessitate a move from static to dynamic methods?\nExplanation: Static benchmarks are susceptible to data contamination because the evaluation data can inadvertently be included in the training data, leading to memorization rather than genuine understanding.\nCorrect Answer: Data contamination risks.\n</output_format>",
    "question": "Why does the evolution of LLM benchmarking necessitate a move from static to dynamic methods?",
    "answer": "Data contamination risks.",
    "explanation": "Static benchmarks are susceptible to data contamination because the evaluation data can inadvertently be included in the training data, leading to memorization rather than genuine understanding.",
    "question_token_count": 18,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 5
  },
  {
    "context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n",
    "topic": "The critical gap in standardized evaluation criteria for dynamic benchmarks and its implications for assessing LLM performance.",
    "response": "`<document_analysis>`\nThe provided context discusses the growing concern of data contamination in LLMs and the shift from static to dynamic benchmarking to address this issue. The core argument revolves around the lack of standardized criteria for evaluating dynamic benchmarks, which the authors identify as a critical gap. The paper proposes design principles for dynamic benchmarking and analyzes existing methods.\n\n**Difficulty Rating:** 8/10. This question probes for a deeper understanding of the methodological challenges in benchmarking LLMs, requiring the expert to consider the implications of lacking standardized evaluation criteria. It requires going beyond simply understanding the *existence* of the gap, but rather understanding its significance.\n\n**Intentional Question Planning:** The question will target the expert's ability to understand the practical consequences of this gap and how it impacts the reliability and comparability of LLM evaluations. The answer will be concise, focusing on the core implication.\n\n**Irrelevant Information:** The GitHub link is irrelevant to the core content and will be disregarded. The abstract introduction is also not relevant, and will be disregarded.\n</document_analysis>`\n\n`<output_format>`\nQuestion: Why is the absence of standardized evaluation criteria for dynamic benchmarks considered a significant limitation in assessing LLM performance?\nExplanation: Without standardized criteria, comparisons between different LLMs using dynamic benchmarks become unreliable and lack a common basis for evaluation, hindering progress in the field.\nCorrect Answer: Unreliable comparisons.\n</output_format>`",
    "question": "Why is the absence of standardized evaluation criteria for dynamic benchmarks considered a significant limitation in assessing LLM performance?",
    "answer": "Unreliable comparisons.",
    "explanation": "Without standardized criteria, comparisons between different LLMs using dynamic benchmarks become unreliable and lack a common basis for evaluation, hindering progress in the field.",
    "question_token_count": 21,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 5
  },
  {
    "context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n",
    "topic": "The significance of accounting for complexity in dynamic benchmarks for LLMs, distinguishing between performance degradation due to data contamination and increased task complexity.",
    "response": "`<document_analysis>`\nThe provided text discusses the importance of accounting for complexity in dynamic benchmarks for Large Language Models (LLMs). It highlights that a performance drop can be attributed to either data contamination or an increase in task complexity. The core challenge lies in accurately measuring this complexity, as existing metrics are often domain-specific. The text introduces the concept of a complexity measurement function \u03a8(\u22c5) and presents a formula for stability based on variance in complexity across trials.\n\nDifficulty: 8/10. This question targets a nuanced understanding of the challenges in dynamic benchmarking and requires the ability to interpret the provided formula. It moves beyond simple recall and asks for an explanation of the stability concept.\n\nIntentional Question Planning: The question will probe the expert's understanding of why complexity must be factored in and the implications of not doing so. The question is designed to assess whether the expert can articulate the distinction between contamination and complexity-related performance drops. The question is challenging because it requires connecting the theoretical formula to the practical implications of benchmark design.\n\nIrrelevant Information: No irrelevant information is present. The entire text is relevant to the question topic.\n</document_analysis>`\n\n`<output_format>`\nQuestion: How does the inclusion of a complexity measurement function, \u03a8(\u22c5), in dynamic benchmarking contribute to differentiating between performance degradation caused by data contamination and an inherent increase in task difficulty?\nExplanation: Incorporating \u03a8(\u22c5) allows for quantifying the complexity of the benchmark, enabling assessment of whether a performance drop stems from data contamination or a more complex task.\nCorrect Answer: Quantifying benchmark complexity.\n</output_format>`",
    "question": "How does the inclusion of a complexity measurement function, \u03a8(\u22c5), in dynamic benchmarking contribute to differentiating between performance degradation caused by data contamination and an inherent increase in task difficulty?",
    "answer": "Quantifying benchmark complexity.",
    "explanation": "Incorporating \u03a8(\u22c5) allows for quantifying the complexity of the benchmark, enabling assessment of whether a performance drop stems from data contamination or a more complex task.",
    "question_token_count": 36,
    "answer_correctness_score": 8,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 9,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 6
  },
  {
    "context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n",
    "topic": "Optimal design principles for dynamic benchmarking in LLMs, as proposed in the study, and their potential for addressing data contamination challenges.",
    "response": "`<document_analysis>\n1. **Thoughtful Content Examination:** The context discusses the growing concern of data contamination in LLMs and the shift from static to dynamic benchmarking to address this. It identifies limitations in enhancing static benchmarks and a lack of standardized criteria for evaluating dynamic benchmarks. The core contribution is the proposal of optimal design principles for dynamic benchmarking and an analysis of existing methods.\n\n2. **Concept Exploration:** The study highlights the crucial need for evaluating dynamic benchmarks. Data contamination is a significant issue in LLMs due to their training on vast internet data, and the proposed design principles aim to improve the effectiveness of dynamic benchmarking in mitigating this risk. The GitHub repository suggests an ongoing effort to collect and analyze various benchmarking techniques.\n\n3. **Strategic Complexity Calibration:** Difficulty: 8/10. This question requires understanding the entire context, identifying the core contribution (design principles), and critically evaluating their potential. It is not a simple recall question.\n\n4. **Intentional Question Planning:** The question aims to probe the expert's understanding of the proposed design principles and their practical implications for addressing data contamination. It encourages reflection on the limitations of existing methods and the potential for future research.\n</document_analysis>\n\n<output_format>\nQuestion: Considering the inherent limitations of static benchmarking and the absence of standardized evaluation criteria for dynamic approaches, how do the proposed optimal design principles for dynamic benchmarking specifically address the challenges posed by data contamination in LLMs?\nExplanation: The study identifies a gap in standardized evaluation and proposes principles to improve dynamic benchmarking, which is crucial for mitigating data contamination risks. The question requires the expert to synthesize these points and articulate the principles' function.\nCorrect Answer: By focusing on standardized evaluation and iterative refinement.\n</output_format>",
    "question": "Considering the inherent limitations of static benchmarking and the absence of standardized evaluation criteria for dynamic approaches, how do the proposed optimal design principles for dynamic benchmarking specifically address the challenges posed by data contamination in LLMs?",
    "answer": "By focusing on standardized evaluation and iterative refinement.",
    "explanation": "The study identifies a gap in standardized evaluation and proposes principles to improve dynamic benchmarking, which is crucial for mitigating data contamination risks. The question requires the expert to synthesize these points and articulate the principles' function.",
    "question_token_count": 41,
    "answer_correctness_score": 8,
    "explanation_validity_score": 8,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 8,
    "avg_answer_token_count": 10
  },
  {
    "context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n",
    "topic": "Explain the role of syntactic transformations (e.g., synonym substitution, morphological variations) in syntactic contamination and how they relate to preserving lexical meaning.",
    "response": "`<document_analysis>\n1. **Thoughtful Content Examination:** The context defines data contamination in LLMs, distinguishing between exact and syntactic contamination. The focus is on how training data overlaps with evaluation benchmarks. Syntactic contamination is specifically addressed, highlighting transformations that preserve lexical meaning.\n2. **Concept Exploration:** The key concept is the subtle nature of syntactic contamination - it's not about exact matches, but about data points that *could* be found after transformations. The definition emphasizes that these transformations must maintain the original meaning (lexical meaning). This implies a focus on superficial changes rather than semantic shifts.\n3. **Strategic Complexity Calibration:** Difficulty: 8/10. This is a challenging question because it requires understanding the nuanced definition of syntactic contamination and the critical role of lexical meaning preservation. It probes the expert's understanding of how syntactic changes can still lead to contamination.\n4. **Intentional Question Planning:** The question is designed to elicit a detailed explanation of the purpose and function of syntactic transformations within the context of data contamination. It requires the expert to articulate why these transformations are problematic despite preserving lexical meaning. The question is crafted to avoid simple recall and encourage a deeper understanding of the underlying principles.\n</document_analysis>`\n\n`<output_format>\nQuestion: How does the preservation of lexical meaning during syntactic transformations contribute to the phenomenon of syntactic contamination in LLMs?\nExplanation: Syntactic contamination occurs when a transformed version of a test data point exists within the training data. The preservation of lexical meaning is crucial because it signifies that the transformed data point retains the original semantic content, allowing the model to potentially memorize and regurgitate the test example during evaluation, even without an exact match.\nCorrect Answer: Preserves semantic content.\n</output_format>`",
    "question": "How does the preservation of lexical meaning during syntactic transformations contribute to the phenomenon of syntactic contamination in LLMs?",
    "answer": "Preserves semantic content.",
    "explanation": "Syntactic contamination occurs when a transformed version of a test data point exists within the training data. The preservation of lexical meaning is crucial because it signifies that the transformed data point retains the original semantic content, allowing the model to potentially memorize and regurgitate the test example during evaluation, even without an exact match.",
    "question_token_count": 22,
    "answer_correctness_score": 8,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 6
  },
  {
    "context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n",
    "topic": "Describe the limitations of current retrieval-based methods for detecting data contamination in LLMs, considering the scale and complexity of training corpora.",
    "response": "`<document_analysis>`\n1. **Thoughtful Content Examination:** The context discusses the problem of data contamination in Large Language Models (LLMs) due to overlap between training and evaluation data. It highlights the challenges posed by the massive scale and complexity of the training corpora, as well as the proprietary nature of training data. The specific focus is on the limitations of retrieval-based detection methods.\n\n2. **Concept Exploration:** The core concept is the difficulty of detecting contamination given the vastness of the data used to train LLMs. The proprietary nature of training data adds another layer of complexity, as it restricts external verification and mitigation efforts. The use of human-annotated and synthetic datasets further increases the risk of contamination.\n\n3. **Strategic Complexity Calibration:** Difficulty: 8/10. This question requires understanding not just the existence of retrieval-based methods, but also a nuanced grasp of *why* they are insufficient in the context of LLM training. It demands an appreciation of the scale and proprietary data issues.\n\n4. **Intentional Question Planning:** The question is designed to elicit a response that goes beyond a simple statement of limitations. It encourages the examinee to consider the interplay of scale, complexity, and data opacity in hindering the effectiveness of retrieval-based techniques. It probes for an understanding of the systemic challenges, rather than just a superficial acknowledgement of the problem.\n`</document_analysis>`\n\n`<output_format>`\nQuestion: How does the nature of LLM training data impede the efficacy of retrieval-based contamination detection methods?\nExplanation: The sheer scale and complexity of LLM training corpora, combined with the proprietary nature of that data, makes it extraordinarily difficult to thoroughly search and exclude evaluation data using retrieval-based techniques.\nCorrect Answer: Scale and data opacity.\n`</output_format>`",
    "question": "How does the nature of LLM training data impede the efficacy of retrieval-based contamination detection methods?",
    "answer": "Scale and data opacity.",
    "explanation": "The sheer scale and complexity of LLM training corpora, combined with the proprietary nature of that data, makes it extraordinarily difficult to thoroughly search and exclude evaluation data using retrieval-based techniques.",
    "question_token_count": 19,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 6
  },
  {
    "context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n",
    "topic": "The methodology of S3Eval and its focus on evaluating LLMs' ability to execute SQL queries on randomly generated SQL tables.",
    "response": "`<document_analysis>`\nThe context describes several evaluation methods for LLMs, focusing on their reasoning abilities. I'll concentrate on S3Eval and its methodology. The key aspects are its use of random SQL queries and randomly generated SQL tables. The difficulty lies in understanding the implications of using *randomly* generated data for evaluation \u2013 it suggests a focus on adaptability and generalizability rather than specific knowledge. The other methods (DyVal, NPHardEval, Xie et al.) are relevant to the broader topic of LLM evaluation but are not the primary focus of the question.\n\nDifficulty: 7/10\nIntent: To probe the understanding of the implications of using random data for evaluating LLMs, particularly in the context of SQL query execution. The question will require the expert to consider the strengths and potential weaknesses of this approach.\n\nthought_process: The question will focus on the randomness aspect of S3Eval, requiring an expert to consider why this design choice is significant for evaluating LLMs. It's designed to be challenging because it requires an understanding of evaluation methodologies and the properties of random data.\n</document_analysis>`\n\n`<output_format>`\nQuestion: What is the primary advantage of evaluating LLMs' SQL reasoning capabilities using randomly generated SQL tables and queries, as employed by S3Eval?\nExplanation: Utilizing random data ensures the LLM's ability to generalize to unseen SQL schemas and query structures, rather than memorizing specific patterns.\nCorrect Answer: Generalization.\n</output_format>`",
    "question": "What is the primary advantage of evaluating LLMs' SQL reasoning capabilities using randomly generated SQL tables and queries, as employed by S3Eval?",
    "answer": "Generalization.",
    "explanation": "Utilizing random data ensures the LLM's ability to generalize to unseen SQL schemas and query structures, rather than memorizing specific patterns.",
    "question_token_count": 29,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 4
  },
  {
    "context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n",
    "topic": "Analyze the potential benefits and limitations of using a multi-agent framework, as demonstrated by BENCHAGENTS, for creating scalable, diverse, and high-quality benchmarks.",
    "response": "`<document_analysis>\nThe provided context details interactive and multi-agent evaluation methods for Large Language Models (LLMs). The focus is on benchmarking techniques, specifically highlighting BENCHAGENTS. The key aspects of BENCHAGENTS are its multi-agent framework, task decomposition into planning, generation, verification, and evaluation stages, and the inclusion of human-in-the-loop feedback.\n\nDifficulty: 8/10 - This requires synthesizing information about the multi-agent approach and critically assessing its benefits and limitations in the context of benchmark creation. It necessitates understanding the specific contributions of BENCHAGENTS.\n\nIntentional Question Planning: The question aims to prompt reflection on the advantages of a structured, multi-agent approach versus more traditional methods. It also encourages consideration of potential drawbacks or challenges associated with such a system, particularly concerning scalability, diversity, and quality.\n\nIrrelevant Information: No irrelevant information is present. The entire context pertains to the topic of LLM evaluation and benchmark creation.\n</document_analysis>\n\n<output_format>\nQuestion: How does the specialized division of labor within BENCHAGENTS\u2019 multi-agent framework contribute to the creation of benchmarks that are both scalable and high-quality, and what inherent limitations might arise from this structured approach?\nExplanation: BENCHAGENTS' success stems from its breakdown of benchmark creation into distinct phases handled by specialized agents, enabling parallel processing and targeted expertise. However, this structure may limit adaptability and introduce biases if the agents' design or feedback loops are flawed.\nCorrect Answer: Structured task decomposition and specialized agents.\n</output_format>",
    "question": "How does the specialized division of labor within BENCHAGENTS\u2019 multi-agent framework contribute to the creation of benchmarks that are both scalable and high-quality, and what inherent limitations might arise from this structured approach?",
    "answer": "Structured task decomposition and specialized agents.",
    "explanation": "BENCHAGENTS' success stems from its breakdown of benchmark creation into distinct phases handled by specialized agents, enabling parallel processing and targeted expertise. However, this structure may limit adaptability and introduce biases if the agents' design or feedback loops are flawed.",
    "question_token_count": 42,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 8
  },
  {
    "context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n",
    "topic": "Discuss the role of data contamination and fairness in the design of dynamic benchmarks, and how these considerations influence the choice of benchmark construction method.",
    "response": "`<document_analysis>`\nThe context discusses dynamic benchmarks for LLM evaluation, highlighting the need for interpretability and addressing data contamination and fairness. The core idea revolves around how dynamic benchmarks are constructed to minimize these issues. The text categorizes these benchmarks into four types: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nThe question topic asks about data contamination and fairness and how they influence the choice of construction method. This aligns directly with the core discussion of dynamic benchmarks and their design principles. The question aims to probe understanding of the nuances of each construction method and their implications for data integrity and unbiased evaluation.\n\nDifficulty: 8/10. Requires synthesizing information about each benchmark type and their impact on contamination and fairness.\n\nIntentional Question Planning: The question encourages the expert to demonstrate a comprehensive understanding of the trade-offs involved in different benchmark construction methods. It forces them to consider the underlying assumptions and potential biases inherent in each approach.\n\nIrrelevant Information: No irrelevant information detected. The entire context pertains to the topic of dynamic benchmarks and their evaluation.\n</document_analysis>`\n\n`<output_format>`\nQuestion: How do the inherent characteristics of rule-based, LLM-based, and temporal cutoff dynamic benchmark generation methods differentially impact the potential for data contamination and ensure fairness in LLM evaluation?\nExplanation: The question probes the expert's understanding of the trade-offs in benchmark construction methods, specifically regarding data contamination and fairness. It requires them to analyze the unique properties of each approach and how those properties affect the integrity and objectivity of the evaluation.\nCorrect Answer: Rule-based offers controlled data; LLM-based risks contamination; temporal cutoff relies on recent data.\n</output_format>`",
    "question": "How do the inherent characteristics of rule-based, LLM-based, and temporal cutoff dynamic benchmark generation methods differentially impact the potential for data contamination and ensure fairness in LLM evaluation?",
    "answer": "Rule-based offers controlled data; LLM-based risks contamination; temporal cutoff relies on recent data.",
    "explanation": "The question probes the expert's understanding of the trade-offs in benchmark construction methods, specifically regarding data contamination and fairness. It requires them to analyze the unique properties of each approach and how those properties affect the integrity and objectivity of the evaluation.",
    "question_token_count": 34,
    "answer_correctness_score": 8,
    "explanation_validity_score": 7,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 19
  },
  {
    "context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n",
    "topic": "Describe how the scoring function, \ud835\udcae(\u22c5), contributes to the overall correctness score in dynamic benchmarks and what a higher score indicates.",
    "response": "<document_analysis>\nThe provided context discusses evaluation criteria for dynamic benchmarking algorithms used to assess Large Language Models (LLMs). The focus is on the 'Correctness' criterion. The text introduces a formula to quantify correctness, where \ud835\udcae(\u22c5) is a scoring function that measures the alignment between transformed dataset outputs and ground truth values. The difficulty rating is 8, as it requires understanding the formula and its implications. The question will probe the understanding of the scoring function's role and the meaning of a higher score. No irrelevant information is present.\n</document_analysis>\n<output_format>\nQuestion: How does the scoring function influence the correctness evaluation of a dynamic benchmark, and what does a resultant higher score signify?\nExplanation: The scoring function, \ud835\udcae(\u22c5), quantifies the alignment between the outputs of the transformed dataset and their corresponding ground truth values. A higher score indicates a greater degree of alignment and, therefore, a higher degree of correctness maintained by the dynamic benchmark.\nCorrect Answer: Greater alignment with ground truth.\n</output_format>",
    "question": "How does the scoring function influence the correctness evaluation of a dynamic benchmark, and what does a resultant higher score signify?",
    "answer": "Greater alignment with ground truth.",
    "explanation": "The scoring function, \ud835\udcae(\u22c5), quantifies the alignment between the outputs of the transformed dataset and their corresponding ground truth values. A higher score indicates a greater degree of alignment and, therefore, a higher degree of correctness maintained by the dynamic benchmark.",
    "question_token_count": 24,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 9,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 7
  },
  {
    "context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n",
    "topic": "The significance of the GitHub repository maintained by the authors in facilitating the collection and advancement of both static and dynamic LLM benchmarking methods.",
    "response": "`<document_analysis>`\nThe provided context discusses the growing concern of data contamination in LLMs and the shift from static to dynamic benchmarking to address this issue. The core argument revolves around analyzing existing methods, identifying limitations, and proposing design principles for dynamic benchmarking. A key element is the mention of a GitHub repository serving as a central hub for collecting both static and dynamic benchmarking methods.\n\n**Difficulty Rating:** 8/10 - This question requires an understanding of the broader context of LLM benchmarking, the purpose of the repository, and its role in the research landscape. It's not a simple factual recall question.\n\n**Intentional Question Planning:** The question aims to evaluate the candidate's comprehension of the practical implications of the research and the importance of open-source resources in the field. It probes their understanding of how such a repository contributes to the progress of LLM benchmarking. The question is designed to require an inference about the value of the resource.\n\n**Irrelevant Information:** The GitHub link itself is irrelevant, as the question focuses on the repository's *significance*, not its specific URL. This will be ignored.\n\n</document_analysis>`\n\n<output_format>\nQuestion: How does the establishment of a publicly accessible repository for LLM benchmarking methods contribute to addressing the challenges posed by data contamination in large language models?\nExplanation: The repository facilitates the collaborative collection and evaluation of both static and dynamic methods, enabling researchers to build upon existing work and identify areas for improvement in mitigating data contamination risks.\nCorrect Answer: Collaborative advancement of benchmarking methods.\n</output_format>",
    "question": "How does the establishment of a publicly accessible repository for LLM benchmarking methods contribute to addressing the challenges posed by data contamination in large language models?",
    "answer": "Collaborative advancement of benchmarking methods.",
    "explanation": "The repository facilitates the collaborative collection and evaluation of both static and dynamic methods, enabling researchers to build upon existing work and identify areas for improvement in mitigating data contamination risks.",
    "question_token_count": 28,
    "answer_correctness_score": 8,
    "explanation_validity_score": 8,
    "question_clarity_score": 7,
    "question_difficulty_score": 6,
    "question_groundedness_score": 8,
    "avg_answer_token_count": 8
  },
  {
    "context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n",
    "topic": "Evaluate the effectiveness of \"No Derivatives\" licenses in conjunction with public key encryption for preventing automated crawling and reuse of test data, considering potential circumvention strategies.",
    "response": "`<document_analysis>`\nThe provided text discusses various methods for securing evaluation data, including encryption and label protection. The question topic focuses on the efficacy of \"No Derivatives\" licenses combined with public key encryption, as proposed by Jacovi et al. (2023), in preventing data reuse. The text highlights that Yang et al. (2023) demonstrated that decontamination methods can be circumvented by minor text variations, implying a potential weakness in relying solely on such approaches. TRUCE utilizes confidential computing and secure multi-party computation for private benchmarking, suggesting a more robust but computationally intensive alternative. The text also acknowledges the limitations of encryption methods, including dependency on strong key management and vulnerability to compromise.\n\nDifficulty: 8/10\nRationale: This question requires the candidate to synthesize information from multiple parts of the text, consider the limitations of the proposed approach (Jacovi et al.), and draw inferences about potential vulnerabilities. It demands an understanding of both the technical aspects of encryption and the broader implications for data security in machine learning. It also requires understanding of the work of Yang et al. and how it relates to the proposed method.\n\nIntentional Question Planning: The question aims to assess the candidate's ability to critically evaluate a specific data protection strategy within the context of broader challenges related to data contamination. It encourages consideration of potential attack vectors and the trade-offs involved in different approaches.\n</document_analysis>`\n\n`<output_format>`\nQuestion: How does the demonstrated vulnerability of decontamination methods to minor text variations, as highlighted by Yang et al. (2023), impact the reliance on \"No Derivatives\" licenses and public key encryption, as proposed by Jacovi et al. (2023), for safeguarding evaluation data?\nExplanation: Yang et al.'s findings suggest that even seemingly robust decontamination methods can be bypassed, implying that simply encrypting data with a \"No Derivatives\" license might not be sufficient to prevent malicious actors from reconstructing or reusing the data.\nCorrect Answer: Insufficient protection.\n</output_format>`",
    "question": "How does the demonstrated vulnerability of decontamination methods to minor text variations, as highlighted by Yang et al. (2023), impact the reliance on \"No Derivatives\" licenses and public key encryption, as proposed by Jacovi et al. (2023), for safeguarding evaluation data?",
    "answer": "Insufficient protection.",
    "explanation": "Yang et al.'s findings suggest that even seemingly robust decontamination methods can be bypassed, implying that simply encrypting data with a \"No Derivatives\" license might not be sufficient to prevent malicious actors from reconstructing or reusing the data.",
    "question_token_count": 58,
    "answer_correctness_score": 6,
    "explanation_validity_score": 5,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 5
  },
  {
    "context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n",
    "topic": "Explain the concept of canary strings and their purpose in the context of LLM static benchmarking, referencing their function as markers for data contamination.",
    "response": "`<document_analysis>`\nThe provided context discusses methods for mitigating data contamination in LLM static benchmarking, specifically focusing on the use of \"canary strings.\" The central idea is that these unique tokens, embedded within datasets, can reveal whether a model has memorized training data instead of generalizing. The text also highlights limitations: developers must be aware of and responsive to these markers, and malicious actors could circumvent the system by intentionally leaking data.\n\n**Difficulty Rating:** 8/10 - Requires understanding of data contamination, memorization vs. generalization in LLMs, and the specific role of canary strings. It also necessitates recognizing the limitations of the method.\n\n**Intentional Question Planning:** The question will probe the understanding of canary strings by asking for their purpose and function within the context of LLM benchmarking. The answer should demonstrate an understanding of how these strings are used to detect data contamination and the inherent limitations of this detection method. The question is designed to assess whether the expert understands the practical implications of canary strings beyond a superficial definition.\n\n**Irrelevant Information:** No irrelevant information was identified. The entire context is relevant to the question topic.\n</document_analysis>\n\n<output_format>\nQuestion: How does the inclusion of deliberately crafted tokens contribute to the evaluation of LLMs during static benchmarking?\nExplanation: Canary strings are designed to identify whether a model has memorized training data rather than generalized from it, by appearing unexpectedly in the model's output.\nCorrect Answer: Data contamination detection.\n</output_format>",
    "question": "How does the inclusion of deliberately crafted tokens contribute to the evaluation of LLMs during static benchmarking?",
    "answer": "Data contamination detection.",
    "explanation": "Canary strings are designed to identify whether a model has memorized training data rather than generalized from it, by appearing unexpectedly in the model's output.",
    "question_token_count": 20,
    "answer_correctness_score": 8,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 5
  },
  {
    "context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n",
    "topic": "Differentiate between exact contamination and syntactic contamination, providing specific examples of each type of data contamination in LLM training.",
    "response": "`<document_analysis>`\nThe provided context discusses data contamination in the training of Large Language Models (LLMs). It defines two types: exact and syntactic contamination. The core of the document is the formal definition of these two types of contamination.\n\n**Thoughtful Content Examination:** The document clearly distinguishes between exact and syntactic contamination. Exact contamination is straightforward - a direct duplication of data. Syntactic contamination is more nuanced, involving transformations of the data.\n\n**Concept Exploration:** The document implicitly highlights the challenge of detecting syntactic contamination, as it involves identifying transformed versions of test data within the training set. The examples provided (punctuation normalization, synonym substitution) illustrate the breadth of possible transformations.\n\n**Strategic Complexity Calibration:** Difficulty: 8. This is a challenging question because it requires the candidate to not only recall the definitions but also to provide meaningful examples demonstrating the difference between the two types of contamination. The examples need to showcase understanding of the transformation aspect of syntactic contamination.\n\n**Intentional Question Planning:** The question is designed to probe the candidate's understanding of the subtle distinction between the two contamination types. The request for examples forces them to apply the definitions to concrete scenarios. The examples must demonstrate an understanding of how syntactic transformations can lead to contamination.\n\nI will formulate a question that requires the candidate to not only differentiate the two but also to apply the concepts to generate examples. The answer will be concise, focusing on the key difference and providing relevant examples.\n`</document_analysis>`\n\n`<output_format>`\nQuestion: How do exact and syntactic contamination differ in the context of LLM training data, and provide a distinct example illustrating each type of contamination?\nExplanation: Exact contamination involves direct duplication of data points, whereas syntactic contamination involves transformed data points that retain lexical meaning.\nCorrect Answer: Exact: verbatim test examples in training; Syntactic: synonym substitution of a benchmark question.\n`</output_format>`",
    "question": "How do exact and syntactic contamination differ in the context of LLM training data, and provide a distinct example illustrating each type of contamination?",
    "answer": "Exact: verbatim test examples in training; Syntactic: synonym substitution of a benchmark question.",
    "explanation": "Exact contamination involves direct duplication of data points, whereas syntactic contamination involves transformed data points that retain lexical meaning.",
    "question_token_count": 27,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 21
  },
  {
    "context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n",
    "topic": "Describe the hybrid generation approach used in LatestEval and DARG, detailing how they combine LLM-based methods with other techniques to create benchmarks.",
    "response": "<document_analysis>\nThe provided context discusses data contamination in LLM benchmarking and explores various methods for creating contamination-free benchmarks. The question topic focuses on hybrid generation approaches, specifically within the context of LatestEval and DARG. The text explicitly details how each method combines LLM-based generation with other techniques: LatestEval combines temporal cutoff and LLM-based generation using real-time content, while DARG integrates LLM-based generation with graph-based methods, perturbing reasoning graphs extracted from existing benchmarks. The difficulty is rated 8/10 as it requires understanding of both the specific methods and the broader context of data contamination in LLM benchmarking. The question is designed to assess the candidate's ability to synthesize information from different parts of the text and explain the nuances of these hybrid approaches. There is no irrelevant information present in the context.\n</document_analysis>\n\n<output_format>\nQuestion: How do LatestEval and DARG address data contamination in LLM benchmarking through hybrid generation techniques?\nExplanation: The question assesses understanding of the specific methodologies used by LatestEval and DARG to mitigate data contamination, requiring the candidate to connect LLM-based generation with other techniques.\nCorrect Answer: LatestEval combines temporal cutoff and LLM generation, while DARG integrates LLM generation with graph-based methods.\n</output_format>",
    "question": "How do LatestEval and DARG address data contamination in LLM benchmarking through hybrid generation techniques?",
    "answer": "LatestEval combines temporal cutoff and LLM generation, while DARG integrates LLM generation with graph-based methods.",
    "explanation": "The question assesses understanding of the specific methodologies used by LatestEval and DARG to mitigate data contamination, requiring the candidate to connect LLM-based generation with other techniques.",
    "question_token_count": 19,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 21
  },
  {
    "context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Discuss the implications of an empty seed dataset \ud835\udc9f for the creation of a dynamic benchmarking dataset.",
    "response": "<document_analysis>\nThe provided text discusses dynamic benchmarking for Large Language Models (LLMs) and introduces the mathematical formulation of a dynamic benchmark. A key component is the transformation function T(\u22c5) which modifies the static benchmark dataset \ud835\udc9f to avoid data contamination. The text then defines the dynamic dataset \ud835\udc9ft as Tt(\ud835\udc9f) for various timestamps t. A crucial point is the scenario where the seed dataset \ud835\udc9f is empty. The text states that in this case, the dynamic benchmarking dataset will be created from scratch.\n\nDifficulty: 8/10 - This question requires understanding the entire dynamic benchmarking framework and its implications, particularly the role of the seed dataset and how the transformation function adapts when the seed is empty. It probes the ability to infer the practical consequences of this specific condition.\n\nIntentional Question Planning: The question aims to explore the ramifications of starting with an empty dataset. It requires the respondent to consider how a benchmark is constructed and validated without any initial data, pushing them beyond a mere factual recall.\n\nIrrelevant Information: The table (Table 3) and image are explicitly ignored as they are not relevant to the core discussion of dynamic benchmarking and the empty seed dataset scenario. The limitations of post-hot detection methods are also irrelevant.\n</document_analysis>\n<output_format>\nQuestion: How does the process of generating a dynamic benchmarking dataset differ when the initial seed dataset is empty?\nExplanation: The text explicitly states that if the initial seed dataset is empty, the dynamic benchmarking dataset is created from scratch, implying a different construction process than when a seed dataset exists.\nCorrect Answer: Created from scratch.\n</output_format>",
    "question": "How does the process of generating a dynamic benchmarking dataset differ when the initial seed dataset is empty?",
    "answer": "Created from scratch.",
    "explanation": "The text explicitly states that if the initial seed dataset is empty, the dynamic benchmarking dataset is created from scratch, implying a different construction process than when a seed dataset exists.",
    "question_token_count": 20,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 5
  },
  {
    "context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n",
    "topic": "Explain how LiveCodeBench continuously collects new coding problems for LLM evaluation and the platforms it leverages for this purpose.",
    "response": "`<document_analysis>\n1. **Thoughtful Content Examination:** The context discusses methods for mitigating data contamination in LLM evaluations by utilizing data collected after the model's knowledge cutoff date. Several benchmarks are presented as examples, each employing different strategies to achieve this. The focus is on how these benchmarks dynamically update their datasets with recent information.\n\n2. **Concept Exploration:** The core concept is temporal cutoff and its application in benchmark design. The examples highlight various approaches to collecting and incorporating fresh data, including math competitions, arXiv papers, coding problems, and forecasting questions. The underlying assumption is that data collected after the cutoff is less likely to have been part of the model's training data.\n\n3. **Strategic Complexity Calibration:** Difficulty: 8/10. This question requires understanding the specific methodology of LiveCodeBench and recalling the specific platform it uses. It demands more than just a superficial understanding of the broader concept of temporal cutoff. The challenge lies in extracting the precise details from the text and presenting them concisely.\n\n4. **Intentional Question Planning:** The question is designed to probe the reader's ability to identify and recall specific details about a particular benchmark. It avoids open-endedness and demands a targeted response focused on the data collection process and platform.\n</document_analysis>`\n\n`<output_format>\nQuestion: What strategy does LiveCodeBench use to maintain a current evaluation dataset, and which platforms does it utilize?\nExplanation: The text explicitly states LiveCodeBench continuously collects new human-written coding problems from online coding competition platforms.\nCorrect Answer: LeetCode\n</output_format>`",
    "question": "What strategy does LiveCodeBench use to maintain a current evaluation dataset, and which platforms does it utilize?",
    "answer": "LeetCode",
    "explanation": "The text explicitly states LiveCodeBench continuously collects new human-written coding problems from online coding competition platforms.",
    "question_token_count": 22,
    "answer_correctness_score": 6,
    "explanation_validity_score": 7,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 4
  },
  {
    "context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n",
    "topic": "Describe the key challenges associated with dynamic benchmarking approaches for LLMs, specifically addressing issues of reliability and reproducibility.",
    "response": "`<document_analysis>`\nThe provided context discusses data contamination in LLM benchmarking and outlines limitations of both static and dynamic approaches. The question topic focuses on the challenges of dynamic benchmarking, specifically reliability and reproducibility. The text explicitly mentions these challenges. The difficulty is rated 7/10 because it requires extracting and synthesizing specific information about dynamic benchmarking from the limitations section. The question is designed to probe understanding of the nuanced issues faced in dynamic benchmarking, which is a critical area of research. No irrelevant information was present.\n</document_analysis>\n\n<output_format>\nQuestion: What hinders the widespread adoption of dynamic benchmarking for LLMs?\nExplanation: The text explicitly states that dynamic approaches \"face challenges in reliability and reproducibility,\" highlighting the primary obstacles to their broader implementation.\nCorrect Answer: Reliability and reproducibility.\n</output_format>",
    "question": "What hinders the widespread adoption of dynamic benchmarking for LLMs?",
    "answer": "Reliability and reproducibility.",
    "explanation": "The text explicitly states that dynamic approaches \"face challenges in reliability and reproducibility,\" highlighting the primary obstacles to their broader implementation.",
    "question_token_count": 14,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 9,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 6
  },
  {
    "context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n",
    "topic": "Describe how IFEval, InfoBench, and C-Eval assess a language model's ability to follow instructions, highlighting the specific real-world scenarios they simulate.",
    "response": "<document_analysis>\nThe provided context describes various benchmarks used to evaluate language models across different capabilities: coding, instruction following, and reasoning. The section on \"Instruction Following\" is the focus for question generation. IFEval, InfoBench, and C-Eval are explicitly mentioned as datasets designed to assess instruction following. The context states that these datasets \"simulate real-world scenarios requiring clear, step-by-step guidance.\" C-Eval specifically focuses on Chinese instructions. The difficulty is rated a 7, as it requires understanding the purpose of these benchmarks and their relation to real-world simulation. The question will probe the expert's understanding of the nuances of these benchmarks and their practical application. No irrelevant or bogus information is present.\n</document_analysis>\n\n<output_format>\nQuestion: What is the primary characteristic of IFEval, InfoBench, and C-Eval benchmarks when evaluating a language model\u2019s instruction-following capabilities?\nExplanation: These benchmarks aim to evaluate a model's ability to comprehend and execute detailed directives by simulating real-world scenarios that require clear, step-by-step guidance.\nCorrect Answer: Real-world scenarios.\n</output_format>",
    "question": "What is the primary characteristic of IFEval, InfoBench, and C-Eval benchmarks when evaluating a language model\u2019s instruction-following capabilities?",
    "answer": "Real-world scenarios.",
    "explanation": "These benchmarks aim to evaluate a model's ability to comprehend and execute detailed directives by simulating real-world scenarios that require clear, step-by-step guidance.",
    "question_token_count": 29,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 5
  },
  {
    "context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n",
    "topic": "Compare and contrast the strengths and weaknesses of GSM-Symbolic, Mathador-LM, and MMLU-CF in the context of LLM evaluation, considering their respective approaches and limitations.",
    "response": "```xml\n<document_analysis>\n1. **Thoughtful Content Examination:** The context discusses limitations of LLM evaluation methods and then details rule-based generation, specifically focusing on GSM-Symbolic, Mathador-LM, and MMLU-CF. The core idea is to highlight different approaches to creating test cases for LLMs.\n\n2. **Concept Exploration:** The strengths lie in their rule-based nature, which helps avoid data contamination and ensures a low collision probability. The weaknesses, though not explicitly stated, can be inferred: GSM-Symbolic relies on templates, Mathador-LM on game rules, and MMLU-CF on multiple-choice structure. This suggests potential limitations in the diversity or complexity of generated problems. The mention of verification being overlooked in live benchmarks is relevant as it highlights a general challenge in the field.\n\n3. **Strategic Complexity Calibration:** Difficulty: 8. This question requires synthesis and comparative analysis across three specific methods, demanding an understanding of their underlying mechanisms and implications. It requires going beyond simple factual recall.\n\n4. **Intentional Question Planning:** The question is designed to probe the expert's understanding of the nuances of each method, encouraging them to articulate both advantages and disadvantages. The prompt explicitly requests a comparison, requiring the expert to identify similarities and differences.\n</document_analysis>\n\n<output_format>\nQuestion: How does the reliance on templates in GSM-Symbolic, game rules in Mathador-LM, and multiple-choice structures in MMLU-CF influence the scope and potential biases of LLM evaluations generated by each method?\nExplanation: The question requires comparing and contrasting the methods' approaches to test generation, examining how their core mechanisms shape the evaluations' characteristics.\nCorrect Answer: Limited diversity.\n</output_format>\n```",
    "question": "How does the reliance on templates in GSM-Symbolic, game rules in Mathador-LM, and multiple-choice structures in MMLU-CF influence the scope and potential biases of LLM evaluations generated by each method?",
    "answer": "Limited diversity.",
    "explanation": "The question requires comparing and contrasting the methods' approaches to test generation, examining how their core mechanisms shape the evaluations' characteristics.",
    "question_token_count": 45,
    "answer_correctness_score": 8,
    "explanation_validity_score": 7,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 4
  },
  {
    "context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n",
    "topic": "How do benchmarks like GSM8K, MATH, AIME 2024, and CNMO 2024 assess a model\u2019s capabilities in solving multi-step math problems, and what are the implications of these assessments for the development of advanced mathematical reasoning in LLMs?",
    "response": "`<document_analysis>\nThe provided text describes several benchmarks used to evaluate Large Language Models (LLMs). The focus is on \"Math\" and \"Knowledge\" benchmarks. The \"Math\" section specifically lists GSM8K, MATH, AIME 2024, and CNMO 2024 as datasets that require models to solve complex, multi-step math problems. The \"Knowledge\" section details benchmarks like NaturalQuestions, TriviaQA, MMLU, BBH, and AGI Eval. Several recent extensions and specialized benchmarks are also mentioned.\n\nThe question topic asks about the assessment of multi-step math problem-solving capabilities and the implications for LLM development. The text provides clear information about the benchmarks used for this assessment, allowing for a direct and challenging question to be formulated. Difficulty: 8.\n\nThe question will probe understanding of the benchmarks' purpose and the broader implications of assessing mathematical reasoning in LLMs.\n</document_analysis>\n\n<output_format>\nQuestion: What characteristic distinguishes datasets such as GSM8K, MATH, AIME 2024, and CNMO 2024 from other LLM evaluation benchmarks?\nExplanation: These datasets specifically test a model's ability to solve complex, multi-step math problems, setting them apart from benchmarks primarily focused on general knowledge retrieval.\nCorrect Answer: Multi-step math problems\n</output_format>",
    "question": "What characteristic distinguishes datasets such as GSM8K, MATH, AIME 2024, and CNMO 2024 from other LLM evaluation benchmarks?",
    "answer": "Multi-step math problems",
    "explanation": "These datasets specifically test a model's ability to solve complex, multi-step math problems, setting them apart from benchmarks primarily focused on general knowledge retrieval.",
    "question_token_count": 31,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 10,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 5
  },
  {
    "context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "The challenges associated with static benchmarks in the context of rapidly evolving LLMs and potential data contamination.",
    "response": "`<document_analysis>`\nThe provided context discusses the evolution of LLM benchmarking and the limitations of static benchmarks. The central idea is that as LLMs improve and are continuously trained on expanding datasets, static benchmarks become less effective due to increasing ease and potential data contamination. The text highlights the development of contamination detectors and dynamic benchmarks as responses to these challenges.\n\nDifficulty Rating: 8/10. This is a high-level conceptual question requiring understanding of the trade-offs in benchmark design and the implications of LLM training practices. It necessitates grasping the problem of data contamination and the rationale behind dynamic benchmarks.\n\nIntentional Question Planning: The question will probe the expert's understanding of why static benchmarks fail in the face of continuous LLM training and data expansion. It aims to elicit a nuanced explanation that goes beyond simply stating the problem, requiring the expert to articulate the underlying mechanisms and potential consequences. The answer should reflect a deep understanding of the challenges in maintaining benchmark relevance and integrity.\n\nIrrelevant Information: The table reference and citations are excluded as they are not essential for understanding the core concept. The image reference is also ignored as it is not available.\n</document_analysis>`\n\n`<output_format>`\nQuestion: How does the principle of continual LLM training on available data fundamentally compromise the utility of fixed benchmark datasets?\nExplanation: Continual training leads to models becoming proficient on benchmark data, rendering the benchmarks trivial or introducing data contamination issues.\nCorrect Answer: Benchmark trivialization and data contamination.\n</output_format>`",
    "question": "How does the principle of continual LLM training on available data fundamentally compromise the utility of fixed benchmark datasets?",
    "answer": "Benchmark trivialization and data contamination.",
    "explanation": "Continual training leads to models becoming proficient on benchmark data, rendering the benchmarks trivial or introducing data contamination issues.",
    "question_token_count": 21,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 8
  },
  {
    "context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n",
    "topic": "The evaluation strategy of NPHardEval, which assesses LLMs' reasoning capabilities on P and NP problems, such as the Traveling Salesman Problem (TSP), utilizing random graphs as inputs.",
    "response": "`<document_analysis>\nThe provided context describes several evaluation methods for assessing the reasoning abilities of Large Language Models (LLMs). I've identified three categories: Table-Based, Graph-Based, and puzzles. The focus is on methods that use randomly generated data structures (SQL tables, graphs) to create challenging reasoning tasks. NPHardEval, specifically, uses random graphs as inputs for problems like the Traveling Salesman Problem (TSP) to evaluate LLM performance.\n\nDifficulty Rating: 8/10. The question requires understanding of the specific evaluation methodology and its connection to the broader context of LLM reasoning assessment.\n\nIntentional Question Planning: The question will focus on the core methodology of NPHardEval. The challenge will be to elicit an explanation that demonstrates understanding of *why* random graphs are used and what aspect of reasoning they are intended to evaluate. The answer must be concise and directly related to the purpose of the evaluation.\n</document_analysis>`\n\n`<output_format>\nQuestion: What is the primary purpose of employing randomly generated graphs in the NPHardEval evaluation framework?\nExplanation: Random graphs are used to assess the LLM's performance on well-known P and NP problems, like the Traveling Salesman Problem.\nCorrect Answer: Performance on NP problems\n</output_format>`",
    "question": "What is the primary purpose of employing randomly generated graphs in the NPHardEval evaluation framework?",
    "answer": "Performance on NP problems",
    "explanation": "Random graphs are used to assess the LLM's performance on well-known P and NP problems, like the Traveling Salesman Problem.",
    "question_token_count": 20,
    "answer_correctness_score": 8,
    "explanation_validity_score": 9,
    "question_clarity_score": 7,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 5
  },
  {
    "context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n",
    "topic": "Describe the mathematical notation used to represent external diversity, including the meaning of each symbol and the overall formula.",
    "response": "<document_analysis>\nThe provided context discusses diversity in transformed datasets, differentiating between external and internal diversity. The external diversity is quantified using a mathematical formula involving expectation, datasets, and a diversity measurement function. The internal diversity is similarly defined but compares different transformation trials. The core task is to understand the mathematical notation for external diversity.\n\nThe difficulty is rated 8/10 because it requires understanding of mathematical notation (expectation, subscripts, superscripts) within the context of dataset diversity, which is a specialized area. It\u2019s not a simple factual recall question.\n\nThe question will be designed to assess the candidate's ability to interpret mathematical expressions in a technical context, focusing specifically on the external diversity formula. It aims to evaluate their understanding of the symbols and their relationship to the concept of dataset diversity. The question is crafted to be challenging, requiring a thorough comprehension of the notation and its implications.\n\nNo irrelevant or bogus information is present. The entire context is relevant to the question topic.\n</document_analysis>\n\n<output_format>\nQuestion: What does the symbol \u0398 represent within the formula for external diversity, and how does its application relate to the overall calculation of external diversity between a transformed dataset and a seed dataset?\nExplanation: The symbol \u0398 represents a function that measures the diversity between two datasets. Its application within the formula calculates the expected diversity between each transformed dataset (\ud835\udc9fi) and the original seed dataset (\ud835\udc9f), providing a quantitative measure of the external diversity.\nCorrect Answer: Diversity measurement function.\n</output_format>",
    "question": "What does the symbol \u0398 represent within the formula for external diversity, and how does its application relate to the overall calculation of external diversity between a transformed dataset and a seed dataset?",
    "answer": "Diversity measurement function.",
    "explanation": "The symbol \u0398 represents a function that measures the diversity between two datasets. Its application within the formula calculates the expected diversity between each transformed dataset (\ud835\udc9fi) and the original seed dataset (\ud835\udc9f), providing a quantitative measure of the external diversity.",
    "question_token_count": 36,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 6
  },
  {
    "context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n",
    "topic": "Detail the four-stage process employed by BENCHAGENTS for automated benchmark creation, specifying the role of each LLM agent and the incorporation of human feedback.",
    "response": "```xml\n<document_analysis>\n1. **Thoughtful Content Examination:** The context describes two categories of LLM evaluation: Interactive Evaluation and Multi-Agent Evaluation. Within Multi-Agent Evaluation, BENCHAGENTS is highlighted as a method for automated benchmark creation. The core of the question will focus on detailing this process.\n\n2. **Concept Exploration:** The text implicitly assumes an understanding of multi-agent systems and LLMs. The key concept is the division of labor within BENCHAGENTS, with each agent specializing in a specific task. Human-in-the-loop feedback is also a crucial element.\n\n3. **Strategic Complexity Calibration:** Difficulty: 8. This is a challenging question because it requires synthesizing information from a concise paragraph and accurately recalling the four distinct stages and their corresponding agent roles. It moves beyond simple recall and asks for a detailed process description.\n\n4. **Intentional Question Planning:** The question is designed to assess the candidate's ability to extract a process description from a text and accurately recall the details of each step, including the agents involved and the role of human feedback. It encourages a structured and comprehensive response.\n</document_analysis>\n\n<output_format>\nQuestion: What are the four distinct stages involved in BENCHAGENTS\u2019 automated benchmark creation process, and what is the specific function of the LLM agent assigned to each stage, alongside the role of human feedback?\nExplanation: The question assesses the understanding of BENCHAGENTS' benchmark creation methodology, demanding a detailed recollection of the four stages and the agent's role in each.\nCorrect Answer: Planning, generation, verification, and evaluation.\n</output_format>\n```",
    "question": "What are the four distinct stages involved in BENCHAGENTS\u2019 automated benchmark creation process, and what is the specific function of the LLM agent assigned to each stage, alongside the role of human feedback?",
    "answer": "Planning, generation, verification, and evaluation.",
    "explanation": "The question assesses the understanding of BENCHAGENTS' benchmark creation methodology, demanding a detailed recollection of the four stages and the agent's role in each.",
    "question_token_count": 41,
    "answer_correctness_score": 10,
    "explanation_validity_score": 6,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 10
  },
  {
    "context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n",
    "topic": "Define the scalability metric presented in the text, including the meaning of each component of the equation \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 / Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ).",
    "response": "`<document_analysis>\n1. **Thoughtful Content Examination:** The context describes a scalability metric for dynamic benchmarking methods. It emphasizes the importance of large datasets to minimize statistical errors while considering the costs associated with generating those datasets. The equation provided quantifies scalability as the ratio of the transformed dataset size to the cost of the transformation.\n\n2. **Concept Exploration:** The core concept is balancing dataset size (for statistical rigor) with the cost of generating it. The equation represents an expectation over the transformation space, implying a consideration of various transformation strategies. The cost function is intentionally broad, encompassing monetary cost, time, and manual effort.\n\n3. **Strategic Complexity Calibration:** Difficulty: 8/10. This question requires not just recalling the equation but also understanding the *meaning* of each component within the broader context of dynamic benchmarking. It's challenging because it demands a grasp of the trade-off between dataset size and cost.\n\n4. **Intentional Question Planning:** The question is designed to elicit a comprehensive explanation of the scalability metric, forcing the respondent to articulate the purpose of each element in the equation and its relationship to the overall goal of efficient and statistically sound benchmarking.\n</document_analysis>\n\n<output_format>\nQuestion: How is the scalability of a dynamic benchmark quantitatively defined, and what does each term in the equation represent?\nExplanation: The equation provided defines scalability as the ratio of the size of the transformed dataset to the cost of the transformation, representing the proportion of data generated per unit cost.\nCorrect Answer: Transformed dataset size / Transformation cost\n</output_format>",
    "question": "How is the scalability of a dynamic benchmark quantitatively defined, and what does each term in the equation represent?",
    "answer": "Transformed dataset size / Transformation cost",
    "explanation": "The equation provided defines scalability as the ratio of the size of the transformed dataset to the cost of the transformation, representing the proportion of data generated per unit cost.",
    "question_token_count": 22,
    "answer_correctness_score": 8,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 8
  },
  {
    "context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n",
    "topic": "Explain how the proprietary nature of LLM training data exacerbates the problem of data contamination and hinders the development of fair and reliable benchmarks.",
    "response": "`<document_analysis>`\n1. **Thoughtful Content Examination:** The context discusses data contamination in LLMs, highlighting the challenges posed by massive, diverse training datasets and subsequent fine-tuning. A key point is the proprietary nature of training data, which complicates the assessment of performance and mitigation of contamination.\n\n2. **Concept Exploration:** The core issue is that if the training data is not publicly available, it's impossible for the broader research community to verify whether evaluation data was inadvertently included in the training set. This lack of transparency makes it difficult to create benchmarks that accurately reflect a model's true capabilities and fairness.\n\n3. **Strategic Complexity Calibration:** Difficulty: 8/10. This question requires synthesizing information about data contamination and the impact of proprietary data, demanding a nuanced understanding of the broader implications for benchmark development. It's not a simple factual recall question.\n\n4. **Intentional Question Planning:** The question aims to elicit a response that demonstrates understanding of why data opacity is a significant obstacle to establishing trustworthy LLM evaluations. It requires explaining the connection between proprietary data and the resulting challenges in ensuring fair and reliable benchmarks. The expected answer should address the community's inability to verify and mitigate overlaps.\n`</document_analysis>`\n\n`<output_format>`\nQuestion: How does the withholding of LLM training data from public scrutiny impact the creation of robust evaluation benchmarks?\nExplanation: The context explicitly states that keeping training data proprietary hinders the community's ability to verify and mitigate potential overlaps between training and evaluation data, which is essential for creating fair and reliable benchmarks.\nCorrect Answer: Impediments verification and mitigation.\n`</output_format>`",
    "question": "How does the withholding of LLM training data from public scrutiny impact the creation of robust evaluation benchmarks?",
    "answer": "Impediments verification and mitigation.",
    "explanation": "The context explicitly states that keeping training data proprietary hinders the community's ability to verify and mitigate potential overlaps between training and evaluation data, which is essential for creating fair and reliable benchmarks.",
    "question_token_count": 20,
    "answer_correctness_score": 8,
    "explanation_validity_score": 9,
    "question_clarity_score": 7,
    "question_difficulty_score": 6,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 8
  },
  {
    "context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n",
    "topic": "The limitations of enhancing static benchmarks in mitigating data contamination risks within LLM evaluation.",
    "response": "`<document_analysis>`\nThe context describes the increasing concern of data contamination in LLMs due to their training data and the shift from static to dynamic benchmarking to address this. The core focus is an analysis of existing static-to-dynamic benchmarking methods. The text explicitly states a critical gap: the lack of standardized criteria for evaluating dynamic benchmarks. This gap forms the basis for proposing design principles for dynamic benchmarking and analyzing the limitations of current dynamic benchmarks. The GitHub repository is mentioned as a resource.\n\nDifficulty: 8/10. The question targets a nuanced understanding of a specific limitation within a broader trend in LLM evaluation. It requires understanding not just *that* there are limitations, but *what* they are and why they are significant in the context of data contamination.\n\nIntentional Question Planning: The question will probe the reader's understanding of the weaknesses of enhancing static benchmarks. The answer will need to draw directly from the text's analysis, requiring careful reading and synthesis. The question will avoid explicitly mentioning \"static benchmarks\" to increase complexity.\n\nIrrelevant Information: The GitHub link is explicitly excluded as it is external and doesn't contribute to the core understanding of the limitations of static benchmarks. The abstract itself is not irrelevant, but the question will draw from the main body of the text to avoid simply paraphrasing the abstract.\n</document_analysis>`\n\n<output_format>\nQuestion: What fundamental impediment restricts the efficacy of augmenting existing LLM evaluation methodologies to prevent data leakage?\nExplanation: The text identifies the lack of standardized evaluation criteria for dynamic benchmarks as a critical gap, highlighting a limitation in the broader effort to move from static to dynamic benchmarking. This deficiency significantly hinders the ability to reliably assess and improve the data contamination mitigation capabilities of LLMs.\nCorrect Answer: Standardized evaluation criteria.\n</output_format>",
    "question": "What fundamental impediment restricts the efficacy of augmenting existing LLM evaluation methodologies to prevent data leakage?",
    "answer": "Standardized evaluation criteria.",
    "explanation": "The text identifies the lack of standardized evaluation criteria for dynamic benchmarks as a critical gap, highlighting a limitation in the broader effort to move from static to dynamic benchmarking. This deficiency significantly hinders the ability to reliably assess and improve the data contamination mitigation capabilities of LLMs.",
    "question_token_count": 20,
    "answer_correctness_score": 1,
    "explanation_validity_score": 2,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 6
  },
  {
    "context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n",
    "topic": "Summarize the overall objective of employing metrics like \"Collision Rate\" and \"Repeat Trials\" in dynamic benchmarking, specifically in relation to evaluating the capabilities of LLMs.",
    "response": "`<document_analysis>`\nThe provided context discusses the challenges of data contamination in dynamic benchmarking of Large Language Models (LLMs). The core issue is that publicly available benchmarking algorithms could be used to train LLMs, potentially compromising the benchmark's ability to accurately reflect the models' true capabilities.\n\nThe concept of \"collision\" is introduced to address this. Collision refers to the overlap between different transformations of the benchmark dataset. The \"Collision Rate\" and \"Repeat Trials\" metrics are proposed to quantify this overlap and assess the benchmark's ability to generate novel test cases.\n\nThe overall objective, as stated in the text, is to evaluate the robustness of a dynamic benchmark against data contamination and to ensure it can still reliably assess LLM capabilities even when exposed to this risk.\n\nDifficulty: 8/10 - Requires understanding the nuances of data contamination and the purpose of the proposed metrics within the broader context of LLM evaluation. It necessitates synthesizing information about the problem, the solution, and the desired outcome.\n\nIntentional Question Planning: The question will focus on eliciting a concise explanation of the *purpose* of the metrics, rather than just defining them. It will encourage the respondent to demonstrate an understanding of why these metrics are important in the context of LLM benchmarking.\n\n</document_analysis>`\n\n`<output_format>`\nQuestion: Why are metrics such as \"Collision Rate\" and \"Repeat Trials\" crucial when dynamically benchmarking LLMs?\nExplanation: These metrics are vital for assessing the benchmark\u2019s robustness against data contamination, ensuring the benchmark can still reliably evaluate LLM capabilities even when exposed to potential training data.\nCorrect Answer: Assess robustness against data contamination.\n</output_format>`",
    "question": "Why are metrics such as \"Collision Rate\" and \"Repeat Trials\" crucial when dynamically benchmarking LLMs?",
    "answer": "Assess robustness against data contamination.",
    "explanation": "These metrics are vital for assessing the benchmark\u2019s robustness against data contamination, ensuring the benchmark can still reliably evaluate LLM capabilities even when exposed to potential training data.",
    "question_token_count": 22,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 8
  },
  {
    "context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n",
    "topic": "Detail the methodology employed by GSM-Symbolic to dynamically generate math benchmarks, emphasizing the use of query templates and placeholder variables.",
    "response": "<document_analysis>\nThe provided context discusses limitations of LLM evaluation benchmarks and explores rule-based test case generation methods. The relevant section focuses on template-based generation, specifically detailing GSM-Symbolic, Mathador-LM, and MMLU-CF. The question topic directly targets GSM-Symbolic, a method utilizing query templates and placeholder variables. The difficulty is rated 7/10 as it requires understanding of a specific technique within a broader discussion. The question is designed to probe understanding of the core mechanism of GSM-Symbolic, demanding the respondent articulate the process of template utilization and variable substitution. There is no irrelevant information present; the entire provided text chunk is relevant to the question topic.\n</document_analysis>\n\n<output_format>\nQuestion: How does GSM-Symbolic approach the creation of dynamic math benchmarks?\nExplanation: GSM-Symbolic creates dynamic math benchmarks by employing query templates containing placeholder variables, which are then randomly populated to generate diverse problem instances.\nCorrect Answer: Query templates with placeholder variables.\n</output_format>",
    "question": "How does GSM-Symbolic approach the creation of dynamic math benchmarks?",
    "answer": "Query templates with placeholder variables.",
    "explanation": "GSM-Symbolic creates dynamic math benchmarks by employing query templates containing placeholder variables, which are then randomly populated to generate diverse problem instances.",
    "question_token_count": 15,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 4,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 7
  },
  {
    "context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n",
    "topic": "Describe how StructEval (Cao et al., 2024) utilizes LLMs and knowledge graphs to expand upon concepts within existing benchmarks, detailing the process of creating extended questions.",
    "response": "```xml\n<document_analysis>\n1. **Thoughtful Content Examination:** The context discusses limitations of rule-generated data and various methods for benchmark rewriting using LLMs. StructEval is presented as a method that utilizes both LLMs and knowledge graphs to expand upon concepts from original benchmarks. The process involves developing a series of extended questions. The other methods (Auto-Dataset, ITD, VarBench) are also briefly described.\n\n2. **Concept Exploration:** The core concept revolves around mitigating contamination in benchmarks and augmenting them with more diverse and challenging questions. StructEval's approach of combining LLMs with knowledge graphs is particularly interesting as it suggests a structured way to generate questions grounded in factual information.\n\n3. **Strategic Complexity Calibration:** Difficulty: 8. This question requires the reader to synthesize information from a single paragraph and explain a specific technique (StructEval) in detail, demonstrating an understanding of its components (LLMs and knowledge graphs) and purpose (generating extended questions). It's not a simple recall question.\n\n4. **Intentional Question Planning:** The question is designed to elicit a concise explanation of StructEval's methodology, probing the user's understanding of how LLMs and knowledge graphs are integrated to create more comprehensive benchmarks. It requires going beyond surface-level understanding to articulate the process of question generation.\n</document_analysis>\n\n<output_format>\nQuestion: How does StructEval (Cao et al., 2024) generate new questions from existing benchmarks?\nExplanation: StructEval uses LLMs and knowledge graphs to expand upon concepts present in the original benchmark, creating a series of extended questions.\nCorrect Answer: LLMs and knowledge graphs\n</output_format>\n```",
    "question": "How does StructEval (Cao et al., 2024) generate new questions from existing benchmarks?",
    "answer": "LLMs and knowledge graphs",
    "explanation": "StructEval uses LLMs and knowledge graphs to expand upon concepts present in the original benchmark, creating a series of extended questions.",
    "question_token_count": 22,
    "answer_correctness_score": 9,
    "explanation_validity_score": 10,
    "question_clarity_score": 7,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 6
  },
  {
    "context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n",
    "topic": "How do pre-defined rules in LLM data generation potentially limit sample diversity and increase the risk of in-distribution contamination, as highlighted by Tu et al. (2024)?",
    "response": "`<document_analysis>`\nThe context discusses limitations and benchmark rewriting techniques in LLM data generation. The core issue highlighted is that pre-defined rules, while useful, can restrict the variety of samples and increase the likelihood of \"in-distribution contamination\" during training. This occurs when the training data closely resembles the data used to generate it, leading to inflated performance metrics that don't generalize well. The provided examples (Auto-Dataset, StructEval, ITD, and VarBench) detail methods for addressing this by using LLMs to rewrite or augment existing benchmarks, with ITD specifically using a contamination detector.\n\nDifficulty: 8/10. This question requires understanding of a technical term (\"in-distribution contamination\") and its connection to the broader issue of sample diversity in LLM training. It also requires identifying the specific limitation described in the text.\n\nIntentional Question Planning: The question is designed to probe the reader's comprehension of a nuanced problem in LLM data generation. The phrasing avoids directly asking \"What is in-distribution contamination?\" but instead requires the reader to explain how pre-defined rules contribute to this issue.\n\nNo irrelevant information was present. The entire text is relevant to the question topic.\n</document_analysis>`\n\n`<output_format>`\nQuestion: What is the potential consequence of relying on pre-defined rules when generating data for LLMs?\nExplanation: Pre-defined rules can limit sample diversity and increase the risk of in-distribution contamination during training, leading to inflated performance metrics.\nCorrect Answer: Limited diversity and contamination.\n</output_format>`",
    "question": "What is the potential consequence of relying on pre-defined rules when generating data for LLMs?",
    "answer": "Limited diversity and contamination.",
    "explanation": "Pre-defined rules can limit sample diversity and increase the risk of in-distribution contamination during training, leading to inflated performance metrics.",
    "question_token_count": 19,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 6
  },
  {
    "context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n",
    "topic": "Describe the \"temporal cutoff\" strategy for LLM evaluation and explain its purpose in mitigating data contamination.",
    "response": "`<document_analysis>`\nThe context discusses the \"temporal cutoff\" strategy for evaluating Large Language Models (LLMs). This strategy involves using data collected *after* the LLM's knowledge cutoff date to construct evaluation datasets. The primary purpose of this approach is to mitigate data contamination, where the LLM might have been inadvertently trained on data that overlaps with the evaluation data, leading to artificially inflated performance scores. Several benchmarks, including LiveBench, AntiLeak-Bench, AcademicEval, LiveCodeBench, LiveAoPSBench, and Forecastbench, employ this strategy by continuously collecting or generating questions based on recent information sources or newly emerged knowledge. The difficulty is rated 8/10 because it requires understanding the subtle issue of data contamination and recognizing how temporal cutoff addresses it, rather than just recalling a definition. The question is designed to probe the candidate's understanding of the *why* behind this technique, not just *what* it is. It also asks for an explanation, encouraging a more thorough response than a simple definition. The question is inherently generalizable; the principles discussed apply to any model evaluation scenario where contamination is a concern. </document_analysis>\n\n<output_format>\nQuestion: How do contemporary LLM benchmarks address the challenge of evaluating models against information beyond their training data\u2019s temporal limits?\nExplanation: The temporal cutoff strategy creates benchmarks using data collected after the LLM\u2019s knowledge cutoff date to prevent the model from leveraging information it was already exposed to during training.\nCorrect Answer: Temporal cutoff strategy\n</output_format>",
    "question": "How do contemporary LLM benchmarks address the challenge of evaluating models against information beyond their training data\u2019s temporal limits?",
    "answer": "Temporal cutoff strategy",
    "explanation": "The temporal cutoff strategy creates benchmarks using data collected after the LLM\u2019s knowledge cutoff date to prevent the model from leveraging information it was already exposed to during training.",
    "question_token_count": 22,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 4
  },
  {
    "context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n",
    "topic": "Define data contamination in the context of Large Language Models (LLMs) and explain why it compromises the validity of performance measurements.",
    "response": "<document_analysis>\nThe context provides a clear definition of data contamination and its subtypes (exact and syntactic) within the realm of LLMs. The core concept revolves around overlap between training and evaluation datasets, which invalidates performance assessments. The text also gives concrete examples of common contamination scenarios. The difficulty is rated at 8/10 because it requires understanding the formal definition and the implications of this overlap. The question will require a domain expert to synthesize the definition and explain the consequences. There is no irrelevant information present.\n</document_analysis>\n\n<output_format>\nQuestion: How does the presence of overlapping data between an LLM's training and evaluation datasets affect the reliability of performance evaluations?\nExplanation: Data contamination, whether exact or syntactic, introduces a scenario where the model has effectively \"seen\" the evaluation data during training. This creates an artificial inflation of performance metrics, as the model isn't generalizing but rather recalling.\nCorrect Answer: Invalidates performance measurements.\n</output_format>",
    "question": "How does the presence of overlapping data between an LLM's training and evaluation datasets affect the reliability of performance evaluations?",
    "answer": "Invalidates performance measurements.",
    "explanation": "Data contamination, whether exact or syntactic, introduces a scenario where the model has effectively \"seen\" the evaluation data during training. This creates an artificial inflation of performance metrics, as the model isn't generalizing but rather recalling.",
    "question_token_count": 23,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 6
  },
  {
    "context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n",
    "topic": "Explain the significance of dataset size in dynamic benchmarking and why smaller datasets can introduce statistical errors.",
    "response": "<document_analysis>\n1. **Thoughtful Content Examination:** The context discusses scalability in dynamic benchmarking, emphasizing the importance of large datasets to minimize statistical errors. It introduces a formula quantifying scalability as the ratio of transformed dataset size to original dataset size, normalized by the transformation cost.\n\n2. **Concept Exploration:** The core concept is balancing dataset size (for statistical robustness) with the cost of generating that data. The equation provided is a formalization of this trade-off. The cost function is intentionally broad, encompassing monetary cost, time, and effort.\n\n3. **Strategic Complexity Calibration:** Difficulty: 8. This question requires understanding the rationale behind the scalability metric, the implications of statistical errors, and interpreting the equation within the context of cost optimization. It's not a simple factual recall.\n\n4. **Intentional Question Planning:** The question aims to assess the candidate's understanding of *why* larger datasets are desirable, not just *that* they are. It probes their ability to connect statistical rigor with practical resource constraints.\n\n</document_analysis>\n\n<output_format>\nQuestion: Why is a larger dataset preferable in dynamic benchmarking, and how does this relate to the potential for statistical errors?\nExplanation: Smaller datasets are prone to higher statistical error rates, necessitating larger datasets to improve the reliability of benchmarking results.\nCorrect Answer: Statistical errors.\n</output_format>",
    "question": "Why is a larger dataset preferable in dynamic benchmarking, and how does this relate to the potential for statistical errors?",
    "answer": "Statistical errors.",
    "explanation": "Smaller datasets are prone to higher statistical error rates, necessitating larger datasets to improve the reliability of benchmarking results.",
    "question_token_count": 23,
    "answer_correctness_score": 4,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 4
  },
  {
    "context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n",
    "topic": "What are the current challenges in evaluating dynamic benchmarks, and why is the lack of standardized criteria a significant concern?",
    "response": "<document_analysis>\nThe provided context discusses data contamination in LLM benchmarking and explores static and dynamic approaches to address this issue. The document highlights the limitations of static benchmarks due to increasing training data sizes and the challenges associated with dynamic benchmarks, including scalability and complexity control. The analysis identifies a significant lack of standardized criteria for evaluating dynamic benchmarks as a key concern. The context clearly delineates the evolution from static to dynamic benchmarks and the shortcomings of both, emphasizing the need for improved evaluation standards for dynamic methods. Difficulty: 8/10. The question aims to probe understanding of the challenges and the importance of standardization in the context of dynamic benchmarking, requiring the expert to synthesize information from different parts of the text.\n</document_analysis>\n\n<output_format>\nQuestion: Why are traditional static methods inadequate for preventing contamination in LLM benchmarking, and what is the consequence of this inadequacy?\nExplanation: The context explicitly states that static methods fall short due to a lack of transparency and high assumptions about contaminated models. This inadequacy prompts the creation of dynamic benchmarks.\nCorrect Answer: Dynamic benchmarks",
    "question": "Why are traditional static methods inadequate for preventing contamination in LLM benchmarking, and what is the consequence of this inadequacy?",
    "answer": "Dynamic benchmarks",
    "explanation": "The context explicitly states that static methods fall short due to a lack of transparency and high assumptions about contaminated models. This inadequacy prompts the creation of dynamic benchmarks.",
    "question_token_count": 24,
    "answer_correctness_score": 8,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 3
  },
  {
    "context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n",
    "topic": "Analyze the challenges associated with the significant human effort required for LLM evaluation data collection and its impact on the scalability and cost-effectiveness of these evaluations.",
    "response": "`<document_analysis>`\nThe provided context discusses limitations and rule-based generation methods for LLM evaluation. The core issue highlighted is the substantial human effort needed for data collection, which presents scalability and cost challenges. The text also mentions the risk of data contamination from recent competitions and the frequent oversight of verification. Rule-based generation offers a potential solution, with template-based methods like GSM-Symbolic, Mathador-LM, and MMLU-CF demonstrating ways to automate test case creation.\n\nDifficulty: 8/10. This question requires synthesis across multiple points in the text, combining the challenges of human effort with the potential of rule-based generation to understand the broader implications. It's not a simple recall question, and demands an understanding of the trade-offs involved.\n\nIntent: The question aims to evaluate the candidate's ability to synthesize information about the limitations of current LLM evaluation practices and the promise of automated methods. It tests their understanding of the practical implications of these limitations.\n\nIrrelevant Information: The citations (White et al., 2024; Jain et al., 2024; Wu et al., 2024; Zhu et al., 2024a; Mirzadeh et al., 2025; Kurtic et al., 2024; Puma et al., 2023; Zhao et al., 2024) are disregarded as they do not contribute to the core argument and are solely for attribution.\n</document_analysis>`\n\n`<output_format>`\nQuestion: How does the reliance on significant human effort in LLM evaluation data collection impact the feasibility of deploying these evaluations at scale, and what strategies are presented to mitigate this challenge?\nExplanation: This question probes the candidate's ability to connect the human effort limitation with the scalability issue and identify potential solutions mentioned in the text. It requires synthesis of information, not just recall.\nCorrect Answer: Rule-based generation.\n</output_format>`",
    "question": "How does the reliance on significant human effort in LLM evaluation data collection impact the feasibility of deploying these evaluations at scale, and what strategies are presented to mitigate this challenge?",
    "answer": "Rule-based generation.",
    "explanation": "This question probes the candidate's ability to connect the human effort limitation with the scalability issue and identify potential solutions mentioned in the text. It requires synthesis of information, not just recall.",
    "question_token_count": 34,
    "answer_correctness_score": 8,
    "explanation_validity_score": 7,
    "question_clarity_score": 8,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 5
  },
  {
    "context": "###### Safety\nSafety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content. Datasets such as RealToxicityPrompts\u00a0(Gehman et\u00a0al., 2020) and ToxiGen\u00a0(Hartvigsen et\u00a0al., 2022) assess resilience against producing harmful outputs.\n\nBy providing a controlled environment to measure these aspects, safety benchmarks play a critical role in guiding the development of models that are not only powerful outputs but also responsible and trustworthy for real-world applications.\n\n\n###### Language\nLanguage benchmarks assess the LLMs\u2019 proficiency in specific languages.\n\nGLUE\u00a0(Wang, 2018) and SuperGLUE\u00a0(Wang et\u00a0al., 2019) cover tasks from sentiment analysis to language inference, while CLUE\u00a0(Xu et\u00a0al., 2020) targets Chinese language.\n\nTypo-fixing\u00a0(Suzgun et\u00a0al., 2022) is also widely used.\n\n\n###### Reading Comprehension\nReading comprehension tasks test a model\u2019s ability to extract and infer information from text. Benchmarks like SQuAD\u00a0(Rajpurkar et\u00a0al., 2018), QuAC\u00a0(Choi et\u00a0al., 2018), and BoolQ\u00a0(Clark et\u00a0al., 2019) challenge models to understand passages and draw logical conclusions.\n",
    "topic": "The role of reading comprehension benchmarks, including SQuAD, QuAC, and BoolQ, in evaluating an LLM\u2019s ability to extract information and draw logical conclusions from text.",
    "response": "`<document_analysis>\n1. **Thoughtful Content Examination:** The context describes several categories of benchmarks used to evaluate Large Language Models (LLMs): Safety, Language, and Reading Comprehension. The focus is on Reading Comprehension, detailing specific benchmarks like SQuAD, QuAC, and BoolQ and their purpose.\n\n2. **Concept Exploration:** The implicit assumption is that reading comprehension is a crucial aspect of LLM competency. The benchmarks listed represent different approaches to testing this ability, from question answering (SQuAD, QuAC) to boolean reasoning (BoolQ). The underlying theory is that a model's ability to understand and reason about text correlates with its overall intelligence and usefulness.\n\n3. **Strategic Complexity Calibration:** Difficulty: 8/10. The question requires understanding not just *what* these benchmarks are, but *why* they are used and what specific skills they test. It demands a nuanced understanding of how different reading comprehension tasks assess different aspects of a model's abilities.\n\n4. **Intentional Question Planning:** The question will probe the evaluator's understanding of the specific challenges each benchmark presents and how they collectively contribute to evaluating an LLM\u2019s reading comprehension capabilities. The question will be designed to encourage a deeper reflection on the nature of reading comprehension and the challenges of evaluating it in LLMs.\n</document_analysis>`\n\n`<output_format>\nQuestion: How do benchmarks such as SQuAD, QuAC, and BoolQ contribute uniquely to the assessment of an LLM\u2019s overall reading comprehension capabilities?\nExplanation: Each benchmark tests different facets of reading comprehension, from answering questions directly to making logical inferences based on boolean statements.\nCorrect Answer: Varied question types and inference tasks.\n</output_format>`",
    "question": "How do benchmarks such as SQuAD, QuAC, and BoolQ contribute uniquely to the assessment of an LLM\u2019s overall reading comprehension capabilities?",
    "answer": "Varied question types and inference tasks.",
    "explanation": "Each benchmark tests different facets of reading comprehension, from answering questions directly to making logical inferences based on boolean statements.",
    "question_token_count": 30,
    "answer_correctness_score": 9,
    "explanation_validity_score": 8,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 9
  },
  {
    "context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "The types of tasks used to evaluate LLMs, specifically instruction-following and coding tasks.",
    "response": "`<document_analysis>`\nThe provided context discusses the importance of LLM benchmarking and the challenges posed by rapidly evolving models and data contamination. It highlights instruction-following and coding tasks as examples of how LLMs are evaluated. The table referenced is not available, so it won't be incorporated into the question. The difficulty is rated 7/10 because it requires understanding the broader implications of static vs. dynamic benchmarks, and how specific tasks relate to this challenge. The question will focus on the inherent limitations of evaluating LLMs using task-specific benchmarks, even when those tasks represent core capabilities. The core argument is that static benchmarks become obsolete as LLMs improve, so the question will probe this point.\n</question_topic>`\n\n`<output_format>`\nQuestion: How do the characteristics of instruction-following and coding tasks contribute to the limitations of static LLM benchmarks?\nExplanation: Static benchmarks, by their nature, remain unchanged over time. Instruction-following and coding tasks, while representing essential LLM capabilities, are susceptible to becoming trivial as models improve and are retrained on increasingly large datasets, ultimately diminishing the benchmark's ability to accurately reflect real-world performance.\nCorrect Answer: Static benchmarks become obsolete.\n</output_format>`",
    "question": "How do the characteristics of instruction-following and coding tasks contribute to the limitations of static LLM benchmarks?",
    "answer": "Static benchmarks become obsolete.",
    "explanation": "Static benchmarks, by their nature, remain unchanged over time. Instruction-following and coding tasks, while representing essential LLM capabilities, are susceptible to becoming trivial as models improve and are retrained on increasingly large datasets, ultimately diminishing the benchmark's ability to accurately reflect real-world performance.",
    "question_token_count": 21,
    "answer_correctness_score": 8,
    "explanation_validity_score": 8,
    "question_clarity_score": 7,
    "question_difficulty_score": 6,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 6
  },
  {
    "context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n",
    "topic": "Evaluate the significance of verification in live LLM benchmarks and the potential consequences of its frequent omission.",
    "response": "`<document_analysis>`\nThe context discusses limitations of LLM benchmark collection processes and explores rule-based test case generation methods. The core topic revolves around the challenges of creating effective and reliable benchmarks, specifically highlighting data contamination and the lack of verification. The difficulty level is rated 7/10 because it requires synthesizing information from multiple parts of the text to address the implied consequences of a common practice. The question will focus on the implications of overlooking verification, requiring a nuanced understanding of the context. Irrelevant information such as citations is ignored.\n\n`thought_process`:\nThe question topic asks about the significance of verification and the consequences of its omission. The text explicitly states that \"Verification is often overlooked in these live benchmarks.\" This is a key observation. The text also mentions data contamination as a consequence of using recent competition data, suggesting a connection between verification and data integrity. The question should probe the expert's understanding of why verification is crucial and what the broader ramifications might be when it is absent. The answer needs to be concise and directly address the core issue.\n\n</document_analysis>`\n\n`<output_format>`\nQuestion: Why is the absence of verification in live LLM benchmarks concerning?\nExplanation: Verification helps prevent data contamination, which can occur when benchmarks include problems reused from recent competitions.\nCorrect Answer: Data contamination.\n</output_format>`",
    "question": "Why is the absence of verification in live LLM benchmarks concerning?",
    "answer": "Data contamination.",
    "explanation": "Verification helps prevent data contamination, which can occur when benchmarks include problems reused from recent competitions.",
    "question_token_count": 13,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 9,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 4
  },
  {
    "context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n",
    "topic": "Describe the approach taken by AcademicEval in designing academic writing tasks for LLM evaluation, focusing on the source of its content.",
    "response": "`<document_analysis>\n1. **Thoughtful Content Examination:** The context discusses methods to mitigate data contamination in LLM evaluations by utilizing data collected after the model's knowledge cutoff date. It then lists several benchmarks (LiveBench, AntiLeak-Bench, AcademicEval, LiveCodeBench, LiveAoPSBench, Forecastbench) that implement this approach, detailing their specific data collection strategies. The focus is on the temporal aspect \u2013 ensuring the data is recent and therefore less likely to have been part of the LLM's training set.\n\n2. **Concept Exploration:** The core concept revolves around \"data contamination,\" where the LLM has already seen the data used for evaluation, rendering the evaluation less meaningful. The benchmarks represent different strategies for avoiding this contamination, ranging from periodically updating questions (LiveBench, LiveCodeBench) to generating questions about entirely new knowledge (AntiLeak-Bench). AcademicEval\u2019s approach is particularly interesting as it focuses on academic writing, a domain that rapidly evolves with new research.\n\n3. **Strategic Complexity Calibration:** Difficulty: 7/10. The question requires identifying a specific benchmark (AcademicEval) and understanding its unique approach. It isn't a simple recall question but requires synthesizing information about AcademicEval from the surrounding context. The challenge lies in extracting the key detail about the source of the content.\n\n4. **Intentional Question Planning:** The question aims to assess the candidate's ability to pinpoint a specific detail within the text and articulate it concisely. It avoids overly broad questions about data contamination and instead hones in on the methodology of one benchmark.\n</document_analysis>\n\n<output_format>\nQuestion: What is the primary data source utilized by AcademicEval for generating academic writing tasks?\nExplanation: The text explicitly states that AcademicEval designs tasks on the \"latest arXiv papers,\" indicating that the source of the content is recent pre-print publications.\nCorrect Answer: arXiv papers\n</output_format>",
    "question": "What is the primary data source utilized by AcademicEval for generating academic writing tasks?",
    "answer": "arXiv papers",
    "explanation": "The text explicitly states that AcademicEval designs tasks on the \"latest arXiv papers,\" indicating that the source of the content is recent pre-print publications.",
    "question_token_count": 17,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 9,
    "question_difficulty_score": 2,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 4
  },
  {
    "context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n",
    "topic": "Discuss the limitations of static benchmarking methods, specifically addressing the issues of transparency and assumptions about contaminated models.",
    "response": "<document_analysis>\nThe provided context discusses data contamination in LLM benchmarking, outlining limitations of both static and dynamic methods. The question topic focuses on the shortcomings of static benchmarking, specifically transparency and assumptions about contaminated models. The text explicitly states that static methods face challenges due to a \"lack of transparency (e.g., label protection)\" and \"high assumptions about contaminated models (e.g., post-hoc detection).\" This provides a clear and concise basis for generating a challenging question. The difficulty level is rated 8/10, as it requires understanding of the nuances of data contamination and evaluation methodologies. No irrelevant or bogus information is present.\n</document_analysis>\n\n<output_format>\nQuestion: How do the transparency issues and assumptions regarding contaminated models inherent in static benchmarking methods hinder their efficacy in evaluating contemporary LLMs?\nExplanation: The question probes the core limitations of static benchmarks, specifically regarding label protection and the reliance on post-hoc detection of contamination. A domain expert should understand how these factors undermine the reliability of the evaluation.\nCorrect Answer: Label protection and post-hoc detection.\n</output_format>",
    "question": "How do the transparency issues and assumptions regarding contaminated models inherent in static benchmarking methods hinder their efficacy in evaluating contemporary LLMs?",
    "answer": "Label protection and post-hoc detection.",
    "explanation": "The question probes the core limitations of static benchmarks, specifically regarding label protection and the reliance on post-hoc detection of contamination. A domain expert should understand how these factors undermine the reliability of the evaluation.",
    "question_token_count": 25,
    "answer_correctness_score": 8,
    "explanation_validity_score": 8,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 8
  },
  {
    "context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n",
    "topic": "Discuss the practical significance of monitoring both external and internal diversity when transforming datasets, and how these metrics might inform decisions about the transformation process.",
    "response": "<document_analysis>\nThe provided text describes external and internal diversity metrics used in dataset transformation. External diversity compares the transformed dataset to the original (seed) dataset, while internal diversity assesses the similarity between different transformation trials. The text introduces mathematical notations for both, using an expectation (\ud835\udd3c) over N datasets and a diversity function \u0398. Examples of \u0398 include N-gram metrics or reference-based metrics like BLEU scores.\n\nThe difficulty level is rated as 8/10. It requires understanding of the mathematical notation, the conceptual difference between external and internal diversity, and the ability to infer practical implications.\n\nThe question will focus on the practical importance of monitoring both diversity types and how they can guide the transformation process. It will require the expert to synthesize the definitions and consider how they relate to real-world dataset transformation scenarios. The question will avoid directly referencing the text, instead prompting for a general discussion.\n</document_analysis>\n\n<output_format>\nQuestion: How do the concepts of external and internal diversity, as applied to dataset transformations, contribute to a robust and reliable transformation pipeline, and what operational adjustments might be triggered by observing trends in these metrics?\nExplanation: The question probes the practical implications of diversity metrics, requiring the expert to go beyond the definitions and consider how they impact the overall transformation process. Understanding the distinction between external and internal diversity is crucial for assessing the quality and stability of the transformation.\nCorrect Answer: Monitoring external diversity ensures the transformed dataset retains relevant characteristics from the original, while internal diversity helps maintain consistency across different transformation attempts. Decreasing external diversity may necessitate adjustments to transformation parameters to preserve essential information, whereas high internal diversity could signal instability in the transformation process requiring debugging or refinement.\n</output_format>",
    "question": "How do the concepts of external and internal diversity, as applied to dataset transformations, contribute to a robust and reliable transformation pipeline, and what operational adjustments might be triggered by observing trends in these metrics?",
    "answer": "Monitoring external diversity ensures the transformed dataset retains relevant characteristics from the original, while internal diversity helps maintain consistency across different transformation attempts. Decreasing external diversity may necessitate adjustments to transformation parameters to preserve essential information, whereas high internal diversity could signal instability in the transformation process requiring debugging or refinement.",
    "explanation": "The question probes the practical implications of diversity metrics, requiring the expert to go beyond the definitions and consider how they impact the overall transformation process. Understanding the distinction between external and internal diversity is crucial for assessing the quality and stability of the transformation.",
    "question_token_count": 40,
    "answer_correctness_score": 9,
    "explanation_validity_score": 8,
    "question_clarity_score": 8,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 58
  },
  {
    "context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n",
    "topic": "The interpretation of the formal stability formulation using the complexity measurement function \u03a8(\u22c5) and variance, and its implications for dynamic benchmarking methods.",
    "response": "`<document_analysis>`\nThe provided context discusses the importance of accounting for complexity in dynamic benchmarks for LLMs, particularly when assessing performance drops. It highlights the difficulty of accurately measuring benchmark complexity and introduces a formal stability formulation using a complexity measurement function \u03a8(\u22c5) and variance. The core idea is that high variance in complexity across trials suggests instability in the benchmarking method.\n\n**Difficulty Rating:** 8/10. This question requires a deep understanding of the stability formulation and its implications for interpreting benchmark results. It goes beyond simply recalling the formula and necessitates an understanding of why high variance is problematic.\n\n**Intentional Question Planning:** The question aims to probe the candidate's ability to connect the formal stability formulation with its practical implications in dynamic benchmarking. It requires them to articulate why high variance in complexity measurements signifies an unreliable benchmarking method. The question encourages reflection on the relationship between complexity, performance, and the validity of dynamic benchmarking.\n\n**Irrelevant Information:** No irrelevant information is present. The entire context is relevant to the question topic.\n</document_analysis>`\n\n<output_format>\nQuestion: Why does a high variance in complexity measurements, as indicated by the formal stability formulation, suggest an unreliable dynamic benchmarking method?\nExplanation: A high variance indicates inconsistencies in how the complexity is measured across different trials, making it difficult to attribute performance drops to data contamination versus genuine task complexity.\nCorrect Answer: Inconsistent complexity measurements.\n</output_format>",
    "question": "Why does a high variance in complexity measurements, as indicated by the formal stability formulation, suggest an unreliable dynamic benchmarking method?",
    "answer": "Inconsistent complexity measurements.",
    "explanation": "A high variance indicates inconsistencies in how the complexity is measured across different trials, making it difficult to attribute performance drops to data contamination versus genuine task complexity.",
    "question_token_count": 25,
    "answer_correctness_score": 8,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 6
  },
  {
    "context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n",
    "topic": "How embedding-based similarity techniques improve upon exact matching methods in detecting data contamination.",
    "response": "<document_analysis>\nThe provided text discusses limitations of label protection and post-hoc detection methods for data contamination in large language models. It specifically focuses on post-hoc detection, detailing how it mitigates contamination by identifying overlaps between training and test datasets. The text outlines two primary approaches: exact matching (n-gram matching at token or word level) and more robust techniques like embedding-based similarity. The question topic centers on the advantages of embedding-based similarity over exact matching.\n\nDifficulty: 8/10. This question requires understanding of both exact matching and embedding-based similarity, and why the latter is preferred, requiring a nuanced grasp of the challenges associated with data contamination detection. It moves beyond a simple factual recall and probes for a deeper understanding of the technical reasons behind the shift in methodology.\n\nThe text explicitly states that \"exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity.\" This provides the direct basis for the question and answer. No irrelevant information is present.\n</document_analysis>\n<output_format>\nQuestion: What is the primary limitation of exact matching techniques in identifying data contamination, and how does this motivate the use of embedding-based similarity methods?\nExplanation: Exact matching, relying on n-gram matching, is prone to false negatives, meaning it can fail to detect contamination. Embedding-based similarity addresses this by capturing semantic relationships that exact matching misses.\nCorrect Answer: False negatives.\n</output_format>",
    "question": "What is the primary limitation of exact matching techniques in identifying data contamination, and how does this motivate the use of embedding-based similarity methods?",
    "answer": "False negatives.",
    "explanation": "Exact matching, relying on n-gram matching, is prone to false negatives, meaning it can fail to detect contamination. Embedding-based similarity addresses this by capturing semantic relationships that exact matching misses.",
    "question_token_count": 28,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 4
  },
  {
    "context": "###### Safety\nSafety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content. Datasets such as RealToxicityPrompts\u00a0(Gehman et\u00a0al., 2020) and ToxiGen\u00a0(Hartvigsen et\u00a0al., 2022) assess resilience against producing harmful outputs.\n\nBy providing a controlled environment to measure these aspects, safety benchmarks play a critical role in guiding the development of models that are not only powerful outputs but also responsible and trustworthy for real-world applications.\n\n\n###### Language\nLanguage benchmarks assess the LLMs\u2019 proficiency in specific languages.\n\nGLUE\u00a0(Wang, 2018) and SuperGLUE\u00a0(Wang et\u00a0al., 2019) cover tasks from sentiment analysis to language inference, while CLUE\u00a0(Xu et\u00a0al., 2020) targets Chinese language.\n\nTypo-fixing\u00a0(Suzgun et\u00a0al., 2022) is also widely used.\n\n\n###### Reading Comprehension\nReading comprehension tasks test a model\u2019s ability to extract and infer information from text. Benchmarks like SQuAD\u00a0(Rajpurkar et\u00a0al., 2018), QuAC\u00a0(Choi et\u00a0al., 2018), and BoolQ\u00a0(Clark et\u00a0al., 2019) challenge models to understand passages and draw logical conclusions.\n",
    "topic": "The significance of safety benchmarks like RealToxicityPrompts and ToxiGen in ensuring LLMs generate non-toxic and ethically aligned content.",
    "response": "<document_analysis>\nThe provided context discusses benchmarks used to evaluate Large Language Models (LLMs) across safety, language proficiency, and reading comprehension. The safety section highlights the importance of benchmarks like RealToxicityPrompts and ToxiGen in assessing an LLM's resilience against producing harmful outputs, and how they guide the development of responsible and trustworthy models. The language section details benchmarks like GLUE, SuperGLUE, and CLUE, which assess proficiency in various languages. The reading comprehension section outlines benchmarks such as SQuAD, QuAC, and BoolQ, which evaluate a model\u2019s ability to extract and infer information from text.\n\nThe difficulty level is rated as 7. The question will probe the understanding of why safety benchmarks are crucial, going beyond a simple recall of their existence. The question will focus on the broader implications of these benchmarks in the development lifecycle of LLMs.\n\nThe intent is to elicit a response that demonstrates an understanding of the role of safety benchmarks in ensuring ethical and responsible LLM development, rather than just reciting the names of the benchmarks.\n</document_analysis>\n<output_format>\nQuestion: How do safety benchmarks contribute to the broader design considerations of Large Language Models beyond simply detecting toxic outputs?\nExplanation: Safety benchmarks aren't just about identifying toxic outputs; they guide the development process to ensure models are responsible and trustworthy, which necessitates incorporating ethical considerations into design.\nCorrect Answer: Responsible design.\n</output_format>",
    "question": "How do safety benchmarks contribute to the broader design considerations of Large Language Models beyond simply detecting toxic outputs?",
    "answer": "Responsible design.",
    "explanation": "Safety benchmarks aren't just about identifying toxic outputs; they guide the development process to ensure models are responsible and trustworthy, which necessitates incorporating ethical considerations into design.",
    "question_token_count": 21,
    "answer_correctness_score": 8,
    "explanation_validity_score": 8,
    "question_clarity_score": 9,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 5
  },
  {
    "context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n",
    "topic": "Explain the concept of data contamination in the context of evaluating Large Language Models (LLMs) and why it poses a challenge for reliable benchmarking.",
    "response": "`<document_analysis>`\nThe provided context discusses methods for creating benchmarks to evaluate Large Language Models (LLMs) while addressing the issue of data contamination. Data contamination arises when the evaluation dataset contains information the LLM was already trained on, which can artificially inflate performance scores and provide a misleading assessment of the model's true capabilities. Several benchmarks (LiveBench, AntiLeak-Bench, AcademicEval, LiveCodeBench, LiveAoPSBench, Forecastbench) are highlighted as examples of approaches that mitigate this contamination by using data collected after the model's knowledge cutoff date or by generating queries about newly emerged knowledge.\n\nThe difficulty is rated 8/10 because it requires understanding the nuanced problem of data contamination and how various benchmarks actively combat it. It goes beyond simple recall and requires a conceptual grasp of the underlying issues.\n\nThe question will probe the understanding of data contamination and its implications for LLM evaluation. The answer will require a concise explanation of the concept and the challenge it presents. The question is designed to be challenging even for domain experts, requiring them to synthesize information about the problem and its mitigation strategies. It avoids explicit references to the text and focuses on a general understanding.\n`</document_analysis>`\n\n<output_format>\nQuestion: Why is it crucial to avoid using data from after a model\u2019s knowledge cutoff date when constructing evaluation datasets for Large Language Models?\nExplanation: Utilizing data beyond the model's knowledge cutoff can lead to artificially inflated performance scores, masking the true capabilities of the model.\nCorrect Answer: Data contamination\n</output_format>",
    "question": "Why is it crucial to avoid using data from after a model\u2019s knowledge cutoff date when constructing evaluation datasets for Large Language Models?",
    "answer": "Data contamination",
    "explanation": "Utilizing data beyond the model's knowledge cutoff can lead to artificially inflated performance scores, masking the true capabilities of the model.",
    "question_token_count": 26,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 3
  },
  {
    "context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n",
    "topic": "What further ethical guidelines should be explored regarding data usage, model transparency, and the broader societal impact of AI benchmarks?",
    "response": "`<document_analysis>`\nThe provided text discusses ethical considerations in LLM evaluations, specifically concerning static and dynamic benchmarks. It highlights potential biases in static benchmarks, privacy/security concerns with dynamic benchmarks, and the risk of misuse of benchmarking results. The question topic asks for further exploration of ethical guidelines.\n\n**Thoughtful Content Examination:** The passage explicitly states a need for \"further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\" This is a direct call for more research and discussion.\n\n**Concept Exploration:** The text implies that current ethical guidelines are insufficient and that there is a need for more robust frameworks to address the challenges posed by AI benchmarking. The potential for harm and disadvantage to specific groups is a key concern.\n\n**Strategic Complexity Calibration:** Difficulty: 8. This question requires the expert to extrapolate beyond the explicitly stated concerns in the text and propose additional guidelines. It necessitates a broader understanding of AI ethics and potential societal impacts. It\u2019s challenging because it\u2019s not a direct recall question, but requires proactive thinking about potential ethical pitfalls.\n\n**Intentional Question Planning:** The question is designed to elicit a response that demonstrates the expert's awareness of the multifaceted ethical landscape surrounding AI benchmarks. The open-ended nature of the question encourages a nuanced and well-reasoned answer. The question intentionally avoids providing any clues or hints, forcing the expert to draw on their broader knowledge.\n`</document_analysis>`\n\n`<output_format>`\nQuestion: Considering the identified risks of bias, privacy breaches, and misuse, what specific, novel ethical guidelines should be prioritized to govern the development and application of AI benchmarks?\nExplanation: The text calls for \"further exploration of ethical guidelines,\" prompting the expert to go beyond the existing concerns and suggest new, specific guidelines.\nCorrect Answer: Independent auditing of benchmark datasets.\n`</output_format>`",
    "question": "Considering the identified risks of bias, privacy breaches, and misuse, what specific, novel ethical guidelines should be prioritized to govern the development and application of AI benchmarks?",
    "answer": "Independent auditing of benchmark datasets.",
    "explanation": "The text calls for \"further exploration of ethical guidelines,\" prompting the expert to go beyond the existing concerns and suggest new, specific guidelines.",
    "question_token_count": 34,
    "answer_correctness_score": 8,
    "explanation_validity_score": 7,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 7
  },
  {
    "context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n",
    "topic": "Describe the significance of addressing potential data contamination in LLM benchmarking, outlining why contaminated benchmarks are problematic and their potential consequences.",
    "response": "`<document_analysis>`\nThe provided context discusses data contamination in LLM benchmarking, specifically focusing on syntactic contamination and its implications. The core argument revolves around the importance of mitigating contamination to ensure accurate evaluation of LLMs. The text highlights that contaminated benchmarks can overestimate model capabilities, leading to misleading conclusions about progress in LLM research and influencing deployment decisions and policy-making. The difficulty rating is 8/10 because it requires a synthesis of the key arguments presented regarding the significance and consequences of contamination, not just a direct recall of a single fact. The question is designed to probe a domain expert's understanding of the broader implications of data contamination beyond just its technical definition. I've chosen an analytical question type to encourage reflection on the impact of flawed benchmarking. The content is educationally valuable and coherent, so question generation is appropriate. </document_analysis>`\n\n<output_format>\nQuestion: Why is it imperative to prevent data contamination in the evaluation of Large Language Models?\nExplanation: The text emphasizes that contaminated benchmarks can produce inaccurate assessments of a model's abilities, potentially misleading researchers and decision-makers regarding progress and real-world applicability.\nCorrect Answer: Misleading assessments of model capabilities.\n</output_format>",
    "question": "Why is it imperative to prevent data contamination in the evaluation of Large Language Models?",
    "answer": "Misleading assessments of model capabilities.",
    "explanation": "The text emphasizes that contaminated benchmarks can produce inaccurate assessments of a model's abilities, potentially misleading researchers and decision-makers regarding progress and real-world applicability.",
    "question_token_count": 17,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 9,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 8
  },
  {
    "context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "The necessity of developing benchmarks for LLMs as they transition into general-purpose task solvers.",
    "response": "`<document_analysis>`\nThe context discusses the evolving need for LLM benchmarks as LLMs become more capable general-purpose task solvers. It highlights the efforts to create benchmarks for instruction-following and coding tasks, acknowledges the limitations of static benchmarks due to rapid LLM evolution and data contamination, and introduces the solutions of contamination detectors and dynamic benchmarks.\n\nDifficulty: 8/10. The question aims to probe the understanding of why benchmarks are crucial and the challenges associated with maintaining their relevance. It requires connecting the evolution of LLMs with the need for adaptive benchmarking strategies. The question avoids directly stating the problem; instead, it prompts the expert to infer the necessity.\n\nIntentional Question Planning: The question is designed to elicit a nuanced response regarding the trade-offs between static and dynamic benchmarks and the underlying reasons for the shift towards dynamic evaluation.\n\nIrrelevant Information: The table reference and citation list are irrelevant to the core concept of benchmarking evolution and are therefore ignored. The image reference is also irrelevant as it is not available.\n</document_analysis>`\n\n`<output_format>`\nQuestion: How does the increasing prevalence of data contamination impact the efficacy of traditional LLM benchmarks?\nExplanation: The context explicitly states that static benchmarks face challenges due to LLMs evolving rapidly and continuing to train on all available data, leading to data contamination issues. This necessitates the development of contamination detectors and dynamic benchmarks.\nCorrect Answer: Data contamination.\n</output_format>`",
    "question": "How does the increasing prevalence of data contamination impact the efficacy of traditional LLM benchmarks?",
    "answer": "Data contamination.",
    "explanation": "The context explicitly states that static benchmarks face challenges due to LLMs evolving rapidly and continuing to train on all available data, leading to data contamination issues. This necessitates the development of contamination detectors and dynamic benchmarks.",
    "question_token_count": 17,
    "answer_correctness_score": 4,
    "explanation_validity_score": 7,
    "question_clarity_score": 5,
    "question_difficulty_score": 8,
    "question_groundedness_score": 7,
    "avg_answer_token_count": 4
  },
  {
    "context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n",
    "topic": "Analyze the distinct approaches employed by PIQA, SIQA, HellaSwag, and WinoGrande to evaluate intuitive reasoning skills in language models.",
    "response": "```xml\n<document_analysis>\n1. **Thoughtful Content Examination:** The provided text describes various benchmarks used to evaluate language models across coding, instruction following, and reasoning. The focus is on reasoning benchmarks: PIQA, SIQA, HellaSwag, and WinoGrande. The text highlights these benchmarks as designed to assess intuitive reasoning skills from multiple perspectives. It also mentions ARC, OpenBookQA, and CommonsenseQA as more challenging sets requiring integration of background knowledge. Finally, C-SimpleQA evaluates factuality in Chinese.\n\n2. **Concept Exploration:** The core concept is the evaluation of language model reasoning capabilities. The text implicitly suggests that different benchmarks might employ different strategies to probe these capabilities. Understanding these differences is key to appreciating the comprehensive evaluation of reasoning.\n\n3. **Strategic Complexity Calibration:** Difficulty: 8. This requires synthesizing information across multiple benchmark descriptions to identify and articulate their distinguishing features. It's not a simple recall question but demands comparative analysis.\n\n4. **Intentional Question Planning:** The question aims to elicit a nuanced understanding of how each benchmark tackles the challenge of assessing intuitive reasoning. It encourages a comparative analysis of their methodologies rather than simply listing them. The question requires a deep dive into the purpose of each benchmark and how its structure contributes to that purpose.\n</document_analysis>\n\n<output_format>\nQuestion: How do PIQA, SIQA, HellaSwag, and WinoGrande differ in their evaluation methodologies for assessing intuitive reasoning in language models?\nExplanation: The question requires identifying the unique approaches each benchmark takes to evaluate reasoning, demanding a synthesis of information from the provided text.\nCorrect Answer: Each benchmark assesses reasoning from a distinct perspective: PIQA uses paired choice questions, SIQA focuses on situational reasoning, HellaSwag utilizes story completion, and WinoGrande employs coreference resolution.\n</output_format>\n```",
    "question": "How do PIQA, SIQA, HellaSwag, and WinoGrande differ in their evaluation methodologies for assessing intuitive reasoning in language models?",
    "answer": "Each benchmark assesses reasoning from a distinct perspective: PIQA uses paired choice questions, SIQA focuses on situational reasoning, HellaSwag utilizes story completion, and WinoGrande employs coreference resolution.",
    "explanation": "The question requires identifying the unique approaches each benchmark takes to evaluate reasoning, demanding a synthesis of information from the provided text.",
    "question_token_count": 32,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 4,
    "avg_answer_token_count": 43
  },
  {
    "context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n",
    "topic": "How do the different phases of LLM development (pre-training and fine-tuning) contribute to the risk of data contamination, and what types of datasets are typically used in each phase?",
    "response": "`<document_analysis>`\nThe provided text segment discusses the challenges of data contamination in Large Language Models (LLMs). It highlights the increased risk due to the massive and diverse training datasets, followed by fine-tuning on human-annotated or synthetic datasets. The difficulty in completely excluding evaluation data and the proprietary nature of training data further complicate the assessment of true performance.\n\n**Difficulty Rating:** 8/10. This is a high-difficulty question because it requires understanding the nuances of LLM development, the different types of datasets used, and how each phase contributes to contamination risks. It's not a simple factual recall question. It demands a synthesis of information about pre-training and fine-tuning and their respective data sources.\n\n**Intentional Question Planning:** The question aims to probe the expert's understanding of the entire LLM development pipeline and how contamination can arise at each stage. It encourages reflection on the implications of using diverse datasets and the challenges posed by proprietary training data. It requires the expert to connect the different phases and datasets to the overall problem of contamination.\n\n**Irrelevant Information:** There is no irrelevant information present. All elements contribute to the central theme of data contamination in LLMs. Citations are appropriately handled and don't detract from the core content.\n</document_analysis>`\n\n`<output_format>`\nQuestion: Considering the distinct developmental stages of LLMs, what inherent characteristics of the data utilized during pre-training and fine-tuning processes collectively contribute to the heightened potential for evaluation data contamination?\nExplanation: The question probes understanding of how both pre-training and fine-tuning, with their distinct datasets, contribute to contamination. The answer needs to articulate that pre-training relies on massive web scrapes, while fine-tuning uses human-annotated or synthetic data, both increasing the likelihood of overlap with evaluation data.\nCorrect Answer: Diverse, large-scale datasets.\n</output_format>`",
    "question": "Considering the distinct developmental stages of LLMs, what inherent characteristics of the data utilized during pre-training and fine-tuning processes collectively contribute to the heightened potential for evaluation data contamination?",
    "answer": "Diverse, large-scale datasets.",
    "explanation": "The question probes understanding of how both pre-training and fine-tuning, with their distinct datasets, contribute to contamination. The answer needs to articulate that pre-training relies on massive web scrapes, while fine-tuning uses human-annotated or synthetic data, both increasing the likelihood of overlap with evaluation data.",
    "question_token_count": 35,
    "answer_correctness_score": 8,
    "explanation_validity_score": 9,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 8
  },
  {
    "context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n",
    "topic": "What are the primary privacy and security concerns associated with dynamic benchmarks, and how can these concerns be addressed while maintaining the benefits of adaptive evaluation?",
    "response": "`<document_analysis>`\nThe provided text discusses ethical considerations in LLM evaluations, specifically focusing on the challenges posed by static and dynamic benchmarks. The core argument revolves around mitigating bias, ensuring fairness, and addressing privacy and security risks.\n\n**Thoughtful Content Examination:** The text highlights the potential for static benchmarks to perpetuate biases and the privacy/security concerns of dynamic benchmarks. It stresses the need for fairness, accountability, and privacy in benchmarking frameworks.\n\n**Concept Exploration:** The text implicitly assumes that LLM evaluations are crucial for responsible AI development. It acknowledges the tension between the benefits of adaptive evaluation (dynamic benchmarks) and the potential risks to privacy and security. The underlying theory is that biased or poorly designed benchmarks can negatively impact AI systems and user groups.\n\n**Strategic Complexity Calibration:** Difficulty: 8/10. This question requires understanding the nuanced trade-offs between static and dynamic benchmarks and applying ethical considerations to the design of evaluation frameworks. It's not a simple factual recall question.\n\n**Intentional Question Planning:** The question is designed to elicit a discussion of the specific privacy and security risks associated with dynamic benchmarks and to probe the respondent's understanding of how to balance these risks with the benefits of adaptive evaluation. It aims to assess their ability to think critically about the ethical implications of AI benchmarking.\n\nI've chosen to focus on the dynamic benchmark aspect as it presents a more complex ethical challenge than static benchmarks. The question requires the respondent to not only identify the concerns but also propose solutions, demonstrating a deeper understanding of the topic.\n</document_analysis>`\n\n`<output_format>`\nQuestion: How can the continual collection and updating of data inherent in dynamic benchmarks be reconciled with the need to safeguard privacy and security?\nExplanation: The text explicitly identifies privacy and security concerns regarding the continual collection and updating of data used in dynamic benchmarks. Addressing these concerns requires careful design and implementation of the benchmarking framework.\nCorrect Answer: Careful design and implementation.\n</output_format>`",
    "question": "How can the continual collection and updating of data inherent in dynamic benchmarks be reconciled with the need to safeguard privacy and security?",
    "answer": "Careful design and implementation.",
    "explanation": "The text explicitly identifies privacy and security concerns regarding the continual collection and updating of data used in dynamic benchmarks. Addressing these concerns requires careful design and implementation of the benchmarking framework.",
    "question_token_count": 26,
    "answer_correctness_score": 8,
    "explanation_validity_score": 8,
    "question_clarity_score": 7,
    "question_difficulty_score": 6,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 7
  },
  {
    "context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n",
    "topic": "Explain the importance of incorporating fairness, accountability, and privacy principles into the design of LLM benchmarking frameworks, providing specific examples of how these principles can be implemented.",
    "response": "`<document_analysis>\n1. **Thoughtful Content Examination:** The provided text discusses ethical considerations surrounding LLM evaluations, specifically focusing on static and dynamic benchmarks. It highlights potential biases in static benchmarks, privacy and security risks in dynamic benchmarks, and the potential for misuse of benchmarking results. The core argument emphasizes the need for fairness, accountability, and privacy in benchmark design.\n\n2. **Concept Exploration:** The text implicitly acknowledges the inherent risks associated with data used in AI systems and the potential for these risks to be amplified through benchmarking processes. It suggests a proactive approach to ethical considerations, rather than reactive measures. The call for further exploration implies that current ethical guidelines may be insufficient.\n\n3. **Strategic Complexity Calibration:** Difficulty: 8/10. This question requires the candidate to synthesize multiple points from the text\u2014the risks of both static and dynamic benchmarks, the specific ethical principles, and the need for proactive design\u2014and then apply that synthesis to propose concrete implementation strategies. It demands a nuanced understanding of the ethical implications of AI benchmarking.\n\n4. **Intentional Question Planning:** The question is designed to elicit a response that demonstrates not just an understanding of the ethical concerns, but also the ability to translate those concerns into practical design considerations. It pushes the candidate to move beyond simply identifying the principles to articulating how they can be actively incorporated into benchmark frameworks.\n</document_analysis>\n\n<output_format>\nQuestion: How can benchmarking frameworks mitigate potential harm to user groups or research domains?\nExplanation: The text emphasizes the need to design frameworks with fairness, accountability, and privacy in mind to avoid unintentionally disadvantaging certain groups.\nCorrect Answer: Proactive design incorporating ethical principles.\n</output_format>",
    "question": "How can benchmarking frameworks mitigate potential harm to user groups or research domains?",
    "answer": "Proactive design incorporating ethical principles.",
    "explanation": "The text emphasizes the need to design frameworks with fairness, accountability, and privacy in mind to avoid unintentionally disadvantaging certain groups.",
    "question_token_count": 15,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 8
  },
  {
    "context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n",
    "topic": "Describe how benchmarks like GLUE, SuperGLUE, and HumanEval utilize label protection to maintain evaluation integrity, and explain why this approach is effective in preventing data contamination.",
    "response": "<document_analysis>\nThe provided context discusses methods for securing evaluation data, specifically focusing on encryption and label protection. Encryption techniques, like those proposed by Jacovi et al. (2023) and TRUCE (Chandran et al., 2024), aim to make data inaccessible to unauthorized parties. However, these methods rely on strong key management and introduce computational overheads. Label protection, as employed in benchmarks like GLUE, SuperGLUE, and HumanEval, involves withholding true answers from public access, enabling only authorized evaluators to use them during assessment. This prevents models from learning or memorizing answers, thereby mitigating data contamination risks.\n\nThe context is well-structured and provides valuable information about data security in model evaluation. The discussion of limitations regarding encryption (key management, computational overhead, vulnerability to compromise) adds nuance. The information is directly relevant to the question topic. There is no irrelevant information to exclude. The difficulty is rated 8/10, requiring a solid understanding of the different data protection approaches and their implications. The question will require the respondent to synthesize information about label protection and its benefits, demonstrating a comprehensive understanding of the concept.\n</document_analysis>\n\n<output_format>\nQuestion: What fundamental principle underlies the utility of label protection strategies, as exemplified by benchmarks like GLUE, SuperGLUE, and HumanEval, in safeguarding evaluation integrity?\nExplanation: The core principle is preventing models from accessing the true answers during training, thus avoiding the learning or memorization of these answers that could artificially inflate performance metrics and compromise the benchmark's validity.\nCorrect Answer: Preventing memorization.\n</output_format>",
    "question": "What fundamental principle underlies the utility of label protection strategies, as exemplified by benchmarks like GLUE, SuperGLUE, and HumanEval, in safeguarding evaluation integrity?",
    "answer": "Preventing memorization.",
    "explanation": "The core principle is preventing models from accessing the true answers during training, thus avoiding the learning or memorization of these answers that could artificially inflate performance metrics and compromise the benchmark's validity.",
    "question_token_count": 35,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 6
  },
  {
    "context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n",
    "topic": "The importance of correctness as a primary evaluation criterion for dynamic benchmarking algorithms in assessing LLMs.",
    "response": "`<document_analysis>\nThe context discusses the evaluation criteria for dynamic benchmarking algorithms used to evaluate Large Language Models (LLMs). It highlights 'Correctness' as a crucial criterion, emphasizing that a flawed benchmark can lead to misleading LLM evaluations. The text provides a formula to quantify correctness, involving transformations (\ud835\udcb3\u1d62, \ud835\udcb4\u1d62), an oracle (\ud835\udca2) providing ground truth, and a scoring function (\ud835\udcae) to measure alignment. The difficulty is rated 8/10 because it requires understanding the formula and its implications regarding benchmark reliability. The question will focus on the ramifications of neglecting correctness, probing the expert's understanding of the broader implications of benchmark design and the potential for flawed evaluations.\n</document_analysis>\n\n<output_format>\nQuestion: Why is ensuring the correctness of a dynamic benchmark paramount, despite the inherent complexities of LLM evaluation?\nExplanation: Failing to guarantee correctness in a dynamic benchmark risks generating a false sense of reliability when assessing LLMs, ultimately leading to inaccurate conclusions about their capabilities.\nCorrect Answer: Misleading evaluations\n</output_format>",
    "question": "Why is ensuring the correctness of a dynamic benchmark paramount, despite the inherent complexities of LLM evaluation?",
    "answer": "Misleading evaluations",
    "explanation": "Failing to guarantee correctness in a dynamic benchmark risks generating a false sense of reliability when assessing LLMs, ultimately leading to inaccurate conclusions about their capabilities.",
    "question_token_count": 20,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 4
  },
  {
    "context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n",
    "topic": "Describe the potential challenges and benefits of using hybrid approaches in dynamic benchmarking, considering the combination of different construction processes.",
    "response": "<document_analysis>\nThe provided context discusses dynamic benchmarking for Large Language Models (LLMs). It outlines four categories of dynamic benchmarks: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches. The core focus of the question topic is on hybrid approaches, requiring an understanding of the strengths and weaknesses of each individual approach to assess the potential of combining them. The text provides enough information to formulate a challenging question that requires synthesis of the different methods. The difficulty is rated 7/10 because it requires drawing connections between the different methods and considering trade-offs. No irrelevant information is present.\n</document_analysis>\n\n<output_format>\nQuestion: How does the integration of temporal cutoff data with LLM-generated data within a hybrid dynamic benchmarking framework impact both the robustness and potential biases of the resulting evaluation?\nExplanation: This question probes the nuanced understanding of hybrid approaches, specifically requiring consideration of how combining data from different sources (temporal cutoff and LLM-generated) affects the benchmark's overall quality. It requires the expert to consider both the benefits (robustness) and drawbacks (potential biases) of such a combination, going beyond a simple description of hybrid approaches.\nCorrect Answer: Increased robustness and reduced contamination, but potentially amplified biases.\n</output_format>",
    "question": "How does the integration of temporal cutoff data with LLM-generated data within a hybrid dynamic benchmarking framework impact both the robustness and potential biases of the resulting evaluation?",
    "answer": "Increased robustness and reduced contamination, but potentially amplified biases.",
    "explanation": "This question probes the nuanced understanding of hybrid approaches, specifically requiring consideration of how combining data from different sources (temporal cutoff and LLM-generated) affects the benchmark's overall quality. It requires the expert to consider both the benefits (robustness) and drawbacks (potential biases) of such a combination, going beyond a simple description of hybrid approaches.",
    "question_token_count": 31,
    "answer_correctness_score": 8,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 6,
    "avg_answer_token_count": 12
  },
  {
    "context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n",
    "topic": "Describe the data source and specific focus of LiveAoPSBench in evaluating LLMs.",
    "response": "`<document_analysis>`\nThe context discusses methods to mitigate data contamination when evaluating Large Language Models (LLMs) by using data collected after the model's knowledge cutoff date. Several benchmarks are introduced as examples: LiveBench, AntiLeak-Bench, AcademicEval, LiveCodeBench, LiveAoPSBench, and Forecastbench. The focus of the question is to identify the specific data source and focus of LiveAoPSBench.\n\nDifficulty: 8/10. This requires identifying a specific benchmark from a list and recalling its data source and focus, demanding attention to detail. The question isn't directly stated but needs to be inferred from the text.\n\nIntentional Question Planning: The question aims to test the ability to extract specific details about a particular benchmark and understand its purpose within the broader context of LLM evaluation. It requires a good grasp of the different benchmarks and their distinct approaches.\n\nIrrelevant Information: None. The entire context is relevant to the question topic.\n</document_analysis>`\n\n`<output_format>`\nQuestion: What is the source of live math problems utilized by LiveAoPSBench for LLM evaluation?\nExplanation: LiveAoPSBench is designed to collect live math problems for LLM evaluation and the source of these problems is specifically the Art of Problem Solving forum.\nCorrect Answer: Art of Problem Solving forum\n</output_format>`",
    "question": "What is the source of live math problems utilized by LiveAoPSBench for LLM evaluation?",
    "answer": "Art of Problem Solving forum",
    "explanation": "LiveAoPSBench is designed to collect live math problems for LLM evaluation and the source of these problems is specifically the Art of Problem Solving forum.",
    "question_token_count": 19,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 10,
    "question_difficulty_score": 2,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 6
  },
  {
    "context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n",
    "topic": "Explain how the \"Collision Rate\" metric is calculated and interpreted, and what it indicates about the potential for contamination between different benchmark trials.",
    "response": "`<document_analysis>`\nThe provided context describes a dynamic benchmarking approach for LLMs and introduces the concept of \"collision\" to address the risk of data contamination. The core focus is on understanding how transformations of a benchmark dataset can lead to overlapping data, which could compromise the benchmark's ability to accurately assess LLM capabilities. The context defines \"Collision Rate\" as the percentage of overlap between two independently transformed versions of the benchmark dataset. This rate directly indicates the potential for contamination between trials. Repeat Trials is a related metric that quantifies the trials needed to regenerate a transformed dataset.\n\nDifficulty: 8/10. This question requires understanding the nuanced definition of \"collision\" and its practical implications within the context of dynamic benchmarking. It goes beyond simple recall and asks for an explanation of the metric and its interpretation.\n\nIntentional Question Planning: The question is designed to probe the understanding of the core concept of \"collision\" and its measurement. It avoids directly asking \"what is Collision Rate?\" but instead requires the expert to explain its calculation and significance, demonstrating a deeper comprehension of its role in assessing benchmark robustness.\n\nIrrelevant Information: No irrelevant information is present. The entire context pertains to the topic of dynamic benchmarking and collision.\n</document_analysis>`\n\n`<output_format>`\nQuestion: How does the Collision Rate metric quantify the potential for contamination in dynamic benchmarking, and what does its value signify?\nExplanation: The Collision Rate measures the overlap between independently transformed versions of a benchmark dataset, representing the degree to which different trials share data, thus indicating contamination potential.\nCorrect Answer: Percentage of overlap between transformed datasets.\n</output_format>`",
    "question": "How does the Collision Rate metric quantify the potential for contamination in dynamic benchmarking, and what does its value signify?",
    "answer": "Percentage of overlap between transformed datasets.",
    "explanation": "The Collision Rate measures the overlap between independently transformed versions of a benchmark dataset, representing the degree to which different trials share data, thus indicating contamination potential.",
    "question_token_count": 23,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 8
  },
  {
    "context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n",
    "topic": "Explain the increasing vulnerability of static benchmarking methods for LLMs as training datasets grow, and discuss the underlying reasons for this trend.",
    "response": "`<document_analysis>`\nThe provided context discusses data contamination in LLM benchmarking, focusing on static and dynamic approaches. The conclusion highlights the increasing vulnerability of static methods as training datasets grow, while acknowledging challenges in the reliability and reproducibility of dynamic approaches. The limitations section further elaborates on the survey's scope and potential gaps, noting the rapidly evolving nature of the field and the need for refinement of dynamic benchmarking criteria.\n\nThe core challenge lies in formulating a question that requires deep understanding of the implications of data contamination and the trade-offs between static and dynamic benchmarking, without explicitly referencing the provided text. The difficulty rating is 8, as it requires a grasp of LLM training dynamics and benchmarking methodologies. The question aims to probe the expert's understanding of why larger training datasets exacerbate the problem of contamination in static benchmarks.\n\nthought_process: The question should focus on the core vulnerability of static methods and the reasoning behind it. It needs to be challenging, requiring the expert to extrapolate beyond the explicit statement in the text. The answer should be concise and directly address the core issue. I will focus on the implications of data contamination for static benchmarks.\n</document_analysis>`\n\n<output_format>\nQuestion: How does the expansion of LLM training datasets compromise the integrity of static benchmarking methodologies?\nExplanation: As training datasets grow, the likelihood of overlap between the benchmark evaluation set and the training data increases, leading to artificially inflated performance scores and undermining the validity of static benchmarks.\nCorrect Answer: Increased overlap with training data.\n</output_format>",
    "question": "How does the expansion of LLM training datasets compromise the integrity of static benchmarking methodologies?",
    "answer": "Increased overlap with training data.",
    "explanation": "As training datasets grow, the likelihood of overlap between the benchmark evaluation set and the training data increases, leading to artificially inflated performance scores and undermining the validity of static benchmarks.",
    "question_token_count": 17,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 7
  },
  {
    "context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n",
    "topic": "Compare and contrast the interactive evaluation methods (LLM-as-an-Interviewer, TreeEval, and KIEval) in terms of their approaches to generating follow-up questions or providing feedback.",
    "response": "`<document_analysis>\nThe provided context describes two categories of LLM evaluation: Interactive Evaluation and Multi-Agent Evaluation. The Interactive Evaluation section details three specific methods: LLM-as-an-Interviewer, TreeEval, and KIEval. The Multi-Agent Evaluation section describes two methods: Benchmark Self-Evolving and BENCHAGENTS.\n\nThe question topic requests a comparison of the interactive evaluation methods regarding their follow-up question generation or feedback mechanisms. This is a suitable topic as the text explicitly outlines these mechanisms for each method.\n\nDifficulty: 8/10. This question requires understanding of each method's approach, synthesizing that information, and articulating the differences. It is not a simple factual recall question.\n\nIntentional Question Planning: The question is designed to encourage a deep understanding of how each method leverages LLMs to dynamically assess other LLMs. It requires the user to identify the core differences in their strategies, promoting a nuanced understanding of the approaches.\n</document_analysis>`\n\n`<output_format>\nQuestion: How do the interactive evaluation methodologies of LLM-as-an-Interviewer, TreeEval, and KIEval differ in their methods for generating subsequent inquiries or offering feedback on LLM responses?\nExplanation: The question requires a comparison of the three methods, focusing on their unique strategies for dynamic evaluation. A correct response needs to articulate the distinct approaches of each method.\nCorrect Answer: LLM-as-an-Interviewer paraphrases existing queries and poses follow-up questions, TreeEval generates subtopics and questions based on previous responses, and KIEval generates follow-up questions based on responses to initial benchmark questions.\n</output_format>`",
    "question": "How do the interactive evaluation methodologies of LLM-as-an-Interviewer, TreeEval, and KIEval differ in their methods for generating subsequent inquiries or offering feedback on LLM responses?",
    "answer": "LLM-as-an-Interviewer paraphrases existing queries and poses follow-up questions, TreeEval generates subtopics and questions based on previous responses, and KIEval generates follow-up questions based on responses to initial benchmark questions.",
    "explanation": "The question requires a comparison of the three methods, focusing on their unique strategies for dynamic evaluation. A correct response needs to articulate the distinct approaches of each method.",
    "question_token_count": 37,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 46
  },
  {
    "context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n",
    "topic": "Explain how AntiLeak-Bench specifically addresses and eliminates potential data contamination when generating evaluation queries.",
    "response": "`<document_analysis>`\nThe provided context discusses methods for evaluating LLMs while mitigating data contamination due to knowledge cutoff dates. Several benchmarks are mentioned: LiveBench, AntiLeak-Bench, AcademicEval, LiveCodeBench, LiveAoPSBench, and Forecastbench.\n\nThe central idea is that using data collected *after* the LLM's knowledge cutoff date can create more reliable benchmarks. The question topic focuses specifically on AntiLeak-Bench.\n\nThe context states that AntiLeak-Bench \"generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\" This is a clear and concise explanation of its approach.\n\nDifficulty: 7/10. It requires understanding the concept of data contamination and how AntiLeak-Bench specifically targets it. It\u2019s not a simple recall question.\n\nThe question will probe understanding of the benchmark's specific technique for avoiding contamination.\n\n</document_analysis>`\n\n`<output_format>`\nQuestion: How does AntiLeak-Bench avoid data contamination during the query generation process?\nExplanation: AntiLeak-Bench focuses on generating questions about information that emerged *after* the LLM's knowledge cutoff date.\nCorrect Answer: Newly emerged knowledge.\n</output_format>`",
    "question": "How does AntiLeak-Bench avoid data contamination during the query generation process?",
    "answer": "Newly emerged knowledge.",
    "explanation": "AntiLeak-Bench focuses on generating questions about information that emerged *after* the LLM's knowledge cutoff date.",
    "question_token_count": 17,
    "answer_correctness_score": 9,
    "explanation_validity_score": 10,
    "question_clarity_score": 9,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 6
  },
  {
    "context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n",
    "topic": "Describe the mathematical notation used to represent internal diversity, including the meaning of each symbol and the overall formula.",
    "response": "<document_analysis>\nThe context provides definitions and formulas for external and internal diversity in transformed datasets. The focus is on the internal diversity formula, which requires careful deconstruction of the mathematical notation. The difficulty is rated 8/10 because it requires understanding of expectation notation, summation, and dataset representation. There is no irrelevant information. The question will focus on eliciting a detailed explanation of the internal diversity formula, testing the expert's ability to interpret mathematical notation within the context of dataset diversity. The question is designed to assess a deep understanding of the formula's components and their implications.\n</document_analysis>\n<output_format>\nQuestion: What does the mathematical expression  \ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj) represent in the context of evaluating transformation trials, and what is the significance of each component?\nExplanation: The expression calculates the expected diversity between transformation trials. The 'i' and 'j' indices iterate through all pairs of trials where i is not equal to j, ensuring each pair is considered only once. The expectation operator (\ud835\udd3c) calculates the average diversity across all such pairs, providing a measure of internal diversity.\nCorrect Answer: Expected diversity between transformation trials.\n</output_format>",
    "question": "What does the mathematical expression  \ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj) represent in the context of evaluating transformation trials, and what is the significance of each component?",
    "answer": "Expected diversity between transformation trials.",
    "explanation": "The expression calculates the expected diversity between transformation trials. The 'i' and 'j' indices iterate through all pairs of trials where i is not equal to j, ensuring each pair is considered only once. The expectation operator (\ud835\udd3c) calculates the average diversity across all such pairs, providing a measure of internal diversity.",
    "question_token_count": 48,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 7
  },
  {
    "context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n",
    "topic": "Compare and contrast the encryption methods proposed by Jacovi et al. (2023) and TRUCE (Chandran et al., 2024), outlining their respective strengths and weaknesses in securing evaluation data.",
    "response": "```xml\n<document_analysis>\n1. **Thoughtful Content Examination:** The context discusses methods for securing evaluation data, specifically encryption and label protection. It highlights approaches like encryption with public keys and \"No Derivatives\" licenses (Jacovi et al., 2023), confidential computing and secure multi-party computation (TRUCE, Chandran et al., 2024), and label withholding (GLUE, SuperGLUE, HumanEval). It also acknowledges limitations of encryption methods, such as key management and computational overhead.\n\n2. **Concept Exploration:** The core concepts revolve around data privacy and preventing data contamination in machine learning model training. Encryption aims to render data inaccessible, while label protection hides the correct answers. The discussion implies a trade-off between security, computational cost, and reliance on secure key management. The vulnerability of encryption if compromised is a critical consideration.\n\n3. **Strategic Complexity Calibration:** Difficulty: 8/10. This question requires comparing and contrasting two distinct encryption methods, understanding their underlying principles, and evaluating their respective strengths and weaknesses. It demands a nuanced understanding of confidential computing and secure multi-party computation, which are advanced topics. The question is designed to challenge a domain expert's ability to synthesize information and critically assess the practical implications of different approaches.\n\n4. **Intentional Question Planning:** The question is designed to probe the expert's understanding of the technical differences between the two methods and their relative effectiveness in real-world scenarios. It encourages them to think about the practical challenges of implementing these methods and the potential vulnerabilities associated with each. The question promotes a comparative analysis rather than a simple recall of facts.\n</document_analysis>\n\n<output_format>\nQuestion: How do the cryptographic approaches of Jacovi et al. (2023) and TRUCE (Chandran et al., 2024) differ in their implementation and what are the implications of these differences for the robustness of evaluation data security?\nExplanation: The question probes the expert\u2019s understanding of the technical distinctions between public key encryption with licensing restrictions and confidential computing techniques, requiring them to analyze the security trade-offs inherent in each approach.\nCorrect Answer: Jacovi et al. rely on public-key encryption and licensing, while TRUCE uses confidential computing and secure multi-party computation, making it more robust but computationally expensive.\n</output_format>\n```",
    "question": "How do the cryptographic approaches of Jacovi et al. (2023) and TRUCE (Chandran et al., 2024) differ in their implementation and what are the implications of these differences for the robustness of evaluation data security?",
    "answer": "Jacovi et al. rely on public-key encryption and licensing, while TRUCE uses confidential computing and secure multi-party computation, making it more robust but computationally expensive.",
    "explanation": "The question probes the expert\u2019s understanding of the technical distinctions between public key encryption with licensing restrictions and confidential computing techniques, requiring them to analyze the security trade-offs inherent in each approach.",
    "question_token_count": 50,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 9,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 34
  },
  {
    "context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Define a dynamic benchmark according to the provided formal notation, explicitly stating the roles of \ud835\udc9f, T(\u22c5), \ud835\udcb3, \ud835\udcb4, and \ud835\udcae(\u22c5).",
    "response": "<document_analysis>\nThe provided context describes limitations of post-hot detection methods and introduces dynamic benchmarking as a solution to the shortcomings of static benchmarking. The core of the text focuses on the formal definition of a dynamic benchmark, outlining its components and how it functions. The difficulty is rated a 7, as it requires understanding and translating formal notation, which is a higher-order skill. The question aims to assess the candidate's ability to comprehend and articulate the formal definition presented, demonstrating a solid grasp of the core concept. There is no irrelevant or bogus information present; all content directly relates to the topic of dynamic benchmarking. The table is disregarded as it does not directly contribute to understanding the formal definition.\n</document_analysis>\n\n<output_format>\nQuestion: What are the components of a dynamic benchmark, and what role does each component play in the benchmarking process?\nExplanation: The question assesses understanding of the formal definition of a dynamic benchmark, requiring the candidate to articulate the roles of each element within the notation.\nCorrect Answer: A dynamic benchmark is defined as (\ud835\udc9f,T(\u22c5)), where \ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(\u22c5)) represents the static benchmark dataset, and T(\u22c5) is a transformation function that modifies the dataset to avoid data contamination.\n</output_format>",
    "question": "What are the components of a dynamic benchmark, and what role does each component play in the benchmarking process?",
    "answer": "A dynamic benchmark is defined as (\ud835\udc9f,T(\u22c5)), where \ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(\u22c5)) represents the static benchmark dataset, and T(\u22c5) is a transformation function that modifies the dataset to avoid data contamination.",
    "explanation": "The question assesses understanding of the formal definition of a dynamic benchmark, requiring the candidate to articulate the roles of each element within the notation.",
    "question_token_count": 22,
    "answer_correctness_score": 9,
    "explanation_validity_score": 7,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 53
  },
  {
    "context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n",
    "topic": "Explain the differences in the types of coding challenges addressed by HumanEval, MBPP, and SWE-Bench, and their relative complexity.",
    "response": "`<document_analysis>\nThe provided text describes various benchmarks used to evaluate language models. The question topic focuses on coding benchmarks specifically, asking for a comparison of HumanEval, MBPP, and SWE-Bench. The text clearly outlines that HumanEval and MBPP test code synthesis and debugging, while SWE-Bench addresses \"more advanced challenges.\" This difference in complexity is the key to answering the question. The difficulty is rated 8/10 because it requires extracting a nuanced comparison from the text and understanding what \"more advanced challenges\" implies in the context of code generation. There is no irrelevant information.\n</document_analysis>\n\n<output_format>\nQuestion: How do the coding benchmarks HumanEval, MBPP, and SWE-Bench differ in terms of the types of challenges they present?\nExplanation: The text explicitly states that HumanEval and MBPP focus on code synthesis and debugging, while SWE-Bench is designed for more advanced challenges.\nCorrect Answer: Advanced challenges",
    "question": "How do the coding benchmarks HumanEval, MBPP, and SWE-Bench differ in terms of the types of challenges they present?",
    "answer": "Advanced challenges",
    "explanation": "The text explicitly states that HumanEval and MBPP focus on code synthesis and debugging, while SWE-Bench is designed for more advanced challenges.",
    "question_token_count": 27,
    "answer_correctness_score": 8,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 3
  },
  {
    "context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n",
    "topic": "Compare and contrast temporal cutoff dynamic benchmarks with rule-based and LLM-based generation approaches, highlighting the advantages and disadvantages of each.",
    "response": "`<document_analysis>`\nThe provided text discusses dynamic benchmarks for LLM evaluation, categorizing them into temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches. The core of the question topic asks for a comparison and contrast of temporal cutoff versus rule-based and LLM-based generation.\n\n*   **Thoughtful Content Examination:** The text outlines the fundamental difference between temporal cutoff (resembling static benchmarks with recent data) and the generative approaches (rule-based and LLM-based). The text also highlights that rule-based transformations are inherently interpretable, while LLM-based transformations require additional mechanisms for transparency.\n*   **Concept Exploration:** The key concept is the distinction between data sourcing (temporal cutoff) and data creation (rule-based/LLM-based). The trade-off between interpretability and the potential for generating novel evaluation data points is also relevant.\n*   **Strategic Complexity Calibration:** Difficulty: 8/10. This requires synthesis of information and application of concepts, going beyond simple recall. It demands a nuanced understanding of the strengths and weaknesses of each approach, not explicitly stated but implied.\n*   **Intentional Question Planning:** The question is designed to assess the ability to synthesize information and evaluate trade-offs in benchmark construction. It requires the examinee to articulate the advantages and disadvantages of each approach, demonstrating a deeper understanding of the underlying principles.\n\n</document_analysis>`\n\n`<output_format>`\nQuestion: How do dynamic benchmarks employing temporal cutoff differ from those utilizing rule-based or LLM-based data generation in terms of their data sourcing and inherent interpretability?\nExplanation: Temporal cutoff benchmarks rely on recently released data, mirroring static benchmarks but with a time constraint, while rule-based and LLM-based benchmarks actively generate new evaluation data. Rule-based methods offer inherent interpretability, while LLM-based methods necessitate additional mechanisms for transparency.\nCorrect Answer: Data sourcing and interpretability.\n</output_format>`",
    "question": "How do dynamic benchmarks employing temporal cutoff differ from those utilizing rule-based or LLM-based data generation in terms of their data sourcing and inherent interpretability?",
    "answer": "Data sourcing and interpretability.",
    "explanation": "Temporal cutoff benchmarks rely on recently released data, mirroring static benchmarks but with a time constraint, while rule-based and LLM-based benchmarks actively generate new evaluation data. Rule-based methods offer inherent interpretability, while LLM-based methods necessitate additional mechanisms for transparency.",
    "question_token_count": 30,
    "answer_correctness_score": 8,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 7
  },
  {
    "context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "The concept of dynamic benchmarks and their intended function in addressing the limitations of static benchmarks.",
    "response": "`<document_analysis>`\nThe provided text discusses the evolution of LLM benchmarking, highlighting the limitations of static benchmarks due to rapid model development and data contamination. The core concept revolves around dynamic benchmarks as a solution to these issues. The difficulty rating is 7/10, as it requires understanding the implications of data contamination and the need for adaptive evaluation methods. The question will focus on eliciting an understanding of why dynamic benchmarks are necessary, probing beyond a simple factual recall of their existence. The table is irrelevant and will be ignored. The question should be challenging for a domain expert, requiring them to articulate the reasoning behind the shift towards dynamic evaluation.\n</question_topic>`\n\n`<output_format>`\nQuestion: How does the inherent nature of continuous LLM training pose a fundamental challenge to the long-term validity of static benchmark evaluations?\nExplanation: Static benchmarks, being fixed datasets, become increasingly susceptible to data contamination as models are continuously trained on expanding datasets, potentially leading to artificially inflated performance scores and a misrepresentation of true generalization capabilities.\nCorrect Answer: Data contamination",
    "question": "How does the inherent nature of continuous LLM training pose a fundamental challenge to the long-term validity of static benchmark evaluations?",
    "answer": "Data contamination",
    "explanation": "Static benchmarks, being fixed datasets, become increasingly susceptible to data contamination as models are continuously trained on expanding datasets, potentially leading to artificially inflated performance scores and a misrepresentation of true generalization capabilities.",
    "question_token_count": 24,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 3
  },
  {
    "context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n",
    "topic": "Discuss the broader implications of data contamination on LLM research, model deployment decisions, and policy-making, synthesizing the information presented in the text.",
    "response": "<document_analysis>\n1. **Thoughtful Content Examination:** The context discusses data contamination in LLMs, specifically syntactic contamination (rephrasing training data). It highlights the debate around whether this constitutes true contamination and argues that it should be considered as such for applications reliant on syntactic information. The text then emphasizes the significance of mitigating contamination to ensure accurate benchmarking and avoid misleading conclusions about LLM capabilities.\n\n2. **Concept Exploration:** The core concept revolves around the challenge of distinguishing between memorization and reasoning in LLMs. Data contamination, particularly syntactic, can inflate performance metrics, making it difficult to assess true generalization ability. The implications extend beyond research to influence practical deployment and policy.\n\n3. **Strategic Complexity Calibration:** Difficulty: 8/10. This question requires synthesizing information from different parts of the text and extrapolating the implications beyond the explicitly stated examples. It demands a nuanced understanding of the core problem and its consequences.\n\n4. **Intentional Question Planning:** The question is designed to encourage the domain expert to connect the problem of data contamination with its broader consequences. It requires them to go beyond simply reciting facts and instead demonstrate an understanding of the implications for the field. The question asks for synthesis of information, rather than retrieval.\n</document_analysis>\n\n<output_format>\nQuestion: How does the presence of data contamination, as discussed, impact the reliability of LLM evaluations and subsequently influence decisions regarding model deployment and the development of relevant policies?\nExplanation: The text explicitly states that contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. The question probes the expert\u2019s understanding of this chain of consequence.\nCorrect Answer: Misleading conclusions about progress.\n</output_format>",
    "question": "How does the presence of data contamination, as discussed, impact the reliability of LLM evaluations and subsequently influence decisions regarding model deployment and the development of relevant policies?",
    "answer": "Misleading conclusions about progress.",
    "explanation": "The text explicitly states that contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. The question probes the expert\u2019s understanding of this chain of consequence.",
    "question_token_count": 32,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 7
  },
  {
    "context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n",
    "topic": "Evaluate the limitations of the presented survey regarding the rapidly evolving nature of LLM development and benchmarking techniques, and discuss the potential impact of these limitations.",
    "response": "`<document_analysis>\n1. **Thoughtful Content Examination:** The context discusses a survey on data contamination in LLM benchmarking, outlining both static and dynamic approaches. It then details limitations of the survey itself, primarily focusing on the rapid evolution of the field, potential omissions of recent methods, the preliminary nature of proposed criteria for dynamic benchmarking, and a lack of fine-grained technical details.\n\n2. **Concept Exploration:** The core concept revolves around the inherent difficulty in creating a static, comprehensive assessment of a rapidly changing technology like LLMs. The limitations highlight the tension between the desire for thoroughness and the reality of constant innovation. The survey's approach, while valuable, is acknowledged to be a snapshot in time.\n\n3. **Strategic Complexity Calibration:** Difficulty: 8. This question requires a deep understanding of the survey's scope, an appreciation for the challenges of benchmarking rapidly evolving AI models, and the ability to extrapolate the potential consequences of the stated limitations. It's not a simple recall question; it demands critical thinking and evaluation.\n\n4. **Intentional Question Planning:** The question is designed to probe the respondent's understanding of the challenges in benchmarking LLMs and their ability to articulate the implications of the survey's acknowledged limitations. It encourages a nuanced perspective on the value and potential shortcomings of such a survey.\n</document_analysis>\n\n<output_format>\nQuestion: How does the acknowledgment of the field's rapid evolution impact the long-term utility and interpretation of the survey's findings regarding LLM benchmarking methods?\nExplanation: The survey's recognition of the fast-paced development in LLMs and benchmarking implies that its conclusions might be quickly outdated, limiting its long-term value.\nCorrect Answer: Limited long-term value.\n</output_format>",
    "question": "How does the acknowledgment of the field's rapid evolution impact the long-term utility and interpretation of the survey's findings regarding LLM benchmarking methods?",
    "answer": "Limited long-term value.",
    "explanation": "The survey's recognition of the fast-paced development in LLMs and benchmarking implies that its conclusions might be quickly outdated, limiting its long-term value.",
    "question_token_count": 29,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 6
  },
  {
    "context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Explain how the dynamic dataset \ud835\udc9ft is generated from the static benchmark dataset \ud835\udc9f, detailing the role of the transformation function and the concept of timestamps.",
    "response": "<document_analysis>\nThe provided context discusses limitations of post-hot detection methods and introduces dynamic benchmarking as a solution to overcome the challenges of static benchmarking. The core of the dynamic benchmarking approach lies in its ability to modify the dataset during evaluation to prevent data contamination. The text defines a dynamic benchmark as a tuple (\ud835\udc9f, T(\u22c5)), where \ud835\udc9f represents the static benchmark dataset and T(\u22c5) is a transformation function. The dynamic dataset \ud835\udc9ft is generated by iteratively applying the transformation function T to the original dataset \ud835\udc9f, creating a sequence of datasets \ud835\udc9f1, \ud835\udc9f2, ..., \ud835\udc9fN, where N is the total number of timestamps. If the initial dataset \ud835\udc9f is empty, the dynamic benchmark dataset is created from scratch.\n\nThe difficulty level is rated as 8/10. This is because it requires the expert to understand the mathematical notation used to describe the dynamic benchmarking process and to synthesize the information about the transformation function and timestamps into a coherent explanation. The question encourages a deep understanding of the dynamic benchmarking methodology and its reliance on iterative transformation. The question is designed to probe the expert\u2019s ability to interpret the formal definition and explain the process in a clear and concise manner.\n\n</document_analysis>\n\n<output_format>\nQuestion: How does the dynamic benchmarking approach iteratively generate evaluation datasets, and what is the significance of the transformation function within this process?\nExplanation: The dynamic benchmarking approach generates evaluation datasets by repeatedly applying a transformation function to a static benchmark dataset. Each iteration produces a new dataset at a specific timestamp.\nCorrect Answer: Iterative transformation function application.\n</output_format>",
    "question": "How does the dynamic benchmarking approach iteratively generate evaluation datasets, and what is the significance of the transformation function within this process?",
    "answer": "Iterative transformation function application.",
    "explanation": "The dynamic benchmarking approach generates evaluation datasets by repeatedly applying a transformation function to a static benchmark dataset. Each iteration produces a new dataset at a specific timestamp.",
    "question_token_count": 25,
    "answer_correctness_score": 8,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 7
  },
  {
    "context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n",
    "topic": "Explain the role and significance of the oracle function, \ud835\udca2(\u22c5), within the provided equation for assessing correctness in dynamic benchmarks, including providing examples of what this function could be.",
    "response": "`<document_analysis>\n1. **Thoughtful Content Examination:** The context discusses evaluation criteria for dynamic benchmarking algorithms used to evaluate Large Language Models (LLMs). A key criterion is \"Correctness,\" which is quantified using an equation. The equation involves an oracle function, \ud835\udca2(\u22c5), which returns the ground truth for a given input. The purpose of this function is to provide an objective reference point for evaluating the correctness of the transformed dataset produced by the dynamic benchmark.\n\n2. **Concept Exploration:** The oracle function is crucial because it addresses the inherent challenge of evaluating dynamic benchmarks \u2013 ensuring that the generated dataset remains faithful to the ground truth. Without an objective reference, it would be difficult to determine whether the benchmark is providing a reliable assessment of LLMs. The example of a domain-specific annotator highlights the need for a reliable and accurate source of ground truth. The equation emphasizes the importance of alignment between the benchmark's outputs and the ground truth.\n\n3. **Strategic Complexity Calibration:** Difficulty: 8/10. This question requires understanding the purpose of the oracle function within the equation, its role in ensuring objectivity, and the ability to provide relevant examples. It demands a grasp of the broader concept of dynamic benchmarking and the challenges associated with evaluating such benchmarks. It is not a simple factual recall question.\n\n4. **Intentional Question Planning:** The question is designed to probe the candidate's understanding of the equation's components and the underlying rationale for using an oracle. It encourages the candidate to consider the practical implications of the oracle's function and the types of entities that could serve as oracles.\n</document_analysis>\n\n<output_format>\nQuestion: What is the purpose of the oracle function, \ud835\udca2(\u22c5), within the provided equation for assessing correctness in dynamic benchmarks, and what are some potential real-world implementations of this function?\nExplanation: The oracle function provides an objective ground truth against which the outputs of the transformed dataset are compared, ensuring the benchmark maintains correctness.\nCorrect Answer: Objective ground truth reference.\n</output_format>",
    "question": "What is the purpose of the oracle function, \ud835\udca2(\u22c5), within the provided equation for assessing correctness in dynamic benchmarks, and what are some potential real-world implementations of this function?",
    "answer": "Objective ground truth reference.",
    "explanation": "The oracle function provides an objective ground truth against which the outputs of the transformed dataset are compared, ensuring the benchmark maintains correctness.",
    "question_token_count": 38,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 6
  },
  {
    "context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n",
    "topic": "Explain the purpose of encryption and label protection in the context of machine learning evaluation data security, detailing how they address the risk of data leakage and contamination.",
    "response": "<document_analysis>\n1. **Thoughtful Content Examination:** The context discusses encryption and label protection as methods to secure evaluation data in machine learning. Encryption aims to make data inaccessible to unauthorized parties, while label protection hides the true answers from public access. Both methods address the risk of data leakage and contamination, which can compromise evaluation integrity. The text also acknowledges limitations of encryption, such as the need for strong key management and potential vulnerabilities.\n\n2. **Concept Exploration:** The core concept revolves around maintaining the integrity of evaluation benchmarks by preventing models from accessing sensitive data (test data and labels) during training. This is crucial for ensuring fair and reliable comparisons between models. The discussion of TRUCE highlights the use of advanced techniques like confidential computing. The vulnerability of encryption methods if keys are compromised is a critical consideration.\n\n3. **Strategic Complexity Calibration:** Difficulty: 8. This is a challenging question because it requires a synthesis of the information regarding both encryption and label protection, and the specific ways each contributes to data security. It also demands understanding of the underlying motivations (preventing data leakage and contamination) and the limitations of these techniques.\n\n4. **Intentional Question Planning:** The question is designed to elicit a comprehensive response that demonstrates understanding of the *purpose* of each method, not just a definition. It encourages the candidate to articulate how they specifically address the risks of data leakage and contamination. The question is open-ended to allow for nuanced explanations and a demonstration of deeper understanding.\n</document_analysis>\n\n<output_format>\nQuestion: How do encryption and label protection strategies safeguard machine learning evaluation data, and what are the fundamental risks they mitigate?\nExplanation: This question probes the candidate\u2019s understanding of the underlying principles of data security in machine learning evaluation. It requires them to articulate not just *what* the strategies are, but *why* they are employed and what specific threats they address.\nCorrect Answer: Preventing data leakage and contamination.\n</output_format>",
    "question": "How do encryption and label protection strategies safeguard machine learning evaluation data, and what are the fundamental risks they mitigate?",
    "answer": "Preventing data leakage and contamination.",
    "explanation": "This question probes the candidate\u2019s understanding of the underlying principles of data security in machine learning evaluation. It requires them to articulate not just *what* the strategies are, but *why* they are employed and what specific threats they address.",
    "question_token_count": 23,
    "answer_correctness_score": 8,
    "explanation_validity_score": 4,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 8
  },
  {
    "context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n",
    "topic": "Detail the methodology employed by LiveBench to construct its benchmark, including the types of data sources it utilizes and its update frequency.",
    "response": "`<document_analysis>\n1. **Thoughtful Content Examination:** The context discusses methods to evaluate LLMs by mitigating data contamination, specifically focusing on the temporal cutoff. It then details several benchmarks that employ this strategy. The core focus is on how these benchmarks gather data *after* the LLM's knowledge cutoff date.\n2. **Concept Exploration:** The concept of \"data contamination\" is central \u2013 the risk that an LLM has already seen the data used to evaluate it. The benchmarks described are designed to avoid this by using recent data. The different approaches highlight the varied ways to address this challenge.\n3. **Strategic Complexity Calibration:** Difficulty: 8/10. This question requires the expert to synthesize information from multiple sentences describing LiveBench, understand its purpose within the broader context of data contamination mitigation, and recall specific details about its data sources and update schedule. It is not a simple recall question.\n4. **Intentional Question Planning:** The question is designed to probe the expert's understanding of LiveBench's practical implementation. It compels them to extract specific details about data collection and update mechanisms. The phrasing avoids directly asking \"What does LiveBench do?\", pushing for a more detailed and descriptive response.\n</document_analysis>\n\n<output_format>\nQuestion: How do benchmarks like LiveBench address the challenge of data contamination when evaluating Large Language Models?\nExplanation: LiveBench employs a methodology of collecting questions based on the latest information sources, such as math competitions from the past 12 months, and updating these questions every few months. This ensures the evaluation data is temporally distinct from the model's training data.\nCorrect Answer: Latest information sources, updated every few months.\n</output_format>",
    "question": "How do benchmarks like LiveBench address the challenge of data contamination when evaluating Large Language Models?",
    "answer": "Latest information sources, updated every few months.",
    "explanation": "LiveBench employs a methodology of collecting questions based on the latest information sources, such as math competitions from the past 12 months, and updating these questions every few months. This ensures the evaluation data is temporally distinct from the model's training data.",
    "question_token_count": 19,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 9,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 10
  },
  {
    "context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n",
    "topic": "Explain why interpretability is crucial in dynamic benchmarking, particularly when LLMs are used to generate transformations, and what measures can be taken to ensure reliability and correctness in those cases.",
    "response": "`<document_analysis>\n1. **Thoughtful Content Examination:** The context discusses dynamic benchmarking, highlighting the challenges of verifying the correctness of transformations due to the large volume of data generated. It emphasizes the importance of interpretability and distinguishes between inherently interpretable (rule-based) and less interpretable (LLM-assisted) transformations. The passage also introduces different categories of dynamic benchmarks.\n\n2. **Concept Exploration:** The core concept revolves around the trade-off between automation (using LLMs for transformation) and the need for assurance (interpretability and validation). The implicit assumption is that automation, while efficient, introduces risks of errors that require mitigation. The passage implicitly touches upon the \"black box\" nature of LLMs and the need for techniques to understand their reasoning.\n\n3. **Strategic Complexity Calibration:** Difficulty: 8/10. This question requires integrating multiple concepts from the passage\u2014dynamic benchmarking, transformation interpretability, LLM-assisted transformations, and validation mechanisms. It necessitates understanding *why* interpretability is important and *how* to achieve it in a complex scenario. It is challenging because it requires synthesis rather than direct recall.\n\n4. **Intentional Question Planning:** The question is designed to assess the candidate's understanding of the core problem (verifying transformations in dynamic benchmarking), the challenges introduced by LLMs, and the potential solutions (explainability tools, human-in-the-loop validation). It encourages them to articulate the rationale behind the need for interpretability and to propose concrete measures for ensuring reliability.\n</document_analysis>\n\n<output_format>\nQuestion: Why is the interpretability of transformations critical in dynamic benchmarking, and what strategies are necessary when these transformations are assisted by large language models?\nExplanation: The passage explicitly states that interpretability reduces the need for extensive manual validation, lowering costs, and that LLM-assisted transformations require additional mechanisms due to the models' lack of transparency.\nCorrect Answer: Explainability tools and human-in-the-loop validation.\n</output_format>",
    "question": "Why is the interpretability of transformations critical in dynamic benchmarking, and what strategies are necessary when these transformations are assisted by large language models?",
    "answer": "Explainability tools and human-in-the-loop validation.",
    "explanation": "The passage explicitly states that interpretability reduces the need for extensive manual validation, lowering costs, and that LLM-assisted transformations require additional mechanisms due to the models' lack of transparency.",
    "question_token_count": 28,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 11
  },
  {
    "context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n",
    "topic": "The approach of DyVal in assessing LLM reasoning using randomly generated directed acyclic graphs (DAGs), including the process of converting these graphs into natural language descriptions and querying for the root node value.",
    "response": "`<document_analysis>\nThe provided text describes several evaluation methods (S3Eval, DyVal, NPHardEval, and Xie et al.) for assessing the reasoning capabilities of Large Language Models (LLMs). The focus is on methods using randomly generated data structures like SQL tables, directed acyclic graphs (DAGs), and random graphs. I will focus on DyVal, as it is the specified topic. The key aspects of DyVal are the construction of DAGs with varying numbers of nodes and edges, the rule-based conversion of these DAGs into natural language descriptions, and the querying of the LLM for the value of the root node. This process allows for control over task difficulty. The difficulty is rated a 7/10 due to the need to understand the nuances of DAG construction and rule-based conversion. The question will focus on the implications of the rule-based conversion process and how it might affect the evaluation of LLM reasoning.\n</document_analysis>\n\n<output_format>\nQuestion: How might the reliance on rule-based conversion to transform directed acyclic graphs into natural language descriptions in DyVal potentially introduce biases or limitations in assessing the true reasoning capabilities of an LLM?\nExplanation: The question probes for an understanding of the limitations introduced by simplifying complex graph structures into natural language using predefined rules. It requires considering how such a conversion might obscure or oversimplify the reasoning process, potentially misrepresenting an LLM's actual capabilities.\nCorrect Answer: Oversimplification of reasoning pathways.\n</output_format>",
    "question": "How might the reliance on rule-based conversion to transform directed acyclic graphs into natural language descriptions in DyVal potentially introduce biases or limitations in assessing the true reasoning capabilities of an LLM?",
    "answer": "Oversimplification of reasoning pathways.",
    "explanation": "The question probes for an understanding of the limitations introduced by simplifying complex graph structures into natural language using predefined rules. It requires considering how such a conversion might obscure or oversimplify the reasoning process, potentially misrepresenting an LLM's actual capabilities.",
    "question_token_count": 36,
    "answer_correctness_score": 8,
    "explanation_validity_score": 8,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 9
  },
  {
    "context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n",
    "topic": "The challenges associated with accurately measuring the complexity of benchmark datasets, including the limitations of domain-specific complexity metrics such as DyVal's graph complexity approach.",
    "response": "`<document_analysis>\n1. **Thoughtful Content Examination:** The context discusses the importance of accounting for complexity in dynamic benchmarks to differentiate between performance drops caused by data contamination and increased task complexity. It highlights the difficulty in accurately measuring this complexity and critiques existing methods like DyVal's graph complexity, noting their domain-specific nature. The context also introduces a formal definition of stability using a complexity measurement function.\n\n2. **Concept Exploration:** The core concept revolves around the challenge of quantifying complexity in benchmark datasets for LLMs. The mention of DyVal suggests a move towards graph-based representations of reasoning problems to assess complexity. The formalization of stability using variance implies a statistical approach to evaluating the reliability of dynamic benchmarking methods. The implicit assumption is that a reliable benchmark should exhibit consistent complexity across trials.\n\n3. **Strategic Complexity Calibration:** Difficulty: 8/10. This question requires understanding of the limitations of current complexity measurement techniques and the implications of those limitations on dynamic benchmarking. It necessitates grasping the formal definition of stability and its connection to complexity variance.\n\n4. **Intentional Question Planning:** The question aims to probe the expert's understanding of why generalizable complexity metrics are so elusive and why domain-specific approaches struggle to provide a robust evaluation. It also intends to assess their ability to connect the concept of variance in complexity to the stability of a benchmark.</document_analysis>\n\n<output_format>\nQuestion: Why do existing methods for measuring benchmark dataset complexity, such as graph complexity, often fail to generalize across different applications?\nExplanation: Domain-specific complexity metrics are tailored to particular problem types, making them unsuitable for assessing the complexity of diverse tasks encountered in broader LLM evaluations.\nCorrect Answer: Domain-specificity.\n</output_format>",
    "question": "Why do existing methods for measuring benchmark dataset complexity, such as graph complexity, often fail to generalize across different applications?",
    "answer": "Domain-specificity.",
    "explanation": "Domain-specific complexity metrics are tailored to particular problem types, making them unsuitable for assessing the complexity of diverse tasks encountered in broader LLM evaluations.",
    "question_token_count": 24,
    "answer_correctness_score": 10,
    "explanation_validity_score": 9,
    "question_clarity_score": 9,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 5
  },
  {
    "context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n",
    "topic": "The purpose and function of CONSTAT as a method for detecting contamination by comparing model performance across benchmarks.",
    "response": "`<document_analysis>\nThe provided text discusses limitations in evaluating large language models (LLMs), particularly regarding label protection and data contamination. It details post-hoc detection methods to mitigate contamination, including n-gram matching, embedding-based similarity, and analysis of model behavior. The section focuses on various techniques, culminating in the introduction of CONSTAT by Dekoninck et al. (2024). The text explicitly states that CONSTAT detects contamination by comparing model performance across benchmarks.\n\nDifficulty: 8/10 - Requires understanding of the broader context of LLM evaluation and the specific role of CONSTAT within that framework. It's not a simple recall question, but requires synthesis of the information.\n\nIntent: The question aims to assess the understanding of CONSTAT's function within the larger context of contamination detection and its reliance on cross-benchmark performance comparison. The question encourages a nuanced understanding of the method's purpose.\n\nIrrelevant Information: No irrelevant information is present. All content contributes to the discussion of LLM evaluation and contamination detection.\n</document_analysis>\n\n<output_format>\nQuestion: How does CONSTAT contribute to the detection of contamination in large language models?\nExplanation: CONSTAT is designed to identify contamination by analyzing and comparing a model's performance across different benchmarks.\nCorrect Answer: Comparing model performance across benchmarks.\n</output_format>",
    "question": "How does CONSTAT contribute to the detection of contamination in large language models?",
    "answer": "Comparing model performance across benchmarks.",
    "explanation": "CONSTAT is designed to identify contamination by analyzing and comparing a model's performance across different benchmarks.",
    "question_token_count": 16,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 7
  },
  {
    "context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n",
    "topic": "The framework developed by Xie et al., which involves automatically constructing Knights and Knaves puzzles with random reasoning graphs to evaluate LLM reasoning abilities.",
    "response": "`<document_analysis>`\nThe provided context describes several evaluation frameworks for assessing the reasoning abilities of Large Language Models (LLMs). It categorizes these frameworks into \"Table-Based\" and \"Graph-Based\" approaches. The focus is on frameworks that utilize randomly generated data structures (SQL tables, directed acyclic graphs, random graphs) to create challenging reasoning tasks for LLMs. The question topic centers on Xie et al.'s approach, which uses Knights and Knaves puzzles generated with random reasoning graphs.\n\nThe difficulty level is rated as 8/10. The question should probe the expert's understanding of the underlying logic and the challenges of evaluating reasoning in such a context. The question should not be easily answered by simply recalling the description but require some inference about the design choices and potential limitations of this approach.\n\nThe intent is to create a question that encourages the expert to consider the complexities of representing and evaluating reasoning within the constraints of a randomly generated puzzle structure. The question will target a deep understanding of the core principles behind the evaluation method and its potential applicability and limitations.\n</document_analysis>`\n\n`<output_format>`\nQuestion: How does the random generation of Knights and Knaves puzzles with reasoning graphs impact the generalizability of the evaluation of LLM reasoning abilities assessed by Xie et al.?\nExplanation: The random generation of puzzles ensures diversity but may limit the evaluation to specific reasoning patterns that arise in those randomly constructed scenarios, potentially hindering assessment of broader reasoning capabilities.\nCorrect Answer: Limited generalizability.\n</output_format>`",
    "question": "How does the random generation of Knights and Knaves puzzles with reasoning graphs impact the generalizability of the evaluation of LLM reasoning abilities assessed by Xie et al.?",
    "answer": "Limited generalizability.",
    "explanation": "The random generation of puzzles ensures diversity but may limit the evaluation to specific reasoning patterns that arise in those randomly constructed scenarios, potentially hindering assessment of broader reasoning capabilities.",
    "question_token_count": 32,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 5
  },
  {
    "context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n",
    "topic": "Explain the role of the contamination detector (Shi et al., 2024) in the ITD framework (Zhu et al., 2024c) and how it informs the LLM\u2019s rewriting process to preserve difficulty levels.",
    "response": "`<document_analysis>\n1. **Thoughtful Content Examination:** The context discusses limitations of rule-generated data and benchmark rewriting techniques using LLMs. Several frameworks (Auto-Dataset, StructEval, ITD, VarBench) are described, with ITD focusing on mitigating contamination in static benchmarks. The key element here is the contamination detector (Shi et al., 2024) used within ITD.\n\n2. **Concept Exploration:** The core concept is contamination in datasets \u2013 where training data overlaps with benchmark data, leading to inflated performance metrics. The ITD framework attempts to address this by identifying contaminated samples and then prompting an LLM to rewrite them. The challenge lies in rewriting while maintaining the original difficulty.\n\n3. **Strategic Complexity Calibration:** This is a high-complexity question (difficulty 8/10) because it requires understanding the *purpose* of the contamination detector within a specific framework and its influence on the subsequent LLM process. It's not simply a factual recall; it requires synthesis and inference.\n\n4. **Intentional Question Planning:** The question aims to assess the candidate's ability to understand how a contamination detector contributes to a broader methodology for dataset improvement. The answer should demonstrate comprehension of the detector's role and its impact on difficulty preservation during rewriting.\n</document_analysis>\n\n<output_format>\nQuestion: How does the contamination detector contribute to the ITD framework\u2019s process of rewriting benchmark samples?\nExplanation: The contamination detector identifies samples in static benchmarks that are potentially contaminated, meaning they overlap with the training data. This information then guides the LLM's rewriting process, ensuring that the rewritten samples retain the original difficulty levels while removing the contamination.\nCorrect Answer: Preserving difficulty levels.\n</output_format>",
    "question": "How does the contamination detector contribute to the ITD framework\u2019s process of rewriting benchmark samples?",
    "answer": "Preserving difficulty levels.",
    "explanation": "The contamination detector identifies samples in static benchmarks that are potentially contaminated, meaning they overlap with the training data. This information then guides the LLM's rewriting process, ensuring that the rewritten samples retain the original difficulty levels while removing the contamination.",
    "question_token_count": 19,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 9,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 6
  },
  {
    "context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "The role and purpose of contamination detectors in assessing the risks associated with benchmark data.",
    "response": "`<document_analysis>`\nThe provided context discusses the evolution of LLM benchmarking and the challenges posed by rapidly evolving LLMs and data contamination. The central idea is that static benchmarks become ineffective over time as models improve and are trained on the benchmark data itself. The context highlights the development of contamination detectors as a response to this issue.\n\n**Concept Exploration:** The context implicitly acknowledges the continuous learning nature of LLMs and the potential for them to \"memorize\" benchmark data, rendering the benchmarks less useful for evaluating genuine generalization ability. Contamination detectors address this by quantifying the extent to which a benchmark dataset has been incorporated into an LLM's training data. Dynamic benchmarks are also mentioned as a mitigation strategy, suggesting a shift toward benchmarks that adapt to model progress.\n\n**Strategic Complexity Calibration:** Difficulty: 7/10. The question requires understanding the problem of data contamination and the specific role of contamination detectors, not just a superficial understanding of benchmarks. It necessitates grasping the implications of LLMs being trained on benchmark data.\n\n**Intentional Question Planning:** The question is designed to assess understanding of *why* contamination detectors are needed, not just *what* they are. It requires the candidate to connect the problem of evolving LLMs with the need for tools to measure data overlap.\n</document_analysis>\n\n<output_format>\nQuestion: How does the inherent training methodology of Large Language Models directly motivate the need for contamination detection techniques within the context of benchmark evaluation?\nExplanation: The continuous training of LLMs on vast datasets, including potentially benchmark data, leads to memorization and diminishes the benchmark's ability to assess true generalization. Contamination detectors are specifically designed to quantify this overlap.\nCorrect Answer: Continuous training",
    "question": "How does the inherent training methodology of Large Language Models directly motivate the need for contamination detection techniques within the context of benchmark evaluation?",
    "answer": "Continuous training",
    "explanation": "The continuous training of LLMs on vast datasets, including potentially benchmark data, leads to memorization and diminishes the benchmark's ability to assess true generalization. Contamination detectors are specifically designed to quantify this overlap.",
    "question_token_count": 26,
    "answer_correctness_score": 8,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 3
  },
  {
    "context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n",
    "topic": "Discuss the computational overhead associated with encryption methods for securing evaluation data, and propose strategies for mitigating this impact without compromising security.",
    "response": "<document_analysis>\n1. **Thoughtful Content Examination:** The context discusses encryption and label protection methods used to secure evaluation data from being used in model training. Encryption, as proposed by Jacovi et al. (2023) and TRUCE (Chandran et al., 2024), involves public key encryption and confidential computing, respectively. However, the text explicitly states that these methods introduce computational overhead. Label protection, used in benchmarks like GLUE and HumanEval, hides true test labels to prevent model memorization.\n\n2. **Concept Exploration:** The text highlights a trade-off between security and computational cost. Encryption offers robust protection but can be computationally expensive. Label protection is simpler but only addresses the risk of models directly memorizing answers. The vulnerability to key compromise in encryption is also a critical consideration. The core concept revolves around balancing data protection with practical feasibility.\n\n3. **Strategic Complexity Calibration:** Difficulty: 8/10. This question requires integrating the information about computational overhead with a broader understanding of security trade-offs and mitigation strategies. It necessitates proposing solutions, not just stating facts from the text.\n\n4. **Intentional Question Planning:** The question is designed to prompt a deeper reflection on the practical implications of encryption. It encourages the expert to consider how to optimize the process without sacrificing security, requiring a nuanced understanding of the limitations and vulnerabilities discussed. The question targets the expert\u2019s ability to synthesize the information and propose realistic solutions.\n</document_analysis>\n\n<output_format>\nQuestion: Considering the computational overhead introduced by encryption methods for securing evaluation data, and the potential vulnerability associated with private key exposure, what architectural or algorithmic modifications could be implemented to reduce this overhead while maintaining a comparable level of security against data leakage and unauthorized access?\nExplanation: This question probes the expert's ability to address a direct challenge presented in the text \u2013 the computational burden of encryption \u2013 while simultaneously considering a key vulnerability (key exposure). It requires a synthesis of security principles and optimization techniques, pushing beyond simple recall of information.\nCorrect Answer: Homomorphic encryption or federated learning.\n</output_format>",
    "question": "Considering the computational overhead introduced by encryption methods for securing evaluation data, and the potential vulnerability associated with private key exposure, what architectural or algorithmic modifications could be implemented to reduce this overhead while maintaining a comparable level of security against data leakage and unauthorized access?",
    "answer": "Homomorphic encryption or federated learning.",
    "explanation": "This question probes the expert's ability to address a direct challenge presented in the text \u2013 the computational burden of encryption \u2013 while simultaneously considering a key vulnerability (key exposure). It requires a synthesis of security principles and optimization techniques, pushing beyond simple recall of information.",
    "question_token_count": 50,
    "answer_correctness_score": 8,
    "explanation_validity_score": 7,
    "question_clarity_score": 6,
    "question_difficulty_score": 9,
    "question_groundedness_score": 8,
    "avg_answer_token_count": 9
  }
]