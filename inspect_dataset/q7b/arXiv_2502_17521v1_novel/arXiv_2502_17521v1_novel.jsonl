[
  {
    "context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n",
    "topic": "VarBench's Strategy in Replacing Variables in Existing Benchmark Samples",
    "response": "<output_format>\nQuestion: How does VarBench's strategy of replacing variables in existing benchmark samples differ fundamentally from other benchmark rewriting techniques like Auto-Dataset and StructEval?\n\nA: By directly rewriting stylistic elements and knowledge retention in new samples.\nB: Through prompting LLMs to rewrite samples while preserving difficulty levels but altering variable values.\nC: By expanding on examined concepts through knowledge graphs to create extended questions.\nD: Using a contamination detector to identify and remove contaminated samples before rewriting.\nExplanation: VarBench's approach focuses on replacing variables rather than rewriting stylistic elements or expanding concepts, distinguishing it from other techniques.\nCorrect Answer: B\n</output_format>\n\n<document_analysis>\nThe context provides a clear differentiation between VarBench's strategy and other benchmark rewriting techniques. VarBench specifically targets variable replacement, which is fundamentally different from methods like Auto-Dataset and StructEval. \n\n- **Option A** is incorrect because Auto-Dataset generates samples that retain stylistics and essential knowledge, not just rewriting.\n- **Option C** is incorrect as StructEval uses knowledge graphs to expand on concepts, unlike VarBench.\n- **Option D** describes ITD\u2019s method, which uses a contamination detector, whereas VarBench's focus is on variable replacement.\n\nVarBench's unique approach makes option B the correct answer, as it accurately captures the essence of its method.\n</document_analysis>",
    "question": "How does VarBench's strategy of replacing variables in existing benchmark samples differ fundamentally from other benchmark rewriting techniques like Auto-Dataset and StructEval?",
    "choices": {
      "A": "By directly rewriting stylistic elements and knowledge retention in new samples.",
      "B": "Through prompting LLMs to rewrite samples while preserving difficulty levels but altering variable values.",
      "C": "By expanding on examined concepts through knowledge graphs to create extended questions.",
      "D": "Using a contamination detector to identify and remove contaminated samples before rewriting."
    },
    "answer": "B",
    "explanation": "VarBench's approach focuses on replacing variables rather than rewriting stylistic elements or expanding concepts, distinguishing it from other techniques.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "question_token_count": 29,
    "avg_answer_token_count": 14
  },
  {
    "context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n",
    "topic": "Differentiating Performance Drops Due to Data Contamination vs. Increased Task Complexity",
    "response": "<document_analysis>\nThe provided context discusses the challenges in distinguishing between performance drops caused by data contamination versus increased task complexity in LLMs. It mentions the importance of accounting for complexity in dynamic benchmarks and highlights existing limitations in measuring complexity effectively. The key points include the notion of using complexity metrics like graph complexity for reasoning problems and the formulation of stability through variance in complexity across different trials.\n\nThe context is relevant and contains meaningful information, making it suitable for generating a challenging question. However, the information provided is more about the conceptual framework rather than specific, concrete examples or detailed applications. Therefore, the question will focus on the deeper understanding required to differentiate between the two scenarios without relying heavily on the exact wording of the text.\n</document_analysis>\n\n<output_format>\nQuestion: When evaluating the performance of an LLM on a transformed dataset, which of the following scenarios would indicate a performance drop primarily due to increased task complexity rather than data contamination?\nA: The performance drop is consistent across multiple runs with the same transformed dataset.\nB: The performance drop occurs despite the dataset being cleaned and verified for quality.\nC: The performance drop is observed in tasks that have been formally proven to be more complex through an established complexity metric.\nD: The performance drop is minimal and occurs only in edge cases of the dataset.\nExplanation: The correct answer lies in recognizing that a consistent performance drop across multiple runs suggests a stable issue rather than variability due to contamination. A drop observed in tasks proven to be more complex indicates a genuine increase in task difficulty rather than contamination issues. Minimal and occasional drops can be attributed to various factors including implementation issues or other confounding variables.\nCorrect Answer: C\n</output_format>",
    "question": "When evaluating the performance of an LLM on a transformed dataset, which of the following scenarios would indicate a performance drop primarily due to increased task complexity rather than data contamination?",
    "choices": {
      "A": "The performance drop is consistent across multiple runs with the same transformed dataset.",
      "B": "The performance drop occurs despite the dataset being cleaned and verified for quality.",
      "C": "The performance drop is observed in tasks that have been formally proven to be more complex through an established complexity metric.",
      "D": "The performance drop is minimal and occurs only in edge cases of the dataset."
    },
    "answer": "C",
    "explanation": "The correct answer lies in recognizing that a consistent performance drop across multiple runs suggests a stable issue rather than variability due to contamination. A drop observed in tasks proven to be more complex indicates a genuine increase in task difficulty rather than contamination issues. Minimal and occasional drops can be attributed to various factors including implementation issues or other confounding variables.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 6,
    "question_token_count": 34,
    "avg_answer_token_count": 17
  },
  {
    "context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "The role of encryption and post-hoc detection in reducing data contamination.",
    "response": "<document_analysis>\nThe provided context primarily discusses the challenges of data contamination in the training of Large Language Models (LLMs) and the methods to mitigate these issues. Key points include the importance of separating training and test data, the use of static and dynamic benchmarking methods, and the role of encryption and post-hoc detection in reducing data contamination. The context is rich in technical details but also includes introductory and concluding sections that do not directly contribute to the core question about encryption and post-hoc detection.\n\nThe context is meaningful and coherent, focusing on the technical aspects of benchmarking and data contamination, making it suitable for generating a challenging question.\n</document_analysis>\n\n<output_format>\nQuestion: How does encryption and post-hoc detection complement each other in mitigating data contamination in LLM benchmarking?\nA: Encryption ensures data remains confidential during transmission, while post-hoc detection helps identify contamination after training.\nB: Encryption prevents data leakage during training, whereas post-hoc detection allows for real-time monitoring of contamination.\nC: Encryption guarantees data integrity before training, and post-hoc detection provides a means to validate the results after training.\nD: Encryption secures data during the training phase, while post-hoc detection offers a way to retrospectively assess contamination in the benchmarking process.\nExplanation: The correct answer is D. Encryption is crucial for securing data during the training phase to prevent unauthorized access or data leakage. Post-hoc detection, on the other hand, is employed to retrospectively assess the benchmarking process and identify any contamination that may have occurred after the training phase. While A, B, and C touch on aspects related to data security and validation, they do not accurately capture the complementary roles of encryption and post-hoc detection in the broader context of LLM benchmarking.\nCorrect Answer: D\n</output_format>",
    "question": "How does encryption and post-hoc detection complement each other in mitigating data contamination in LLM benchmarking?",
    "choices": {
      "A": "Encryption ensures data remains confidential during transmission, while post-hoc detection helps identify contamination after training.",
      "B": "Encryption prevents data leakage during training, whereas post-hoc detection allows for real-time monitoring of contamination.",
      "C": "Encryption guarantees data integrity before training, and post-hoc detection provides a means to validate the results after training.",
      "D": "Encryption secures data during the training phase, while post-hoc detection offers a way to retrospectively assess contamination in the benchmarking process."
    },
    "answer": "D",
    "explanation": "The correct answer is D. Encryption is crucial for securing data during the training phase to prevent unauthorized access or data leakage. Post-hoc detection, on the other hand, is employed to retrospectively assess the benchmarking process and identify any contamination that may have occurred after the training phase. While A, B, and C touch on aspects related to data security and validation, they do not accurately capture the complementary roles of encryption and post-hoc detection in the broader context of LLM benchmarking.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "question_token_count": 19,
    "avg_answer_token_count": 22
  },
  {
    "context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "The necessity of transparent benchmarking in the context of LLM development.",
    "response": "<document_analysis>\nThe provided context discusses the importance of transparent benchmarking in the development of Large Language Models (LLMs). It highlights the issue of data contamination, where benchmark data might be included in the training data of LLMs, leading to an inflated assessment of their performance. The context also mentions the evolution from static to dynamic benchmarking methods and introduces criteria for evaluating dynamic benchmarks. Given this rich content, we aim to generate a highly insightful and challenging question that encourages deep engagement with the material.\n</document_analysis>\n\n<output_format>\nQuestion: What critical criterion does the paper propose for evaluating dynamic benchmarks that addresses a key limitation of existing methods in mitigating data contamination?\nA: Transparency in the benchmarking process.\nB: Continuous updating of benchmark datasets based on the timestamps of LLM training.\nC: Regeneration of benchmark data to reconstruct original benchmarks.\nD: Criteria for assessing the robustness of dynamic benchmarks against data contamination.\nExplanation: The question requires a deep understanding of the paper's proposed criteria for evaluating dynamic benchmarks. While options B and C describe methods for mitigating data contamination, they do not directly address the evaluation criteria. Option A is too broad and does not specifically relate to the evaluation aspect. Option D correctly identifies the focus of the proposed criteria, which aims to evaluate the robustness of dynamic benchmarks against data contamination.\nCorrect Answer: D\n</output_format>",
    "question": "What critical criterion does the paper propose for evaluating dynamic benchmarks that addresses a key limitation of existing methods in mitigating data contamination?",
    "choices": {
      "A": "Transparency in the benchmarking process.",
      "B": "Continuous updating of benchmark datasets based on the timestamps of LLM training.",
      "C": "Regeneration of benchmark data to reconstruct original benchmarks.",
      "D": "Criteria for assessing the robustness of dynamic benchmarks against data contamination."
    },
    "answer": "D",
    "explanation": "The question requires a deep understanding of the paper's proposed criteria for evaluating dynamic benchmarks. While options B and C describe methods for mitigating data contamination, they do not directly address the evaluation criteria. Option A is too broad and does not specifically relate to the evaluation aspect. Option D correctly identifies the focus of the proposed criteria, which aims to evaluate the robustness of dynamic benchmarks against data contamination.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 4,
    "question_difficulty_score": 7,
    "question_groundedness_score": 6,
    "question_token_count": 25,
    "avg_answer_token_count": 11
  },
  {
    "context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n",
    "topic": "Definition and Importance of Scalability in Dynamic Benchmarking",
    "response": "<document_analysis>\nThe context discusses scalability in the context of dynamic benchmarking, focusing on the trade-off between the size of the transformed dataset and the associated costs. It introduces a mathematical representation of scalability using an expectation over the transformation space, highlighting the importance of generating larger datasets while minimizing costs. The question should probe a deep understanding of this concept and its practical implications.\n</document_analysis>\n\n<output_format>\nQuestion: What does the scalability metric in dynamic benchmarking primarily measure, and how is it represented mathematically?\nA: The efficiency of transforming small datasets into larger ones while minimizing costs.\nB: The proportion of data that can be generated per unit cost, represented as E[\u2225Ti(\ud835\udc9f)\u2225/\u2225\ud835\udc9f\u2225]E, parallel, T, subscript, i, left parenthesis, \ud835\udc9f, right parenthesis, parallel, divided by, parallel, \ud835\udc9f, parallel, end parallel, start_script E end_script.\nC: The ratio of the total cost to the size of the transformed dataset.\nD: The accuracy of the benchmarking results relative to the original dataset size.\nExplanation: The scalability metric primarily measures the efficiency of transforming small datasets into larger ones while minimizing costs, which is mathematically represented as the expectation over the transformation space, E[\u2225Ti(\ud835\udc9f)\u2225/\u2225\ud835\udc9f\u2225], where \u2225Ti(\ud835\udc9f)\u2225 represents the size of the transformed dataset and \u2225\ud835\udc9f\u2225 represents the size of the original dataset.\nCorrect Answer: B\n</output_format>",
    "question": "What does the scalability metric in dynamic benchmarking primarily measure, and how is it represented mathematically?",
    "choices": {
      "A": "The efficiency of transforming small datasets into larger ones while minimizing costs.",
      "B": "The proportion of data that can be generated per unit cost, represented as E[\u2225Ti(\ud835\udc9f)\u2225/\u2225\ud835\udc9f\u2225]E, parallel, T, subscript, i, left parenthesis, \ud835\udc9f, right parenthesis, parallel, divided by, parallel, \ud835\udc9f, parallel, end parallel, start_script E end_script.",
      "C": "The ratio of the total cost to the size of the transformed dataset.",
      "D": "The accuracy of the benchmarking results relative to the original dataset size."
    },
    "answer": "B",
    "explanation": "The scalability metric primarily measures the efficiency of transforming small datasets into larger ones while minimizing costs, which is mathematically represented as the expectation over the transformation space, E[\u2225Ti(\ud835\udc9f)\u2225/\u2225\ud835\udc9f\u2225], where \u2225Ti(\ud835\udc9f)\u2225 represents the size of the transformed dataset and \u2225\ud835\udc9f\u2225 represents the size of the original dataset.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "question_token_count": 19,
    "avg_answer_token_count": 28
  },
  {
    "context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n",
    "topic": "Purpose and Methods of Constructing Reliable Benchmarks to Evaluate Language Models",
    "response": "<output_format>\nQuestion: What critical issue does LiveBench aim to address through its methodology of collecting questions based on the latest information sources?\nA: It ensures the model's responses are not contaminated by outdated data.\nB: It evaluates the model's ability to handle complex mathematical concepts.\nC: It tests the model's performance on a variety of coding problems.\nD: It assesses the model's proficiency in generating academic writing tasks.\nExplanation: LiveBench focuses on collecting questions based on the latest information to avoid contamination from outdated data. While other benchmarks may test various aspects like coding or academic writing, LiveBench specifically aims to mitigate data contamination.\nCorrect Answer: A\n</output_format>\n\n<document_analysis>\nThe context discusses various benchmarks designed to ensure that language models are evaluated using up-to-date data. LiveBench is explicitly described as collecting questions based on the latest information sources to help evaluate the model while mitigating data contamination. Therefore, the correct answer should reflect this primary objective.\n\nOptions B, C, and D describe other benchmarks' purposes but do not align with LiveBench's specific focus on avoiding data contamination from outdated sources.\n</document_analysis>",
    "question": "What critical issue does LiveBench aim to address through its methodology of collecting questions based on the latest information sources?",
    "choices": {
      "A": "It ensures the model's responses are not contaminated by outdated data.",
      "B": "It evaluates the model's ability to handle complex mathematical concepts.",
      "C": "It tests the model's performance on a variety of coding problems.",
      "D": "It assesses the model's proficiency in generating academic writing tasks."
    },
    "answer": "A",
    "explanation": "LiveBench focuses on collecting questions based on the latest information to avoid contamination from outdated data. While other benchmarks may test various aspects like coding or academic writing, LiveBench specifically aims to mitigate data contamination.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 5,
    "question_groundedness_score": 10,
    "question_token_count": 23,
    "avg_answer_token_count": 12
  },
  {
    "context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n",
    "topic": "Potential methods to counteract the effectiveness of canary strings by malicious actors attempting to leak benchmarking data.",
    "response": "<document_analysis>\nThe provided context discusses the limitations of using canary strings as a method to detect data contamination in language models. It highlights that the effectiveness of canary strings is contingent upon developers' awareness and responsiveness. The text also mentions that malicious actors might attempt to counteract the effectiveness of canary strings by intentionally including them in their outputs to avoid detection. This suggests that there are strategies malicious actors could employ to leak benchmarking data while still evading the detection mechanisms provided by canary strings.\n\nGiven this, we need to craft a question that challenges domain experts to think deeply about potential countermeasures against such sophisticated evasion tactics.\n</document_analysis>\n\n<output_format>\nQuestion: What strategy might a malicious actor employ to effectively use canary strings against their intended purpose, thereby leaking benchmarking data without detection?\nA: Incorporating canary strings into the model's own training data in a manner that mimics legitimate usage.\nB: Modifying the canary strings to become indistinguishable from other common tokens used in the model's training dataset.\nC: Using a different type of token altogether that is not recognized as a canary string.\nD: Randomizing the placement of canary strings in the output to avoid consistent patterns.\nExplanation: The correct answer is B because modifying the canary strings to be indistinguishable from other common tokens would make it difficult for model developers to identify and filter out the contaminated data, thus allowing the malicious actor to leak benchmarking data.\nCorrect Answer: B\n</output_format>",
    "question": "What strategy might a malicious actor employ to effectively use canary strings against their intended purpose, thereby leaking benchmarking data without detection?",
    "choices": {
      "A": "Incorporating canary strings into the model's own training data in a manner that mimics legitimate usage.",
      "B": "Modifying the canary strings to become indistinguishable from other common tokens used in the model's training dataset.",
      "C": "Using a different type of token altogether that is not recognized as a canary string.",
      "D": "Randomizing the placement of canary strings in the output to avoid consistent patterns."
    },
    "answer": "B",
    "explanation": "The correct answer is B because modifying the canary strings to be indistinguishable from other common tokens would make it difficult for model developers to identify and filter out the contaminated data, thus allowing the malicious actor to leak benchmarking data.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 4,
    "question_token_count": 26,
    "avg_answer_token_count": 19
  },
  {
    "context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n",
    "topic": "Examine a model's performance in answering short questions in Chinese by assessing its factuality using C-SimpleQA.",
    "response": "<document_analysis>\nThe provided context does not contain meaningful, coherent, or educationally valuable content related to the performance evaluation of models using C-SimpleQA. Instead, it discusses various coding and instruction benchmarks used in evaluating models. Therefore, no question-answer pair can be generated that meets the specified criteria based solely on this context.\n</document_analysis>\n\n<output_format>\nQuestion: Since the provided context does not discuss the evaluation of models using C-SimpleQA, what type of benchmark would be most appropriate for assessing a model's factuality in answering short questions in Chinese?\nA: Instruction Following benchmarks\nB: Reasoning benchmarks\nC: Coding benchmarks\nD: None of the above\nExplanation: The context focuses on various types of benchmarks but does not mention C-SimpleQA. A domain expert would recognize that none of the listed benchmarks directly relate to the specific task of evaluating factuality in short Chinese questions.\nCorrect Answer: D\n</output_format>",
    "question": "Since the provided context does not discuss the evaluation of models using C-SimpleQA, what type of benchmark would be most appropriate for assessing a model's factuality in answering short questions in Chinese?",
    "choices": {
      "A": "Instruction Following benchmarks",
      "B": "Reasoning benchmarks",
      "C": "Coding benchmarks",
      "D": "None of the above"
    },
    "answer": "D",
    "explanation": "The context focuses on various types of benchmarks but does not mention C-SimpleQA. A domain expert would recognize that none of the listed benchmarks directly relate to the specific task of evaluating factuality in short Chinese questions.",
    "answer_correctness_score": 1,
    "explanation_validity_score": 1,
    "question_clarity_score": 2,
    "question_difficulty_score": 5,
    "question_groundedness_score": 2,
    "question_token_count": 39,
    "avg_answer_token_count": 4
  },
  {
    "context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n",
    "topic": "Structure and Components of the Seed Dataset in Static Benchmarks",
    "response": "<document_analysis>\nThe provided context focuses on the structure and components of a seed dataset used in static benchmarks. It specifies that a static benchmark is defined by three main components: input prompts (\\(\\mathcal{X}\\)), expected outputs (\\(\\mathcal{Y}\\)), and a scoring function (\\(\\mathcal{S}(\\cdot)\\)). The seed dataset serves as the basis for evaluating the performance of language models across various tasks. The key points to consider are the nature of input prompts, the definition of expected outputs, and the role of the scoring function. Given the technical nature of the content, a highly challenging question should require a deep understanding of these components and their interplay.\n\nAfter careful consideration, the question should focus on the subtle nuances between the input prompts and expected outputs, and how the scoring function evaluates the model's performance. This will ensure that the question is challenging and requires a sophisticated understanding of the benchmarking process.\n</document_analysis>\n\n<output_format>\nQuestion: In the context of a seed dataset for a static benchmark, how does the scoring function \\(\\mathcal{S}(\\cdot)\\) primarily differ in its evaluation criteria compared to the characteristics of the expected outputs \\(\\mathcal{Y}\\)?\n\nA: The scoring function assesses the semantic similarity between the model's outputs and the expected outputs, while the expected outputs specify the exact solutions or responses required.\n\nB: The scoring function measures the syntactic correctness of the model's outputs, whereas the expected outputs provide the actual data used to train the models.\n\nC: The scoring function evaluates the diversity of the model's outputs, while the expected outputs define the set of acceptable responses.\n\nD: The scoring function calculates the computational efficiency of the model's outputs, whereas the expected outputs provide the theoretical limits for performance.\n\nExplanation: The scoring function \\(\\mathcal{S}(\\cdot)\\) typically measures how well the model's outputs align with the expected outputs in terms of correctness and relevance. While the expected outputs specify the desired outcomes, the scoring function quantitatively evaluates this alignment.\n\nCorrect Answer: A\n</output_format>",
    "question": "In the context of a seed dataset for a static benchmark, how does the scoring function \\(\\mathcal{S}(\\cdot)\\) primarily differ in its evaluation criteria compared to the characteristics of the expected outputs \\(\\mathcal{Y}\\)?",
    "choices": {
      "A": "The scoring function assesses the semantic similarity between the model's outputs and the expected outputs, while the expected outputs specify the exact solutions or responses required.",
      "B": "The scoring function measures the syntactic correctness of the model's outputs, whereas the expected outputs provide the actual data used to train the models.",
      "C": "The scoring function evaluates the diversity of the model's outputs, while the expected outputs define the set of acceptable responses.",
      "D": "The scoring function calculates the computational efficiency of the model's outputs, whereas the expected outputs provide the theoretical limits for performance."
    },
    "answer": "A",
    "explanation": "The scoring function \\(\\mathcal{S}(\\cdot)\\) typically measures how well the model's outputs align with the expected outputs in terms of correctness and relevance. While the expected outputs specify the desired outcomes, the scoring function quantitatively evaluates this alignment.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "question_token_count": 43,
    "avg_answer_token_count": 25
  },
  {
    "context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n",
    "topic": "Fairness, accountability, and privacy considerations in the design of benchmarking frameworks.",
    "response": "<output_format>\nQuestion: How can a benchmarking framework be designed to ensure fairness, accountability, and privacy while mitigating the risk of bias and misuse?\n\nA: By using exclusively static benchmarks to avoid privacy concerns.\nB: Through the incorporation of diverse and regularly updated datasets, transparent methodologies, and robust governance structures.\nC: Implementing strict access controls on all benchmarking data and prohibiting public sharing of results.\nD: Limiting the scope of benchmarking to closed, private domains where data can be freely collected and shared without oversight.\n\nExplanation: The correct answer requires a deep understanding of the multifaceted issues involved in benchmarking frameworks and the need for a balanced approach. Option B addresses the key aspects of fairness, accountability, and privacy by ensuring the use of diverse and up-to-date datasets, transparent methodologies, and robust governance, which are essential for mitigating risks of bias and misuse. Options A, C, and D are either overly restrictive or fail to address multiple dimensions of the problem.\n\nCorrect Answer: B\n</output_format>",
    "question": "How can a benchmarking framework be designed to ensure fairness, accountability, and privacy while mitigating the risk of bias and misuse?",
    "choices": {
      "A": "By using exclusively static benchmarks to avoid privacy concerns.",
      "B": "Through the incorporation of diverse and regularly updated datasets, transparent methodologies, and robust governance structures.",
      "C": "Implementing strict access controls on all benchmarking data and prohibiting public sharing of results.",
      "D": "Limiting the scope of benchmarking to closed, private domains where data can be freely collected and shared without oversight."
    },
    "answer": "B",
    "explanation": "The correct answer requires a deep understanding of the multifaceted issues involved in benchmarking frameworks and the need for a balanced approach. Option B addresses the key aspects of fairness, accountability, and privacy by ensuring the use of diverse and up-to-date datasets, transparent methodologies, and robust governance, which are essential for mitigating risks of bias and misuse. Options A, C, and D are either overly restrictive or fail to address multiple dimensions of the problem.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 9,
    "question_token_count": 25,
    "avg_answer_token_count": 17
  },
  {
    "context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n",
    "topic": "Importance of addressing contamination for assessing generalization and robustness in LLMs.",
    "response": "<document_analysis>\nThe context emphasizes the importance of addressing contamination in evaluating LLMs' generalization and robustness. It discusses syntactic contamination as a form of data contamination and highlights the significance of understanding and mitigating contamination to ensure valid benchmarks. The key points include the challenge of distinguishing between recall and reasoning, the impact of contaminated benchmarks on model comparisons and real-world applicability, and the necessity for robust methods to identify and prevent contamination. The question should probe the deep understanding of contamination's role in benchmarking and its implications for LLM evaluation.\n</document_analysis>\n\n<output_format>\nQuestion: What is the primary reason for considering syntactic transformations as a form of contamination in LLM benchmarking, despite the debate surrounding their nature?\nA: To enhance the model's syntactic recall capabilities.\nB: To ensure the model's reasoning capability is adequately tested.\nC: Because syntactic transformations are easier to implement than semantic changes.\nD: To align the test data more closely with the training data.\nExplanation: The correct answer is B because the context states that syntactic transformations are considered contamination primarily to test the model's reasoning capability, not just its recall of memorized information. This distinction is crucial for evaluating the model's true reasoning abilities rather than its capacity to reproduce learned patterns.\nCorrect Answer: B\n</output_format>",
    "question": "What is the primary reason for considering syntactic transformations as a form of contamination in LLM benchmarking, despite the debate surrounding their nature?",
    "choices": {
      "A": "To enhance the model's syntactic recall capabilities.",
      "B": "To ensure the model's reasoning capability is adequately tested.",
      "C": "Because syntactic transformations are easier to implement than semantic changes.",
      "D": "To align the test data more closely with the training data."
    },
    "answer": "B",
    "explanation": "The correct answer is B because the context states that syntactic transformations are considered contamination primarily to test the model's reasoning capability, not just its recall of memorized information. This distinction is crucial for evaluating the model's true reasoning abilities rather than its capacity to reproduce learned patterns.",
    "answer_correctness_score": 8,
    "explanation_validity_score": 8,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "question_token_count": 26,
    "avg_answer_token_count": 11
  },
  {
    "context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n",
    "topic": "Assessing a model\u2019s capacity to handle diverse and intricate math tasks, as seen in recent challenges like AIME 2024 and CNMO 2024.",
    "response": "<document_analysis>\nThe provided context discusses various benchmarks and datasets used to assess a model's ability to handle complex and diverse math tasks, particularly in the context of recent challenges like AIME 2024 and CNMO 2024. The context is relevant and provides specific examples of benchmarks and challenges, which can be used to formulate a challenging and insightful question.\n</document_analysis>\n\n<output_format>\nQuestion: In the context of evaluating a model's capacity to handle diverse and intricate math tasks as seen in recent challenges like AIME 2024 and CNMO 2024, which of the following statements best reflects the nature of the challenge posed by these competitions?\n\nA: These challenges primarily test a model's ability to recall and apply basic mathematical formulas.\nB: These challenges require a model to solve multi-step problems that involve creative thinking and understanding of advanced mathematical concepts.\nC: These challenges focus on testing a model's proficiency in arithmetic operations and simple problem-solving techniques.\nD: These challenges are designed to assess a model's speed in performing calculations rather than its depth of mathematical understanding.\n\nExplanation: The recent challenges like AIME 2024 and CNMO 2024 are known for their complexity and require models to work through multi-step, intricate math problems that often demand creative thinking and understanding of advanced mathematical concepts. Therefore, option B accurately captures the nature of these challenges.\n\nCorrect Answer: B\n</output_format>",
    "question": "In the context of evaluating a model's capacity to handle diverse and intricate math tasks as seen in recent challenges like AIME 2024 and CNMO 2024, which of the following statements best reflects the nature of the challenge posed by these competitions?",
    "choices": {
      "A": "These challenges primarily test a model's ability to recall and apply basic mathematical formulas.",
      "B": "These challenges require a model to solve multi-step problems that involve creative thinking and understanding of advanced mathematical concepts.",
      "C": "These challenges focus on testing a model's proficiency in arithmetic operations and simple problem-solving techniques.",
      "D": "These challenges are designed to assess a model's speed in performing calculations rather than its depth of mathematical understanding."
    },
    "answer": "B",
    "explanation": "The recent challenges like AIME 2024 and CNMO 2024 are known for their complexity and require models to work through multi-step, intricate math problems that often demand creative thinking and understanding of advanced mathematical concepts. Therefore, option B accurately captures the nature of these challenges.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 1,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "question_token_count": 51,
    "avg_answer_token_count": 19
  },
  {
    "context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "The need for dynamic benchmarks to mitigate issues associated with static benchmarks.",
    "response": "<document_analysis>\nThe context discusses the importance of developing comprehensive benchmarks for evaluating LLMs and highlights the limitations of static benchmarks as LLMs improve over time. It mentions contamination detectors and dynamic benchmarks as solutions to these issues. The focus is on the need for dynamic benchmarks to adapt to the evolving capabilities of LLMs and address potential data contamination problems.\n\nThe context provides a balanced overview of the topic but lacks specific technical details about dynamic benchmarks, making it suitable for generating a challenging question that requires a deep understanding of the underlying concepts.\n</document_analysis>\n\n<output_format>\nQuestion: What is the primary reason for transitioning from static benchmarks to dynamic benchmarks in the evaluation of LLMs?\nA: To ensure consistent performance metrics across different models.\nB: To adapt to the rapid evolution of LLMs and reduce the risk of data contamination.\nC: To simplify the benchmarking process by standardizing evaluation methods.\nD: To reduce the computational resources required for benchmarking.\nExplanation: Dynamic benchmarks are designed to adapt to the changing capabilities of LLMs and mitigate the risk of data contamination, which is a significant issue with static benchmarks as LLMs evolve rapidly and continue to train on new data.\nCorrect Answer: B\n</output_format>",
    "question": "What is the primary reason for transitioning from static benchmarks to dynamic benchmarks in the evaluation of LLMs?",
    "choices": {
      "A": "To ensure consistent performance metrics across different models.",
      "B": "To adapt to the rapid evolution of LLMs and reduce the risk of data contamination.",
      "C": "To simplify the benchmarking process by standardizing evaluation methods.",
      "D": "To reduce the computational resources required for benchmarking."
    },
    "answer": "B",
    "explanation": "Dynamic benchmarks are designed to adapt to the changing capabilities of LLMs and mitigate the risk of data contamination, which is a significant issue with static benchmarks as LLMs evolve rapidly and continue to train on new data.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 9,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "question_token_count": 21,
    "avg_answer_token_count": 12
  },
  {
    "context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n",
    "topic": "Human Effort Required in Data Collection and Benchmark Updates",
    "response": "<document_analysis>\nThe context discusses the limitations of human effort required in data collection and benchmark updates, along with different methods of generating test cases for evaluating language models (LLMs). It highlights the challenges of maintaining up-to-date benchmarks and the importance of verification processes. The context does not provide any specific data points or examples that are crucial for generating a highly insightful and challenging question. Therefore, it is important to focus on the broader themes and implications of human effort in benchmarking and the potential for data contamination.\n</document_analysis>\n\n<output_format>\nQuestion: How does the reliance on human effort in data collection and benchmark updates impact the long-term reliability and validity of language model evaluations?\nA: It enhances the accuracy of evaluations through meticulous human oversight.\nB: It significantly increases the risk of data contamination and outdated information.\nC: It reduces the need for verification processes due to automated rule-based generation.\nD: It diminishes the importance of using recent competition data for evaluation.\nCorrect Answer: B\nExplanation: The context emphasizes the continuous human involvement required for data collection and benchmark updates, highlighting the risk of data contamination and the need for verification. These factors suggest that relying heavily on human effort can introduce inaccuracies over time, making option B the most accurate response.\n</output_format>",
    "question": "How does the reliance on human effort in data collection and benchmark updates impact the long-term reliability and validity of language model evaluations?",
    "choices": {
      "A": "It enhances the accuracy of evaluations through meticulous human oversight.",
      "B": "It significantly increases the risk of data contamination and outdated information.",
      "C": "It reduces the need for verification processes due to automated rule-based generation.",
      "D": "It diminishes the importance of using recent competition data for evaluation."
    },
    "answer": "B",
    "explanation": "The context emphasizes the continuous human involvement required for data collection and benchmark updates, highlighting the risk of data contamination and the need for verification. These factors suggest that relying heavily on human effort can introduce inaccuracies over time, making option B the most accurate response.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 6,
    "question_token_count": 26,
    "avg_answer_token_count": 13
  },
  {
    "context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "The importance of maintaining the separation between training and testing data in LLM evaluation.",
    "response": "<document_analysis>\nThe provided context discusses the importance of maintaining the separation between training and testing data in LLM evaluation. It emphasizes the critical issue of data contamination and the necessity of separating training and testing data to accurately assess model performance. The context does not contain any hyperlinks, advertisements, headers, footers, or irrelevant content, so all the information is relevant and meaningful for generating a question.\n\nGiven the complexity of the topic, a highly challenging question would require a deep understanding of the nuances involved in LLM evaluation and the potential consequences of failing to maintain proper data separation.\n</document_analysis>\n\n<output_format>\nQuestion: Why is the separation between training and testing data critical in the evaluation of Large Language Models (LLMs), and what are the potential risks of violating this separation?\nA: To prevent overfitting and ensure the model's generalizability, thereby providing a true assessment of its performance.\nB: To avoid data contamination, which can lead to inflated and misleading performance metrics.\nC: To comply with ethical standards in AI research and development.\nD: To simplify the model training process and reduce computational costs.\nExplanation: The correct answer is B, as data contamination can lead to models performing well on seen data but poorly on unseen data, providing an inaccurate representation of their true capabilities.\nCorrect Answer: B\n</output_format>",
    "question": "Why is the separation between training and testing data critical in the evaluation of Large Language Models (LLMs), and what are the potential risks of violating this separation?",
    "choices": {
      "A": "To prevent overfitting and ensure the model's generalizability, thereby providing a true assessment of its performance.",
      "B": "To avoid data contamination, which can lead to inflated and misleading performance metrics.",
      "C": "To comply with ethical standards in AI research and development.",
      "D": "To simplify the model training process and reduce computational costs."
    },
    "answer": "B",
    "explanation": "The correct answer is B, as data contamination can lead to models performing well on seen data but poorly on unseen data, providing an inaccurate representation of their true capabilities.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "question_token_count": 33,
    "avg_answer_token_count": 15
  },
  {
    "context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n",
    "topic": "The need for standardized criteria in evaluating both static and dynamic benchmarks for LLMs.",
    "response": "<document_analysis>\nThe context discusses the challenges and limitations of evaluating Large Language Models (LLMs) using both static and dynamic benchmarks. It highlights the importance of standardized criteria for evaluating dynamic benchmarks, emphasizing the need for transparent and scalable methods. The context does not contain irrelevant or promotional information. Therefore, we can proceed with generating a question that challenges domain experts to think deeply about the implications of standardized criteria for dynamic benchmarks.\n</document_analysis>\n\n<output_format>\nQuestion: What is a key challenge in the current development of dynamic benchmarks for LLMs, and how does the lack of standardized criteria exacerbate this issue?\nA: Ensuring transparency and avoiding high assumptions about contaminated models.\nB: Balancing the correctness of benchmarks with their scalability.\nC: Controlling complexity in dynamic benchmarks to ensure efficient evaluation.\nD: Addressing data contamination issues inherent in static benchmarks.\nExplanation: The correct answer is B. The context mentions that while dynamic benchmarks address challenges like transparency and high assumptions about contaminated models, they introduce new issues such as balancing correctness with scalability. Standardized criteria for dynamic benchmarks are crucial to tackle this challenge effectively.\nCorrect Answer: B\n</output_format>",
    "question": "What is a key challenge in the current development of dynamic benchmarks for LLMs, and how does the lack of standardized criteria exacerbate this issue?",
    "choices": {
      "A": "Ensuring transparency and avoiding high assumptions about contaminated models.",
      "B": "Balancing the correctness of benchmarks with their scalability.",
      "C": "Controlling complexity in dynamic benchmarks to ensure efficient evaluation.",
      "D": "Addressing data contamination issues inherent in static benchmarks."
    },
    "answer": "B",
    "explanation": "The correct answer is B. The context mentions that while dynamic benchmarks address challenges like transparency and high assumptions about contaminated models, they introduce new issues such as balancing correctness with scalability. Standardized criteria for dynamic benchmarks are crucial to tackle this challenge effectively.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 8,
    "question_token_count": 30,
    "avg_answer_token_count": 11
  },
  {
    "context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n",
    "topic": "Proposed Optimal Design Principles for Dynamic Benchmarking",
    "response": "<document_analysis>\nThe context focuses on the transition from static to dynamic benchmarking methods for large language models (LLMs) to address data contamination issues. It highlights the limitations of current static benchmarks and the gaps in evaluating dynamic benchmarks. The key points include:\n- Enhanced static benchmarks have inherent limitations.\n- There is a critical gap in standardized evaluation criteria for dynamic benchmarks.\n- Proposed optimal design principles for dynamic benchmarking are essential for future research.\n\nGiven the emphasis on design principles and the gap in evaluation criteria, a challenging question should probe the expert's understanding of these principles and their application.\n</document_analysis>\n\n<output_format>\nQuestion: According to the proposed optimal design principles for dynamic benchmarking, which of the following is NOT a key aspect that should be considered to ensure robustness against data contamination?\nA: Incorporating real-time feedback mechanisms to adapt benchmarks dynamically.\nB: Ensuring the benchmarking process is transparent and reproducible.\nC: Using fixed datasets to maintain consistency across different benchmarking periods.\nD: Developing standardized metrics to evaluate the performance of LLMs under varying conditions.\n\nExplanation: The correct answer requires a deep understanding of the proposed design principles. While incorporating real-time feedback mechanisms, ensuring transparency, and developing standardized metrics are crucial, using fixed datasets contradicts the principle of dynamic adaptation.\nCorrect Answer: C\n</output_format>",
    "question": "According to the proposed optimal design principles for dynamic benchmarking, which of the following is NOT a key aspect that should be considered to ensure robustness against data contamination?",
    "choices": {
      "A": "Incorporating real-time feedback mechanisms to adapt benchmarks dynamically.",
      "B": "Ensuring the benchmarking process is transparent and reproducible.",
      "C": "Using fixed datasets to maintain consistency across different benchmarking periods.",
      "D": "Developing standardized metrics to evaluate the performance of LLMs under varying conditions."
    },
    "answer": "C",
    "explanation": "The correct answer requires a deep understanding of the proposed design principles. While incorporating real-time feedback mechanisms, ensuring transparency, and developing standardized metrics are crucial, using fixed datasets contradicts the principle of dynamic adaptation.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 1,
    "question_difficulty_score": 8,
    "question_groundedness_score": 2,
    "question_token_count": 32,
    "avg_answer_token_count": 12
  },
  {
    "context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n",
    "topic": "Limitations of Existing Static Benchmarking Methods",
    "response": "<document_analysis>\nThe provided context discusses the shift from static to dynamic benchmarking methods for Large Language Models (LLMs), focusing on the limitations of existing approaches. It highlights the importance of enhancing static benchmarks and identifies gaps in the evaluation of dynamic benchmarks. The key points include:\n- Data contamination is a growing concern due to LLMs' reliance on Internet-derived training corpora.\n- There is a need for standardized criteria to evaluate dynamic benchmarks.\n- Current methods have inherent limitations that are not fully addressed.\n\nGiven these points, the challenge is to create a question that probes a deep understanding of the limitations of static benchmarking methods while encouraging critical thinking about the proposed solutions.\n</document_analysis>\n\n<output_format>\nQuestion: What is a critical limitation of existing static benchmarking methods in the context of mitigating data contamination risks for Large Language Models (LLMs)?\nA: They do not adequately account for the diverse sources of data contamination.\nB: They fail to provide a standardized framework for evaluating the effectiveness of mitigation strategies.\nC: They are too complex to implement and interpret.\nD: They do not effectively capture the nuances of real-world data usage scenarios.\nExplanation: The correct answer is B. Existing static benchmarking methods often lack standardized criteria for evaluating the success of data contamination mitigation strategies, which makes it difficult to compare and improve upon different methods.\nCorrect Answer: B\n</output_format>",
    "question": "What is a critical limitation of existing static benchmarking methods in the context of mitigating data contamination risks for Large Language Models (LLMs)?",
    "choices": {
      "A": "They do not adequately account for the diverse sources of data contamination.",
      "B": "They fail to provide a standardized framework for evaluating the effectiveness of mitigation strategies.",
      "C": "They are too complex to implement and interpret.",
      "D": "They do not effectively capture the nuances of real-world data usage scenarios."
    },
    "answer": "B",
    "explanation": "The correct answer is B. Existing static benchmarking methods often lack standardized criteria for evaluating the success of data contamination mitigation strategies, which makes it difficult to compare and improve upon different methods.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 6,
    "question_token_count": 27,
    "avg_answer_token_count": 13
  },
  {
    "context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n",
    "topic": "Methods to mitigate data contamination in LLM benchmarks, including temporal cutoff, LLM-based, and graph-based generation.",
    "response": "<output_format>\nQuestion: How does C2LEVA ensure the contamination-free construction of bilingual evaluation datasets?\nA: By exclusively using temporal cutoff to filter out contaminated samples.\nB: Through a hybrid approach combining temporal cutoff, LLM-based, and graph-based generation techniques.\nC: Utilizing only LLM-based generation to ensure sample faithfulness.\nD: Applying graph-based perturbation exclusively to extract reasoning graphs from benchmarks.\nExplanation: C2LEVA incorporates all three methods\u2014temporal cutoff, LLM-based generation, and graph-based generation\u2014to construct contamination-free bilingual evaluation datasets. This comprehensive approach ensures a thorough mitigation of contamination risks.\nCorrect Answer: B\n</output_format>\n\n<document_analysis>\nThe context discusses various methods to mitigate data contamination in LLM benchmarks. Specifically, C2LEVA is mentioned as a method that \"incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\" These methods include temporal cutoff, LLM-based generation, and graph-based generation. The other options either focus on a single method or combine only two of the three methods mentioned, making option B the most accurate representation of C2LEVA's approach.\n</document_analysis>",
    "question": "How does C2LEVA ensure the contamination-free construction of bilingual evaluation datasets?",
    "choices": {
      "A": "By exclusively using temporal cutoff to filter out contaminated samples.",
      "B": "Through a hybrid approach combining temporal cutoff, LLM-based, and graph-based generation techniques.",
      "C": "Utilizing only LLM-based generation to ensure sample faithfulness.",
      "D": "Applying graph-based perturbation exclusively to extract reasoning graphs from benchmarks."
    },
    "answer": "B",
    "explanation": "C2LEVA incorporates all three methods\u2014temporal cutoff, LLM-based generation, and graph-based generation\u2014to construct contamination-free bilingual evaluation datasets. This comprehensive approach ensures a thorough mitigation of contamination risks.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "question_token_count": 17,
    "avg_answer_token_count": 14
  },
  {
    "context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n",
    "topic": "The necessity and challenges of developing fair and transparent benchmarks for LLMs.",
    "response": "<output_format>\nQuestion: What is the primary reason for the increased difficulty in developing fair and transparent benchmarks for LLMs despite the existence of various detection methods?\n\nA: The proprietary nature of training data makes it hard to assess performance accurately.\nB: The scale and complexity of the training corpora increase the risk of data contamination.\nC: The reliance on human-annotated and synthetic datasets for fine-tuning introduces new biases.\nD: The use of web-scraped data in pre-training phases ensures a diverse dataset.\n\nExplanation: The difficulty arises primarily from the scale and complexity of the training corpora, which makes it challenging to entirely exclude evaluation data, even with existing detection methods. This challenge is exacerbated by the proprietary nature of training data, but the core issue lies in the sheer size and diversity of the training datasets.\n\nCorrect Answer: B\n</output_format>\n\n<document_analysis>\nThe context emphasizes the scale and complexity of the training corpora as the primary challenge in developing fair and transparent benchmarks for LLMs. While the proprietary nature of training data, the use of human-annotated and synthetic datasets, and web-scraped data contribute to the problem, they are secondary factors. The question is designed to probe a deep understanding of the core issue, requiring the respondent to recognize the fundamental difficulty posed by the vast and varied training datasets.\n</document_analysis>",
    "question": "What is the primary reason for the increased difficulty in developing fair and transparent benchmarks for LLMs despite the existence of various detection methods?",
    "choices": {
      "A": "The proprietary nature of training data makes it hard to assess performance accurately.",
      "B": "The scale and complexity of the training corpora increase the risk of data contamination.",
      "C": "The reliance on human-annotated and synthetic datasets for fine-tuning introduces new biases.",
      "D": "The use of web-scraped data in pre-training phases ensures a diverse dataset."
    },
    "answer": "B",
    "explanation": "The difficulty arises primarily from the scale and complexity of the training corpora, which makes it challenging to entirely exclude evaluation data, even with existing detection methods. This challenge is exacerbated by the proprietary nature of training data, but the core issue lies in the sheer size and diversity of the training datasets.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 9,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "question_token_count": 27,
    "avg_answer_token_count": 17
  },
  {
    "context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n",
    "topic": "Challenges and Limitations of Current Dynamic Benchmarking Methods",
    "response": "<output_format>\nQuestion: What is the primary gap identified in the current dynamic benchmarking methods that the authors aim to address through their proposed design principles?\n\nA: The lack of standardized criteria for evaluating the effectiveness of dynamic benchmarks.\nB: The reliance on static benchmarks that cannot adapt to changing data conditions.\nC: The insufficient data contamination mitigation strategies in dynamic benchmarks.\nD: The inability to fully automate the dynamic benchmarking process.\n\nExplanation: The question probes the authors' identification of the main gap in current dynamic benchmarking methods, requiring a deep understanding of the context provided. It challenges the respondent to distinguish between various issues discussed and focus on the most critical one highlighted in the paper.\n\nCorrect Answer: A\n</output_format>\n\n<document_analysis>\nThe context emphasizes the shift from static to dynamic benchmarking methods aimed at addressing data contamination risks in Large Language Models (LLMs). The authors identify a critical gap: the lack of standardized criteria for evaluating dynamic benchmarks. They propose a series of design principles to address this gap and analyze existing limitations. Options B, C, and D, while related to benchmarking issues, are not the primary gap identified by the authors. Therefore, option A is the correct choice.\n</document_analysis>",
    "question": "What is the primary gap identified in the current dynamic benchmarking methods that the authors aim to address through their proposed design principles?",
    "choices": {
      "A": "The lack of standardized criteria for evaluating the effectiveness of dynamic benchmarks.",
      "B": "The reliance on static benchmarks that cannot adapt to changing data conditions.",
      "C": "The insufficient data contamination mitigation strategies in dynamic benchmarks.",
      "D": "The inability to fully automate the dynamic benchmarking process."
    },
    "answer": "A",
    "explanation": "The question probes the authors' identification of the main gap in current dynamic benchmarking methods, requiring a deep understanding of the context provided. It challenges the respondent to distinguish between various issues discussed and focus on the most critical one highlighted in the paper.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 8,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "question_token_count": 25,
    "avg_answer_token_count": 12
  },
  {
    "context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n",
    "topic": "Advantages and Collision Probability of Rule-Based Generation Method",
    "response": "<document_analysis>\nThe context primarily discusses the rule-based generation method, highlighting its advantage of having a low collision probability. However, it also mentions limitations such as the need for human effort for updates and potential data contamination issues. The context does not provide extensive detail on the exact nature of the rule-based generation method or how it avoids collisions. Therefore, we can focus on the key aspect of the low collision probability and challenge the domain expert's understanding of this concept.\n</document_analysis>\n\n<output_format>\nQuestion: How does the rule-based generation method ensure a low collision probability in producing test cases, and what theoretical underpinning might support this claim?\nA: By using a deterministic rule set that minimizes the chance of generating identical test cases through strict adherence to predefined structures.\nB: Through the implementation of randomization techniques that introduce variability while maintaining unique characteristics of each test case.\nC: By employing sophisticated machine learning algorithms to predict and avoid generating similar test cases.\nD: It relies on manual verification and review processes to eliminate duplicate cases after generation.\nExplanation: The correct answer is A. The rule-based generation method ensures low collision probability by strictly following a deterministic rule set, thereby minimizing the chance of generating identical test cases. This is supported by the theoretical underpinning of formal language theory and combinatorial mathematics, which ensure that each test case is uniquely identifiable based on the defined rules.\nCorrect Answer: A\n</output_format>",
    "question": "How does the rule-based generation method ensure a low collision probability in producing test cases, and what theoretical underpinning might support this claim?",
    "choices": {
      "A": "By using a deterministic rule set that minimizes the chance of generating identical test cases through strict adherence to predefined structures.",
      "B": "Through the implementation of randomization techniques that introduce variability while maintaining unique characteristics of each test case.",
      "C": "By employing sophisticated machine learning algorithms to predict and avoid generating similar test cases.",
      "D": "It relies on manual verification and review processes to eliminate duplicate cases after generation."
    },
    "answer": "A",
    "explanation": "The correct answer is A. The rule-based generation method ensures low collision probability by strictly following a deterministic rule set, thereby minimizing the chance of generating identical test cases. This is supported by the theoretical underpinning of formal language theory and combinatorial mathematics, which ensure that each test case is uniquely identifiable based on the defined rules.",
    "answer_correctness_score": 9,
    "explanation_validity_score": 8,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 4,
    "question_token_count": 29,
    "avg_answer_token_count": 18
  },
  {
    "context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n",
    "topic": "Analyze the implications of a low correctness score in the context of evaluating LLMs and discuss strategies to improve the correctness of dynamic benchmarks.",
    "response": "<output_format>\nQuestion: How might a low correctness score in a dynamic benchmark impact the evaluation of LLMs, and what strategies can be employed to enhance the accuracy of such benchmarks?\n\nA: It would indicate unreliable benchmark results, potentially misleading the evaluation of LLM performance, and strategies to improve could include enhancing the oracle function's reliability and rigorously validating the ground truth annotations.\n\nB: It would suggest overfitting to training data, and strategies might involve increasing the size of the training dataset and fine-tuning the models extensively.\n\nC: It would highlight the need for more sophisticated scoring functions, suggesting the development of complex algorithms to better align model outputs with ground truths.\n\nD: It would reveal the benchmarks' sensitivity to noise, recommending the implementation of robust error handling mechanisms and regularization techniques.\n\nExplanation: A low correctness score in a dynamic benchmark suggests that the generated datasets may not accurately reflect real-world scenarios, leading to potentially misleading evaluations of LLMs. Enhancing the reliability of the oracle function and rigorously validating ground truth annotations are effective strategies to improve the correctness of dynamic benchmarks. Options B, C, and D address different aspects of model evaluation but do not directly relate to the issue of correctness in dynamic benchmarks as described.\n\nCorrect Answer: A\n</output_format>\n\n<document_analysis>\nThe context discusses the importance of correctness in dynamic benchmarks for evaluating LLMs. It explains that a low correctness score can lead to misleading evaluations, and proposes a formula to quantify correctness. The formula involves comparing transformed dataset outputs to their ground truth values through an oracle function. The question is designed to probe the implications of a low correctness score and strategies to improve it, encouraging a deep understanding of the concept and its application.\n</document_analysis>",
    "question": "How might a low correctness score in a dynamic benchmark impact the evaluation of LLMs, and what strategies can be employed to enhance the accuracy of such benchmarks?",
    "choices": {
      "A": "It would indicate unreliable benchmark results, potentially misleading the evaluation of LLM performance, and strategies to improve could include enhancing the oracle function's reliability and rigorously validating the ground truth annotations.",
      "B": "It would suggest overfitting to training data, and strategies might involve increasing the size of the training dataset and fine-tuning the models extensively.",
      "C": "It would highlight the need for more sophisticated scoring functions, suggesting the development of complex algorithms to better align model outputs with ground truths.",
      "D": "It would reveal the benchmarks' sensitivity to noise, recommending the implementation of robust error handling mechanisms and regularization techniques."
    },
    "answer": "A",
    "explanation": "A low correctness score in a dynamic benchmark suggests that the generated datasets may not accurately reflect real-world scenarios, leading to potentially misleading evaluations of LLMs. Enhancing the reliability of the oracle function and rigorously validating ground truth annotations are effective strategies to improve the correctness of dynamic benchmarks. Options B, C, and D address different aspects of model evaluation but do not directly relate to the issue of correctness in dynamic benchmarks as described.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 6,
    "question_token_count": 32,
    "avg_answer_token_count": 28
  },
  {
    "context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n",
    "topic": "Components of the Scalability Formula",
    "response": "<document_analysis>\nThe provided context focuses on the mathematical representation of scalability in dynamic benchmarking methods. It introduces the formula that quantifies scalability, involving the size of the transformed dataset and the original dataset, as well as a cost function. The formula itself and its interpretation are key elements, but there is no explicit mention of the actual components or parameters of the formula. Therefore, the context is relevant and contains meaningful information for a domain expert.\n</document_analysis>\n\n<output_format>\nQuestion: What does the term \\(\\parallel T_i(\\mathcal{D}) \\parallel\\) represent in the scalability formula, and how does it contribute to the overall scalability measure?\n\nA: The size of the original dataset.\nB: The size of the transformed dataset.\nC: The cost associated with the transformation process.\nD: The ratio of the original dataset size to the transformed dataset size.\n\nExplanation: \\(\\parallel T_i(\\mathcal{D}) \\parallel\\) represents the size of the transformed dataset, which is a crucial component in the scalability formula as it directly affects the overall measure of scalability. The formula balances the size of the transformed dataset with the cost of the transformation process to ensure an optimal dynamic benchmark.\nCorrect Answer: B\n</output_format>",
    "question": "What does the term \\(\\parallel T_i(\\mathcal{D}) \\parallel\\) represent in the scalability formula, and how does it contribute to the overall scalability measure?",
    "choices": {
      "A": "The size of the original dataset.",
      "B": "The size of the transformed dataset.",
      "C": "The cost associated with the transformation process.",
      "D": "The ratio of the original dataset size to the transformed dataset size."
    },
    "answer": "B",
    "explanation": "\\(\\parallel T_i(\\mathcal{D}) \\parallel\\) represents the size of the transformed dataset, which is a crucial component in the scalability formula as it directly affects the overall measure of scalability. The formula balances the size of the transformed dataset with the cost of the transformation process to ensure an optimal dynamic benchmark.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 5,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "question_token_count": 33,
    "avg_answer_token_count": 9
  },
  {
    "context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Criteria for evaluating the effectiveness of dynamic benchmarks in LLM evaluation.",
    "response": "<document_analysis>\nThe provided context discusses the challenges of evaluating large language models (LLMs) and introduces the concept of dynamic benchmarks as a solution to mitigate the risk of data contamination. The context covers the background of data contamination, the limitations of static benchmarks, and the introduction of dynamic benchmarks. It also mentions the need for criteria to evaluate dynamic benchmarks.\n\nThe context is rich with technical details and provides a thorough discussion of the issues and solutions in LLM evaluation. Given this, the question generated should challenge the domain expert's ability to deeply understand and reflect on the nuances of dynamic benchmarking and its evaluation criteria.\n</document_analysis>\n\n<output_format>\nQuestion: According to the context, what key criterion must a dynamic benchmark satisfy to ensure robust evaluation of LLMs while minimizing the risk of data contamination?\nA: It must update benchmark datasets based on the timestamps of LLM training.\nB: It must regenerate benchmark data to reconstruct original benchmarks.\nC: It must be transparent and traceable to the exact training data of the LLM.\nD: It must incorporate a comprehensive set of evaluative criteria that consider the dynamic nature of LLM training data.\nExplanation: The correct answer is D, as the context emphasizes the need for a set of criteria to evaluate dynamic benchmarks comprehensively. The other options represent specific methods mentioned in the text but do not encapsulate the broader requirement for robust evaluation criteria.\nCorrect Answer: D\n</output_format>",
    "question": "According to the context, what key criterion must a dynamic benchmark satisfy to ensure robust evaluation of LLMs while minimizing the risk of data contamination?",
    "choices": {
      "A": "It must update benchmark datasets based on the timestamps of LLM training.",
      "B": "It must regenerate benchmark data to reconstruct original benchmarks.",
      "C": "It must be transparent and traceable to the exact training data of the LLM.",
      "D": "It must incorporate a comprehensive set of evaluative criteria that consider the dynamic nature of LLM training data."
    },
    "answer": "D",
    "explanation": "The correct answer is D, as the context emphasizes the need for a set of criteria to evaluate dynamic benchmarks comprehensively. The other options represent specific methods mentioned in the text but do not encapsulate the broader requirement for robust evaluation criteria.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 4,
    "question_difficulty_score": 7,
    "question_groundedness_score": 8,
    "question_token_count": 29,
    "avg_answer_token_count": 15
  },
  {
    "context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n",
    "topic": "Understanding the structure and objectives of knowledge benchmarks, including datasets such as NaturalQuestions, TriviaQA, and MMLU.",
    "response": "<document_analysis>\nThe provided context focuses primarily on math benchmarks and control benchmarks, but it does not directly discuss NaturalQuestions, TriviaQA, and MMLU, which are crucial for understanding knowledge benchmarks as specified in the `<question_topic>`. Therefore, I will generate a question that probes the deeper understanding of these knowledge benchmarks and their objectives.\n\nThe context also includes irrelevant information such as math benchmarks and various control benchmarks, which will be ignored as they do not pertain to the specific topic of knowledge benchmarks and the mentioned datasets.\n</document_analysis>\n\n<output_format>\nQuestion: What is a distinguishing characteristic of the NaturalQuestions dataset compared to TriviaQA and MMLU in terms of the type of knowledge retrieval it emphasizes?\nA: NaturalQuestions focuses on real-world information retrieval.\nB: NaturalQuestions is designed for multi-domain tasks across various domains.\nC: NaturalQuestions specializes in technical and long-context challenges.\nD: NaturalQuestions requires models to solve multi-step math problems.\nExplanation: NaturalQuestions (Kwiatkowski et al., 2019) is known for its focus on real-world, commonsense questions that require common sense reasoning and contextual understanding, distinguishing it from TriviaQA and MMLU which focus more on factual and multi-domain knowledge retrieval.\nCorrect Answer: A\n</output_format>",
    "question": "What is a distinguishing characteristic of the NaturalQuestions dataset compared to TriviaQA and MMLU in terms of the type of knowledge retrieval it emphasizes?",
    "choices": {
      "A": "NaturalQuestions focuses on real-world information retrieval.",
      "B": "NaturalQuestions is designed for multi-domain tasks across various domains.",
      "C": "NaturalQuestions specializes in technical and long-context challenges.",
      "D": "NaturalQuestions requires models to solve multi-step math problems."
    },
    "answer": "A",
    "explanation": "NaturalQuestions (Kwiatkowski et al., 2019) is known for its focus on real-world, commonsense questions that require common sense reasoning and contextual understanding, distinguishing it from TriviaQA and MMLU which focus more on factual and multi-domain knowledge retrieval.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 8,
    "question_token_count": 31,
    "avg_answer_token_count": 11
  },
  {
    "context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n",
    "topic": "Evaluation Criteria Using Scoring Functions",
    "response": "<document_analysis>\nThe provided context discusses static benchmarks used to evaluate the performance of language models across various tasks. It focuses on the formulation of these benchmarks, highlighting the role of a scoring function in evaluating model outputs. The central idea revolves around how the scoring function compares the model's responses to the expected outputs, indicating the need for a deeply reflective question that probes the nuances of this evaluation process.\n</document_analysis>\n\n<output_format>\nQuestion: What aspect of the scoring function \\( \\mathcal{S}(\\cdot) \\) challenges even domain experts in evaluating the quality of an LLM's outputs?\nA: Its ability to accurately capture the complexity of human language and reasoning.\nB: Its capacity to handle edge cases and rare scenarios that may not be present in the training data.\nC: Its reliance on subjective interpretations of expected outputs.\nD: Its deterministic nature in providing a single score for each output.\nExplanation: The scoring function must consider the intricacies and subtleties of natural language and reasoning, which can vary significantly across different contexts and tasks. This requires a deep understanding of both the model's outputs and the expected results, making it a highly challenging aspect for even domain experts.\nCorrect Answer: A\n</output_format>",
    "question": "What aspect of the scoring function \\( \\mathcal{S}(\\cdot) \\) challenges even domain experts in evaluating the quality of an LLM's outputs?",
    "choices": {
      "A": "Its ability to accurately capture the complexity of human language and reasoning.",
      "B": "Its capacity to handle edge cases and rare scenarios that may not be present in the training data.",
      "C": "Its reliance on subjective interpretations of expected outputs.",
      "D": "Its deterministic nature in providing a single score for each output."
    },
    "answer": "A",
    "explanation": "The scoring function must consider the intricacies and subtleties of natural language and reasoning, which can vary significantly across different contexts and tasks. This requires a deep understanding of both the model's outputs and the expected results, making it a highly challenging aspect for even domain experts.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 4,
    "question_difficulty_score": 7,
    "question_groundedness_score": 3,
    "question_token_count": 29,
    "avg_answer_token_count": 14
  },
  {
    "context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n",
    "topic": "Balancing correctness and scalability in dynamic LLM benchmarks.",
    "response": "<output_format>\nQuestion: What critical issue does the proposed research highlight in the development of dynamic LLM benchmarks, and how might it impact their effectiveness?\nA: Balancing correctness and scalability can lead to inefficiencies in evaluation.\nB: Lack of standardized criteria hinders the development of reliable dynamic benchmarks.\nC: Insufficient transparency in labeling poses significant challenges.\nD: High assumptions about contaminated models affect post-hoc detection accuracy.\nExplanation: The question requires a deep understanding of the challenges identified in developing dynamic LLM benchmarks, particularly focusing on the balance between correctness and scalability, which is a subtle yet critical aspect of the research findings.\nCorrect Answer: A\n</output_format>\n\n<document_analysis>\nThe provided context discusses various aspects of LLM benchmarking, highlighting the importance of balancing correctness and scalability in dynamic benchmarks. The passage mentions that while dynamic benchmarks address some limitations of static benchmarks, they introduce new issues, such as balancing correctness with scalability. This issue is crucial because it directly impacts the effectiveness of the benchmarks, making it a central theme for discussion and future research directions. The other options, while relevant to the broader context, do not capture the specific challenge highlighted in the question as precisely as option A.\n</document_analysis>",
    "question": "What critical issue does the proposed research highlight in the development of dynamic LLM benchmarks, and how might it impact their effectiveness?",
    "choices": {
      "A": "Balancing correctness and scalability can lead to inefficiencies in evaluation.",
      "B": "Lack of standardized criteria hinders the development of reliable dynamic benchmarks.",
      "C": "Insufficient transparency in labeling poses significant challenges.",
      "D": "High assumptions about contaminated models affect post-hoc detection accuracy."
    },
    "answer": "A",
    "explanation": "The question requires a deep understanding of the challenges identified in developing dynamic LLM benchmarks, particularly focusing on the balance between correctness and scalability, which is a subtle yet critical aspect of the research findings.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 8,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "question_token_count": 25,
    "avg_answer_token_count": 12
  },
  {
    "context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n",
    "topic": "Understanding the limitations of label protection in research and its impact on performance metrics.",
    "response": "<output_format>\nQuestion: How does label protection affect the evaluation of performance metrics in machine learning research, and what is a potential consequence of relying solely on centralized evaluation systems?\nA: It enhances the reproducibility of results by providing standardized metrics.\nB: It restricts the ability to perform detailed error analysis and hinders reproducibility.\nC: It improves the accuracy of performance metrics by reducing bias.\nD: It has no significant impact on the evaluation process.\nExplanation: The context states that label protection limits transparency and independent verification, forcing reliance on centralized evaluation systems which can impede detailed error analysis and reproducibility.\nCorrect Answer: B\n</output_format>\n\n<document_analysis>\nThe context highlights the limitations of label protection, particularly how it restricts transparency and independent verification, and forces researchers to depend on centralized evaluation systems. This dependence impedes detailed error analysis and reproducibility. Therefore, the question is designed to probe the expert's understanding of these limitations and their consequences.\n\nThe provided options are designed to be plausible yet subtly incorrect:\n- Option A suggests an enhancement in reproducibility, which is contrary to the stated limitations.\n- Option B accurately reflects the negative impact on detailed error analysis and reproducibility.\n- Option C implies an improvement in accuracy, which is not mentioned and contradicts the stated limitations.\n- Option D incorrectly suggests that label protection has no significant impact, which is false based on the provided context.\n</document_analysis>",
    "question": "How does label protection affect the evaluation of performance metrics in machine learning research, and what is a potential consequence of relying solely on centralized evaluation systems?",
    "choices": {
      "A": "It enhances the reproducibility of results by providing standardized metrics.",
      "B": "It restricts the ability to perform detailed error analysis and hinders reproducibility.",
      "C": "It improves the accuracy of performance metrics by reducing bias.",
      "D": "It has no significant impact on the evaluation process."
    },
    "answer": "B",
    "explanation": "The context states that label protection limits transparency and independent verification, forcing reliance on centralized evaluation systems which can impede detailed error analysis and reproducibility.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "question_token_count": 30,
    "avg_answer_token_count": 12
  },
  {
    "context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n",
    "topic": "Auto-Dataset Methodology for Generating New Benchmark Samples",
    "response": "<document_analysis>\nThe provided context discusses various methodologies for generating new benchmark samples, particularly focusing on Auto-Dataset. It emphasizes the ability to generate diverse and high-quality samples by prompting LLMs to create new instances that either retain the style and knowledge of the original samples or introduce related questions at different cognitive levels. The methodology avoids the limitations of pre-defined rules and reduces the risk of in-distribution contamination. However, the context does not delve deeply into the specific mechanisms or challenges of implementing these methodologies, nor does it provide detailed examples of the generated samples.\n\nGiven the focus on the unique aspects of Auto-Dataset, a highly challenging question would require a deep understanding of its approach and its impact on benchmark generation.\n</document_analysis>\n\n<output_format>\nQuestion: How does Auto-Dataset ensure diversity and avoid in-distribution contamination compared to other methodologies?\nA: By employing LLMs to generate new samples that either retain the stylistics and essential knowledge of the original or present related questions at different cognitive levels.\nB: By defining strict rules for sample generation and using a contamination detector to identify and remove contaminated samples.\nC: By rewriting samples from existing static benchmarks to ensure they remain uncontaminated.\nD: By utilizing knowledge graphs to expand on examined concepts and generate new questions.\nExplanation: The correct answer involves understanding that Auto-Dataset uses LLMs to create new samples that either retain the original style and knowledge or present related questions at varying cognitive levels, thereby avoiding the limitations of predefined rules and reducing the risk of contamination. Other methods might rely on rewriting existing samples or using knowledge graphs, but they do not capture the essence of Auto-Dataset's approach as described.\nCorrect Answer: A\n</output_format>",
    "question": "How does Auto-Dataset ensure diversity and avoid in-distribution contamination compared to other methodologies?",
    "choices": {
      "A": "By employing LLMs to generate new samples that either retain the stylistics and essential knowledge of the original or present related questions at different cognitive levels.",
      "B": "By defining strict rules for sample generation and using a contamination detector to identify and remove contaminated samples.",
      "C": "By rewriting samples from existing static benchmarks to ensure they remain uncontaminated.",
      "D": "By utilizing knowledge graphs to expand on examined concepts and generate new questions."
    },
    "answer": "A",
    "explanation": "The correct answer involves understanding that Auto-Dataset uses LLMs to create new samples that either retain the original style and knowledge or present related questions at varying cognitive levels, thereby avoiding the limitations of predefined rules and reducing the risk of contamination. Other methods might rely on rewriting existing samples or using knowledge graphs, but they do not capture the essence of Auto-Dataset's approach as described.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 7,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "question_token_count": 18,
    "avg_answer_token_count": 20
  },
  {
    "context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n",
    "topic": "StructEval Approach in Expanding Examined Concepts via Knowledge Graphs",
    "response": "<output_format>\nQuestion: How does StructEval approach the expansion of examined concepts in benchmarks through the integration of knowledge graphs?\nA: By prompting LLMs to generate stylistically similar yet semantically varied samples.\nB: By employing LLMs to rewrite samples from static benchmarks, maintaining their difficulty levels.\nC: By using knowledge graphs to extend questions from the original benchmark, thereby exploring related concepts at varying cognitive levels.\nD: By detecting contaminated samples in static benchmarks and prompting LLMs to rewrite them.\nExplanation: StructEval specifically uses LLMs and knowledge graphs to expand on the original benchmark questions by developing a series of extended questions that explore related concepts at different cognitive levels, as described in the provided context.\nCorrect Answer: C\n</output_format>\n\n<document_analysis>\nThe context describes various approaches to expanding and rewriting samples from benchmarks. StructEval is clearly distinguished by its method of using LLMs and knowledge graphs to extend questions from the original benchmark, focusing on exploring related concepts at different cognitive levels. This approach directly aligns with option C. Options A, B, and D describe other methodologies (Auto-Dataset, ITD, VarBench) mentioned in the context but do not match the specific description of StructEval.\n</document_analysis>",
    "question": "How does StructEval approach the expansion of examined concepts in benchmarks through the integration of knowledge graphs?",
    "choices": {
      "A": "By prompting LLMs to generate stylistically similar yet semantically varied samples.",
      "B": "By employing LLMs to rewrite samples from static benchmarks, maintaining their difficulty levels.",
      "C": "By using knowledge graphs to extend questions from the original benchmark, thereby exploring related concepts at varying cognitive levels.",
      "D": "By detecting contaminated samples in static benchmarks and prompting LLMs to rewrite them."
    },
    "answer": "C",
    "explanation": "StructEval specifically uses LLMs and knowledge graphs to expand on the original benchmark questions by developing a series of extended questions that explore related concepts at different cognitive levels, as described in the provided context.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "question_token_count": 20,
    "avg_answer_token_count": 17
  },
  {
    "context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n",
    "topic": "Strategies and limitations in detecting and mitigating data contamination in LLM training processes.",
    "response": "<document_analysis>\nThe provided context discusses the challenges of detecting and mitigating data contamination in Large Language Model (LLM) training processes. Key points include:\n1. LLMs are trained on massive, diverse datasets often scraped from the web, increasing the risk of evaluation data overlap.\n2. Post-training fine-tuning on annotated or synthetic datasets further complicates the issue.\n3. Retrieval-based detection methods exist but are limited by the scale and complexity of training corpora.\n4. Proprietary nature of training data complicates accurate assessment and benchmarking.\n5. Opaqueness of training data further exacerbates contamination risks.\n\nGiven these points, the question should probe a deep understanding of the nuances involved in detecting and mitigating contamination, requiring reflection on the inherent difficulties and the need for transparent practices.\n</document_analysis>\n\n<output_format>\nQuestion: What is the primary reason that makes it difficult to entirely exclude evaluation data during LLM training, despite the existence of retrieval-based detection methods?\nA: The sheer size and diversity of the training corpora.\nB: The proprietary nature of the training data.\nC: The lack of comprehensive retrieval-based detection methods.\nD: The ease of accessing and using large-scale datasets for training.\nExplanation: The correct answer is A. Despite the existence of retrieval-based detection methods, the sheer size and diversity of training corpora make it practically impossible to ensure complete exclusion of evaluation data. The other options are plausible but less directly related to the core challenge.\nCorrect Answer: A\n</output_format>",
    "question": "What is the primary reason that makes it difficult to entirely exclude evaluation data during LLM training, despite the existence of retrieval-based detection methods?",
    "choices": {
      "A": "The sheer size and diversity of the training corpora.",
      "B": "The proprietary nature of the training data.",
      "C": "The lack of comprehensive retrieval-based detection methods.",
      "D": "The ease of accessing and using large-scale datasets for training."
    },
    "answer": "A",
    "explanation": "The correct answer is A. Despite the existence of retrieval-based detection methods, the sheer size and diversity of training corpora make it practically impossible to ensure complete exclusion of evaluation data. The other options are plausible but less directly related to the core challenge.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "question_token_count": 28,
    "avg_answer_token_count": 11
  },
  {
    "context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n",
    "topic": "The importance of ethical guidelines in data usage and model transparency within AI benchmarking.",
    "response": "<document_analysis>\nThe provided context emphasizes the importance of ethical guidelines in data usage and model transparency within AI benchmarking. It discusses the risks associated with static and dynamic benchmarks, including the perpetuation of biases, privacy and security concerns, and the potential for misuse. The context highlights the need for fairness, accountability, and privacy in benchmarking frameworks. Given this, the question should delve into the nuances of these ethical considerations and their implications for AI benchmarking.\n</document_analysis>\n\n<output_format>\nQuestion: How do ethical guidelines address the dual challenges of data bias and model transparency in AI benchmarking?\nA: By mandating regular audits of data sources and model performance metrics to ensure fairness and accountability.\nB: Through the establishment of transparent data policies and the implementation of robust validation techniques to prevent bias.\nC: By limiting the use of static benchmarks and encouraging the exclusive use of dynamic benchmarks to enhance adaptability.\nD: Ensuring that all models undergo rigorous testing and approval processes before being used in benchmarks.\nExplanation: The correct answer lies in recognizing the comprehensive nature of ethical guidelines, which involve both data policy transparency and the prevention of bias through validation techniques. Options A, C, and D address specific aspects but do not fully capture the breadth of ethical guidelines discussed.\nCorrect Answer: B\n</output_format>",
    "question": "How do ethical guidelines address the dual challenges of data bias and model transparency in AI benchmarking?",
    "choices": {
      "A": "By mandating regular audits of data sources and model performance metrics to ensure fairness and accountability.",
      "B": "Through the establishment of transparent data policies and the implementation of robust validation techniques to prevent bias.",
      "C": "By limiting the use of static benchmarks and encouraging the exclusive use of dynamic benchmarks to enhance adaptability.",
      "D": "Ensuring that all models undergo rigorous testing and approval processes before being used in benchmarks."
    },
    "answer": "B",
    "explanation": "The correct answer lies in recognizing the comprehensive nature of ethical guidelines, which involve both data policy transparency and the prevention of bias through validation techniques. Options A, C, and D address specific aspects but do not fully capture the breadth of ethical guidelines discussed.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 9,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 6,
    "question_token_count": 19,
    "avg_answer_token_count": 19
  },
  {
    "context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n",
    "topic": "Categorization of dynamic benchmarks and their respective characteristics.",
    "response": "<document_analysis>\nThe context provides information about the categorization of dynamic benchmarks and their characteristics. It mentions that dynamic benchmarks can be categorized into four types: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches. The focus is on the differences between these categories, particularly their construction processes and unique features. The key themes include the interpretability requirement for transformations, the nature of data collection, and the methods used to generate evaluation data points. The information is structured and detailed, providing a clear basis for generating a challenging question.\n</document_analysis>\n\n<output_format>\nQuestion: What characteristic distinguishes hybrid approaches in dynamic benchmarking from the other categories?\nA: They rely solely on manually crafted rules.\nB: They use a combination of rule-based and LLM-based generation techniques.\nC: They focus exclusively on collecting new data from recent releases.\nD: They are less transparent and harder to validate.\nExplanation: Hybrid approaches incorporate elements from both rule-based and LLM-based generation, offering a balanced approach but requiring validation mechanisms to ensure reliability.\nCorrect Answer: B\n</output_format>",
    "question": "What characteristic distinguishes hybrid approaches in dynamic benchmarking from the other categories?",
    "choices": {
      "A": "They rely solely on manually crafted rules.",
      "B": "They use a combination of rule-based and LLM-based generation techniques.",
      "C": "They focus exclusively on collecting new data from recent releases.",
      "D": "They are less transparent and harder to validate."
    },
    "answer": "B",
    "explanation": "Hybrid approaches incorporate elements from both rule-based and LLM-based generation, offering a balanced approach but requiring validation mechanisms to ensure reliability.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "question_token_count": 14,
    "avg_answer_token_count": 11
  },
  {
    "context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Methods for regenerating benchmark data to reduce contamination in LLM evaluation.",
    "response": "<document_analysis>\nThe provided context focuses on methods for mitigating data contamination in the evaluation of Large Language Models (LLMs) through dynamic benchmarking. The key points are:\n- Data contamination occurs when benchmark data is included in the training phase of LLMs, leading to inflated performance assessments.\n- Static benchmarking methods are prone to contamination and are being replaced by dynamic methods.\n- One dynamic method involves continuously updating benchmarks based on the timestamps of LLM training.\n- Another method involves regenerating benchmark data to reconstruct original benchmarks.\n- The context emphasizes the need for criteria to evaluate dynamic benchmarks.\n\nGiven the complexity and nuance of the topic, we aim to generate a highly challenging and reflective question that encourages deep engagement with the content.\n</document_analysis>\n\n<output_format>\nQuestion: What is the primary advantage of regenerating benchmark data over simply updating benchmarks based on the timestamps of LLM training, in the context of mitigating data contamination?\nA: Regenerating benchmark data allows for the reconstruction of the original problem space, ensuring that the model is evaluated on fresh data that was not used during its training.\nB: Regenerating benchmark data provides real-time updates, making it more responsive to the latest trends and developments in the field.\nC: Regenerating benchmark data is less computationally intensive than updating benchmarks based on timestamps.\nD: Regenerating benchmark data ensures that the evaluation process remains transparent, allowing for public scrutiny of the model's performance.\nExplanation: Regenerating benchmark data offers the primary advantage of ensuring that the model is evaluated on fresh data that was not used during its training, thus providing a more accurate and reliable measure of performance. This approach helps in mitigating data contamination by creating a new and independent set of problems for the model to solve.\nCorrect Answer: A\n</output_format>",
    "question": "What is the primary advantage of regenerating benchmark data over simply updating benchmarks based on the timestamps of LLM training, in the context of mitigating data contamination?",
    "choices": {
      "A": "Regenerating benchmark data allows for the reconstruction of the original problem space, ensuring that the model is evaluated on fresh data that was not used during its training.",
      "B": "Regenerating benchmark data provides real-time updates, making it more responsive to the latest trends and developments in the field.",
      "C": "Regenerating benchmark data is less computationally intensive than updating benchmarks based on timestamps.",
      "D": "Regenerating benchmark data ensures that the evaluation process remains transparent, allowing for public scrutiny of the model's performance."
    },
    "answer": "A",
    "explanation": "Regenerating benchmark data offers the primary advantage of ensuring that the model is evaluated on fresh data that was not used during its training, thus providing a more accurate and reliable measure of performance. This approach helps in mitigating data contamination by creating a new and independent set of problems for the model to solve.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "question_token_count": 31,
    "avg_answer_token_count": 24
  },
  {
    "context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n",
    "topic": "Evaluation Framework for LLMs on NP-Hard Problems",
    "response": "<document_analysis>\nThe provided context discusses various frameworks for evaluating the reasoning abilities of Large Language Models (LLMs) in handling NP-hard problems. The frameworks include table-based evaluation using SQL queries, graph-based methods such as directed acyclic graphs (DAGs) and random graphs for NP-hard problems, and puzzle construction. The context does not contain detailed information about a specific evaluation framework for LLMs on NP-hard problems, but rather describes different approaches. Since the focus is on evaluation frameworks for LLMs on NP-hard problems, and the context provides a general overview of different methods, a question that probes deeper understanding of these frameworks would be beneficial.\n</document_analysis>\n\n<output_format>\nQuestion: Which of the following frameworks uses a combination of randomly generated graphs and natural language descriptions to evaluate LLMs' reasoning abilities for NP-hard problems?\nA: S3Eval\nB: DyVal\nC: NPHardEval\nD: Xie et al.'s method for Knights and Knaves puzzles\nExplanation: DyVal (Zhu et al., 2024a) constructs directed acyclic graphs (DAGs) with varying numbers of nodes and edges, transforms these graphs into natural language descriptions, and then evaluates the LLM's reasoning capability by querying for the root node value. This approach combines graph generation and natural language processing to assess the model's ability to reason about complex structures.\nCorrect Answer: B\n</output_format>",
    "question": "Which of the following frameworks uses a combination of randomly generated graphs and natural language descriptions to evaluate LLMs' reasoning abilities for NP-hard problems?",
    "choices": {
      "A": "S3Eval",
      "B": "DyVal",
      "C": "NPHardEval",
      "D": "Xie et al.'s method for Knights and Knaves puzzles"
    },
    "answer": "B",
    "explanation": "DyVal (Zhu et al., 2024a) constructs directed acyclic graphs (DAGs) with varying numbers of nodes and edges, transforms these graphs into natural language descriptions, and then evaluates the LLM's reasoning capability by querying for the root node value. This approach combines graph generation and natural language processing to assess the model's ability to reason about complex structures.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "question_token_count": 29,
    "avg_answer_token_count": 6
  },
  {
    "context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n",
    "topic": "The potential perpetuation of biases through the use of static benchmarks and methods to mitigate these biases.",
    "response": "<document_analysis>\nThe context discusses the use of static and dynamic benchmarks in evaluating language models (LLMs) and the associated ethical considerations. It highlights the potential for static benchmarks to perpetuate biases if not carefully constructed, despite their benefits in enhancing transparency and fairness. The context also mentions the challenges posed by dynamic benchmarks, including privacy and security concerns. Given the emphasis on the potential perpetuation of biases through static benchmarks, a question that delves into the nuances and implications of this issue would be highly insightful.\n</document_analysis>\n\n<output_format>\nQuestion: How might static benchmarks inadvertently perpetuate biases, and what measures can be taken to mitigate this risk?\nA: By relying on outdated or biased data sources, thus reinforcing existing prejudices in AI systems.\nB: Through the lack of adaptability, making them unsuitable for diverse user groups.\nC: By being too transparent, allowing adversaries to exploit weaknesses in the evaluation process.\nD: Due to their reliance on continuous data collection, leading to potential privacy violations.\nExplanation: The correct answer is A. Static benchmarks can perpetuate biases if they are based on outdated or biased data sources. This can reinforce existing prejudices in AI systems, even if the intention was to mitigate bias.\nCorrect Answer: A\n</output_format>",
    "question": "How might static benchmarks inadvertently perpetuate biases, and what measures can be taken to mitigate this risk?",
    "choices": {
      "A": "By relying on outdated or biased data sources, thus reinforcing existing prejudices in AI systems.",
      "B": "Through the lack of adaptability, making them unsuitable for diverse user groups.",
      "C": "By being too transparent, allowing adversaries to exploit weaknesses in the evaluation process.",
      "D": "Due to their reliance on continuous data collection, leading to potential privacy violations."
    },
    "answer": "A",
    "explanation": "The correct answer is A. Static benchmarks can perpetuate biases if they are based on outdated or biased data sources. This can reinforce existing prejudices in AI systems, even if the intention was to mitigate bias.",
    "answer_correctness_score": 8,
    "explanation_validity_score": 9,
    "question_clarity_score": 9,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "question_token_count": 21,
    "avg_answer_token_count": 16
  },
  {
    "context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n",
    "topic": "Investigating the assessment of technical and long-context challenges through datasets like AlpacaEval and ArenaHard.",
    "response": "<document_analysis>\nThe context primarily discusses various benchmarks used to evaluate language models, focusing on datasets that assess technical and long-context challenges. The text mentions ControlBench, FRAMES, and GPQA Diamond targeting such challenges, but also highlights two specific datasets: AlpacaEval and ArenaHard, which provide open-domain evaluations. The focus here is on how these datasets are used to test the ability of models to handle complex, multi-faceted tasks.\n</document_analysis>\n\n<output_format>\nQuestion: How do AlpacaEval and ArenaHard contribute uniquely to assessing a model's ability to handle technical and long-context challenges compared to other datasets mentioned in the context?\n\nA: By providing a more diverse range of technical and long-context problems than traditional math benchmarks.\nB: Through their focus on open-domain evaluations that require deep integration of knowledge across multiple domains.\nC: By offering controlled environments with well-defined tasks unlike the more open-ended nature of ControlBench and FRAMES.\nD: They emphasize natural language understanding over problem-solving capabilities, making them distinct from math and knowledge benchmarks.\n\nExplanation: AlpacaEval and ArenaHard are designed to test models' abilities to handle complex, open-domain scenarios rather than structured math problems or straightforward knowledge retrieval. This makes them unique in their emphasis on integrating diverse knowledge and handling intricate, context-rich tasks.\nCorrect Answer: B\n</output_format>",
    "question": "How do AlpacaEval and ArenaHard contribute uniquely to assessing a model's ability to handle technical and long-context challenges compared to other datasets mentioned in the context?",
    "choices": {
      "A": "By providing a more diverse range of technical and long-context problems than traditional math benchmarks.",
      "B": "Through their focus on open-domain evaluations that require deep integration of knowledge across multiple domains.",
      "C": "By offering controlled environments with well-defined tasks unlike the more open-ended nature of ControlBench and FRAMES.",
      "D": "They emphasize natural language understanding over problem-solving capabilities, making them distinct from math and knowledge benchmarks."
    },
    "answer": "B",
    "explanation": "AlpacaEval and ArenaHard are designed to test models' abilities to handle complex, open-domain scenarios rather than structured math problems or straightforward knowledge retrieval. This makes them unique in their emphasis on integrating diverse knowledge and handling intricate, context-rich tasks.",
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "question_token_count": 32,
    "avg_answer_token_count": 19
  },
  {
    "context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n",
    "topic": "Transparency issues in benchmarking and the need for safeguards against misuse of benchmarking results.",
    "response": "<output_format>\nQuestion: What subtle ethical risk might arise from the dynamic nature of benchmarks, despite their intended adaptability?\nA: Increased computational costs for model training.\nB: Heightened potential for continuous bias amplification.\nC: Simplified data management processes.\nD: Enhanced user trust in AI systems.\nExplanation: The dynamic nature of benchmarks introduces ongoing data collection and updating, which can lead to continuous bias amplification if not managed properly. This risk is subtle yet critical, as it can perpetuate existing biases over time without proper oversight.\nCorrect Answer: B\n</output_format>\n\n<document_analysis>\nThoughtful Content Examination:\nThe context discusses the potential pitfalls of benchmarks, particularly focusing on the dynamic nature of benchmarks which can lead to continuous bias amplification if not handled carefully. It highlights the importance of managing these risks to ensure fairness and avoid unintended harm.\n\nConcept Exploration:\nThe question aims to probe the domain expert's understanding of the subtle ethical risk associated with the dynamic benchmarks, which is the potential for continuous bias amplification. This requires recognizing that while dynamic benchmarks offer adaptability, they also introduce ongoing data collection and updating, which can inadvertently perpetuate biases.\n\nStrategic Complexity Calibration:\nThis question is rated 9 out of 10 in terms of difficulty. It challenges the domain expert to recognize the nuanced ethical risk associated with the dynamic nature of benchmarks and understand the implications of continuous data collection and updating.\n\nIntentional Question Planning:\nThe question invites deeper reflection on the ethical considerations of benchmarking, encouraging a critical engagement with the material. It requires a high level of understanding to identify the subtle but significant risk of continuous bias amplification.\n</document_analysis>",
    "question": "What subtle ethical risk might arise from the dynamic nature of benchmarks, despite their intended adaptability?",
    "choices": {
      "A": "Increased computational costs for model training.",
      "B": "Heightened potential for continuous bias amplification.",
      "C": "Simplified data management processes.",
      "D": "Enhanced user trust in AI systems."
    },
    "answer": "B",
    "explanation": "The dynamic nature of benchmarks introduces ongoing data collection and updating, which can lead to continuous bias amplification if not managed properly. This risk is subtle yet critical, as it can perpetuate existing biases over time without proper oversight.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "question_token_count": 19,
    "avg_answer_token_count": 8
  },
  {
    "context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n",
    "topic": "The planning, generation, verification, and evaluation phases in multi-agent benchmark creation, as described in BENCHAGENTS.",
    "response": "<output_format>\nQuestion: What is the primary role of each specialized LLM agent during the benchmark creation process as described in BENCHAGENTS?\nA: Planning - defines the scope and structure of the benchmark.\nB: Generation - creates initial test cases and scenarios based on the defined scope.\nC: Verification - assesses the accuracy and consistency of the generated test cases.\nD: Evaluation - conducts the final assessment through multi-agent collaboration and feedback.\nExplanation: The question requires a deep understanding of the roles assigned to different LLM agents in the BENCHAGENTS process. It challenges the respondent to comprehend the distinct responsibilities of each agent without simply regurgitating the text.\nCorrect Answer: D\n</output_format>\n\n<document_analysis>\nThe provided context describes the BENCHAGENTS process, which involves a multi-agent framework for benchmark creation. The text mentions four main phases: planning, generation, verification, and evaluation, each handled by a specialized LLM agent. The context does not explicitly detail the tasks performed by each phase, but it provides a clear structure that can be analyzed to formulate a challenging question.\n\nThe question is designed to test a deep understanding of the roles within the BENCHAGENTS process, requiring the respondent to synthesize the information rather than simply recalling it. This ensures that the question is highly technical and thought-provoking, aligning with the objective of evaluating domain expertise.\n</document_analysis>",
    "question": "What is the primary role of each specialized LLM agent during the benchmark creation process as described in BENCHAGENTS?",
    "choices": {
      "A": "Planning - defines the scope and structure of the benchmark.",
      "B": "Generation - creates initial test cases and scenarios based on the defined scope.",
      "C": "Verification - assesses the accuracy and consistency of the generated test cases.",
      "D": "Evaluation - conducts the final assessment through multi-agent collaboration and feedback."
    },
    "answer": "D",
    "explanation": "The question requires a deep understanding of the roles assigned to different LLM agents in the BENCHAGENTS process. It challenges the respondent to comprehend the distinct responsibilities of each agent without simply regurgitating the text.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 6,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "question_token_count": 24,
    "avg_answer_token_count": 13
  },
  {
    "context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n",
    "topic": "Importance of Standardized Evaluation Tools in Model Performance Assessment",
    "response": "<document_analysis>\nThe provided context focuses on static benchmarks used to evaluate various aspects of model performance. It emphasizes the importance of standardized evaluation tools in assessing model abilities across different tasks. The context highlights the structure of these benchmarks, defining them as datasets consisting of input prompts, expected outputs, and a scoring function. Given this, the question should probe a deeper understanding of the significance of such tools in the assessment of model performance, encouraging a reflective and insightful response from domain experts.\n</document_analysis>\n\n<output_format>\nQuestion: What is the primary advantage of using static benchmarks as standardized evaluation tools in assessing model performance, particularly when compared to other forms of evaluation methods?\nA: They provide a subjective and flexible framework for evaluating models.\nB: They allow for a consistent and comparable measurement of model capabilities across diverse tasks.\nC: They enable models to adapt to new and unforeseen challenges dynamically.\nD: They facilitate real-time interaction and immediate feedback between models and evaluators.\nExplanation: The correct answer lies in recognizing that static benchmarks offer a standardized and consistent method for evaluating model performance across various tasks, making them comparable and objective.\nCorrect Answer: B\n</output_format>",
    "question": "What is the primary advantage of using static benchmarks as standardized evaluation tools in assessing model performance, particularly when compared to other forms of evaluation methods?",
    "choices": {
      "A": "They provide a subjective and flexible framework for evaluating models.",
      "B": "They allow for a consistent and comparable measurement of model capabilities across diverse tasks.",
      "C": "They enable models to adapt to new and unforeseen challenges dynamically.",
      "D": "They facilitate real-time interaction and immediate feedback between models and evaluators."
    },
    "answer": "B",
    "explanation": "The correct answer lies in recognizing that static benchmarks offer a standardized and consistent method for evaluating model performance across various tasks, making them comparable and objective.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 4,
    "question_token_count": 29,
    "avg_answer_token_count": 14
  },
  {
    "context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n",
    "topic": "Analyzing the role of recent extensions in refining knowledge benchmarks, such as MMLU-Redux and MMLU-Pro.",
    "response": "<output_format>\nQuestion: How do MMLU-Redux and MMLU-Pro contribute to the refinement of knowledge benchmarks in a manner distinct from other recent extensions?\n\nA: By focusing exclusively on technical and long-context challenges.\n\nB: Through enhanced multi-domain testing and detailed performance tracking.\n\nC: By integrating control and framing challenges into the assessment framework.\n\nD: Utilizing open-domain evaluations to gauge broad understanding and applicability.\n\nExplanation: MMLU-Redux and MMLU-Pro specifically refine knowledge benchmarks by providing more detailed and comprehensive multi-domain testing compared to other recent extensions, which often focus on different types of challenges like technical skills or long-context problems.\n\nCorrect Answer: B\n</output_format>\n\n<document_analysis>\nThe context discusses various benchmark datasets and tests but emphasizes MMLU-Redux and MMLU-Pro as recent extensions that refine knowledge benchmarks. It mentions that other benchmarks like ControlBench, FRAMES, and GPQA Diamond focus on different aspects such as technical and long-context challenges. The key distinction is that MMLU-Redux and MMLU-Pro enhance multi-domain testing and detailed performance tracking, making them distinct in their approach to refining knowledge benchmarks.\n\nThe question is designed to probe the domain expert's understanding of how these specific extensions contribute to benchmark refinement in a unique way, requiring a deep understanding of the nuances between MMLU-Redux, MMLU-Pro, and other recent extensions.\n</document_analysis>",
    "question": "How do MMLU-Redux and MMLU-Pro contribute to the refinement of knowledge benchmarks in a manner distinct from other recent extensions?",
    "choices": {
      "A": "By focusing exclusively on technical and long-context challenges.",
      "B": "Through enhanced multi-domain testing and detailed performance tracking.",
      "C": "By integrating control and framing challenges into the assessment framework.",
      "D": "Utilizing open-domain evaluations to gauge broad understanding and applicability."
    },
    "answer": "B",
    "explanation": "MMLU-Redux and MMLU-Pro specifically refine knowledge benchmarks by providing more detailed and comprehensive multi-domain testing compared to other recent extensions, which often focus on different types of challenges like technical skills or long-context problems.",
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "question_token_count": 29,
    "avg_answer_token_count": 11
  },
  {
    "context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Strategies for mitigating data contamination in static benchmarking of LLMs.",
    "response": "<output_format>\nQuestion: What critical factor makes the risk of data contamination particularly high in the context of LLM benchmarking, and how does this challenge traditional separation principles?\nA: The extensive training data collection from the internet increases the likelihood of contamination, violating the principle of separating training and test data.\nB: The use of dynamic benchmarking methods decreases the risk of contamination by updating datasets.\nC: The transparency of static benchmarks allows for easier detection of contamination.\nD: The rapid evolution of LLM architectures makes it difficult to track training data sources.\nExplanation: This question requires a deep understanding of the unique challenges posed by LLMs in benchmarking due to their extensive training data collection from the internet. It probes the critical factor of increased contamination risk and the violation of traditional separation principles.\nCorrect Answer: A\n</output_format>\n\n<document_analysis>\nThe context highlights that LLMs, trained on vast amounts of Internet-sourced data, are particularly susceptible to data contamination during benchmarking. This contamination is a significant challenge because it violates the traditional principle of separating training and test data. Therefore, option A accurately identifies the critical factor making data contamination particularly high in the context of LLM benchmarking. \n\nOption B addresses a method to mitigate contamination but does not directly answer the question about the critical factor increasing contamination risk. Option C refers to a characteristic of static benchmarks but does not address the unique issue of LLMs. Option D discusses the complexity of tracking training data sources but does not capture the essence of the risk associated with the extensive training data collection from the internet.\n\nThe question is designed to provoke critical thinking about the unique challenges posed by LLMs and the inherent risks in their benchmarking process.\n</document_analysis>",
    "question": "What critical factor makes the risk of data contamination particularly high in the context of LLM benchmarking, and how does this challenge traditional separation principles?",
    "choices": {
      "A": "The extensive training data collection from the internet increases the likelihood of contamination, violating the principle of separating training and test data.",
      "B": "The use of dynamic benchmarking methods decreases the risk of contamination by updating datasets.",
      "C": "The transparency of static benchmarks allows for easier detection of contamination.",
      "D": "The rapid evolution of LLM architectures makes it difficult to track training data sources."
    },
    "answer": "A",
    "explanation": "This question requires a deep understanding of the unique challenges posed by LLMs in benchmarking due to their extensive training data collection from the internet. It probes the critical factor of increased contamination risk and the violation of traditional separation principles.",
    "answer_correctness_score": 9,
    "explanation_validity_score": 7,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 8,
    "question_token_count": 28,
    "avg_answer_token_count": 17
  },
  {
    "context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n",
    "topic": "Specific Examples of Up-to-Date Data Sources Used in LiveBench",
    "response": "<document_analysis>\nThe provided context discusses various up-to-date data sources used in different live benchmarking systems. Key points include the use of recent data sources such as math competitions, arXiv papers, and prediction markets. The focus is on maintaining temporal relevance to avoid data contamination. However, the context does not provide specific examples of current data sources used in LiveBench. This suggests that while other benchmarks like AntiLeak-Bench and AcademicEval are explicitly mentioned to use recent data, LiveBench's specific data sources are not detailed in the text.\n</document_analysis>\n\n<output_format>\nQuestion: What specific types of data sources does LiveBench utilize to ensure up-to-date information for its benchmarks?\nA: Recent math competitions from the past 12 months\nB: Newly published arXiv papers\nC: Human-written coding problems from online platforms\nD: Daily updated forecasting questions from prediction markets\nExplanation: LiveBench (White et al., 2024) is described as collecting questions based on the latest information source, but the text does not specify the exact types of data sources used. Given the context, options A, B, and C are plausible sources that align with the benchmark's aim to use recent data, while option D is less likely as LiveBench focuses on questions rather than forecasting questions.\nCorrect Answer: A\n</output_format>",
    "question": "What specific types of data sources does LiveBench utilize to ensure up-to-date information for its benchmarks?",
    "choices": {
      "A": "Recent math competitions from the past 12 months",
      "B": "Newly published arXiv papers",
      "C": "Human-written coding problems from online platforms",
      "D": "Daily updated forecasting questions from prediction markets"
    },
    "answer": "A",
    "explanation": "LiveBench (White et al., 2024) is described as collecting questions based on the latest information source, but the text does not specify the exact types of data sources used. Given the context, options A, B, and C are plausible sources that align with the benchmark's aim to use recent data, while option D is less likely as LiveBench focuses on questions rather than forecasting questions.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 5,
    "question_groundedness_score": 10,
    "question_token_count": 21,
    "avg_answer_token_count": 8
  },
  {
    "context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n",
    "topic": "Comparison Between Different LLM Evaluation Techniques",
    "response": "<document_analysis>\nThe context provides a detailed comparison between different techniques used to evaluate the reasoning abilities of Large Language Models (LLMs). It mentions three main categories: table-based evaluation (using S3Eval), graph-based evaluation (including DyVal and NPHardEval), and puzzle-based evaluation (Xie et al.). The table-based method uses SQL queries on randomly generated tables, while the graph-based methods utilize various types of graphs, including DAGs and random P/NP problem instances. Puzzle-based evaluation involves automatically generated Knights and Knaves puzzles. The context is informative and useful but does not contain any irrelevant or promotional information. Therefore, we can proceed with generating a highly challenging question based on this content.\n</document_analysis>\n\n<output_format>\nQuestion: What key feature distinguishes S3Eval from other graph-based evaluation techniques like DyVal and NPHardEval?\nA: S3Eval uses natural language descriptions derived from SQL tables.\nB: S3Eval evaluates reasoning skills using random SQL queries on SQL tables.\nC: S3Eval assesses the reasoning capabilities using randomly generated DAGs.\nD: S3Eval tests LLMs on well-known P and NP problems.\nExplanation: S3Eval specifically uses SQL queries on randomly generated tables, which is a distinct feature compared to the graph-based techniques that use DAGs, random P/NP problems, or Knights and Knaves puzzles.\nCorrect Answer: B\n</output_format>",
    "question": "What key feature distinguishes S3Eval from other graph-based evaluation techniques like DyVal and NPHardEval?",
    "choices": {
      "A": "S3Eval uses natural language descriptions derived from SQL tables.",
      "B": "S3Eval evaluates reasoning skills using random SQL queries on SQL tables.",
      "C": "S3Eval assesses the reasoning capabilities using randomly generated DAGs.",
      "D": "S3Eval tests LLMs on well-known P and NP problems."
    },
    "answer": "B",
    "explanation": "S3Eval specifically uses SQL queries on randomly generated tables, which is a distinct feature compared to the graph-based techniques that use DAGs, random P/NP problems, or Knights and Knaves puzzles.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "question_token_count": 23,
    "avg_answer_token_count": 14
  },
  {
    "context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n",
    "topic": "The impact of training corpus size on the effectiveness of static benchmarks in LLM evaluation.",
    "response": "<output_format>\nQuestion: How does the increase in training corpus size affect the effectiveness of static benchmarks in evaluating large language models (LLMs), and what mathematical relationship is suggested for this impact?\nA: It has no effect; static benchmarks remain equally reliable regardless of training corpus size.\nB: It improves the effectiveness; larger corpora lead to fewer contaminations.\nC: It decreases the effectiveness; the probability of contamination increases proportionally with the product of training and test set sizes.\nD: It enhances the effectiveness by reducing the reliance on human annotation.\nExplanation: The question invites a deep understanding of the mathematical relationship described in the context and its implications on benchmark reliability. The answer requires recognizing the inverse relationship between contamination probability and the ratio of training to test set sizes.\nCorrect Answer: C\n</output_format>\n\n<document_analysis>\nThe context provides a clear statement about the impact of training corpus size on static benchmarks, indicating that the probability of contamination increases with the product of the training and test set sizes. This relationship is mathematically represented as \\( \\text{Pr}_{\\text{contam}} \\proportional |\\mathcal{D}_{\\text{train}}| \\cdot |\\mathcal{D}_{\\text{test}}|^{-1} \\). Therefore, option C accurately reflects this relationship, while the other options either suggest no change, an improvement, or an enhancement unrelated to the stated relationship.\n</document_analysis>",
    "question": "How does the increase in training corpus size affect the effectiveness of static benchmarks in evaluating large language models (LLMs), and what mathematical relationship is suggested for this impact?",
    "choices": {
      "A": "It has no effect; static benchmarks remain equally reliable regardless of training corpus size.",
      "B": "It improves the effectiveness; larger corpora lead to fewer contaminations.",
      "C": "It decreases the effectiveness; the probability of contamination increases proportionally with the product of training and test set sizes.",
      "D": "It enhances the effectiveness by reducing the reliance on human annotation."
    },
    "answer": "C",
    "explanation": "The question invites a deep understanding of the mathematical relationship described in the context and its implications on benchmark reliability. The answer requires recognizing the inverse relationship between contamination probability and the ratio of training to test set sizes.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 10,
    "question_token_count": 34,
    "avg_answer_token_count": 17
  },
  {
    "context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n",
    "topic": "Impact of contamination on the validity of LLM benchmarks.",
    "response": "<document_analysis>\nThe provided context discusses the impact of contamination on the validity of Large Language Model (LLM) benchmarks. It outlines different types of contamination, particularly syntactic contamination, and emphasizes the significance of understanding and mitigating such issues. The core of the context highlights the potential for contamination to overestimate LLM capabilities, thus undermining the reliability of benchmark results. This information is crucial for evaluating the true performance and applicability of LLMs in practical scenarios.\n</document_analysis>\n\n<output_format>\nQuestion: How does syntactic contamination primarily affect the evaluation of LLMs in benchmarking?\nA: By allowing LLMs to recall memorized training data rather than demonstrating true reasoning.\nB: By enhancing the LLM's ability to handle unseen data.\nC: By improving the accuracy of the LLM on syntactically similar tasks.\nD: By reducing the computational resources needed for benchmarking.\nExplanation: Syntactic contamination involves rephrasing test data using prefix strings, derived from the training data. This form of contamination can make it appear that an LLM is recalling memorized information rather than demonstrating true reasoning capability, thereby overestimating its performance.\nCorrect Answer: A\n</output_format>",
    "question": "How does syntactic contamination primarily affect the evaluation of LLMs in benchmarking?",
    "choices": {
      "A": "By allowing LLMs to recall memorized training data rather than demonstrating true reasoning.",
      "B": "By enhancing the LLM's ability to handle unseen data.",
      "C": "By improving the accuracy of the LLM on syntactically similar tasks.",
      "D": "By reducing the computational resources needed for benchmarking."
    },
    "answer": "A",
    "explanation": "Syntactic contamination involves rephrasing test data using prefix strings, derived from the training data. This form of contamination can make it appear that an LLM is recalling memorized information rather than demonstrating true reasoning capability, thereby overestimating its performance.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 5,
    "question_groundedness_score": 10,
    "question_token_count": 15,
    "avg_answer_token_count": 13
  },
  {
    "context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Quality Assessment of Dynamic Benchmarks",
    "response": "<output_format>\nQuestion: What critical limitation does dynamic benchmarking aim to overcome compared to static benchmarking, and how does it achieve this through its definition?\nA: Dynamic benchmarking overcomes the limitation of restricted access to the training dataset by modifying the dataset during evaluation.\nB: It addresses the issue of data contamination by using a transformation function that changes the dataset at each timestamp.\nC: Dynamic benchmarking aims to provide a more transparent evaluation by always starting from a non-empty seed dataset.\nD: It ensures faithfulness in evaluation by allowing infinite timestamps for modification.\nExplanation: Dynamic benchmarking overcomes the limitation of static benchmarking by introducing a transformation function \\( T(\\cdot) \\) that modifies the dataset during evaluation, thus avoiding data contamination and providing a flexible evaluation framework.\nCorrect Answer: B\n</output_format>\n\n<document_analysis>\nThe context introduces dynamic benchmarking as a solution to the limitations faced by static benchmarking, particularly regarding data contamination and the need for a flexible evaluation framework. The definition of the dynamic benchmarking framework includes a transformation function \\( T(\\cdot) \\) that modifies the dataset at each timestamp \\( t \\), which directly addresses the issue of data contamination. Therefore, option B accurately captures the critical aspect of dynamic benchmarking aimed at overcoming data contamination issues.\n\nOption A is partially correct but incomplete; it does not mention the transformation function. Option C is incorrect because the definition does not specify that the seed dataset must be non-empty. Option D is incorrect because the definition does not guarantee infinite timestamps.\n</document_analysis>",
    "question": "What critical limitation does dynamic benchmarking aim to overcome compared to static benchmarking, and how does it achieve this through its definition?",
    "choices": {
      "A": "Dynamic benchmarking overcomes the limitation of restricted access to the training dataset by modifying the dataset during evaluation.",
      "B": "It addresses the issue of data contamination by using a transformation function that changes the dataset at each timestamp.",
      "C": "Dynamic benchmarking aims to provide a more transparent evaluation by always starting from a non-empty seed dataset.",
      "D": "It ensures faithfulness in evaluation by allowing infinite timestamps for modification."
    },
    "answer": "B",
    "explanation": "Dynamic benchmarking overcomes the limitation of static benchmarking by introducing a transformation function \\( T(\\cdot) \\) that modifies the dataset during evaluation, thus avoiding data contamination and providing a flexible evaluation framework.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 7,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "question_token_count": 25,
    "avg_answer_token_count": 19
  },
  {
    "context": "###### Safety\nSafety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content. Datasets such as RealToxicityPrompts\u00a0(Gehman et\u00a0al., 2020) and ToxiGen\u00a0(Hartvigsen et\u00a0al., 2022) assess resilience against producing harmful outputs.\n\nBy providing a controlled environment to measure these aspects, safety benchmarks play a critical role in guiding the development of models that are not only powerful outputs but also responsible and trustworthy for real-world applications.\n\n\n###### Language\nLanguage benchmarks assess the LLMs\u2019 proficiency in specific languages.\n\nGLUE\u00a0(Wang, 2018) and SuperGLUE\u00a0(Wang et\u00a0al., 2019) cover tasks from sentiment analysis to language inference, while CLUE\u00a0(Xu et\u00a0al., 2020) targets Chinese language.\n\nTypo-fixing\u00a0(Suzgun et\u00a0al., 2022) is also widely used.\n\n\n###### Reading Comprehension\nReading comprehension tasks test a model\u2019s ability to extract and infer information from text. Benchmarks like SQuAD\u00a0(Rajpurkar et\u00a0al., 2018), QuAC\u00a0(Choi et\u00a0al., 2018), and BoolQ\u00a0(Clark et\u00a0al., 2019) challenge models to understand passages and draw logical conclusions.\n",
    "topic": "The design and purpose of reading comprehension benchmarks, such as SQuAD, QuAC, and BoolQ, in assessing LLMs' ability to extract and infer information from text.",
    "response": "<output_format>\nQuestion: What critical skill do benchmarks like SQuAD, QuAC, and BoolQ primarily assess in large language models (LLMs), and how does this relate to their broader application?\n\nA: The ability to generate non-toxic and ethically aligned content.\nB: Proficiency in specific languages, as measured by GLUE and SuperGLUE.\nC: The capacity to extract and infer information from text, crucial for practical applications.\nD: The robustness against producing harmful outputs, as evaluated by datasets like RealToxicityPrompts.\n\nExplanation: While safety benchmarks ensure ethical behavior and robustness, reading comprehension benchmarks like SQuAD, QuAC, and BoolQ are specifically designed to test a model's ability to extract and infer information from text, which is fundamental for various practical applications requiring text understanding.\n\nCorrect Answer: C\n</output_format>\n\n<document_analysis>\nThe provided context discusses different types of benchmarks used to evaluate large language models (LLMs). The passage highlights safety benchmarks, language benchmarks, and reading comprehension benchmarks. However, the question is designed to focus on the reading comprehension benchmarks, particularly SQuAD, QuAC, and BoolQ, and their purpose in assessing LLMs.\n\nOption A refers to safety benchmarks, which are not directly related to reading comprehension tasks. Option B pertains to language benchmarks like GLUE and SuperGLUE, which test language proficiency rather than reading comprehension. Option D is about safety benchmarks, which ensure ethical behavior and robustness against harmful outputs. Option C accurately identifies the primary skill assessed by reading comprehension benchmarks\u2014extracting and inferring information from text\u2014and emphasizes its importance for practical applications.\n\nThe question is designed to be challenging because it requires distinguishing between different types of benchmarks and understanding the unique role of reading comprehension benchmarks in assessing a model's ability to understand and work with text.\n</document_analysis>",
    "question": "What critical skill do benchmarks like SQuAD, QuAC, and BoolQ primarily assess in large language models (LLMs), and how does this relate to their broader application?",
    "choices": {
      "A": "The ability to generate non-toxic and ethically aligned content.",
      "B": "Proficiency in specific languages, as measured by GLUE and SuperGLUE.",
      "C": "The capacity to extract and infer information from text, crucial for practical applications.",
      "D": "The robustness against producing harmful outputs, as evaluated by datasets like RealToxicityPrompts."
    },
    "answer": "C",
    "explanation": "While safety benchmarks ensure ethical behavior and robustness, reading comprehension benchmarks like SQuAD, QuAC, and BoolQ are specifically designed to test a model's ability to extract and infer information from text, which is fundamental for various practical applications requiring text understanding.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 9,
    "question_token_count": 37,
    "avg_answer_token_count": 16
  },
  {
    "context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n",
    "topic": "Explain the mathematical formula for measuring correctness in dynamic benchmarks and interpret its components.",
    "response": "<output_format>\nQuestion: What does the formula for measuring the correctness of dynamic benchmarks represent, and what are the key components involved?\nA: It represents the expected alignment between the transformed dataset's outputs and their corresponding ground truth values.\nB: It measures the accuracy of the transformation process in generating the dataset.\nC: It calculates the probability that the benchmark results are reliable.\nD: It assesses the consistency between the benchmark's output and the ground truth.\nExplanation: The formula quantifies the correctness of the dynamic benchmark by comparing the outputs of the transformations to their ground truth values using a scoring function. The key components are the input/output pairs of the transformation, the ground truth provided by an oracle, and the scoring function that evaluates the alignment.\nCorrect Answer: A\n</output_format>\n\n<document_analysis>\nThe context describes a formula for measuring the correctness of dynamic benchmarks. It involves the input and output of transformations (\\(\\mathcal{X}_i\\) and \\(\\mathcal{Y}_i\\)), an oracle function (\\(\\mathcal{G}(\u22c5)\\)) that provides the ground truth, and a scoring function (\\(\\mathcal{S}(\u22c5)\\)). The formula aims to measure the expected alignment between the transformed dataset's outputs and their corresponding ground truth values. While the other options touch upon related concepts, they do not precisely capture the essence of the formula described, which is the alignment measurement between transformed outputs and ground truth.\n\nThe formula is designed to ensure that the dynamic benchmark maintains correctness relative to the ground truth, thus providing a reliable and valid evaluation of LLMs. The question is crafted to test the candidate's understanding of the formula's purpose and its key components, ensuring a deep engagement with the material.\n</document_analysis>",
    "question": "What does the formula for measuring the correctness of dynamic benchmarks represent, and what are the key components involved?",
    "choices": {
      "A": "It represents the expected alignment between the transformed dataset's outputs and their corresponding ground truth values.",
      "B": "It measures the accuracy of the transformation process in generating the dataset.",
      "C": "It calculates the probability that the benchmark results are reliable.",
      "D": "It assesses the consistency between the benchmark's output and the ground truth."
    },
    "answer": "A",
    "explanation": "The formula quantifies the correctness of the dynamic benchmark by comparing the outputs of the transformations to their ground truth values using a scoring function. The key components are the input/output pairs of the transformation, the ground truth provided by an oracle, and the scoring function that evaluates the alignment.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "question_token_count": 22,
    "avg_answer_token_count": 15
  },
  {
    "context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n",
    "topic": "Analysis of model behavior under different conditions to detect data contamination, such as through masked inputs and partial completions.",
    "response": "<output_format>\nQuestion: How can post-hoc detection methods reveal model behavior under conditions that indicate data contamination, and what techniques are employed to ensure robustness in identifying such behaviors?\n\nA: By analyzing model responses to masked inputs and partial completions to detect memorization patterns and by comparing performance across different datasets.\nB: Through direct overlap detection using n-gram matching and embedding-based similarity to identify exact matches and near-matches in the test and train sets.\nC: By evaluating the model's preference for original versus paraphrased test cases and using improved mapping metrics to assess embedding-based similarity.\nD: Using post-hoc detection to monitor label protection mechanisms and ensuring transparency in performance metrics.\n\nExplanation: The question requires a deep understanding of the nuances in post-hoc detection methods, particularly how they analyze model behavior under specific conditions to detect data contamination. It challenges the respondent to recognize the importance of various techniques such as masked inputs, partial completions, and embedding-based similarity, while also understanding their role in ensuring robust identification of contamination.\n\nCorrect Answer: A\n</output_format>\n\n<document_analysis>\nThe context discusses various post-hoc detection techniques aimed at mitigating data contamination in models. It highlights the use of masked inputs and partial completions as methods to observe model behavior under different conditions. These techniques are crucial for detecting memorization and other forms of contamination. While other methods like n-gram matching and embedding-based similarity are mentioned, the focus of the question is on how these techniques contribute to revealing model behavior under conditions that indicate contamination.\n\nOption A correctly captures the essence of the post-hoc detection methods discussed, emphasizing the analysis of model responses to masked inputs and partial completions. Options B, C, and D, while related to the broader topic of post-hoc detection, do not fully encapsulate the challenge of revealing model behavior under specific conditions that indicate data contamination.\n</document_analysis>",
    "question": "How can post-hoc detection methods reveal model behavior under conditions that indicate data contamination, and what techniques are employed to ensure robustness in identifying such behaviors?",
    "choices": {
      "A": "By analyzing model responses to masked inputs and partial completions to detect memorization patterns and by comparing performance across different datasets.",
      "B": "Through direct overlap detection using n-gram matching and embedding-based similarity to identify exact matches and near-matches in the test and train sets.",
      "C": "By evaluating the model's preference for original versus paraphrased test cases and using improved mapping metrics to assess embedding-based similarity.",
      "D": "Using post-hoc detection to monitor label protection mechanisms and ensuring transparency in performance metrics."
    },
    "answer": "A",
    "explanation": "The question requires a deep understanding of the nuances in post-hoc detection methods, particularly how they analyze model behavior under specific conditions to detect data contamination. It challenges the respondent to recognize the importance of various techniques such as masked inputs, partial completions, and embedding-based similarity, while also understanding their role in ensuring robust identification of contamination.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "question_token_count": 30,
    "avg_answer_token_count": 23
  },
  {
    "context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n",
    "topic": "Value and Future Potential of Continuous Collection of Benchmarking Methods through GitHub Repository",
    "response": "<output_format>\nQuestion: What is the primary challenge in the transition from static to dynamic benchmarking methods for LLMs as highlighted in the document, and how does the proposed solution address this challenge?\n\nA: The primary challenge is the lack of standardized criteria for evaluating dynamic benchmarks, and the proposed solution is to establish a series of optimal design principles for dynamic benchmarking.\n\nB: The primary challenge is the inherent limitations of enhancing static benchmarks, and the proposed solution is to focus solely on improving static benchmarks further.\n\nC: The primary challenge is the difficulty in collecting enough data for dynamic benchmarks, and the proposed solution is to create a GitHub repository to continuously gather various benchmarking methods.\n\nD: The primary challenge is the reliance on Internet-derived training corpora, and the proposed solution is to reduce the size of the training corpora used for LLMs.\n\nExplanation: The question requires a deep understanding of the document's main argument and identifies the critical gap in the current benchmarking practices. It challenges the respondent to recognize the importance of standardization in dynamic benchmarking and the innovative approach of using a GitHub repository for continuous collection and improvement of benchmarking methods.\n\nCorrect Answer: A\n</output_format>\n\n<document_analysis>\nThe context discusses the shift from static to dynamic benchmarking methods for LLMs to address data contamination risks. It identifies the lack of standardized criteria for evaluating dynamic benchmarks as a critical gap and proposes establishing optimal design principles to address this issue. The document also mentions the creation of a GitHub repository to continuously collect benchmarking methods. Answer option A accurately captures both the primary challenge and the proposed solution, while the other options either misrepresent or oversimplify the content.\n</document_analysis>",
    "question": "What is the primary challenge in the transition from static to dynamic benchmarking methods for LLMs as highlighted in the document, and how does the proposed solution address this challenge?",
    "choices": {
      "A": "The primary challenge is the lack of standardized criteria for evaluating dynamic benchmarks, and the proposed solution is to establish a series of optimal design principles for dynamic benchmarking.",
      "B": "The primary challenge is the inherent limitations of enhancing static benchmarks, and the proposed solution is to focus solely on improving static benchmarks further.",
      "C": "The primary challenge is the difficulty in collecting enough data for dynamic benchmarks, and the proposed solution is to create a GitHub repository to continuously gather various benchmarking methods.",
      "D": "The primary challenge is the reliance on Internet-derived training corpora, and the proposed solution is to reduce the size of the training corpora used for LLMs."
    },
    "answer": "A",
    "explanation": "The question requires a deep understanding of the document's main argument and identifies the critical gap in the current benchmarking practices. It challenges the respondent to recognize the importance of standardization in dynamic benchmarking and the innovative approach of using a GitHub repository for continuous collection and improvement of benchmarking methods.",
    "answer_correctness_score": 9,
    "explanation_validity_score": 7,
    "question_clarity_score": 1,
    "question_difficulty_score": 8,
    "question_groundedness_score": 1,
    "question_token_count": 34,
    "avg_answer_token_count": 31
  },
  {
    "context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n",
    "topic": "The potential benefits and challenges of dynamic benchmarking methods for LLMs.",
    "response": "<output_format>\nQuestion: What key challenge does dynamic benchmarking for LLMs face that limits its current utility despite showing promise?\nA: Lack of standardized evaluation procedures\nB: Insufficient coverage of recent methods and tools\nC: High computational costs\nD: Difficulty in achieving consistent results across different datasets\nExplanation: Dynamic benchmarking shows promise but faces challenges in reliability and reproducibility, which can hinder its current utility despite its potential benefits.\nCorrect Answer: A\n</output_format>\n\n<document_analysis>\nThoughtful Content Examination:\nThe context discusses the limitations and challenges of dynamic benchmarking for LLMs, highlighting both the potential benefits and the current obstacles. Key points include the vulnerability of static methods to contamination as datasets grow, the promise of dynamic methods, and the challenges they face in reliability and reproducibility.\n\nConcept Exploration:\nDynamic benchmarking is highlighted as promising but requires standardization and further validation. The challenge of achieving consistent results across different datasets is a crucial issue that limits its current utility.\n\nStrategic Complexity Calibration:\nThis question is rated 8 out of 10 in terms of difficulty. It requires a deep understanding of the nuances between static and dynamic benchmarking methods and their respective advantages and limitations.\n\nIntentional Question Planning:\nThe question is designed to probe the domain expert's ability to identify the specific challenge that hinders the practical application of dynamic benchmarking methods, even though they show promise. This encourages reflection on the practical aspects of benchmarking and the importance of standardization and consistency.\n</document_analysis>",
    "question": "What key challenge does dynamic benchmarking for LLMs face that limits its current utility despite showing promise?",
    "choices": {
      "A": "Lack of standardized evaluation procedures",
      "B": "Insufficient coverage of recent methods and tools",
      "C": "High computational costs",
      "D": "Difficulty in achieving consistent results across different datasets"
    },
    "answer": "A",
    "explanation": "Dynamic benchmarking shows promise but faces challenges in reliability and reproducibility, which can hinder its current utility despite its potential benefits.",
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 7,
    "question_difficulty_score": 6,
    "question_groundedness_score": 9,
    "question_token_count": 20,
    "avg_answer_token_count": 7
  },
  {
    "context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n",
    "topic": "Use of Query Templates with Placeholder Variables in GSM-Symbolic for Benchmark Creation",
    "response": "<document_analysis>\nThe context discusses various methods for creating benchmark tests, particularly focusing on GSM-Symbolic, which uses query templates with placeholder variables. The text does not contain any irrelevant or promotional information. However, the discussion is quite high-level and does not provide deep, nuanced details that would be necessary for a highly challenging question. Nonetheless, we will proceed to craft a question that challenges domain experts on their understanding of the implications and subtleties of using placeholder variables in GSM-Symbolic.\n</document_analysis>\n\n<output_format>\nQuestion: How does the use of placeholder variables in GSM-Symbolic's query templates impact the robustness of the generated benchmarks compared to rule-based generation methods, and what are the potential implications for data contamination in LLM evaluations?\n\nA: It enhances robustness by allowing for a more varied set of problem instances, reducing the likelihood of data contamination due to repeated problem instances.\nB: It increases the risk of data contamination as placeholders may reuse previously seen problem structures, but it also reduces the collision probability of generated test cases.\nC: It has no impact on robustness or data contamination since both methods rely on predefined rules and templates.\nD: It decreases the collision probability significantly but does not address the issue of data contamination in benchmark creation.\n\nExplanation: The correct answer is A. Using placeholder variables in GSM-Symbolic allows for a broader range of problem instances, reducing the chance that any particular problem instance is repeated. This diversity in problem instances makes the benchmarks more robust and less prone to data contamination. Rule-based generation, on the other hand, might generate similar problem instances due to limited rule variations, increasing the risk of contamination.\n\nCorrect Answer: A\n</output_format>",
    "question": "How does the use of placeholder variables in GSM-Symbolic's query templates impact the robustness of the generated benchmarks compared to rule-based generation methods, and what are the potential implications for data contamination in LLM evaluations?",
    "choices": {
      "A": "It enhances robustness by allowing for a more varied set of problem instances, reducing the likelihood of data contamination due to repeated problem instances.",
      "B": "It increases the risk of data contamination as placeholders may reuse previously seen problem structures, but it also reduces the collision probability of generated test cases.",
      "C": "It has no impact on robustness or data contamination since both methods rely on predefined rules and templates.",
      "D": "It decreases the collision probability significantly but does not address the issue of data contamination in benchmark creation."
    },
    "answer": "A",
    "explanation": "The correct answer is A. Using placeholder variables in GSM-Symbolic allows for a broader range of problem instances, reducing the chance that any particular problem instance is repeated. This diversity in problem instances makes the benchmarks more robust and less prone to data contamination. Rule-based generation, on the other hand, might generate similar problem instances due to limited rule variations, increasing the risk of contamination.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "question_token_count": 43,
    "avg_answer_token_count": 24
  },
  {
    "context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Challenges faced by static benchmarks as LLMs evolve and the impact on benchmark reliability.",
    "response": "<document_analysis>\nThe context discusses the evolving nature of LLMs and the challenges faced by static benchmarks. It highlights the importance of developing comprehensive benchmarks to assess various aspects of model performance. However, it also points out that static benchmarks may become outdated or contaminated over time, which affects their reliability. This suggests that the question should focus on the implications of using static benchmarks for evolving LLMs and the potential solutions mentioned.\n</document_analysis>\n\n<output_format>\nQuestion: What critical issue arises when using static benchmarks for evaluating the performance of rapidly evolving LLMs, and what is the proposed solution to address this issue?\n\nA: Static benchmarks may no longer accurately reflect the capabilities of newer models due to their rapid evolution, leading to potential data contamination, but no solution has been proposed yet.\nB: Static benchmarks may become too easy for advanced LLMs, leading to unreliable results, and dynamic benchmarks have been proposed to adapt to the evolving models.\nC: Static benchmarks are insufficient for assessing the general-purpose task-solving abilities of LLMs, necessitating the development of new benchmark types.\nD: Static benchmarks are effective for evaluating LLMs as they remain consistent over time, and no issues arise during rapid model evolution.\nExplanation: The correct answer is B. Static benchmarks may become too easy for advanced LLMs, leading to unreliable results. Dynamic benchmarks have been proposed to adapt to the evolving models and address the issue of data contamination.\nCorrect Answer: B\n</output_format>",
    "question": "What critical issue arises when using static benchmarks for evaluating the performance of rapidly evolving LLMs, and what is the proposed solution to address this issue?",
    "choices": {
      "A": "Static benchmarks may no longer accurately reflect the capabilities of newer models due to their rapid evolution, leading to potential data contamination, but no solution has been proposed yet.",
      "B": "Static benchmarks may become too easy for advanced LLMs, leading to unreliable results, and dynamic benchmarks have been proposed to adapt to the evolving models.",
      "C": "Static benchmarks are insufficient for assessing the general-purpose task-solving abilities of LLMs, necessitating the development of new benchmark types.",
      "D": "Static benchmarks are effective for evaluating LLMs as they remain consistent over time, and no issues arise during rapid model evolution."
    },
    "answer": "B",
    "explanation": "The correct answer is B. Static benchmarks may become too easy for advanced LLMs, leading to unreliable results. Dynamic benchmarks have been proposed to adapt to the evolving models and address the issue of data contamination.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "question_token_count": 30,
    "avg_answer_token_count": 28
  },
  {
    "context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n",
    "topic": "Explain the mechanism of using a \"No Derivatives\" license in combination with public key encryption to secure test data, and discuss its effectiveness in preventing automated reuse.",
    "response": "<document_analysis>\nThe provided context discusses the use of encryption with a \"No Derivatives\" license to secure test data. It mentions that Jacovi et al. (2023) propose combining public key encryption with a \"No Derivatives\" license to prevent automated reuse. However, the context does not provide extensive details on the exact mechanism or the effectiveness of this combination. There is a brief mention of Yang et al. (2023) showing that advanced decontamination methods can be defeated by minor text variations, which implies that additional layers of security are necessary. \n\nThe context also highlights the limitations of such methods, including the need for strong key management and the introduction of computational overhead. Given the limited information, we can infer that the question should focus on the nuances of combining these two mechanisms and their effectiveness.\n</document_analysis>\n\n<output_format>\nQuestion: How does combining public key encryption with a \"No Derivatives\" license enhance the security of test data against automated reuse, and what challenges might still exist despite this combination?\n\nA: Public key encryption ensures data confidentiality, while the \"No Derivatives\" license prevents unauthorized modifications, thus safeguarding against automated reuse.\nB: The combination allows for secure transmission but fails to address the issue of data being easily extracted through advanced decontamination techniques.\nC: Both mechanisms ensure data integrity and prevent any form of reuse or modification, making automated attacks ineffective.\nD: Public key encryption alone is sufficient to prevent automated reuse, making the \"No Derivatives\" license redundant in this context.\n\nExplanation: While public key encryption protects the confidentiality of the data, the \"No Derivatives\" license is intended to prevent unauthorized derivative works. However, advanced decontamination techniques can bypass these protections, making the combination less effective than initially thought.\nCorrect Answer: B\n</output_format>",
    "question": "How does combining public key encryption with a \"No Derivatives\" license enhance the security of test data against automated reuse, and what challenges might still exist despite this combination?",
    "choices": {
      "A": "Public key encryption ensures data confidentiality, while the \"No Derivatives\" license prevents unauthorized modifications, thus safeguarding against automated reuse.",
      "B": "The combination allows for secure transmission but fails to address the issue of data being easily extracted through advanced decontamination techniques.",
      "C": "Both mechanisms ensure data integrity and prevent any form of reuse or modification, making automated attacks ineffective.",
      "D": "Public key encryption alone is sufficient to prevent automated reuse, making the \"No Derivatives\" license redundant in this context."
    },
    "answer": "B",
    "explanation": "While public key encryption protects the confidentiality of the data, the \"No Derivatives\" license is intended to prevent unauthorized derivative works. However, advanced decontamination techniques can bypass these protections, making the combination less effective than initially thought.",
    "answer_correctness_score": 8,
    "explanation_validity_score": 9,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "question_token_count": 34,
    "avg_answer_token_count": 23
  },
  {
    "context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n",
    "topic": "Definition and Components of Static Benchmarks",
    "response": "<document_analysis>\nThe provided context focuses on defining static benchmarks and their components. It emphasizes the importance of these benchmarks in evaluating model performance across various tasks. The key elements of a static benchmark are the seed dataset, input prompts, expected outputs, and a scoring function. The context does not delve deeply into the nuances or potential challenges associated with these components, nor does it provide specific examples or detailed explanations.\n\nGiven this, the question should probe the deeper understanding of these components and their application, encouraging critical reflection and insight.\n</document_analysis>\n\n<output_format>\nQuestion: What critical aspect of a static benchmark can most significantly influence the perceived performance of a language model, even if the underlying scoring function is robust and well-defined?\nA: The quality and diversity of the input prompts (\ud835\udcb3).\nB: The precision and coverage of the expected outputs (\ud835\udcb4).\nC: The complexity of the scoring function (\ud835\udcae(\u22c5)).\nD: The size and representativeness of the seed dataset (\ud835\udc9f).\nExplanation: The choice of input prompts can dramatically affect the model's perceived performance because the same scoring function might yield different results based on the nature and variety of the inputs. High-quality, diverse prompts can better showcase the model's capabilities, while poorly chosen prompts might obscure its true abilities.\nCorrect Answer: A\n</output_format>",
    "question": "What critical aspect of a static benchmark can most significantly influence the perceived performance of a language model, even if the underlying scoring function is robust and well-defined?",
    "choices": {
      "A": "The quality and diversity of the input prompts (\ud835\udcb3).",
      "B": "The precision and coverage of the expected outputs (\ud835\udcb4).",
      "C": "The complexity of the scoring function (\ud835\udcae(\u22c5)).",
      "D": "The size and representativeness of the seed dataset (\ud835\udc9f)."
    },
    "answer": "A",
    "explanation": "The choice of input prompts can dramatically affect the model's perceived performance because the same scoring function might yield different results based on the nature and variety of the inputs. High-quality, diverse prompts can better showcase the model's capabilities, while poorly chosen prompts might obscure its true abilities.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 1,
    "question_difficulty_score": 6,
    "question_groundedness_score": 4,
    "question_token_count": 32,
    "avg_answer_token_count": 13
  },
  {
    "context": "###### Safety\nSafety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content. Datasets such as RealToxicityPrompts\u00a0(Gehman et\u00a0al., 2020) and ToxiGen\u00a0(Hartvigsen et\u00a0al., 2022) assess resilience against producing harmful outputs.\n\nBy providing a controlled environment to measure these aspects, safety benchmarks play a critical role in guiding the development of models that are not only powerful outputs but also responsible and trustworthy for real-world applications.\n\n\n###### Language\nLanguage benchmarks assess the LLMs\u2019 proficiency in specific languages.\n\nGLUE\u00a0(Wang, 2018) and SuperGLUE\u00a0(Wang et\u00a0al., 2019) cover tasks from sentiment analysis to language inference, while CLUE\u00a0(Xu et\u00a0al., 2020) targets Chinese language.\n\nTypo-fixing\u00a0(Suzgun et\u00a0al., 2022) is also widely used.\n\n\n###### Reading Comprehension\nReading comprehension tasks test a model\u2019s ability to extract and infer information from text. Benchmarks like SQuAD\u00a0(Rajpurkar et\u00a0al., 2018), QuAC\u00a0(Choi et\u00a0al., 2018), and BoolQ\u00a0(Clark et\u00a0al., 2019) challenge models to understand passages and draw logical conclusions.\n",
    "topic": "The importance and methods of evaluating LLMs' safety through benchmark datasets.",
    "response": "<document_analysis>\nThe provided context focuses primarily on different types of benchmarks used for evaluating LLMs, particularly emphasizing safety benchmarks. It mentions safety benchmarks like RealToxicityPrompts and ToxiGen, which assess the resilience of LLMs against harmful outputs. The rest of the context discusses other types of benchmarks such as language proficiency and reading comprehension, which are not directly related to the evaluation of LLMs' safety. Therefore, the question should be crafted around the concept of safety benchmarks and their importance.\n</document_analysis>\n\n<output_format>\nQuestion: Why are safety benchmarks critical for the development of LLMs, and what specific method does RealToxicityPrompts employ to evaluate LLMs?\nA: They ensure LLMs produce non-toxic and ethically aligned content, using controlled environments to assess resilience against harmful outputs.\nB: They enhance the language proficiency of LLMs by testing their ability to understand and generate content in multiple languages.\nC: They improve the reading comprehension skills of LLMs by challenging them to extract and infer information from text.\nD: They solely focus on the efficiency and speed of LLMs in processing large volumes of data.\nExplanation: RealToxicityPrompts evaluates LLMs by measuring their resilience against producing harmful outputs, which is a key aspect of safety benchmarks. This method directly addresses the robustness of LLMs in generating non-toxic content.\nCorrect Answer: A\n</output_format>",
    "question": "Why are safety benchmarks critical for the development of LLMs, and what specific method does RealToxicityPrompts employ to evaluate LLMs?",
    "choices": {
      "A": "They ensure LLMs produce non-toxic and ethically aligned content, using controlled environments to assess resilience against harmful outputs.",
      "B": "They enhance the language proficiency of LLMs by testing their ability to understand and generate content in multiple languages.",
      "C": "They improve the reading comprehension skills of LLMs by challenging them to extract and infer information from text.",
      "D": "They solely focus on the efficiency and speed of LLMs in processing large volumes of data."
    },
    "answer": "A",
    "explanation": "RealToxicityPrompts evaluates LLMs by measuring their resilience against producing harmful outputs, which is a key aspect of safety benchmarks. This method directly addresses the robustness of LLMs in generating non-toxic content.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 7,
    "question_difficulty_score": 6,
    "question_groundedness_score": 8,
    "question_token_count": 30,
    "avg_answer_token_count": 21
  },
  {
    "context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n",
    "topic": "How hybrid approaches integrate different elements of dynamic benchmark construction.",
    "response": "<document_analysis>\nThe provided context discusses the interpretability of dynamic benchmarking and categorizes dynamic benchmarks into four types: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches. The focus on hybrid approaches indicates they integrate elements of both rule-based and LLM-based methods. However, the context does not provide specific details on how these integration points work, leaving room for deeper exploration and reflection.\n</document_analysis>\n\n<output_format>\nQuestion: How do hybrid approaches in dynamic benchmark construction typically integrate the strengths of rule-based and LLM-based methods?\nA: By combining predefined rules with LLM-generated data to validate each other's accuracy.\nB: By alternating between rule-based and LLM-based methods to ensure comprehensive coverage.\nC: By using LLMs to generate data and rules to filter out biased or irrelevant information.\nD: By manually validating LLM outputs against rule-based criteria to enhance reliability.\nExplanation: The correct answer requires understanding that hybrid approaches leverage both predefined rules and LLM-generated data, but the question asks for a deeper integration rather than just validation or coverage. Option A captures this integration most accurately.\nCorrect Answer: A\n</output_format>",
    "question": "How do hybrid approaches in dynamic benchmark construction typically integrate the strengths of rule-based and LLM-based methods?",
    "choices": {
      "A": "By combining predefined rules with LLM-generated data to validate each other's accuracy.",
      "B": "By alternating between rule-based and LLM-based methods to ensure comprehensive coverage.",
      "C": "By using LLMs to generate data and rules to filter out biased or irrelevant information.",
      "D": "By manually validating LLM outputs against rule-based criteria to enhance reliability."
    },
    "answer": "A",
    "explanation": "The correct answer requires understanding that hybrid approaches leverage both predefined rules and LLM-generated data, but the question asks for a deeper integration rather than just validation or coverage. Option A captures this integration most accurately.",
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 4,
    "question_token_count": 21,
    "avg_answer_token_count": 15
  },
  {
    "context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n",
    "topic": "The limitations of the current survey due to the rapid evolution of LLM development and unaccounted recent methods.",
    "response": "<document_analysis>\nThe context discusses the limitations of the current survey in the context of LLM benchmarking. Key points include the rapid evolution of LLM development, the potential unaccounted recent methods, and the preliminary nature of the proposed dynamic benchmarking criteria. The survey also mentions its focus on high-level concepts rather than detailed technical implementations.\n\nGiven the emphasis on the rapid evolution of LLMs and the unaccounted recent methods, a highly probing question would require a deep understanding of the implications of these factors on the survey's comprehensiveness and future relevance.\n</document_analysis>\n\n<output_format>\nQuestion: How does the rapid evolution of LLM development and the potential unaccounted recent methods impact the comprehensiveness and future relevance of the current survey's findings?\nA: The rapid evolution means that the survey might have overlooked critical new methods, reducing its comprehensiveness and future relevance.\nB: Recent developments will likely be incorporated into future surveys, so the current findings remain robust.\nC: The unaccounted recent methods suggest that the survey's findings are broadly applicable but require periodic updates.\nD: The survey's focus on high-level concepts ensures its comprehensiveness and future relevance despite the rapid evolution.\nExplanation: The correct answer is A because the rapid evolution of LLM development implies that many new and potentially crucial methods may not have been included in the survey. This makes the survey less comprehensive and reduces its future relevance as newer, more relevant methods continue to emerge.\nCorrect Answer: A\n</output_format>",
    "question": "How does the rapid evolution of LLM development and the potential unaccounted recent methods impact the comprehensiveness and future relevance of the current survey's findings?",
    "choices": {
      "A": "The rapid evolution means that the survey might have overlooked critical new methods, reducing its comprehensiveness and future relevance.",
      "B": "Recent developments will likely be incorporated into future surveys, so the current findings remain robust.",
      "C": "The unaccounted recent methods suggest that the survey's findings are broadly applicable but require periodic updates.",
      "D": "The survey's focus on high-level concepts ensures its comprehensiveness and future relevance despite the rapid evolution."
    },
    "answer": "A",
    "explanation": "The correct answer is A because the rapid evolution of LLM development implies that many new and potentially crucial methods may not have been included in the survey. This makes the survey less comprehensive and reduces its future relevance as newer, more relevant methods continue to emerge.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 9,
    "question_token_count": 32,
    "avg_answer_token_count": 21
  },
  {
    "context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n",
    "topic": "Template-Based MMLU-CF Generation Process for Multiple-Choice Questions",
    "response": "<output_format>\nQuestion: How does MMLU-CF ensure the authenticity and diversity of its generated multiple-choice questions?\nA: By shuffling answer choices and replacing incorrect options with \"None of the other choices.\"\nB: Using predefined rules to minimize collision probability.\nC: Applying query templates with placeholder variables that are dynamically filled.\nD: Relying on temporal cutoffs to incorporate recent competition data.\nExplanation: MMLU-CF ensures the authenticity and diversity of its generated questions through the process of shuffling answer choices and replacing incorrect options with \"None of the other choices,\" which maintains the integrity of the multiple-choice format while introducing variability. Options A, C, and D describe other methods but do not align with the specific approach used by MMLU-CF as detailed in the context.\nCorrect Answer: A\n</output_format>\n\n<document_analysis>\nThe context discusses various methods for generating test cases, including rule-based generation, template-based approaches, and the specific technique used by MMLU-CF. MMLU-CF's method involves using templates for multiple-choice questions and then shuffling answer choices and replacing incorrect options with \"None of the other choices.\" This process ensures both authenticity and diversity. Other methods mentioned (rule-based generation, query templates, and temporal cutoffs) are distinct and do not involve the specifics of MMLU-CF\u2019s approach, making them plausible but incorrect options.\n</document_analysis>",
    "question": "How does MMLU-CF ensure the authenticity and diversity of its generated multiple-choice questions?",
    "choices": {
      "A": "By shuffling answer choices and replacing incorrect options with \"None of the other choices.\"",
      "B": "Using predefined rules to minimize collision probability.",
      "C": "Applying query templates with placeholder variables that are dynamically filled.",
      "D": "Relying on temporal cutoffs to incorporate recent competition data."
    },
    "answer": "A",
    "explanation": "MMLU-CF ensures the authenticity and diversity of its generated questions through the process of shuffling answer choices and replacing incorrect options with \"None of the other choices,\" which maintains the integrity of the multiple-choice format while introducing variability. Options A, C, and D describe other methods but do not align with the specific approach used by MMLU-CF as detailed in the context.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 2,
    "question_groundedness_score": 10,
    "question_token_count": 20,
    "avg_answer_token_count": 13
  },
  {
    "context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n",
    "topic": "Understanding and Mitigating In-distribution Contamination in LLM Benchmarks",
    "response": "<document_analysis>\nThe provided context discusses various methods to mitigate in-distribution contamination in LLM benchmarks. It highlights issues with pre-defined rules limiting sample diversity and the risk of in-distribution contamination. The context then introduces several techniques such as Auto-Dataset, StructEval, ITD, and VarBench, each addressing different aspects of benchmark rewriting and contamination mitigation.\n\nThe analysis reveals that the context is primarily focused on the methods used to address contamination, but does not delve deeply into the nuances of each method or the underlying principles. Therefore, the question should probe the deeper understanding of these techniques and their implications rather than surface-level facts.\n</document_analysis>\n\n<output_format>\nQuestion: How does Auto-Dataset ensure that the generated new samples retain both the stylistics and essential knowledge of the original samples while also presenting related questions at different cognitive levels?\nA: By employing LLMs to generate new samples that preserve the original style and knowledge but also introduce variations in complexity.\nB: Through the use of knowledge graphs to expand on the original concepts, thereby creating related questions at varying cognitive levels.\nC: By leveraging a contamination detector to first identify contaminated samples before prompting LLMs to rewrite them.\nD: Utilizing LLMs to replace variables in the original samples, thus generating new samples that are structurally similar but conceptually distinct.\nExplanation: The correct answer involves understanding that Auto-Dataset generates new samples by preserving the original style and knowledge but also introduces different cognitive levels through varied question formulations. The other options either refer to different techniques or miss the key aspect of retaining both style and knowledge.\nCorrect Answer: A\n</output_format>",
    "question": "How does Auto-Dataset ensure that the generated new samples retain both the stylistics and essential knowledge of the original samples while also presenting related questions at different cognitive levels?",
    "choices": {
      "A": "By employing LLMs to generate new samples that preserve the original style and knowledge but also introduce variations in complexity.",
      "B": "Through the use of knowledge graphs to expand on the original concepts, thereby creating related questions at varying cognitive levels.",
      "C": "By leveraging a contamination detector to first identify contaminated samples before prompting LLMs to rewrite them.",
      "D": "Utilizing LLMs to replace variables in the original samples, thus generating new samples that are structurally similar but conceptually distinct."
    },
    "answer": "A",
    "explanation": "The correct answer involves understanding that Auto-Dataset generates new samples by preserving the original style and knowledge but also introduces different cognitive levels through varied question formulations. The other options either refer to different techniques or miss the key aspect of retaining both style and knowledge.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "question_token_count": 34,
    "avg_answer_token_count": 22
  },
  {
    "context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n",
    "topic": "Privacy and security concerns associated with dynamic benchmarks and strategies to address them.",
    "response": "<document_analysis>\nThe context discusses the ethical considerations surrounding both static and dynamic benchmarks, emphasizing the need for transparency, fairness, and privacy. While the text mentions several aspects, it focuses significantly on the privacy and security concerns related to dynamic benchmarks and the measures needed to address them. Key points include the continuous collection and updating of data, which can lead to privacy issues, and the potential for misuse of benchmarking results.\n\nThe text does not provide specific technical details or examples, but it highlights the importance of designing benchmarking frameworks that consider these ethical dimensions. Given this, the question should encourage a deep reflection on the implications of dynamic benchmarks and strategies to mitigate their associated risks.\n</document_analysis>\n\n<output_format>\nQuestion: What is a crucial step in designing dynamic benchmarks to address privacy and security concerns, considering the continuous collection and updating of data?\nA: Implementing strong encryption and anonymization techniques for data.\nB: Establishing strict access controls and auditing mechanisms.\nC: Regularly conducting independent audits of the benchmarking process.\nD: Publishing detailed documentation on the data sources and collection methods.\nExplanation: The correct approach involves a multi-layered strategy that includes robust data protection measures and oversight to ensure privacy and security.\nCorrect Answer: A\n</output_format>",
    "question": "What is a crucial step in designing dynamic benchmarks to address privacy and security concerns, considering the continuous collection and updating of data?",
    "choices": {
      "A": "Implementing strong encryption and anonymization techniques for data.",
      "B": "Establishing strict access controls and auditing mechanisms.",
      "C": "Regularly conducting independent audits of the benchmarking process.",
      "D": "Publishing detailed documentation on the data sources and collection methods."
    },
    "answer": "A",
    "explanation": "The correct approach involves a multi-layered strategy that includes robust data protection measures and oversight to ensure privacy and security.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 8,
    "question_clarity_score": 4,
    "question_difficulty_score": 6,
    "question_groundedness_score": 8,
    "question_token_count": 26,
    "avg_answer_token_count": 11
  },
  {
    "context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Structure of a Dynamic Benchmark Dataset",
    "response": "<output_format>\nQuestion: What is the primary reason for employing a transformation function \\( T(\\cdot) \\) in a dynamic benchmark dataset and how does it impact the evaluation of LLMs?\n\nA: To ensure the static benchmark dataset remains unchanged during evaluation, avoiding any form of data contamination.\nB: To modify the data set dynamically to prevent possible data contamination, ensuring a fair evaluation of LLMs.\nC: To facilitate easier access to the training dataset, thus enhancing the transparency of the evaluation process.\nD: To increase the memorization capacity of the LLMs being evaluated, thereby improving their performance metrics.\n\nExplanation: The transformation function \\( T(\\cdot) \\) is crucial because it modifies the data set during benchmarking to avoid possible data contamination, which impacts the evaluation of LLMs by ensuring fairness and integrity of the results. Option A is incorrect because the function's purpose is to change the dataset, not keep it unchanged. Option C is incorrect as the function's main goal is not related to accessing the training dataset but rather to preventing contamination. Option D is incorrect because increasing memorization is not the goal; instead, the focus is on maintaining the fidelity of the evaluation.\n\nCorrect Answer: B\n</output_format>",
    "question": "What is the primary reason for employing a transformation function \\( T(\\cdot) \\) in a dynamic benchmark dataset and how does it impact the evaluation of LLMs?",
    "choices": {
      "A": "To ensure the static benchmark dataset remains unchanged during evaluation, avoiding any form of data contamination.",
      "B": "To modify the data set dynamically to prevent possible data contamination, ensuring a fair evaluation of LLMs.",
      "C": "To facilitate easier access to the training dataset, thus enhancing the transparency of the evaluation process.",
      "D": "To increase the memorization capacity of the LLMs being evaluated, thereby improving their performance metrics."
    },
    "answer": "B",
    "explanation": "The transformation function \\( T(\\cdot) \\) is crucial because it modifies the data set during benchmarking to avoid possible data contamination, which impacts the evaluation of LLMs by ensuring fairness and integrity of the results. Option A is incorrect because the function's purpose is to change the dataset, not keep it unchanged. Option C is incorrect as the function's main goal is not related to accessing the training dataset but rather to preventing contamination. Option D is incorrect because increasing memorization is not the goal; instead, the focus is on maintaining the fidelity of the evaluation.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "question_token_count": 33,
    "avg_answer_token_count": 19
  },
  {
    "context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n",
    "topic": "Strategies for Identifying and Mitigating Data Contamination in LLM Training Data",
    "response": "<document_analysis>\nThe provided context discusses two types of data contamination in the training data of large language models (LLMs): exact contamination and syntactic contamination. Exact contamination involves direct duplicates between the training and test datasets, while syntactic contamination refers to test data points that can be derived from the training dataset through syntactic transformations. The context does not contain any irrelevant or promotional information and provides a clear and valuable educational insight into the topic.\n\nGiven the nature of the context, we aim to generate a highly challenging question that probes the nuanced understanding of these concepts and their implications. The question should encourage reflection on the subtleties of data contamination and its mitigation strategies.\n</document_analysis>\n\n<output_format>\nQuestion: In the context of mitigating data contamination in LLM training data, which of the following scenarios would be classified as syntactic contamination, assuming no exact duplicates exist?\n\nA: A test example containing a typo that matches a training example exactly upon correcting the typo.\nB: A test example identical to a training example except for minor punctuation differences.\nC: A test example that, through a series of morphological variations, closely mirrors a training example's lexical meaning but maintains distinct wording.\nD: A test example where a code snippet from a benchmark implementation is included in the training corpus verbatim.\n\nExplanation: Syntactic contamination involves finding a test data point in the training dataset after applying syntactic transformations such as punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning. Option C accurately describes this scenario, as it involves transforming a training example through these methods to create a test example with a similar meaning but different wording.\n\nCorrect Answer: C\n</output_format>",
    "question": "In the context of mitigating data contamination in LLM training data, which of the following scenarios would be classified as syntactic contamination, assuming no exact duplicates exist?",
    "choices": {
      "A": "A test example containing a typo that matches a training example exactly upon correcting the typo.",
      "B": "A test example identical to a training example except for minor punctuation differences.",
      "C": "A test example that, through a series of morphological variations, closely mirrors a training example's lexical meaning but maintains distinct wording.",
      "D": "A test example where a code snippet from a benchmark implementation is included in the training corpus verbatim."
    },
    "answer": "C",
    "explanation": "Syntactic contamination involves finding a test data point in the training dataset after applying syntactic transformations such as punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning. Option C accurately describes this scenario, as it involves transforming a training example through these methods to create a test example with a similar meaning but different wording.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "question_token_count": 31,
    "avg_answer_token_count": 20
  },
  {
    "context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n",
    "topic": "Data Contamination Risk in Live Benchmarks Using Recent Competition Problems",
    "response": "<output_format>\nQuestion: How does the risk of data contamination in live benchmarks using recent competition problems affect the reliability of LLM evaluations, and what method could mitigate this issue?\n\nA: It has no significant impact on reliability since recent problems are sufficiently different.\nB: It can lead to biased results due to reused problems, and rule-based generation can mitigate this by synthesizing new test cases.\nC: It improves reliability by ensuring that LLMs are tested on current trends.\nD: It decreases the need for continuous human updates to the benchmark systems.\nExplanation: The context highlights the risk of data contamination in live benchmarks using recent competition problems, which can lead to biased results. Rule-based generation can mitigate this issue by synthesizing new test cases, thereby reducing the likelihood of reusing previously seen problems.\nCorrect Answer: B\n</output_format>",
    "question": "How does the risk of data contamination in live benchmarks using recent competition problems affect the reliability of LLM evaluations, and what method could mitigate this issue?",
    "choices": {
      "A": "It has no significant impact on reliability since recent problems are sufficiently different.",
      "B": "It can lead to biased results due to reused problems, and rule-based generation can mitigate this by synthesizing new test cases.",
      "C": "It improves reliability by ensuring that LLMs are tested on current trends.",
      "D": "It decreases the need for continuous human updates to the benchmark systems."
    },
    "answer": "B",
    "explanation": "The context highlights the risk of data contamination in live benchmarks using recent competition problems, which can lead to biased results. Rule-based generation can mitigate this issue by synthesizing new test cases, thereby reducing the likelihood of reusing previously seen problems.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 8,
    "question_token_count": 30,
    "avg_answer_token_count": 17
  },
  {
    "context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n",
    "topic": "The importance of complexity control in the design of efficient LLM benchmarks.",
    "response": "<output_format>\nQuestion: What is the primary reason for the inefficiency in certain dynamic benchmarks despite their advantages over static benchmarks?\nA: Insufficient complexity control leading to overly simplistic or redundant sample generation.\nB: Lack of standardized evaluation criteria hindering comparison across different benchmarks.\nC: Over-reliance on human annotation for reliability, increasing costs and time.\nD: High computational requirements for real-time generation of large datasets.\nExplanation: The context mentions that \"We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\" This indicates that complexity control is crucial for efficiency but is often overlooked in certain dynamic benchmarks.\nCorrect Answer: A\n</output_format>",
    "question": "What is the primary reason for the inefficiency in certain dynamic benchmarks despite their advantages over static benchmarks?",
    "choices": {
      "A": "Insufficient complexity control leading to overly simplistic or redundant sample generation.",
      "B": "Lack of standardized evaluation criteria hindering comparison across different benchmarks.",
      "C": "Over-reliance on human annotation for reliability, increasing costs and time.",
      "D": "High computational requirements for real-time generation of large datasets."
    },
    "answer": "A",
    "explanation": "The context mentions that \"We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\" This indicates that complexity control is crucial for efficiency but is often overlooked in certain dynamic benchmarks.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 4,
    "question_difficulty_score": 7,
    "question_groundedness_score": 8,
    "question_token_count": 21,
    "avg_answer_token_count": 14
  },
  {
    "context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n",
    "topic": "Dynamic benchmark generation using multi-agent frameworks, as demonstrated in Benchmark Self-Evolving and BENCHAGENTS.",
    "response": "<output_format>\nQuestion: How does BENCHAGENTS utilize multi-agent frameworks to enhance the dynamic benchmark generation process compared to traditional static benchmarks?\nA: By splitting the benchmark creation process into distinct phases such as planning, generation, verification, and evaluation, each managed by specialized LLM agents.\nB: Through the continuous evolution of benchmarks by adding new tasks and subtasks based on agent collaboration and feedback.\nC: By employing a human-in-the-loop approach to provide immediate feedback and guide the benchmark's development.\nD: Utilizing a single LLM to generate a wide range of questions and tasks, reducing the complexity of the benchmarking process.\nExplanation: BENCHAGENTS uses a multi-agent framework that divides the benchmark creation into multiple stages, each handled by different agents, which allows for more structured and scalable benchmark generation. This approach is fundamentally different from traditional static benchmarks that do not evolve dynamically.\nCorrect Answer: A\n</output_format>\n\n<document_analysis>\nThe context discusses various evaluation methods and the innovative use of multi-agent frameworks in benchmark generation. BENCHAGENTS specifically employs a multi-agent system that divides the benchmark creation process into planning, generation, verification, and evaluation stages, each handled by specialized agents. This method is highlighted as a significant improvement over traditional static benchmarks, which do not have such a structured and collaborative approach.\n\nThe question focuses on BENCHAGENTS' unique methodology, requiring an understanding of the multi-agent framework and its application in benchmark generation. It avoids simply repeating the information verbatim but rather requires the respondent to synthesize and compare BENCHAGENTS with traditional methods.\n</document_analysis>",
    "question": "How does BENCHAGENTS utilize multi-agent frameworks to enhance the dynamic benchmark generation process compared to traditional static benchmarks?",
    "choices": {
      "A": "By splitting the benchmark creation process into distinct phases such as planning, generation, verification, and evaluation, each managed by specialized LLM agents.",
      "B": "Through the continuous evolution of benchmarks by adding new tasks and subtasks based on agent collaboration and feedback.",
      "C": "By employing a human-in-the-loop approach to provide immediate feedback and guide the benchmark's development.",
      "D": "Utilizing a single LLM to generate a wide range of questions and tasks, reducing the complexity of the benchmarking process."
    },
    "answer": "A",
    "explanation": "BENCHAGENTS uses a multi-agent framework that divides the benchmark creation into multiple stages, each handled by different agents, which allows for more structured and scalable benchmark generation. This approach is fundamentally different from traditional static benchmarks that do not evolve dynamically.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 4,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "question_token_count": 24,
    "avg_answer_token_count": 23
  },
  {
    "context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Definition and Rationale Behind Dynamic Benchmarking",
    "response": "<output_format>\nQuestion: What is the primary rationale behind using dynamic benchmarking over static benchmarking in evaluating Large Language Models (LLMs)?\n\nA: To adapt the benchmarking process to changes in the LLM's performance over time by dynamically altering the evaluation datasets.\nB: To reduce the computational resources required for benchmarking by limiting the size of the datasets used.\nC: To simplify the evaluation process by using a fixed, static dataset that remains unchanged throughout the testing period.\nD: To ensure complete access to the training dataset, thereby avoiding legal and privacy constraints.\n\nExplanation: The primary rationale behind dynamic benchmarking is its ability to adapt to changes in the LLM's performance over time by dynamically altering the evaluation datasets. This approach addresses the limitations of static benchmarking, particularly the challenges posed by legal and privacy constraints and varying assumptions about model behavior.\nCorrect Answer: A\n</output_format>\n\n<document_analysis>\nThe provided context discusses the limitations of static benchmarking and introduces dynamic benchmarking as a solution. The key idea is to modify the dataset during the benchmarking process to avoid data contamination, which aligns with option A. Options B, C, and D do not accurately capture the essence of dynamic benchmarking as described in the context.\n</document_analysis>",
    "question": "What is the primary rationale behind using dynamic benchmarking over static benchmarking in evaluating Large Language Models (LLMs)?",
    "choices": {
      "A": "To adapt the benchmarking process to changes in the LLM's performance over time by dynamically altering the evaluation datasets.",
      "B": "To reduce the computational resources required for benchmarking by limiting the size of the datasets used.",
      "C": "To simplify the evaluation process by using a fixed, static dataset that remains unchanged throughout the testing period.",
      "D": "To ensure complete access to the training dataset, thereby avoiding legal and privacy constraints."
    },
    "answer": "A",
    "explanation": "The primary rationale behind dynamic benchmarking is its ability to adapt to changes in the LLM's performance over time by dynamically altering the evaluation datasets. This approach addresses the limitations of static benchmarking, particularly the challenges posed by legal and privacy constraints and varying assumptions about model behavior.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "question_token_count": 22,
    "avg_answer_token_count": 19
  },
  {
    "context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n",
    "topic": "Examples of Exact and Syntactic Contamination in LLM Training and Testing Data",
    "response": "<document_analysis>\nThe provided context discusses two types of data contamination in LLMs: exact and syntactic contamination. It explains that exact contamination happens when there is a direct duplicate between the training and testing datasets, whereas syntactic contamination involves slight alterations to test data points that still preserve their original meaning and can be found in the training set through syntactic transformations. This analysis helps identify key concepts and nuances that can be leveraged to craft a challenging question.\n</document_analysis>\n\n<output_format>\nQuestion: Which of the following scenarios exemplifies syntactic contamination in the context of LLM training and testing data?\n\nA: An exact match of a test example found in the training dataset.\n\nB: A test example with slightly altered punctuation but retains its original meaning, found in the training dataset.\n\nC: A new test example created by combining two different training examples.\n\nD: A test example with completely different wording but identical meaning, found in the training dataset.\n\nExplanation: Syntactic contamination refers to situations where a test data point can be found in the training dataset after undergoing syntactic transformations such as punctuation normalization, whitespace modification, or synonym substitution, while preserving the original meaning. Therefore, option B correctly describes syntactic contamination.\n\nCorrect Answer: B\n</output_format>",
    "question": "Which of the following scenarios exemplifies syntactic contamination in the context of LLM training and testing data?",
    "choices": {
      "A": "An exact match of a test example found in the training dataset.",
      "B": "A test example with slightly altered punctuation but retains its original meaning, found in the training dataset.",
      "C": "A new test example created by combining two different training examples.",
      "D": "A test example with completely different wording but identical meaning, found in the training dataset."
    },
    "answer": "B",
    "explanation": "Syntactic contamination refers to situations where a test data point can be found in the training dataset after undergoing syntactic transformations such as punctuation normalization, whitespace modification, or synonym substitution, while preserving the original meaning. Therefore, option B correctly describes syntactic contamination.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 5,
    "question_difficulty_score": 6,
    "question_groundedness_score": 9,
    "question_token_count": 20,
    "avg_answer_token_count": 16
  },
  {
    "context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n",
    "topic": "Discuss the role of the oracle function \ud835\udca2\u2062(\u22c5) in ensuring the correctness of dynamic benchmarks and provide examples of what this function could be.",
    "response": "<document_analysis>\nThe provided context discusses the importance of correctness in dynamic benchmarking algorithms for evaluating LLMs. It introduces an equation to measure correctness, which relies on an oracle function \\( \\mathcal{G}(\\cdot) \\). This function is crucial because it ensures that the generated dataset's outputs align with their true ground truth values. The context provides an example where \\( \\mathcal{G}(\\cdot) \\) could be a domain-specific annotator, but it does not explicitly define other possible forms of the oracle function.\n\nThe question should probe the candidate's deep understanding of the role of \\( \\mathcal{G}(\\cdot) \\) and encourage them to think about various possible implementations of this function. The question should also challenge the candidate to consider the nuances and implications of using different types of oracle functions.\n</document_analysis>\n\n<output_format>\nQuestion: What are two different possible implementations of the oracle function \\( \\mathcal{G}(\\cdot) \\) that could be used in ensuring the correctness of a dynamic benchmark, and what are the implications of each implementation?\nA: A domain-specific annotator and a machine learning model trained on ground truth data.\nB: A random number generator and a pre-defined set of rules.\nC: An expert human evaluator and a neural network model.\nD: A statistical model and a deterministic algorithm.\nExplanation: The correct implementation of \\( \\mathcal{G}(\\cdot) \\) depends on the nature of the data and the context. A domain-specific annotator or a machine learning model trained on ground truth data can ensure high accuracy but may be costly and time-consuming. On the other hand, a neural network model or an expert human evaluator might offer more flexibility and adaptability but could introduce biases or inconsistencies. A random number generator or pre-defined rules would be simplistic and may not capture complex patterns in the data.\nCorrect Answer: A\n</output_format>",
    "question": "What are two different possible implementations of the oracle function \\( \\mathcal{G}(\\cdot) \\) that could be used in ensuring the correctness of a dynamic benchmark, and what are the implications of each implementation?",
    "choices": {
      "A": "A domain-specific annotator and a machine learning model trained on ground truth data.",
      "B": "A random number generator and a pre-defined set of rules.",
      "C": "An expert human evaluator and a neural network model.",
      "D": "A statistical model and a deterministic algorithm."
    },
    "answer": "A",
    "explanation": "The correct implementation of \\( \\mathcal{G}(\\cdot) \\) depends on the nature of the data and the context. A domain-specific annotator or a machine learning model trained on ground truth data can ensure high accuracy but may be costly and time-consuming. On the other hand, a neural network model or an expert human evaluator might offer more flexibility and adaptability but could introduce biases or inconsistencies. A random number generator or pre-defined rules would be simplistic and may not capture complex patterns in the data.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 8,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 6,
    "question_token_count": 41,
    "avg_answer_token_count": 12
  },
  {
    "context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n",
    "topic": "Assessment of LLM Reasoning Using Directed Acyclic Graphs (DAGs)",
    "response": "<document_analysis>\nThe context provides a detailed overview of different methods used to evaluate the reasoning abilities of large language models (LLMs). It primarily focuses on two categories: table-based and graph-based evaluations. Specifically, it mentions the use of directed acyclic graphs (DAGs) in evaluating LLMs, detailing the process of constructing these graphs and transforming them into natural language descriptions for evaluation purposes. This information is crucial for generating a question that probes deeper understanding and critical thinking about the evaluation methods.\n</document_analysis>\n\n<output_format>\nQuestion: How does the evaluation process of LLMs using directed acyclic graphs (DAGs) ensure a controlled assessment of reasoning capabilities?\nA: By constructing DAGs with varying numbers of nodes and edges to adjust the difficulty level and then converting them into natural language descriptions.\nB: By synthesizing random tables and querying LLMs for specific values within the tables.\nC: By generating random graphs and evaluating LLMs on well-known P and NP problems like the Traveling Salesman Problem.\nD: By automatically constructing Knights and Knaves puzzles with random reasoning graphs and querying LLMs.\nExplanation: The correct answer involves understanding the multi-step process of creating DAGs, adjusting their complexity, converting them into natural language, and querying the LLMs to test reasoning. Option A accurately describes this process, whereas the other options pertain to different evaluation methods (tables, NP-hard problems, and puzzles).\nCorrect Answer: A\n</output_format>",
    "question": "How does the evaluation process of LLMs using directed acyclic graphs (DAGs) ensure a controlled assessment of reasoning capabilities?",
    "choices": {
      "A": "By constructing DAGs with varying numbers of nodes and edges to adjust the difficulty level and then converting them into natural language descriptions.",
      "B": "By synthesizing random tables and querying LLMs for specific values within the tables.",
      "C": "By generating random graphs and evaluating LLMs on well-known P and NP problems like the Traveling Salesman Problem.",
      "D": "By automatically constructing Knights and Knaves puzzles with random reasoning graphs and querying LLMs."
    },
    "answer": "A",
    "explanation": "The correct answer involves understanding the multi-step process of creating DAGs, adjusting their complexity, converting them into natural language, and querying the LLMs to test reasoning. Option A accurately describes this process, whereas the other options pertain to different evaluation methods (tables, NP-hard problems, and puzzles).",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "question_token_count": 25,
    "avg_answer_token_count": 21
  },
  {
    "context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n",
    "topic": "Methods for ensuring fairness in dynamic benchmarks while minimizing data contamination.",
    "response": "<document_analysis>\nThe context discusses methods for ensuring fairness in dynamic benchmarks while minimizing data contamination. It primarily focuses on the transformation process and categorization of dynamic benchmarks. However, the provided text does not directly address strategies for ensuring fairness or methods for minimizing data contamination. Therefore, the content is deemed insufficient to generate a meaningful and highly challenging question-answer pair that requires deep expertise and engagement with the topic as requested.\n</document_analysis>\n\n<output_format>\nQuestion: How can dynamic benchmarks ensure fairness and minimize data contamination without relying on extensive manual validation?\nA: By employing rule-based or manually crafted transformations.\nB: Through the use of advanced explainability tools and human-in-the-loop validation.\nC: Implementing hybrid approaches combining various dynamic benchmark techniques.\nD: Utilizing temporal cutoff for data collection and analysis.\nExplanation: The correct answer requires understanding the nuances of ensuring fairness and minimizing data contamination in dynamic benchmarks, which goes beyond the scope of the provided context. The question encourages reflection on the limitations of the current methods and the need for more sophisticated techniques.\nCorrect Answer: B\n</output_format>",
    "question": "How can dynamic benchmarks ensure fairness and minimize data contamination without relying on extensive manual validation?",
    "choices": {
      "A": "By employing rule-based or manually crafted transformations.",
      "B": "Through the use of advanced explainability tools and human-in-the-loop validation.",
      "C": "Implementing hybrid approaches combining various dynamic benchmark techniques.",
      "D": "Utilizing temporal cutoff for data collection and analysis."
    },
    "answer": "B",
    "explanation": "The correct answer requires understanding the nuances of ensuring fairness and minimizing data contamination in dynamic benchmarks, which goes beyond the scope of the provided context. The question encourages reflection on the limitations of the current methods and the need for more sophisticated techniques.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 8,
    "question_clarity_score": 7,
    "question_difficulty_score": 6,
    "question_groundedness_score": 8,
    "question_token_count": 18,
    "avg_answer_token_count": 12
  },
  {
    "context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n",
    "topic": "ITD's Use of a Contamination Detector for Sample Rewriting",
    "response": "<document_analysis>\nThe context discusses various methods for rewriting samples from static benchmarks, focusing on ITD's approach. ITD uses a contamination detector to identify contaminated samples, ensuring that the rewritten samples maintain their difficulty levels. This involves a sophisticated interplay between technology and methodology. Key points to consider are:\n1. The use of a contamination detector.\n2. The goal of preserving difficulty levels during rewriting.\n3. The interaction between the contamination detector and the LLM.\n4. The potential for in-distribution contamination highlighted in other works.\n\nGiven the need for a highly challenging question, we aim to probe the deeper understanding of how the contamination detector interacts with the rewriting process, and the implications of this for maintaining sample quality.\n</document_analysis>\n\n<output_format>\nQuestion: How does the contamination detector in ITD's method ensure that the rewritten samples retain their difficulty levels, and what are the potential challenges in achieving this?\nA: By prompting an LLM to rewrite samples after identification, without altering the difficulty.\nB: Through direct modification of the original sample's content to remove contamination.\nC: By using a contamination detector to identify contaminated samples, and then prompting an LLM to rewrite them while preserving the original difficulty.\nD: The contamination detector automatically adjusts the difficulty level based on the identified contamination.\nExplanation: The correct answer is C because ITD first identifies contaminated samples using a detector and then prompts an LLM to rewrite them, ensuring the difficulty remains intact. This requires a deep understanding of the interaction between the detector and the rewriting process. Options A and B oversimplify the process, while D incorrectly suggests automatic adjustment.\nCorrect Answer: C\n</output_format>",
    "question": "How does the contamination detector in ITD's method ensure that the rewritten samples retain their difficulty levels, and what are the potential challenges in achieving this?",
    "choices": {
      "A": "By prompting an LLM to rewrite samples after identification, without altering the difficulty.",
      "B": "Through direct modification of the original sample's content to remove contamination.",
      "C": "By using a contamination detector to identify contaminated samples, and then prompting an LLM to rewrite them while preserving the original difficulty.",
      "D": "The contamination detector automatically adjusts the difficulty level based on the identified contamination."
    },
    "answer": "C",
    "explanation": "The correct answer is C because ITD first identifies contaminated samples using a detector and then prompts an LLM to rewrite them, ensuring the difficulty remains intact. This requires a deep understanding of the interaction between the detector and the rewriting process. Options A and B oversimplify the process, while D incorrectly suggests automatic adjustment.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 7,
    "question_token_count": 31,
    "avg_answer_token_count": 17
  },
  {
    "context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n",
    "topic": "Transition from Static to Dynamic Benchmarking Methods in LLMs",
    "response": "<document_analysis>\nThe context discusses the transition from static to dynamic benchmarking methods in Large Language Models (LLMs) to address data contamination risks. It emphasizes the limitations of current static benchmarks and highlights the need for standardized criteria to evaluate dynamic benchmarks. The key points to consider are:\n- The evolution from static to dynamic benchmarking methods.\n- Limitations of current static benchmarks.\n- Lack of standardized criteria for evaluating dynamic benchmarks.\n- Proposed optimal design principles for dynamic benchmarking.\n- The importance of a GitHub repository for collecting and evaluating these benchmarks.\n\nThe question should probe the deeper implications and requirements of transitioning to dynamic benchmarking methods while considering the limitations and proposed solutions.\n</document_analysis>\n\n<output_format>\nQuestion: What is a critical challenge in adopting dynamic benchmarking methods for LLMs, and how does it relate to the limitations of current static benchmarks?\nA: Ensuring the standardized evaluation criteria for dynamic benchmarks is crucial because current static benchmarks lack such standards.\nB: The primary challenge lies in the computational cost of dynamically updating benchmarks frequently.\nC: Dynamic benchmarks are less reliable than static benchmarks due to their complexity.\nD: There is no significant difference between static and dynamic benchmarks in terms of mitigating data contamination.\nExplanation: The answer requires a deep understanding of the context, particularly the need for standardized criteria in dynamic benchmarks and the limitations of current static benchmarks.\nCorrect Answer: A\n</output_format>",
    "question": "What is a critical challenge in adopting dynamic benchmarking methods for LLMs, and how does it relate to the limitations of current static benchmarks?",
    "choices": {
      "A": "Ensuring the standardized evaluation criteria for dynamic benchmarks is crucial because current static benchmarks lack such standards.",
      "B": "The primary challenge lies in the computational cost of dynamically updating benchmarks frequently.",
      "C": "Dynamic benchmarks are less reliable than static benchmarks due to their complexity.",
      "D": "There is no significant difference between static and dynamic benchmarks in terms of mitigating data contamination."
    },
    "answer": "A",
    "explanation": "The answer requires a deep understanding of the context, particularly the need for standardized criteria in dynamic benchmarks and the limitations of current static benchmarks.",
    "answer_correctness_score": 9,
    "explanation_validity_score": 8,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "question_token_count": 28,
    "avg_answer_token_count": 16
  },
  {
    "context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n",
    "topic": "Definition and examples of syntactic contamination in LLM datasets.",
    "response": "<document_analysis>\nThe provided context discusses syntactic contamination in LLM datasets, focusing on how syntactic transformations can lead to misleading evaluations of models' capabilities. The key points include:\n1. Syntactic contamination involves rephrasing test data using a prefix string derived from training data.\n2. Debates exist regarding whether such transformations qualify as contamination.\n3. Some NLP applications rely on syntactic information, making these transformations relevant for evaluation.\n4. Understanding and mitigating contamination is crucial for valid benchmarking and robust LLM development.\n\nGiven these points, the question should delve into the nuances of syntactic contamination and challenge the expert's understanding of its implications.\n</document_analysis>\n\n<output_format>\nQuestion: How does syntactic contamination affect the evaluation of LLMs in applications that primarily depend on syntactic information?\nA: It provides a more accurate assessment of the model's reasoning capabilities.\nB: It may lead to overestimating the model's ability to handle unseen data by leveraging memorized information.\nC: It enhances the model's ability to generalize across different contexts.\nD: It has no significant impact on the evaluation process.\nExplanation: The correct answer is B because syntactic contamination can lead to models recalling memorized information rather than demonstrating true reasoning capabilities, thus potentially overestimating their performance.\nCorrect Answer: B\n</output_format>",
    "question": "How does syntactic contamination affect the evaluation of LLMs in applications that primarily depend on syntactic information?",
    "choices": {
      "A": "It provides a more accurate assessment of the model's reasoning capabilities.",
      "B": "It may lead to overestimating the model's ability to handle unseen data by leveraging memorized information.",
      "C": "It enhances the model's ability to generalize across different contexts.",
      "D": "It has no significant impact on the evaluation process."
    },
    "answer": "B",
    "explanation": "The correct answer is B because syntactic contamination can lead to models recalling memorized information rather than demonstrating true reasoning capabilities, thus potentially overestimating their performance.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 9,
    "question_token_count": 20,
    "avg_answer_token_count": 14
  },
  {
    "context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n",
    "topic": "Discuss the advantages and disadvantages of employing robust encryption methods versus label protection in maintaining the integrity of model evaluation, using specific examples from the literature.",
    "response": "<output_format>\nQuestion: What are the key differences between employing robust encryption methods and label protection in maintaining the integrity of model evaluation, and what are their respective advantages and disadvantages as highlighted in the literature?\n\nA: Robust encryption ensures data confidentiality but introduces computational overhead and relies on strong key management; label protection hides true answers to prevent model learning, but may still allow model exposure through other means.\n\nB: Robust encryption prevents data leakage and model exposure but requires complex key management; label protection mitigates risk of data contamination but does not protect the integrity of the test set itself.\n\nC: Robust encryption and label protection both prevent model exposure but differ in implementation complexity and reliance on key management; robust encryption adds computational load while label protection does not.\n\nD: Robust encryption and label protection both require strong key management but differ in their impact on model learning; robust encryption can be bypassed with minor text variations, while label protection can be compromised if test labels are leaked.\n\nExplanation: This question requires a deep understanding of both encryption methods and label protection, their advantages, and their limitations as described in the literature. It asks for a nuanced comparison that reflects the complexities and trade-offs involved in choosing between these two approaches.\n\nCorrect Answer: A\n</output_format>\n\n<document_analysis>\nThe context discusses robust encryption methods and label protection techniques for maintaining the integrity of model evaluation. It highlights the advantages and disadvantages of each method. \n\n- Robust encryption is effective in securing data but has dependencies on strong key management and introduces computational overhead.\n- Label protection hides true answers, preventing model learning but may still expose models to other forms of data contamination.\n\nThe question aims to explore these aspects, requiring a deep understanding of the methods' strengths and weaknesses.\n</document_analysis>",
    "question": "What are the key differences between employing robust encryption methods and label protection in maintaining the integrity of model evaluation, and what are their respective advantages and disadvantages as highlighted in the literature?",
    "choices": {
      "A": "Robust encryption ensures data confidentiality but introduces computational overhead and relies on strong key management; label protection hides true answers to prevent model learning, but may still allow model exposure through other means.",
      "B": "Robust encryption prevents data leakage and model exposure but requires complex key management; label protection mitigates risk of data contamination but does not protect the integrity of the test set itself.",
      "C": "Robust encryption and label protection both prevent model exposure but differ in implementation complexity and reliance on key management; robust encryption adds computational load while label protection does not.",
      "D": "Robust encryption and label protection both require strong key management but differ in their impact on model learning; robust encryption can be bypassed with minor text variations, while label protection can be compromised if test labels are leaked."
    },
    "answer": "A",
    "explanation": "This question requires a deep understanding of both encryption methods and label protection, their advantages, and their limitations as described in the literature. It asks for a nuanced comparison that reflects the complexities and trade-offs involved in choosing between these two approaches.",
    "answer_correctness_score": 9,
    "explanation_validity_score": 8,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "question_token_count": 36,
    "avg_answer_token_count": 36
  },
  {
    "context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Techniques for continuously updating benchmark datasets to minimize data contamination.",
    "response": "<document_analysis>\nThe provided context discusses the issue of data contamination in benchmarking Large Language Models (LLMs) and introduces techniques for mitigating this problem. The focus is on continuous updates of benchmark datasets to minimize contamination. The context provides valuable information on the evolution of benchmarking methods from static to dynamic, emphasizing the importance of keeping benchmark datasets up-to-date to avoid data leakage during training.\n\nThe context is relevant and contains meaningful, coherent, and educationally valuable content. Therefore, it is suitable for generating a challenging question.\n</document_analysis>\n\n<output_format>\nQuestion: What is a key challenge in implementing continuous updates to benchmark datasets for LLMs to ensure they are not contaminated, and how does this differ from traditional static benchmarking methods?\nA: Ensuring that the latest training data is excluded from the benchmark dataset.\nB: Guaranteeing that the benchmark dataset remains static over time to maintain consistency.\nC: Regularly refreshing the benchmark dataset to reflect recent training data while avoiding contamination.\nD: Limiting the size of the benchmark dataset to reduce complexity.\nExplanation: The key challenge is to refresh the benchmark dataset regularly to reflect recent training data without including it in the dataset, which is a significant departure from static benchmarking where datasets remain unchanged over time.\nCorrect Answer: C\n</output_format>",
    "question": "What is a key challenge in implementing continuous updates to benchmark datasets for LLMs to ensure they are not contaminated, and how does this differ from traditional static benchmarking methods?",
    "choices": {
      "A": "Ensuring that the latest training data is excluded from the benchmark dataset.",
      "B": "Guaranteeing that the benchmark dataset remains static over time to maintain consistency.",
      "C": "Regularly refreshing the benchmark dataset to reflect recent training data while avoiding contamination.",
      "D": "Limiting the size of the benchmark dataset to reduce complexity."
    },
    "answer": "C",
    "explanation": "The key challenge is to refresh the benchmark dataset regularly to reflect recent training data without including it in the dataset, which is a significant departure from static benchmarking where datasets remain unchanged over time.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 8,
    "question_token_count": 34,
    "avg_answer_token_count": 15
  },
  {
    "context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "The concept of contamination detectors and their role in quantifying contamination risks in benchmarks.",
    "response": "<output_format>\nQuestion: How do contamination detectors contribute to the development of robust benchmarks for evolving LLMs?\n\nA: By identifying outdated aspects of the benchmarks that no longer accurately reflect model performance.\nB: By quantifying the risk of data contamination and helping to update benchmarks dynamically.\nC: By assessing the ability of LLMs to follow instructions and generate code.\nD: By evaluating the overall performance of LLMs across multiple tasks and scenarios.\n\nExplanation: Contamination detectors specifically aim to measure and address the risk of data contamination in benchmarks, which is crucial as LLMs evolve and new data is continually incorporated. This ensures that benchmarks remain relevant and accurately reflect the true capabilities of the models.\n\nCorrect Answer: B\n</output_format>\n\n<document_analysis>\nThe context discusses the challenges faced by static benchmarks as LLMs evolve, particularly the issue of data contamination. It mentions contamination detectors as tools to quantify these risks and proposes dynamic benchmarks to address them. The question is designed to probe deeper understanding of the role of contamination detectors rather than focusing on other aspects like specific benchmarking tasks or overall model performance assessments.\n</document_analysis>",
    "question": "How do contamination detectors contribute to the development of robust benchmarks for evolving LLMs?",
    "choices": {
      "A": "By identifying outdated aspects of the benchmarks that no longer accurately reflect model performance.",
      "B": "By quantifying the risk of data contamination and helping to update benchmarks dynamically.",
      "C": "By assessing the ability of LLMs to follow instructions and generate code.",
      "D": "By evaluating the overall performance of LLMs across multiple tasks and scenarios."
    },
    "answer": "B",
    "explanation": "Contamination detectors specifically aim to measure and address the risk of data contamination in benchmarks, which is crucial as LLMs evolve and new data is continually incorporated. This ensures that benchmarks remain relevant and accurately reflect the true capabilities of the models.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "question_token_count": 17,
    "avg_answer_token_count": 15
  },
  {
    "context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n",
    "topic": "Mathematical Representation of Scalability",
    "response": "<document_analysis>\nThe context provides a mathematical representation of scalability, focusing on the relationship between the size of the transformed dataset and the original dataset, adjusted for the cost of the transformation process. It introduces a formula that quantifies scalability as a ratio involving the expectation over the transformation space. The formula includes elements such as the norm of the transformed dataset, the norm of the original dataset, and a cost function. Given the complexity and the abstract nature of the concept, the question should aim to test deep understanding of the formula's components and their interplay.\n\nThe question should be challenging yet fair, avoiding straightforward recall and encouraging reflection on the implications and nuances of the formula.\n</document_analysis>\n\n<output_format>\nQuestion: What does the term \\(\\left\\lVert T_i(\\mathcal{D}) \\right\\rVert / \\left\\lVert \\mathcal{D} \\right\\rVert\\) in the scalability equation represent, and how does the cost function \\( \\textsf{Cost}(\\cdot) \\) influence the overall measure of scalability?\n\nA: The ratio of the size of the transformed dataset to the original dataset, with no impact from the cost function.\nB: The proportion of data that can be generated per unit cost, with the cost function affecting the overall measure.\nC: The efficiency of the transformation process, independent of the cost function.\nD: The total cost of the transformation process divided by the size of the original dataset.\nExplanation: The term \\(\\left\\lVert T_i(\\mathcal{D}) \\right\\rVert / \\left\\lVert \\mathcal{D} \\right\\rVert\\) represents the ratio of the size of the transformed dataset to the original dataset. However, the overall measure of scalability is influenced by the cost function \\( \\textsf{Cost}(\\cdot) \\), which accounts for the monetary, time, or manual effort costs associated with the transformation process.\nCorrect Answer: B\n</output_format>",
    "question": "What does the term \\(\\left\\lVert T_i(\\mathcal{D}) \\right\\rVert / \\left\\lVert \\mathcal{D} \\right\\rVert\\) in the scalability equation represent, and how does the cost function \\( \\textsf{Cost}(\\cdot) \\) influence the overall measure of scalability?",
    "choices": {
      "A": "The ratio of the size of the transformed dataset to the original dataset, with no impact from the cost function.",
      "B": "The proportion of data that can be generated per unit cost, with the cost function affecting the overall measure.",
      "C": "The efficiency of the transformation process, independent of the cost function.",
      "D": "The total cost of the transformation process divided by the size of the original dataset."
    },
    "answer": "B",
    "explanation": "The term \\(\\left\\lVert T_i(\\mathcal{D}) \\right\\rVert / \\left\\lVert \\mathcal{D} \\right\\rVert\\) represents the ratio of the size of the transformed dataset to the original dataset. However, the overall measure of scalability is influenced by the cost function \\( \\textsf{Cost}(\\cdot) \\), which accounts for the monetary, time, or manual effort costs associated with the transformation process.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "question_token_count": 62,
    "avg_answer_token_count": 19
  },
  {
    "context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n",
    "topic": "Techniques Employed by LLMs to Rewrite Benchmark Samples",
    "response": "<document_analysis>\nThe context primarily discusses various techniques employed by Large Language Models (LLMs) to rewrite samples from existing benchmarks. It mentions specific methods such as Auto-Dataset, StructEval, ITD, and VarBench, each employing unique strategies to either preserve the original's stylistics and essential knowledge, expand examined concepts, identify and rewrite contaminated samples, or replace variables in samples. The information focuses on methodologies rather than detailed implementation specifics, making the context rich for probing questions on the nuances of these techniques.\n</document_analysis>\n\n<output_format>\nQuestion: What is the primary method used by the ITD technique to ensure the rewritten samples maintain their original difficulty levels while addressing contamination?\nA: Prompting LLMs to rewrite samples while preserving stylistics.\nB: Expanding on the original concepts using knowledge graphs.\nC: Utilizing a contamination detector to identify and prompt LLMs to rewrite samples.\nD: Identifying and replacing variables in the samples.\nExplanation: The correct answer is C, as the ITD technique specifically mentions using a contamination detector to identify contaminated samples and prompting an LLM to rewrite them while preserving their difficulty levels. This requires understanding the unique approach of ITD in handling contaminated samples compared to other techniques.\nCorrect Answer: C\n</output_format>",
    "question": "What is the primary method used by the ITD technique to ensure the rewritten samples maintain their original difficulty levels while addressing contamination?",
    "choices": {
      "A": "Prompting LLMs to rewrite samples while preserving stylistics.",
      "B": "Expanding on the original concepts using knowledge graphs.",
      "C": "Utilizing a contamination detector to identify and prompt LLMs to rewrite samples.",
      "D": "Identifying and replacing variables in the samples."
    },
    "answer": "C",
    "explanation": "The correct answer is C, as the ITD technique specifically mentions using a contamination detector to identify contaminated samples and prompting an LLM to rewrite them while preserving their difficulty levels. This requires understanding the unique approach of ITD in handling contaminated samples compared to other techniques.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 1,
    "question_difficulty_score": 5,
    "question_groundedness_score": 8,
    "question_token_count": 26,
    "avg_answer_token_count": 12
  },
  {
    "context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n",
    "topic": "Analyzing the Impact of Data Contamination on LLM Evaluation Through Dynamic Benchmarking Metrics",
    "response": "<document_analysis>\nThe context focuses on the impact of data contamination on the evaluation of Large Language Models (LLMs) through dynamic benchmarking metrics. Key concepts include collision rate and repeat trials, which measure the overlap and variability of transformed benchmark datasets. The discussion centers around the challenge of balancing transparency and the risk of data contamination in benchmarking. The question should probe a deeper understanding of these metrics and their implications for LLM evaluation.\n</document_analysis>\n\n<output_format>\nQuestion: How does the collision rate and repeat trials metric together influence the reliability of dynamic benchmarking in evaluating LLMs' true capabilities, considering the risk of data contamination?\nA: By measuring the overlap of transformed datasets, collision rate highlights potential contamination, while repeat trials indicate the ease of regenerating existing transformations.\nB: They ensure that each benchmark trial is unique, thereby completely eliminating data contamination risks.\nC: These metrics help in assessing the robustness of the benchmark by identifying both the extent of overlap and the variability of generated test cases.\nD: Collision rate alone is sufficient to ensure the benchmark's effectiveness in LLM evaluation.\nExplanation: The correct answer requires understanding that both collision rate and repeat trials are interrelated and necessary for comprehensive evaluation. Collision rate identifies overlap, which indicates potential contamination, while repeat trials measure the variability and uniqueness of test cases.\nCorrect Answer: C\n</output_format>",
    "question": "How does the collision rate and repeat trials metric together influence the reliability of dynamic benchmarking in evaluating LLMs' true capabilities, considering the risk of data contamination?",
    "choices": {
      "A": "By measuring the overlap of transformed datasets, collision rate highlights potential contamination, while repeat trials indicate the ease of regenerating existing transformations.",
      "B": "They ensure that each benchmark trial is unique, thereby completely eliminating data contamination risks.",
      "C": "These metrics help in assessing the robustness of the benchmark by identifying both the extent of overlap and the variability of generated test cases.",
      "D": "Collision rate alone is sufficient to ensure the benchmark's effectiveness in LLM evaluation."
    },
    "answer": "C",
    "explanation": "The correct answer requires understanding that both collision rate and repeat trials are interrelated and necessary for comprehensive evaluation. Collision rate identifies overlap, which indicates potential contamination, while repeat trials measure the variability and uniqueness of test cases.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "question_token_count": 32,
    "avg_answer_token_count": 21
  },
  {
    "context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n",
    "topic": "Test a model's intuitive reasoning skills using benchmarks like PIQA, SIQA, HellaSwag, and WinoGrande.",
    "response": "<output_format>\nQuestion: What unique aspects of human reasoning are tested by benchmarks like PIQA, SIQA, HellaSwag, and WinoGrande, and how do they differ from other types of language comprehension benchmarks?\n\nA: These benchmarks test a model\u2019s ability to apply everyday knowledge and logical reasoning in scenarios that require nuanced understanding and inference, distinguishing them from coding and instruction-following benchmarks which focus more on procedural tasks.\nB: They assess a model\u2019s capacity for creative problem-solving and complex logical reasoning, setting them apart from benchmarks that primarily test fact retrieval or syntactic understanding.\nC: These benchmarks evaluate a model\u2019s skill in debugging code and synthesizing code from instructions, making them different from reasoning benchmarks that focus on common sense and everyday knowledge application.\nD: They measure a model\u2019s proficiency in understanding and executing detailed step-by-step instructions, distinguishing them from reasoning benchmarks that deal with broader, more abstract concepts.\n\nExplanation: The correct answer is B, as PIQA, SIQA, HellaSwag, and WinoGrande specifically target intuitive reasoning, logical deduction, and the application of common sense in scenarios that require nuanced understanding and inference. This contrasts with coding and instruction-following benchmarks, which focus more on procedural tasks and step-by-step execution.\n\nCorrect Answer: B\n</output_format>\n\n<document_analysis>\nThe context provides a detailed overview of various language comprehension benchmarks, including those focused on reasoning skills like PIQA, SIQA, HellaSwag, and WinoGrande. It is essential to craft a question that highlights the unique aspects of these benchmarks and distinguishes them from other types of language comprehension benchmarks. The question must be challenging enough to probe a domain expert\u2019s understanding of the nuances between different benchmark categories. The proposed question focuses on the unique aspects of reasoning benchmarks and how they differ from other types of benchmarks, encouraging a deep reflection on the nature of these tests.\n</document_analysis>",
    "question": "What unique aspects of human reasoning are tested by benchmarks like PIQA, SIQA, HellaSwag, and WinoGrande, and how do they differ from other types of language comprehension benchmarks?",
    "choices": {
      "A": "These benchmarks test a model\u2019s ability to apply everyday knowledge and logical reasoning in scenarios that require nuanced understanding and inference, distinguishing them from coding and instruction-following benchmarks which focus more on procedural tasks.",
      "B": "They assess a model\u2019s capacity for creative problem-solving and complex logical reasoning, setting them apart from benchmarks that primarily test fact retrieval or syntactic understanding.",
      "C": "These benchmarks evaluate a model\u2019s skill in debugging code and synthesizing code from instructions, making them different from reasoning benchmarks that focus on common sense and everyday knowledge application.",
      "D": "They measure a model\u2019s proficiency in understanding and executing detailed step-by-step instructions, distinguishing them from reasoning benchmarks that deal with broader, more abstract concepts."
    },
    "answer": "B",
    "explanation": "The correct answer is B, as PIQA, SIQA, HellaSwag, and WinoGrande specifically target intuitive reasoning, logical deduction, and the application of common sense in scenarios that require nuanced understanding and inference. This contrasts with coding and instruction-following benchmarks, which focus more on procedural tasks and step-by-step execution.",
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "question_token_count": 42,
    "avg_answer_token_count": 33
  },
  {
    "context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n",
    "topic": "Multi-round interaction techniques in evaluating LLMs, including their application in LLM-as-an-Interviewer and TreeEval.",
    "response": "<document_analysis>\nThe context introduces various methods for evaluating Large Language Models (LLMs) through interactive and multi-agent frameworks. It specifically mentions LLM-as-an-Interviewer, TreeEval, and KIEval as examples of interactive evaluation techniques, and Benchmark Self-Evolving and BENCHAGENTS as examples of multi-agent evaluation techniques. Given the detailed nature of the descriptions, a highly probing question can be crafted that requires a deep understanding of the intricacies and differences between these methods.\n</document_analysis>\n\n<output_format>\nQuestion: Which of the following best describes the key difference between the interactive evaluation techniques (like LLM-as-an-Interviewer and TreeEval) and the multi-agent evaluation techniques (like Benchmark Self-Evolving and BENCHAGENTS)?\n\nA: Interactive techniques focus on evaluating models through structured, multi-round interactions designed to simulate human-like questioning, while multi-agent techniques involve automated agents working together to create dynamic benchmarks.\n\nB: Interactive techniques involve multiple agents working together to create benchmarks, while multi-agent techniques focus on evaluating models through structured, multi-round interactions.\n\nC: Interactive techniques are less effective than multi-agent techniques because they rely on human-like questioning rather than automated processes.\n\nD: Interactive techniques generate follow-up questions based on the model's response, whereas multi-agent techniques do not involve follow-up questioning.\n\nExplanation: Option A accurately captures the essence of the distinction between interactive and multi-agent evaluation techniques. Interactive methods such as LLM-as-an-Interviewer and TreeEval are designed to simulate a human-like conversation to evaluate the model, while multi-agent methods like Benchmark Self-Evolving and BENCHAGENTS leverage automation and cooperation among multiple agents to create and evolve benchmarks.\n\nCorrect Answer: A\n</output_format>",
    "question": "Which of the following best describes the key difference between the interactive evaluation techniques (like LLM-as-an-Interviewer and TreeEval) and the multi-agent evaluation techniques (like Benchmark Self-Evolving and BENCHAGENTS)?",
    "choices": {
      "A": "Interactive techniques focus on evaluating models through structured, multi-round interactions designed to simulate human-like questioning, while multi-agent techniques involve automated agents working together to create dynamic benchmarks.",
      "B": "Interactive techniques involve multiple agents working together to create benchmarks, while multi-agent techniques focus on evaluating models through structured, multi-round interactions.",
      "C": "Interactive techniques are less effective than multi-agent techniques because they rely on human-like questioning rather than automated processes.",
      "D": "Interactive techniques generate follow-up questions based on the model's response, whereas multi-agent techniques do not involve follow-up questioning."
    },
    "answer": "A",
    "explanation": "Option A accurately captures the essence of the distinction between interactive and multi-agent evaluation techniques. Interactive methods such as LLM-as-an-Interviewer and TreeEval are designed to simulate a human-like conversation to evaluate the model, while multi-agent methods like Benchmark Self-Evolving and BENCHAGENTS leverage automation and cooperation among multiple agents to create and evolve benchmarks.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "question_token_count": 45,
    "avg_answer_token_count": 26
  },
  {
    "context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n",
    "topic": "The need for further refinement and validation of dynamic benchmarking criteria in real-world applications.",
    "response": "<document_analysis>\nThe context discusses the limitations of current benchmarking practices for large language models (LLMs), particularly focusing on the need for further refinement and validation of dynamic benchmarking criteria in real-world applications. It highlights the importance of addressing the challenges of reliability and reproducibility in dynamic benchmarking and suggests that future research should aim to standardize these methods.\n\nThe context does not contain any hyperlinks, advertisements, headers, footers, navigation menus, disclaimers, social media buttons, or other irrelevant elements. However, it does emphasize the necessity for further research and validation, indicating that dynamic benchmarking criteria are still in an early stage of development and require more robust testing and refinement.\n</document_analysis>\n\n<output_format>\nQuestion: What is the key challenge that future research should address to improve the reliability of dynamic benchmarking criteria in LLMs?\nA: Ensuring that benchmarking criteria are static and unchanging.\nB: Developing more sophisticated data collection methods for static benchmarks.\nC: Standardizing and validating dynamic benchmarking criteria in diverse real-world applications.\nD: Reducing the size of training datasets to minimize contamination.\nExplanation: The context emphasizes the need for further refinement and validation of dynamic benchmarking criteria in real-world applications, highlighting the current challenges in reliability and reproducibility. Option C directly addresses this need by focusing on standardizing and validating these criteria across different scenarios.\nCorrect Answer: C\n</output_format>",
    "question": "What is the key challenge that future research should address to improve the reliability of dynamic benchmarking criteria in LLMs?",
    "choices": {
      "A": "Ensuring that benchmarking criteria are static and unchanging.",
      "B": "Developing more sophisticated data collection methods for static benchmarks.",
      "C": "Standardizing and validating dynamic benchmarking criteria in diverse real-world applications.",
      "D": "Reducing the size of training datasets to minimize contamination."
    },
    "answer": "C",
    "explanation": "The context emphasizes the need for further refinement and validation of dynamic benchmarking criteria in real-world applications, highlighting the current challenges in reliability and reproducibility. Option C directly addresses this need by focusing on standardizing and validating these criteria across different scenarios.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "question_token_count": 23,
    "avg_answer_token_count": 12
  },
  {
    "context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Transformation Function in Dynamic Benchmarking",
    "response": "<output_format>\nQuestion: What is the primary role of the transformation function \\( T(\\cdot) \\) in dynamic benchmarking?\nA: To modify the evaluation data set dynamically to mitigate potential data contamination.\nB: To ensure full access to the training dataset is maintained.\nC: To provide a static evaluation environment that remains constant over time.\nD: To enhance the memorization capabilities of the LLM being evaluated.\nExplanation: The transformation function \\( T(\\cdot) \\) is crucial in dynamic benchmarking as it adapts the dataset during evaluation to avoid contamination, which aligns with the description of modifying the data set to avoid possible data contamination.\nCorrect Answer: A\n</output_format>\n\n<document_analysis>\nThe context discusses the limitations of static benchmarking and introduces dynamic benchmarking as a solution. It defines the components of a dynamic benchmark and emphasizes the role of the transformation function \\( T(\\cdot) \\). The transformation function is described as modifying the dataset to avoid contamination, which directly supports option A. Options B, C, and D do not accurately capture the primary role of the transformation function as explained in the text. This question tests the understanding of the core concept of dynamic benchmarking and the specific function of the transformation component.\n</document_analysis>",
    "question": "What is the primary role of the transformation function \\( T(\\cdot) \\) in dynamic benchmarking?",
    "choices": {
      "A": "To modify the evaluation data set dynamically to mitigate potential data contamination.",
      "B": "To ensure full access to the training dataset is maintained.",
      "C": "To provide a static evaluation environment that remains constant over time.",
      "D": "To enhance the memorization capabilities of the LLM being evaluated."
    },
    "answer": "A",
    "explanation": "The transformation function \\( T(\\cdot) \\) is crucial in dynamic benchmarking as it adapts the dataset during evaluation to avoid contamination, which aligns with the description of modifying the data set to avoid possible data contamination.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "question_token_count": 20,
    "avg_answer_token_count": 13
  },
  {
    "context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n",
    "topic": "The benefits and limitations of LLM-based generation approaches in creating novel evaluation data points.",
    "response": "<document_analysis>\nThe provided context discusses the benefits and limitations of LLM-based generation approaches in creating novel evaluation data points. It highlights that while LLMs offer strong generative capabilities, their transparency and traceability are crucial for ensuring the reliability and correctness of the generated data. The context does not delve deeply into the nuances of LLM-based generation but rather provides a broad overview. Given this, we need to design a question that encourages deep reflection on the implications of LLM transparency and its impact on evaluation data creation.\n</document_analysis>\n\n<output_format>\nQuestion: How does the transparency of LLMs influence the reliability and interpretability of data generated for evaluating LLMs?\nA: Transparency in LLMs enhances interpretability, but limits the scale of generated data.\nB: Transparency in LLMs is crucial for ensuring the reliability and interpretability of generated data, necessitating additional validation mechanisms.\nC: Transparency in LLMs reduces the need for manual validation, thus increasing efficiency.\nD: Transparency in LLMs has no significant impact on the reliability or interpretability of generated data.\nExplanation: The correct answer is B because LLMs' transparency is essential for ensuring reliability and interpretability, often requiring additional validation mechanisms like explainability tools or human-in-the-loop validation to maintain trustworthiness.\nCorrect Answer: B\n</output_format>",
    "question": "How does the transparency of LLMs influence the reliability and interpretability of data generated for evaluating LLMs?",
    "choices": {
      "A": "Transparency in LLMs enhances interpretability, but limits the scale of generated data.",
      "B": "Transparency in LLMs is crucial for ensuring the reliability and interpretability of generated data, necessitating additional validation mechanisms.",
      "C": "Transparency in LLMs reduces the need for manual validation, thus increasing efficiency.",
      "D": "Transparency in LLMs has no significant impact on the reliability or interpretability of generated data."
    },
    "answer": "B",
    "explanation": "The correct answer is B because LLMs' transparency is essential for ensuring reliability and interpretability, often requiring additional validation mechanisms like explainability tools or human-in-the-loop validation to maintain trustworthiness.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "question_token_count": 22,
    "avg_answer_token_count": 19
  },
  {
    "context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n",
    "topic": "The comprehensiveness of the survey in covering high-level concepts versus detailed technical implementation guidelines.",
    "response": "<output_format>\nQuestion: What aspect does the survey emphasize less compared to its coverage of high-level concepts?\nA: Comprehensive technical implementation guidelines\nB: Detailed analysis of static benchmarking methods\nC: Evaluation of dynamic benchmarking methods' reliability\nD: Overview of recent advancements in LLM benchmarking\nExplanation: The survey focuses more on high-level concepts and less on providing detailed technical implementation guidelines, which limits its applicability for practitioners needing in-depth implementation details.\nCorrect Answer: A\n</output_format>\n\n<document_analysis>\nThe context clearly states that the survey emphasizes high-level concepts and may not delve into all the fine-grained technical details of specific methods. This directly indicates that the survey lacks comprehensive technical implementation guidelines, making option A the correct answer.\n\nOptions B, C, and D are included to provide plausible alternatives:\n- B: Although the survey does cover static benchmarking methods, it does so at a high level rather than in detail.\n- C: The survey mentions evaluating the reliability of dynamic benchmarking methods but does not focus on this aspect more than others.\n- D: While the survey covers recent advancements, it notes that some recent methods or tools may not have been fully covered, indicating a focus on high-level concepts rather than a comprehensive review of recent advancements.\n\nThe question is designed to challenge domain experts to reflect on the balance between high-level concepts and detailed technical implementation guidelines, ensuring it meets the requirement for high complexity and difficulty.\n</document_analysis>",
    "question": "What aspect does the survey emphasize less compared to its coverage of high-level concepts?",
    "choices": {
      "A": "Comprehensive technical implementation guidelines",
      "B": "Detailed analysis of static benchmarking methods",
      "C": "Evaluation of dynamic benchmarking methods' reliability",
      "D": "Overview of recent advancements in LLM benchmarking"
    },
    "answer": "A",
    "explanation": "The survey focuses more on high-level concepts and less on providing detailed technical implementation guidelines, which limits its applicability for practitioners needing in-depth implementation details.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 1,
    "question_difficulty_score": 5,
    "question_groundedness_score": 9,
    "question_token_count": 17,
    "avg_answer_token_count": 7
  },
  {
    "context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "The limitations and challenges associated with current dynamic benchmarking methods.",
    "response": "<output_format>\nQuestion: What is a significant limitation of current dynamic benchmarking methods in mitigating data contamination in large language models?\nA: They rely too heavily on timestamps for updating benchmark datasets, which may not accurately reflect the latest training data.\nB: They do not provide clear criteria for evaluating the effectiveness of dynamic benchmarks themselves.\nC: They overemphasize post-hoc contamination detection rather than preventing contamination during the training phase.\nD: They are too complex to implement, requiring significant computational resources and expertise.\nExplanation: The correct answer is B, as the paper explicitly states that no existing work discusses criteria for evaluating dynamic benchmarks themselves, indicating a significant gap in the current methodology.\nCorrect Answer: B\n</output_format>\n\n<document_analysis>\nThe provided context discusses various aspects of benchmarking large language models (LLMs), particularly focusing on static and dynamic benchmarking methods. It highlights the importance of mitigating data contamination, a significant issue arising from the training data of LLMs. \n\nThe context mentions the evolution from static to dynamic benchmarking methods but notes that no systematic survey has summarized the progress of dynamic benchmarking methods. It further points out that while there are various dynamic benchmarking approaches, they do not fully satisfy the proposed criteria for evaluating dynamic benchmarks. This directly leads to the formulated question, which probes the limitations of current dynamic benchmarking methods.\n\nOption A is incorrect because the context does not suggest that reliance on timestamps is the sole or most significant limitation. Option C is also incorrect as the context does not indicate that existing methods overemphasize post-hoc detection to the detriment of prevention. Option D is incorrect as the complexity of implementation is not discussed as a major limitation in the provided context.\n\nOption B accurately reflects the main limitation highlighted in the paper, emphasizing the need for a set of criteria to evaluate the effectiveness of dynamic benchmarks.\n</document_analysis>",
    "question": "What is a significant limitation of current dynamic benchmarking methods in mitigating data contamination in large language models?",
    "choices": {
      "A": "They rely too heavily on timestamps for updating benchmark datasets, which may not accurately reflect the latest training data.",
      "B": "They do not provide clear criteria for evaluating the effectiveness of dynamic benchmarks themselves.",
      "C": "They overemphasize post-hoc contamination detection rather than preventing contamination during the training phase.",
      "D": "They are too complex to implement, requiring significant computational resources and expertise."
    },
    "answer": "B",
    "explanation": "The correct answer is B, as the paper explicitly states that no existing work discusses criteria for evaluating dynamic benchmarks themselves, indicating a significant gap in the current methodology.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 9,
    "question_difficulty_score": 6,
    "question_groundedness_score": 8,
    "question_token_count": 20,
    "avg_answer_token_count": 18
  },
  {
    "context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n",
    "topic": "Impact of Graph Size on LLM Performance in NP-Hard Problems",
    "response": "<document_analysis>\nThe provided context discusses various frameworks and approaches for evaluating LLMs (Large Language Models) in terms of their ability to reason about graph structures, particularly in the context of NP-hard problems. It mentions different methods like S3Eval, DyVal, NPHardEval, and Xie et al.'s work, each utilizing random graphs to test the models' performance. The focus on graph size as a variable to control task difficulty is crucial here. However, the context does not explicitly discuss the impact of graph size on LLM performance in NP-hard problems directly. Therefore, while the context provides a rich background, it does not offer a straightforward basis for generating a highly challenging, insightful, and probing question that requires deep reflection and nuanced understanding.\n</document_analysis>\n\n<output_format>\nQuestion: How does the variation in graph size affect an LLM's performance in solving NP-hard problems, as evidenced by the evaluation frameworks discussed in the context?\nA: Larger graphs enhance LLMs' ability to generalize and handle complex problem structures effectively.\nB: Smaller graphs are more challenging for LLMs due to the increased complexity in reasoning.\nC: Graph size has no significant impact on LLM performance since LLMs are equally adept at handling all sizes.\nD: The optimal graph size for LLM performance varies depending on the specific NP-hard problem being evaluated.\nExplanation: This question challenges the domain expert to consider the intricacies of how different graph sizes influence LLM performance in NP-hard problems. It requires a deep understanding of the evaluation frameworks discussed and the nuances of graph complexity.\nCorrect Answer: A\n</output_format>",
    "question": "How does the variation in graph size affect an LLM's performance in solving NP-hard problems, as evidenced by the evaluation frameworks discussed in the context?",
    "choices": {
      "A": "Larger graphs enhance LLMs' ability to generalize and handle complex problem structures effectively.",
      "B": "Smaller graphs are more challenging for LLMs due to the increased complexity in reasoning.",
      "C": "Graph size has no significant impact on LLM performance since LLMs are equally adept at handling all sizes.",
      "D": "The optimal graph size for LLM performance varies depending on the specific NP-hard problem being evaluated."
    },
    "answer": "A",
    "explanation": "This question challenges the domain expert to consider the intricacies of how different graph sizes influence LLM performance in NP-hard problems. It requires a deep understanding of the evaluation frameworks discussed and the nuances of graph complexity.",
    "answer_correctness_score": 6,
    "explanation_validity_score": 4,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "question_token_count": 30,
    "avg_answer_token_count": 19
  },
  {
    "context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n",
    "topic": "Evaluating the effectiveness of post-hoc detection methods like CONSTAT in identifying contamination across various benchmarks.",
    "response": "<document_analysis>\nThe provided context discusses the limitations of label protection and introduces post-hoc detection methods, particularly focusing on the CONSTAT method proposed by Dekoninck et al. (2024). The text explains how post-hoc detection identifies contamination through various techniques, including embedding-based similarity and analyzing model behavior under different conditions. The context does not provide specific details about the effectiveness of these methods across multiple benchmarks, which is crucial for evaluating their true utility. Therefore, the question will probe the domain expert's ability to critically evaluate the robustness and applicability of post-hoc detection methods like CONSTAT.\n</document_analysis>\n\n<output_format>\nQuestion: How does the CONSTAT method by Dekoninck et al. (2024) contribute to the evaluation of model performance across different benchmarks in detecting data contamination, and what challenges might arise in its application?\n\nA: By comparing model performance consistency across benchmarks, it helps identify contamination but may overlook subtle shifts in model behavior.\nB: It enhances data integrity by directly removing contaminated samples before training but faces difficulties in large-scale datasets.\nC: It relies on token-level n-gram matching to detect overlaps, ensuring high accuracy but limiting scalability.\nD: It uses embedding-based similarity to detect contamination, providing a flexible approach that can handle variations in benchmark datasets.\n\nExplanation: The correct answer is A. While CONSTAT compares model performance consistency across benchmarks to detect contamination, it might miss subtle changes in model behavior that could indicate contamination. Options B, C, and D describe other aspects of post-hoc detection methods but do not capture the unique contribution and limitations of the CONSTAT method in the context of evaluating model performance across different benchmarks.\n\nCorrect Answer: A\n</output_format>",
    "question": "How does the CONSTAT method by Dekoninck et al. (2024) contribute to the evaluation of model performance across different benchmarks in detecting data contamination, and what challenges might arise in its application?",
    "choices": {
      "A": "By comparing model performance consistency across benchmarks, it helps identify contamination but may overlook subtle shifts in model behavior.",
      "B": "It enhances data integrity by directly removing contaminated samples before training but faces difficulties in large-scale datasets.",
      "C": "It relies on token-level n-gram matching to detect overlaps, ensuring high accuracy but limiting scalability.",
      "D": "It uses embedding-based similarity to detect contamination, providing a flexible approach that can handle variations in benchmark datasets."
    },
    "answer": "A",
    "explanation": "The correct answer is A. While CONSTAT compares model performance consistency across benchmarks to detect contamination, it might miss subtle changes in model behavior that could indicate contamination. Options B, C, and D describe other aspects of post-hoc detection methods but do not capture the unique contribution and limitations of the CONSTAT method in the context of evaluating model performance across different benchmarks.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 9,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "question_token_count": 42,
    "avg_answer_token_count": 21
  },
  {
    "context": "###### Safety\nSafety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content. Datasets such as RealToxicityPrompts\u00a0(Gehman et\u00a0al., 2020) and ToxiGen\u00a0(Hartvigsen et\u00a0al., 2022) assess resilience against producing harmful outputs.\n\nBy providing a controlled environment to measure these aspects, safety benchmarks play a critical role in guiding the development of models that are not only powerful outputs but also responsible and trustworthy for real-world applications.\n\n\n###### Language\nLanguage benchmarks assess the LLMs\u2019 proficiency in specific languages.\n\nGLUE\u00a0(Wang, 2018) and SuperGLUE\u00a0(Wang et\u00a0al., 2019) cover tasks from sentiment analysis to language inference, while CLUE\u00a0(Xu et\u00a0al., 2020) targets Chinese language.\n\nTypo-fixing\u00a0(Suzgun et\u00a0al., 2022) is also widely used.\n\n\n###### Reading Comprehension\nReading comprehension tasks test a model\u2019s ability to extract and infer information from text. Benchmarks like SQuAD\u00a0(Rajpurkar et\u00a0al., 2018), QuAC\u00a0(Choi et\u00a0al., 2018), and BoolQ\u00a0(Clark et\u00a0al., 2019) challenge models to understand passages and draw logical conclusions.\n",
    "topic": "The structure and goals of safety benchmarks in measuring LLMs' resilience against producing harmful outputs.",
    "response": "<document_analysis>\nThe context primarily discusses various benchmarks used to evaluate LLMs, focusing on safety benchmarks and their importance in measuring resilience against harmful outputs. While the provided context includes information on other types of benchmarks (language, reading comprehension), these are not directly relevant to the topic of safety benchmarks and their goals. Therefore, the focus will be on the safety benchmarks section.\n</document_analysis>\n\n<output_format>\nQuestion: What is the primary objective of safety benchmarks in the context of Large Language Models (LLMs)?\nA: To measure the LLM's ability to produce non-toxic and ethically aligned content under controlled environments.\nB: To evaluate the LLM's proficiency in specific languages like English and Chinese.\nC: To test the LLM's capacity for language translation between different languages.\nD: To assess the LLM's performance in reading comprehension tasks.\nExplanation: The primary objective of safety benchmarks is to ensure that LLMs generate non-toxic and ethically aligned content. This is directly stated in the context: \"Safety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content.\"\nCorrect Answer: A\n</output_format>",
    "question": "What is the primary objective of safety benchmarks in the context of Large Language Models (LLMs)?",
    "choices": {
      "A": "To measure the LLM's ability to produce non-toxic and ethically aligned content under controlled environments.",
      "B": "To evaluate the LLM's proficiency in specific languages like English and Chinese.",
      "C": "To test the LLM's capacity for language translation between different languages.",
      "D": "To assess the LLM's performance in reading comprehension tasks."
    },
    "answer": "A",
    "explanation": "The primary objective of safety benchmarks is to ensure that LLMs generate non-toxic and ethically aligned content. This is directly stated in the context: \"Safety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content.\"",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 9,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "question_token_count": 20,
    "avg_answer_token_count": 15
  },
  {
    "context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Advancements in dynamic benchmarking techniques for LLMs.",
    "response": "<output_format>\nQuestion: How does the proposed framework for evaluating dynamic benchmarks differ from traditional static benchmarks in terms of addressing data contamination, and what are the specific criteria outlined for dynamic benchmarks?\nA: It evaluates the dynamic benchmarks based on their ability to adapt and update in real-time to minimize contamination, whereas static benchmarks are evaluated based on their initial design.\nB: It emphasizes the importance of continuous improvement and adaptation through dynamic updates, while static benchmarks rely on predefined datasets.\nC: It introduces a set of criteria for evaluating dynamic benchmarks that includes real-time adaptability, transparency, and robustness against contamination, distinguishing it from static benchmarks that focus on initial validation.\nD: It relies solely on post-hoc contamination detection methods to ensure dynamic benchmarks are free from contamination, similar to static benchmarks.\nExplanation: The correct answer is C. The proposed framework for evaluating dynamic benchmarks introduces specific criteria that emphasize real-time adaptability, transparency, and robustness against contamination. This distinguishes it from traditional static benchmarks, which focus more on initial validation and predefined datasets.\nCorrect Answer: C\n</output_format>\n\n<document_analysis>\nThe provided context discusses the advancements in dynamic benchmarking techniques for LLMs, highlighting the limitations of static benchmarks in addressing data contamination. The context mentions that static benchmarks are evaluated based on initial design and predefined datasets, whereas dynamic benchmarks are evaluated based on their ability to adapt and update in real-time. It further states that a systematic survey is needed to evaluate dynamic benchmarks using specific criteria, which include real-time adaptability, transparency, and robustness against contamination. Therefore, the question is crafted to probe the domain expert's understanding of the differences between static and dynamic benchmarks in terms of addressing data contamination and the specific criteria outlined for dynamic benchmarks.\n</document_analysis>",
    "question": "How does the proposed framework for evaluating dynamic benchmarks differ from traditional static benchmarks in terms of addressing data contamination, and what are the specific criteria outlined for dynamic benchmarks?",
    "choices": {
      "A": "It evaluates the dynamic benchmarks based on their ability to adapt and update in real-time to minimize contamination, whereas static benchmarks are evaluated based on their initial design.",
      "B": "It emphasizes the importance of continuous improvement and adaptation through dynamic updates, while static benchmarks rely on predefined datasets.",
      "C": "It introduces a set of criteria for evaluating dynamic benchmarks that includes real-time adaptability, transparency, and robustness against contamination, distinguishing it from static benchmarks that focus on initial validation.",
      "D": "It relies solely on post-hoc contamination detection methods to ensure dynamic benchmarks are free from contamination, similar to static benchmarks."
    },
    "answer": "C",
    "explanation": "The correct answer is C. The proposed framework for evaluating dynamic benchmarks introduces specific criteria that emphasize real-time adaptability, transparency, and robustness against contamination. This distinguishes it from traditional static benchmarks, which focus more on initial validation and predefined datasets.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "question_token_count": 33,
    "avg_answer_token_count": 28
  },
  {
    "context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n",
    "topic": "Evaluation Methods for LLMs Using Random SQL Queries",
    "response": "<output_format>\nQuestion: How might an evaluation method that uses random SQL queries differ fundamentally from those using graph-based approaches, and what implication does this have for assessing LLMs' reasoning abilities?\n\nA: It measures LLMs' capacity to handle structured data versus logical reasoning tasks, indicating that graph-based methods better capture complex logical dependencies.\n\nB: It focuses solely on LLMs' database query optimization skills rather than their general reasoning capabilities.\n\nC: It assesses LLMs' ability to perform arithmetic operations within database contexts, which is less relevant for evaluating logical reasoning.\n\nD: It evaluates LLMs' proficiency in natural language processing rather than their reasoning skills.\n\nExplanation: The question requires a deep understanding of the fundamental differences between table-based and graph-based evaluation methods. While SQL query evaluations test LLMs' ability to manipulate structured data, graph-based methods assess more complex logical reasoning tasks, making them better suited for evaluating broader reasoning abilities.\n\nCorrect Answer: A\n</output_format>\n\n<document_analysis>\nThe provided context discusses various evaluation methods for LLMs, including S3Eval for SQL queries and several graph-based methods like DyVal, NPHardEval, and Xie et al.'s approach. The question is designed to probe the fundamental differences between these two methodologies and their implications for assessing LLMs' reasoning abilities. Answer A captures the essence of the distinction, highlighting the focus on structured data handling versus complex logical reasoning. Other options are plausible but less accurate or relevant to the core distinction being tested.\n</document_analysis>",
    "question": "How might an evaluation method that uses random SQL queries differ fundamentally from those using graph-based approaches, and what implication does this have for assessing LLMs' reasoning abilities?",
    "choices": {
      "A": "It measures LLMs' capacity to handle structured data versus logical reasoning tasks, indicating that graph-based methods better capture complex logical dependencies.",
      "B": "It focuses solely on LLMs' database query optimization skills rather than their general reasoning capabilities.",
      "C": "It assesses LLMs' ability to perform arithmetic operations within database contexts, which is less relevant for evaluating logical reasoning.",
      "D": "It evaluates LLMs' proficiency in natural language processing rather than their reasoning skills."
    },
    "answer": "A",
    "explanation": "The question requires a deep understanding of the fundamental differences between table-based and graph-based evaluation methods. While SQL query evaluations test LLMs' ability to manipulate structured data, graph-based methods assess more complex logical reasoning tasks, making them better suited for evaluating broader reasoning abilities.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "question_token_count": 34,
    "avg_answer_token_count": 21
  },
  {
    "context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n",
    "topic": "Evaluate the potential vulnerabilities of encryption-based methods to minor text variations and suggest strategies to mitigate these risks.",
    "response": "<document_analysis>\nThe provided context focuses on encryption methods for securing evaluation data, highlighting their effectiveness but also pointing out potential vulnerabilities related to minor text variations. The text mentions that even advanced decontamination methods can be defeated by such variations, which suggests that encryption alone may not be sufficient to protect against sophisticated attacks. Additionally, the context does not directly discuss label protection in detail, focusing more on encryption and its limitations. Therefore, we will craft a question that encourages deep reflection on the nuances of encryption methods and their weaknesses, particularly regarding minor text variations and potential mitigation strategies.\n</document_analysis>\n\n<output_format>\nQuestion: What strategic approach can be taken to mitigate the risk of minor text variations compromising encryption-based methods for securing evaluation data?\nA: Implementing strict access controls and regular key rotation to minimize the risk of private keys being exposed.\nB: Utilizing hash functions and checksums to detect and prevent unauthorized alterations to encrypted texts.\nC: Enhancing encryption algorithms to be more resilient against small changes in plaintext.\nD: Developing a robust labeling system where labels are obfuscated and only accessible to authorized evaluators.\nExplanation: The correct answer is B because hash functions and checksums can help detect any unauthorized modifications to the encrypted data, thereby providing an additional layer of security. While options A, C, and D are valid strategies, they do not directly address the issue of minor text variations in the encrypted data, which is a focus of the question.\nCorrect Answer: B\n</output_format>",
    "question": "What strategic approach can be taken to mitigate the risk of minor text variations compromising encryption-based methods for securing evaluation data?",
    "choices": {
      "A": "Implementing strict access controls and regular key rotation to minimize the risk of private keys being exposed.",
      "B": "Utilizing hash functions and checksums to detect and prevent unauthorized alterations to encrypted texts.",
      "C": "Enhancing encryption algorithms to be more resilient against small changes in plaintext.",
      "D": "Developing a robust labeling system where labels are obfuscated and only accessible to authorized evaluators."
    },
    "answer": "B",
    "explanation": "The correct answer is B because hash functions and checksums can help detect any unauthorized modifications to the encrypted data, thereby providing an additional layer of security. While options A, C, and D are valid strategies, they do not directly address the issue of minor text variations in the encrypted data, which is a focus of the question.",
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 4,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "question_token_count": 24,
    "avg_answer_token_count": 18
  },
  {
    "context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n",
    "topic": "Adherence to Mathador Games Rules in Mathador-LM for Dynamic Math Benchmark Generation",
    "response": "<document_analysis>\nThe provided context discusses different methods for generating dynamic math benchmarks, focusing on Mathador-LM's approach. The key points are:\n1. The limitations of human effort and data contamination in live benchmarks.\n2. The rule-based generation method and its low collision probability.\n3. Template-based methods, such as GSM-Symbolic and MMLU-CF.\n4. Mathador-LM specifically uses predefined rules from Mathador games to generate evaluation queries.\n\nThe central theme is the adherence to specific rules for generating test cases, particularly in Mathador-LM. The question should delve into the nuances of rule adherence and its implications.\n</document_analysis>\n\n<output_format>\nQuestion: How does Mathador-LM ensure the validity and uniqueness of its generated queries compared to template-based methods?\nA: By strictly following the rules of Mathador games, thus minimizing collisions and ensuring adherence to game-specific constraints.\nB: Through frequent human verification of each generated query.\nC: By using complex algorithms to predict and avoid duplicate queries.\nD: By relying on recent mathematical competition problems for inspiration.\nExplanation: Mathador-LM's unique approach lies in its strict adherence to predefined rules from the Mathador games, which minimizes collisions and ensures that each query adheres to specific game constraints, unlike template-based methods which may introduce variability through placeholders and randomization.\nCorrect Answer: A\n</output_format>",
    "question": "How does Mathador-LM ensure the validity and uniqueness of its generated queries compared to template-based methods?",
    "choices": {
      "A": "By strictly following the rules of Mathador games, thus minimizing collisions and ensuring adherence to game-specific constraints.",
      "B": "Through frequent human verification of each generated query.",
      "C": "By using complex algorithms to predict and avoid duplicate queries.",
      "D": "By relying on recent mathematical competition problems for inspiration."
    },
    "answer": "A",
    "explanation": "Mathador-LM's unique approach lies in its strict adherence to predefined rules from the Mathador games, which minimizes collisions and ensures that each query adheres to specific game constraints, unlike template-based methods which may introduce variability through placeholders and randomization.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 8,
    "question_token_count": 22,
    "avg_answer_token_count": 13
  },
  {
    "context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n",
    "topic": "Debate surrounding syntactic transformations as a form of data contamination.",
    "response": "<output_format>\nQuestion: What is the primary argument against considering syntactic transformations as a form of data contamination in the context of evaluating language models?\n\nA: Syntactic transformations do not genuinely represent new information and can be easily distinguished from memorized information.\n\nB: Language models are unable to reason about syntactic structures, making syntactic transformations irrelevant.\n\nC: Such transformations are merely stylistic and do not affect the model's reasoning capabilities.\n\nD: The presence of syntactic transformations in test data does not necessarily indicate that the model has learned the information but could be a result of the model's ability to rephrase or manipulate existing data.\n\nExplanation: The correct answer requires a nuanced understanding of the debate around syntactic transformations. Option A is incorrect because the text suggests it is challenging to distinguish between memorized information and reasoning capabilities. Option B is too simplistic and overlooks the complexity of the issue. Option C is too narrow and misses the broader point of the debate. Option D captures the essence of the debate, acknowledging the difficulty in attributing the model's performance solely to reasoning versus manipulation.\n\nCorrect Answer: D\n</output_format>\n\n<document_analysis>\nThe context discusses the ongoing debate regarding syntactic transformations as a form of data contamination. It highlights the difficulty in distinguishing between the model's reasoning capabilities and its ability to recall memorized information. The text emphasizes the importance of considering syntactic transformations as contamination because some NLP applications rely heavily on syntactic information. The key argument against considering syntactic transformations as contamination is that it is challenging to determine whether the model's performance is due to its reasoning or simply manipulating existing information, making option D the most appropriate answer.\n</document_analysis>",
    "question": "What is the primary argument against considering syntactic transformations as a form of data contamination in the context of evaluating language models?",
    "choices": {
      "A": "Syntactic transformations do not genuinely represent new information and can be easily distinguished from memorized information.",
      "B": "Language models are unable to reason about syntactic structures, making syntactic transformations irrelevant.",
      "C": "Such transformations are merely stylistic and do not affect the model's reasoning capabilities.",
      "D": "The presence of syntactic transformations in test data does not necessarily indicate that the model has learned the information but could be a result of the model's ability to rephrase or manipulate existing data."
    },
    "answer": "D",
    "explanation": "The correct answer requires a nuanced understanding of the debate around syntactic transformations. Option A is incorrect because the text suggests it is challenging to distinguish between memorized information and reasoning capabilities. Option B is too simplistic and overlooks the complexity of the issue. Option C is too narrow and misses the broader point of the debate. Option D captures the essence of the debate, acknowledging the difficulty in attributing the model's performance solely to reasoning versus manipulation.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 6,
    "question_groundedness_score": 9,
    "question_token_count": 24,
    "avg_answer_token_count": 22
  },
  {
    "context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "The impact of data contamination on the evaluation of large language models (LLMs).",
    "response": "<document_analysis>\nThe provided context discusses the issue of data contamination in the evaluation of large language models (LLMs) and introduces various methods to mitigate this problem. The text does not contain any irrelevant or promotional content. The discussion is centered around the challenges posed by data contamination and the proposed solutions for addressing them. This content is highly relevant and valuable for domain experts in NLP and LLM evaluation.\n</document_analysis>\n\n<output_format>\nQuestion: How does data contamination impact the evaluation of large language models (LLMs), and what are the proposed methods to mitigate its effects?\nA: It leads to an inflated and misleading assessment of LLM performance, and mitigating it requires methods like continuous updating of benchmark datasets and regenerating benchmark data.\nB: It enhances the performance of LLMs by providing more diverse training data, and there are no significant methods to mitigate its effects.\nC: It has minimal impact on LLM evaluation, and existing static benchmarks are sufficient to ensure reliable performance metrics.\nD: It primarily affects the computational efficiency of LLMs during training and does not significantly impact their evaluation.\nExplanation: The correct answer is A because data contamination inflates LLM performance, and methods like continuous updates and regeneration of benchmark data help mitigate this issue. Options B, C, and D are incorrect as they either misrepresent the impact of data contamination or fail to acknowledge the proposed mitigation strategies.\nCorrect Answer: A\n</output_format>",
    "question": "How does data contamination impact the evaluation of large language models (LLMs), and what are the proposed methods to mitigate its effects?",
    "choices": {
      "A": "It leads to an inflated and misleading assessment of LLM performance, and mitigating it requires methods like continuous updating of benchmark datasets and regenerating benchmark data.",
      "B": "It enhances the performance of LLMs by providing more diverse training data, and there are no significant methods to mitigate its effects.",
      "C": "It has minimal impact on LLM evaluation, and existing static benchmarks are sufficient to ensure reliable performance metrics.",
      "D": "It primarily affects the computational efficiency of LLMs during training and does not significantly impact their evaluation."
    },
    "answer": "A",
    "explanation": "The correct answer is A because data contamination inflates LLM performance, and methods like continuous updates and regeneration of benchmark data help mitigate this issue. Options B, C, and D are incorrect as they either misrepresent the impact of data contamination or fail to acknowledge the proposed mitigation strategies.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 9,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "question_token_count": 27,
    "avg_answer_token_count": 24
  },
  {
    "context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n",
    "topic": "Quantifying the Reliability of Dynamic Benchmarks in Reflecting True LLM Capabilities",
    "response": "<output_format>\nQuestion: How does the concept of \"collision\" in dynamic benchmarking affect the reliability of a benchmark in reflecting true LLM capabilities, and what metrics are proposed to evaluate this aspect?\n\nA: It decreases reliability because collisions indicate potential data contamination and overlap, leading to less diverse test cases.\nB: It increases reliability as it ensures consistent results across different transformations.\nC: It has no impact since collisions only affect the training phase, not the evaluation phase.\nD: It enhances reliability by providing insights into the benchmark's ability to generate novel variations and detect contamination.\n\nExplanation: Collision refers to the extent of overlap between different transformations of the benchmark dataset. High collision rates indicate more overlap, which can limit the benchmark's ability to generate novel and diverse test cases. Metrics like Collision Rate and Repeat Trials are proposed to quantify this overlap and the number of trials needed to regenerate an existing dataset. These metrics help ensure the benchmark remains effective despite potential training data contamination.\n\nCorrect Answer: A\n</output_format>",
    "question": "How does the concept of \"collision\" in dynamic benchmarking affect the reliability of a benchmark in reflecting true LLM capabilities, and what metrics are proposed to evaluate this aspect?",
    "choices": {
      "A": "It decreases reliability because collisions indicate potential data contamination and overlap, leading to less diverse test cases.",
      "B": "It increases reliability as it ensures consistent results across different transformations.",
      "C": "It has no impact since collisions only affect the training phase, not the evaluation phase.",
      "D": "It enhances reliability by providing insights into the benchmark's ability to generate novel variations and detect contamination."
    },
    "answer": "A",
    "explanation": "Collision refers to the extent of overlap between different transformations of the benchmark dataset. High collision rates indicate more overlap, which can limit the benchmark's ability to generate novel and diverse test cases. Metrics like Collision Rate and Repeat Trials are proposed to quantify this overlap and the number of trials needed to regenerate an existing dataset. These metrics help ensure the benchmark remains effective despite potential training data contamination.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "question_token_count": 34,
    "avg_answer_token_count": 17
  },
  {
    "context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n",
    "topic": "Evaluate a model's ability to generate and debug code using benchmark tests like HumanEval, MBPP, and SWE-Bench.",
    "response": "<document_analysis>\nThe context primarily discusses various benchmarks used to evaluate the capabilities of models in different domains. It specifically mentions coding benchmarks (HumanEval, MBPP, and SWE-Bench) for assessing a model's ability to generate and debug code. The text also touches upon other types of benchmarks like instruction following and reasoning. While the context provides useful information, the focus on coding benchmarks makes the most sense for the given question topic. Other parts of the context can be disregarded as they are less relevant to evaluating a model's coding abilities.\n</document_analysis>\n\n<output_format>\nQuestion: Which of the following benchmarks would best evaluate a model's capability to generate and debug complex software systems, including handling edge cases and integrating with existing codebases?\n\nA: HumanEval\nB: MBPP\nC: SWE-Bench\nD: C-Eval\n\nExplanation: SWE-Bench is designed to address more advanced challenges in software engineering, including complex code synthesis and debugging tasks that require handling edge cases and integrating with existing codebases, making it the most suitable benchmark among the options provided.\n\nCorrect Answer: C\n</output_format>",
    "question": "Which of the following benchmarks would best evaluate a model's capability to generate and debug complex software systems, including handling edge cases and integrating with existing codebases?",
    "choices": {
      "A": "HumanEval",
      "B": "MBPP",
      "C": "SWE-Bench",
      "D": "C-Eval"
    },
    "answer": "C",
    "explanation": "SWE-Bench is designed to address more advanced challenges in software engineering, including complex code synthesis and debugging tasks that require handling edge cases and integrating with existing codebases, making it the most suitable benchmark among the options provided.",
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "question_token_count": 32,
    "avg_answer_token_count": 3
  }
]