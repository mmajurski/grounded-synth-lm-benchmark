[
  {
    "context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n",
    "topic": "Analyze how preserving stylistic integrity and core knowledge content during rewriting affects benchmark validity and fairness.",
    "response": "<document_analysis>\nThe context discusses various benchmark rewriting methods that retain stylistic and knowledge content (Auto-Dataset, StructEval, ITD, VarBench) to generate new samples while addressing contamination and difficulty preservation. Central themes include maintaining construct validity by controlling stylistic variance, mitigating contamination, and balancing sample diversity and fairness. Implicit is the trade-off between preserving original biases versus fostering unbiased, diverse benchmarks. A probing question should explore how style and content preservation impacts both validity and fairness in dataset creation. Difficulty calibrated at 8/10 to challenge domain experts on trade-offs in dataset rewriting methodologies.\n</document_analysis>\n\n<output_format>\nQuestion: In benchmark rewriting, how does retaining original stylistic patterns and core knowledge simultaneously influence both the construct validity and fairness of the resulting dataset?\n\nExplanation: Preserving style minimizes superficial variance to uphold construct validity but also replicates existing biases and limits diversity, thereby undermining fairness.\n\nCorrect Answer: It preserves construct validity by controlling non-conceptual variance yet perpetuates biases and reduces diversity, compromising fairness.\n</output_format>",
    "question": "In benchmark rewriting, how does retaining original stylistic patterns and core knowledge simultaneously influence both the construct validity and fairness of the resulting dataset?",
    "answer": "It preserves construct validity by controlling non-conceptual variance yet perpetuates biases and reduces diversity, compromising fairness.",
    "explanation": "Preserving style minimizes superficial variance to uphold construct validity but also replicates existing biases and limits diversity, thereby undermining fairness.",
    "question_token_count": 28,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 22
  },
  {
    "context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n",
    "topic": "A comparative analysis of encryption\u2010based protection versus label withholding in terms of security guarantees, operational costs, and usability.",
    "response": "<document_analysis>\nThe context contrasts two evaluation data protection strategies: encryption (public\u2010key encryption, confidential computing, secure multi\u2010party computation) and label protection (withholding true answers). Encryption provides strong confidentiality but incurs computational overhead and key management vulnerabilities; label withholding is operationally simpler and more usable but offers weaker security (only covering labels, not data content). A probing question can ask for a comparative analysis of their trade\u2010offs across security guarantees, operational costs, and usability. Difficulty is calibrated at 8/10 to challenge domain experts to synthesize these aspects succinctly.\n</document_analysis>\n\n<output_format>\nQuestion: Contrast the primary trade\u2010offs between encryption\u2010based data protection and label withholding in evaluation benchmarks regarding their security assurances, computational overhead, and usability.\n\nExplanation: Encryption requires robust key management and introduces computational overhead to secure data and parameters, whereas label withholding is operationally lightweight and easier to use but only conceals answer labels, providing weaker confidentiality.\n\nCorrect Answer: Encryption provides stronger confidentiality with higher computational and key management costs; label withholding offers minimal overhead and greater usability but weaker security guarantees.\n</output_format>",
    "question": "Contrast the primary trade\u2010offs between encryption\u2010based data protection and label withholding in evaluation benchmarks regarding their security assurances, computational overhead, and usability.",
    "answer": "Encryption provides stronger confidentiality with higher computational and key management costs; label withholding offers minimal overhead and greater usability but weaker security guarantees.",
    "explanation": "Encryption requires robust key management and introduces computational overhead to secure data and parameters, whereas label withholding is operationally lightweight and easier to use but only conceals answer labels, providing weaker confidentiality.",
    "question_token_count": 30,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 26
  },
  {
    "context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n",
    "topic": "Methodology of DARG for integrating reasoning\u2010graph extraction and rule\u2010based perturbation to produce new benchmark samples",
    "response": "<document_analysis>\nThe context describes DARG\u2019s hybrid generation methodology: it integrates LLM-based and graph-based generation by (1) extracting reasoning graphs from existing benchmarks and (2) perturbing those graphs using predefined rules to create new samples. This two-step pipeline addresses static benchmark contamination by ensuring novel, logically coherent examples. The content is meaningful and directly informs the question topic.\nDifficulty: 7/10 \u2013 requires understanding of hybrid benchmark generation and the benefits of graph-based perturbations versus pure LLM generation.\n</document_analysis>\n\n<output_format>\nQuestion: What key advantage does incorporating rule-based perturbation of extracted reasoning graphs offer in DARG\u2019s hybrid generation pipeline compared to relying solely on LLM-based generation?\nExplanation: Rule-based perturbation systematically modifies the logical structure of reasoning graphs, ensuring controlled complexity and preserving coherence, which pure LLM outputs may lack.\nCorrect Answer: It provides controlled complexity and preserves logical consistency by systematically modifying reasoning graphs.\n</output_format>",
    "question": "What key advantage does incorporating rule-based perturbation of extracted reasoning graphs offer in DARG\u2019s hybrid generation pipeline compared to relying solely on LLM-based generation?",
    "answer": "It provides controlled complexity and preserves logical consistency by systematically modifying reasoning graphs.",
    "explanation": "Rule-based perturbation systematically modifies the logical structure of reasoning graphs, ensuring controlled complexity and preserving coherence, which pure LLM outputs may lack.",
    "question_token_count": 30,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 8,
    "avg_answer_token_count": 15
  },
  {
    "context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n",
    "topic": "Discussion of the research gaps and omissions identified in the survey, including recent methods and emerging benchmarking practices.",
    "response": "<document_analysis>\nThe context outlines the survey\u2019s conclusions and limitations, specifically noting omissions such as recent methods/tools, emerging benchmarking practices, the need for refinement/validation of proposed dynamic benchmarking criteria in real-world settings, and lack of fine-grained technical details. For a domain expert, probing the acknowledged shortfall in the proposed dynamic benchmarking criteria\u2019s applicability invites critical reflection on bridging theoretical proposals with practical validation.\nDifficulty: 8/10\n</document_analysis>\n\n<output_format>\nQuestion: What essential validation step for the proposed dynamic benchmarking criteria does the survey acknowledge is still required for practical adoption?\nExplanation: The survey notes that its criteria for dynamic benchmarking are preliminary and must undergo further refinement and real-world validation before they can be reliably applied.\nCorrect Answer: Further refinement and validation in real-world applications\n</output_format>",
    "question": "What essential validation step for the proposed dynamic benchmarking criteria does the survey acknowledge is still required for practical adoption?",
    "answer": "Further refinement and validation in real-world applications",
    "explanation": "The survey notes that its criteria for dynamic benchmarking are preliminary and must undergo further refinement and real-world validation before they can be reliably applied.",
    "question_token_count": 22,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 9
  },
  {
    "context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n",
    "topic": "Analysis of trade\u2011offs between benchmark stability and responsiveness to new data in dynamic approaches.",
    "response": "<document_analysis>\nThe context discusses ethical considerations in LLM benchmarks, contrasting static and dynamic approaches. Key points include: static benchmarks may perpetuate biases due to outdated data; dynamic benchmarks adapt to new data but introduce privacy/security concerns and potential misuse. It stresses the need for fairness, accountability, and privacy in benchmarking frameworks. This content is coherent and suitable for generating a deep, technical question exploring the trade\u2011off between stability and adaptability when using dynamic benchmarks.\n</document_analysis>\n\n<output_format>\nQuestion: Which combination of mechanisms can a dynamic LLM benchmarking framework employ to remain responsive to emerging data while preserving user privacy and ensuring evaluation fairness?\n\nExplanation: Integrating differential privacy, continuous bias auditing, and transparent update logs balances adaptability, data protection, and accountability.\n\nCorrect Answer: Integrating differential privacy in data collection, continuous bias auditing, and transparent update logs\n</output_format>",
    "question": "Which combination of mechanisms can a dynamic LLM benchmarking framework employ to remain responsive to emerging data while preserving user privacy and ensuring evaluation fairness?",
    "answer": "Integrating differential privacy in data collection, continuous bias auditing, and transparent update logs",
    "explanation": "Integrating differential privacy, continuous bias auditing, and transparent update logs balances adaptability, data protection, and accountability.",
    "question_token_count": 27,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 8,
    "avg_answer_token_count": 17
  },
  {
    "context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n",
    "topic": "Discussing the trade-offs and complementary roles of direct overlap detection versus behavior-based contamination analyses.",
    "response": "<document_analysis>\nThe context discusses limitations of direct overlap detection methods (n-gram matching leading to false negatives) and introduces behavior-based contamination analyses (memorization via masked inputs, partial completions, performance shifts) as complementary approaches. Key contrasts include surface-level vs. semantic detection and trade-offs between sensitivity and interpretability. These insights support a question probing the complementary strengths and weaknesses of both methods and the benefit of integrating them.\nPlanned question difficulty: 8/10 (requires synthesis of technical detection methods and understanding of their interplay).\n</document_analysis>\n\n<output_format>\nQuestion: Contrast the detection capabilities and blind spots of n\u2011gram based exact matching versus masked\u2011input memorization analysis in post\u2011hoc contamination detection, and explain how integrating these approaches enhances overall diagnostic robustness.\n\nExplanation: Exact n\u2011gram matching reliably finds verbatim reuse but misses paraphrases or semantic overlaps, while masked\u2011input memorization reveals learned continuations even when text is altered; combining them covers both surface duplicates and deep memorization.\n\nCorrect Answer: N\u2011gram matching catches exact reuse but overlooks paraphrases, whereas masked\u2011input analysis exposes memorized outputs on perturbed inputs, and their integration ensures both surface\u2011level overlaps and deeper memorization patterns are detected.\n</output_format>",
    "question": "Contrast the detection capabilities and blind spots of n\u2011gram based exact matching versus masked\u2011input memorization analysis in post\u2011hoc contamination detection, and explain how integrating these approaches enhances overall diagnostic robustness.",
    "answer": "N\u2011gram matching catches exact reuse but overlooks paraphrases, whereas masked\u2011input analysis exposes memorized outputs on perturbed inputs, and their integration ensures both surface\u2011level overlaps and deeper memorization patterns are detected.",
    "explanation": "Exact n\u2011gram matching reliably finds verbatim reuse but misses paraphrases or semantic overlaps, while masked\u2011input memorization reveals learned continuations even when text is altered; combining them covers both surface duplicates and deep memorization.",
    "question_token_count": 40,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 44
  },
  {
    "context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n",
    "topic": "In-depth examination of rule-based test case generation, focusing on the mechanisms by which predefined rules achieve extremely low collision probability.",
    "response": "<document_analysis>\nThe context describes rule-based generation as synthesizing new test cases using predefined rules, highlighting its extremely low collision probability. It contrasts with template-based approaches but does not elaborate on the mechanism. To probe an expert\u2019s deep understanding, the question should ask how rule-based methods leverage rule design to minimize duplication. The difficulty is calibrated at about 8/10, requiring knowledge of combinatorial design and sampling theory in test generation.\n</document_analysis>\n\n<output_format>\nQuestion: In rule-based test case generation, which mechanism underlies its extremely low collision probability, and how does this mechanism conceptually minimize duplicate instances?\n\nExplanation: It hinges on combining independent rule parameters to exponentially expand the instance space, making repeated sampling statistically negligible.\n\nCorrect Answer: Independent parametrized rule templates combinatorially generate instances across multiple orthogonal dimensions, yielding a vast space where collisions are exceedingly improbable.\n</output_format>",
    "question": "In rule-based test case generation, which mechanism underlies its extremely low collision probability, and how does this mechanism conceptually minimize duplicate instances?",
    "answer": "Independent parametrized rule templates combinatorially generate instances across multiple orthogonal dimensions, yielding a vast space where collisions are exceedingly improbable.",
    "explanation": "It hinges on combining independent rule parameters to exponentially expand the instance space, making repeated sampling statistically negligible.",
    "question_token_count": 29,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 27
  },
  {
    "context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n",
    "topic": "Critically evaluate the rationale and methodology behind Auto-Dataset\u2019s twofold sample generation strategy and its use of Bloom\u2019s taxonomy for cognitive-level variation.",
    "response": "<document_analysis>\nThe context outlines several LLM-based benchmark rewriting methods, focusing on Auto-Dataset which uses a twofold strategy: (1) style\u2011and\u2011knowledge preservation of original samples, and (2) generation of related questions mapped to different cognitive levels per Bloom\u2019s taxonomy. The rationale is to mitigate data contamination, enhance diversity, and control difficulty systematically. Underlying theories include Bloom\u2019s taxonomy\u2019s six cognitive hierarchies to ensure varied cognitive demand. This invites an expert to probe how combining stylistic fidelity with taxonomy\u2011guided question generation operationalizes diversity and contamination control.\nDifficulty rating: 8/10 \u2013 requires synthesis of prompt\u2011engineering rationale, Bloom\u2019s taxonomy, and contamination concerns.\n</document_analysis>\n\n<output_format>\nQuestion: How does Auto\u2011Dataset\u2019s bifurcated generation strategy leverage Bloom\u2019s taxonomy to simultaneously preserve original sample fidelity and expand cognitive diversity, and why is this dual approach critical for mitigating in\u2011distribution contamination?\nExplanation: Auto\u2011Dataset instructs LLMs to create one set of samples that mirror the stylistic and knowledge aspects of the original and a second set of questions explicitly targeted at distinct Bloom\u2019s cognitive levels; this ensures both fidelity to the source material and systematic variation in reasoning demands, thereby reducing overfitting to contaminated benchmarks.\nCorrect Answer: By prompting LLMs to produce style\u2011preserving duplicates alongside Bloom\u2019s\u2011level\u2013specific questions, Auto\u2011Dataset maintains domain fidelity while diversifying cognitive tasks to prevent in\u2011distribution contamination.\n</output_format>",
    "question": "How does Auto\u2011Dataset\u2019s bifurcated generation strategy leverage Bloom\u2019s taxonomy to simultaneously preserve original sample fidelity and expand cognitive diversity, and why is this dual approach critical for mitigating in\u2011distribution contamination?",
    "answer": "By prompting LLMs to produce style\u2011preserving duplicates alongside Bloom\u2019s\u2011level\u2013specific questions, Auto\u2011Dataset maintains domain fidelity while diversifying cognitive tasks to prevent in\u2011distribution contamination.",
    "explanation": "Auto\u2011Dataset instructs LLMs to create one set of samples that mirror the stylistic and knowledge aspects of the original and a second set of questions explicitly targeted at distinct Bloom\u2019s cognitive levels; this ensures both fidelity to the source material and systematic variation in reasoning demands, thereby reducing overfitting to contaminated benchmarks.",
    "question_token_count": 41,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 7,
    "question_difficulty_score": 9,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 39
  },
  {
    "context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n",
    "topic": "Discuss the mathematical role of the expectation operator in averaging diversity scores across datasets or dataset pairs.",
    "response": "<document_analysis>\n1. Content Examination: The context defines two diversity metrics\u2014external and internal\u2014using an expectation operator over dataset indices. Both metrics average a pairwise diversity function \u0398 across either single datasets relative to a seed (external) or dataset pairs (internal).\n2. Concept Exploration: The expectation operator E implicitly sums scores across the index set and normalizes by its cardinality (N for external, N(N\u20131) for internal), thus producing a mean diversity. Understanding this requires recognition of uniform weighting and normalization in finite discrete expectations.\n3. Difficulty: Rated 8/10\u2014requires expert-level understanding of discrete expectation notation, normalization factors, and their implications in metric definitions.\n4. Question Planning: The question prompts analysis of how E transforms a set of pairwise diversity scores into a single metric and the importance of its implicit normalization, pushing the expert to articulate the mathematical role of expectation in these formulas.\n</document_analysis>\n\n<output_format>\nQuestion: In the definitions of external and internal diversity, how does the expectation operator convert the collection of \u0398-diversity scores into a single metric, and what normalization does it implicitly apply?\nExplanation: The expectation operator sums all \u0398 scores over the specified index set and divides by the number of terms, yielding the uniform average diversity.\nCorrect Answer: It uniformly averages \u0398 over N datasets (external) or N(N\u22121) ordered pairs (internal) by summing all scores and dividing by the respective count.\n</output_format>",
    "question": "In the definitions of external and internal diversity, how does the expectation operator convert the collection of \u0398-diversity scores into a single metric, and what normalization does it implicitly apply?",
    "answer": "It uniformly averages \u0398 over N datasets (external) or N(N\u22121) ordered pairs (internal) by summing all scores and dividing by the respective count.",
    "explanation": "The expectation operator sums all \u0398 scores over the specified index set and divides by the number of terms, yielding the uniform average diversity.",
    "question_token_count": 37,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 33
  },
  {
    "context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n",
    "topic": "Examining behavior-based post-hoc methods that detect memorization through masked input experiments.",
    "response": "<document_analysis>\nThe context outlines post-hoc contamination detection methods, emphasizing behavior-based techniques like masked-input experiments (Ranaldi et\u00a0al. 2024; Chang et\u00a0al. 2023) that reveal memorization by checking if a model restores hidden tokens verbatim. The key nuance is distinguishing true memorization (exact span recall) from generalization (pattern-based completion). A high-level question probes recognition of the definitive behavioral indicator\u2014verbatim span reconstruction despite context changes\u2014highlighting a deep understanding of how such tests isolate memorized content.\n</document_analysis>\n\n<output_format>\nQuestion: In behavior-based post-hoc detection using masked-input experiments, what specific model response most definitively indicates memorization rather than learned generalization?\nExplanation: A model that has memorized data will reproduce the masked tokens exactly, even when the surrounding context is altered, whereas a generalized model would generate more variable completions.\nCorrect Answer: Verbatim reconstruction of masked spans under context perturbations\n</output_format>",
    "question": "In behavior-based post-hoc detection using masked-input experiments, what specific model response most definitively indicates memorization rather than learned generalization?",
    "answer": "Verbatim reconstruction of masked spans under context perturbations",
    "explanation": "A model that has memorized data will reproduce the masked tokens exactly, even when the surrounding context is altered, whereas a generalized model would generate more variable completions.",
    "question_token_count": 27,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 7,
    "avg_answer_token_count": 10
  },
  {
    "context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Reflect on how continuous training on benchmarking data can bias LLM evaluation outcomes and suggest mitigation strategies.",
    "response": "<document_analysis>\nThe context discusses the evolution of LLM benchmarking, highlighting static benchmarks for instruction-following and coding tasks, the challenges posed by rapid model improvements and data contamination, and proposed solutions such as contamination detectors and dynamic benchmarks. Key themes include the unreliability of unchanging test sets, contamination risk from models training on benchmark data, and the need for dynamic updating. This informs a question probing experts on the deep relationship between continuous training, benchmark contamination, and mitigation via dynamic benchmarks.\n</document_analysis>\n\n<output_format>\nQuestion: What specific contamination risk arises when LLMs continuously train on static benchmark datasets, and how do dynamic benchmarks mitigate this issue while maintaining comprehensive performance evaluation?\n\nExplanation: Continuous exposure to static benchmarks leads models to \u201cmemorize\u201d test items, inflating performance scores; dynamic benchmarks counteract this by regularly refreshing test instances to prevent prior exposure without narrowing task coverage.\n\nCorrect Answer: Static benchmarks become contaminated as models train on the very test data they will be evaluated on, and dynamic benchmarks refresh and rotate examples to avoid exposure while preserving evaluation breadth.\n</output_format>",
    "question": "What specific contamination risk arises when LLMs continuously train on static benchmark datasets, and how do dynamic benchmarks mitigate this issue while maintaining comprehensive performance evaluation?",
    "answer": "Static benchmarks become contaminated as models train on the very test data they will be evaluated on, and dynamic benchmarks refresh and rotate examples to avoid exposure while preserving evaluation breadth.",
    "explanation": "Continuous exposure to static benchmarks leads models to \u201cmemorize\u201d test items, inflating performance scores; dynamic benchmarks counteract this by regularly refreshing test instances to prevent prior exposure without narrowing task coverage.",
    "question_token_count": 30,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 34
  },
  {
    "context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n",
    "topic": "Statistical assumptions underlying the calculation of Collision Rate and Repeat Trials",
    "response": "<document_analysis>\nThe context introduces two metrics\u2014Collision Rate and Repeat Trials\u2014to evaluate dynamic benchmarks. Collision Rate measures overlap (probability of the same transformed dataset in two independent trials), while Repeat Trials measures the expected number of trials to reproduce an existing transformation. Both metrics assume a probabilistic model in which each transformation is sampled from a fixed set of possible transformations. To derive Collision Rate as a probability and Repeat Trials via a geometric distribution (expected trials until a \u201csuccess\u201d), one must assume independent, uniform sampling over this finite transformation space. Without these assumptions, the reciprocal relationship and geometric model no longer hold, undermining the interpretability of both metrics.\n</document_analysis>\n\n<output_format>\nQuestion: Which core statistical assumption about the transformation process allows the Collision Rate to serve as the success probability in a geometric model for Repeat Trials?\nExplanation: Modeling the number of trials until regenerating a dataset as a geometric distribution requires that each transformation be drawn independently and uniformly from the space of all possible transformations, making the Collision Rate the constant success probability.\nCorrect Answer: Independent, uniform sampling of transformations\n</output_format>",
    "question": "Which core statistical assumption about the transformation process allows the Collision Rate to serve as the success probability in a geometric model for Repeat Trials?",
    "answer": "Independent, uniform sampling of transformations",
    "explanation": "Modeling the number of trials until regenerating a dataset as a geometric distribution requires that each transformation be drawn independently and uniformly from the space of all possible transformations, making the Collision Rate the constant success probability.",
    "question_token_count": 27,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 3,
    "question_difficulty_score": 8,
    "question_groundedness_score": 6,
    "avg_answer_token_count": 7
  },
  {
    "context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Critically evaluate the assumptions about model behavior (e.g., memorization and perplexity) used in overlap detection and discuss their potential failure modes.",
    "response": "<document_analysis>\nThe context highlights that post-hoc overlap detection assumes contaminated data yield higher memorization and lower perplexity, yet these heuristics can fail. A probing question should ask domain experts to identify specific scenarios (e.g., privacy-preserving training, data augmentation, domain adaptation) where these signals invert or vanish, and to propose stronger detection metrics (e.g., embedding similarity, influence functions). Difficulty: 9/10.\n</document_analysis>\n\n<output_format>\nQuestion: What training or data-processing scenarios can decouple the typical correlation between contamination, increased memorization, and reduced perplexity, and which alternative detection methods can compensate when these heuristics fail?\nExplanation: Certain practices like differential privacy or aggressive data augmentation mask memorization and perplexity cues, so overlap detection must leverage internal model signals such as embedding similarity or influence-based metrics.\nCorrect Answer: Differential privacy or strong data augmentation can equalize memorization and perplexity, requiring embedding-space similarity or gradient-influence detection.\n</output_format>",
    "question": "What training or data-processing scenarios can decouple the typical correlation between contamination, increased memorization, and reduced perplexity, and which alternative detection methods can compensate when these heuristics fail?",
    "answer": "Differential privacy or strong data augmentation can equalize memorization and perplexity, requiring embedding-space similarity or gradient-influence detection.",
    "explanation": "Certain practices like differential privacy or aggressive data augmentation mask memorization and perplexity cues, so overlap detection must leverage internal model signals such as embedding similarity or influence-based metrics.",
    "question_token_count": 37,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 9,
    "question_groundedness_score": 2,
    "avg_answer_token_count": 26
  },
  {
    "context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n",
    "topic": "Formally define the scalability metric as presented and describe each mathematical component (\u2225T_i(D)\u2225, \u2225D\u2225, Cost(\u22c5), expectation).",
    "response": "<document_analysis>\nThe context defines a single equation for scalability as an expectation over the transformation space. It specifies each component: \u2225T_i(D)\u2225 (size of transformed dataset), \u2225D\u2225 (size of original dataset), Cost(\u00b7) (transformation cost), and expectation over all T_i. All content is coherent and educational, so a question-answer pair may be generated.\n</document_analysis>\n\n<output_format>\nQuestion: Provide the formal definition of the scalability metric for dynamic benchmarks and explain the meaning of each symbol: \u2225T_i(D)\u2225, \u2225D\u2225, Cost(\u00b7), and the expectation.\n\nExplanation: The scalability metric is defined as the expected data generated per unit cost normalized by the original dataset size. Each term denotes the size of transformed data, original data size, resource cost, and averaging across all transformations.\n\nCorrect Answer: Scalability = E_{T_i}[\u2225T_i(D)\u2225\u00a0/\u00a0(\u2225D\u2225\u00b7Cost(T_i))], where \u2225T_i(D)\u2225 is the number of examples after transformation, \u2225D\u2225 the original dataset size, Cost(T_i) the resource cost for T_i, and expectation averages over the transformation space.\n</output_format>",
    "question": "Provide the formal definition of the scalability metric for dynamic benchmarks and explain the meaning of each symbol: \u2225T_i(D)\u2225, \u2225D\u2225, Cost(\u00b7), and the expectation.",
    "answer": "Scalability = E_{T_i}[\u2225T_i(D)\u2225\u00a0/\u00a0(\u2225D\u2225\u00b7Cost(T_i))], where \u2225T_i(D)\u2225 is the number of examples after transformation, \u2225D\u2225 the original dataset size, Cost(T_i) the resource cost for T_i, and expectation averages over the transformation space.",
    "explanation": "The scalability metric is defined as the expected data generated per unit cost normalized by the original dataset size. Each term denotes the size of transformed data, original data size, resource cost, and averaging across all transformations.",
    "question_token_count": 38,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 70
  },
  {
    "context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n",
    "topic": "Criteria for robust contamination detection amid rapidly evolving LLM architectures and data sources.",
    "response": "<document_analysis>\nThe context discusses two main points: (1) the definition and debate around syntactic contamination (rephrased test data derived from training with added prefixes) and (2) the significance of identifying and preventing contamination to ensure benchmarks validly measure LLMs\u2019 generalization, robustness, and real-world applicability. A challenging question for domain experts should probe the subtle distinction between model memorization and reasoning, focusing on how to detect true novelty versus surface-level overlap. The answer must be concise yet reflect deep understanding of semantic versus syntactic criteria.\n</document_analysis>\n\n<output_format>\nQuestion: What critical property must a contamination detection framework evaluate to distinguish an LLM\u2019s true reasoning ability from mere recall of rephrased training data?\nExplanation: Effective contamination detection goes beyond surface syntax to assess whether test inputs introduce genuinely new semantic content that the model has not memorized.\nCorrect Answer: Semantic novelty beyond syntactic similarity\n</output_format>",
    "question": "What critical property must a contamination detection framework evaluate to distinguish an LLM\u2019s true reasoning ability from mere recall of rephrased training data?",
    "answer": "Semantic novelty beyond syntactic similarity",
    "explanation": "Effective contamination detection goes beyond surface syntax to assess whether test inputs introduce genuinely new semantic content that the model has not memorized.",
    "question_token_count": 29,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 6
  },
  {
    "context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n",
    "topic": "Examine potential biases and limitations present in current benchmark datasets across coding, instruction following, and reasoning, and propose strategies for more equitable evaluation.",
    "response": "<document_analysis>\nThe context lists various benchmarks across coding, instruction following, and reasoning but does not explicitly discuss biases or limitations. However, one can infer that these benchmarks may share common shortcomings such as limited domain coverage, language and cultural biases, and skewed difficulty distributions. A probing question can ask experts to identify such an inferred limitation (e.g., dataset homogeneity or demographic narrowness) and propose a high-level mitigation strategy (e.g., diversifying data sources and integrating fairness metrics).\nDifficulty rating: 8/10\n</document_analysis>\n\n<output_format>\nQuestion: What overarching limitation stemming from domain and demographic narrowness affects coding, instruction-following, and reasoning benchmarks, and what concise strategy can mitigate it for more equitable evaluation?\nExplanation: The answer pinpoints narrow scope as the core bias and suggests diversification and fairness integration as the remedy.\nCorrect Answer: Their narrow domain and demographic scope, mitigated by building multilingual, multi-domain, culturally diverse benchmarks with integrated fairness metrics.\n</output_format>",
    "question": "What overarching limitation stemming from domain and demographic narrowness affects coding, instruction-following, and reasoning benchmarks, and what concise strategy can mitigate it for more equitable evaluation?",
    "answer": "Their narrow domain and demographic scope, mitigated by building multilingual, multi-domain, culturally diverse benchmarks with integrated fairness metrics.",
    "explanation": "The answer pinpoints narrow scope as the core bias and suggests diversification and fairness integration as the remedy.",
    "question_token_count": 35,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 8,
    "avg_answer_token_count": 26
  },
  {
    "context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n",
    "topic": "Synthesize lessons from existing temporal cutoff benchmarks to propose a comprehensive set of best practices for future LLM benchmark development.",
    "response": "<document_analysis>\nThe context outlines a variety of benchmarks that all employ post\u2013knowledge-cutoff data\u2014ranging from math problems and coding challenges to academic papers and forecasting questions\u2014and refresh this data periodically to prevent contamination and keep evaluations current. The unified insight is the need for continuous collection of fresh, domain-diverse data beyond the model\u2019s cutoff. This suggests best practices for future benchmarks: implement regular, automated updates of new data from reliable sources across multiple fields to guarantee relevance and integrity. Difficulty: 8.\n</document_analysis>\n\n<output_format>\nQuestion: What key design principle should guide future temporal cutoff benchmarks, as evidenced by LiveBench, AntiLeak-Bench, AcademicEval, LiveCodeBench, LiveAoPSBench, and Forecastbench?\nExplanation: Each of these benchmarks relies on continuously updated, post\u2013knowledge-cutoff data across diverse domains to prevent contamination and maintain evaluation relevance.\nCorrect Answer: Continuously integrate fresh, post-cutoff data streams from multiple domains with regular updates to ensure benchmark novelty and avoid data leakage.\n</output_format>",
    "question": "What key design principle should guide future temporal cutoff benchmarks, as evidenced by LiveBench, AntiLeak-Bench, AcademicEval, LiveCodeBench, LiveAoPSBench, and Forecastbench?",
    "answer": "Continuously integrate fresh, post-cutoff data streams from multiple domains with regular updates to ensure benchmark novelty and avoid data leakage.",
    "explanation": "Each of these benchmarks relies on continuously updated, post\u2013knowledge-cutoff data across diverse domains to prevent contamination and maintain evaluation relevance.",
    "question_token_count": 40,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 26
  },
  {
    "context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n",
    "topic": "Set\u2010theoretic characterization of exact contamination as the existence of any data point d in both D_train and D_test.",
    "response": "<document_analysis>\nThe context formally defines exact contamination as the existence of any data point d that belongs to both D_train and D_test. A higher\u2010level set\u2010theoretic insight involves relating this condition to the inclusion\u2013exclusion principle on set cardinalities: contamination \u21d4 |D_train \u2229 D_test| \u2265 1 \u21d4 |D_train \u222a D_test| < |D_train| + |D_test|.\nThis connection is not explicitly stated but can be derived by a domain expert using fundamental set theory. Thus, an advanced question probes this derived inequality.\n</document_analysis>\n<output_format>\nQuestion: Using the principle of inclusion\u2013exclusion for finite sets, what inequality involving |D_train \u222a D_test| signals the presence of exact contamination between D_train and D_test?\n\nExplanation: If there is at least one shared element then |D_train \u222a D_test| = |D_train| + |D_test| \u2212 |D_train \u2229 D_test|, so a nonempty intersection implies |D_train \u222a D_test| < |D_train| + |D_test|.\n\nCorrect Answer: |D_train \u222a D_test| < |D_train| + |D_test|\n</output_format>",
    "question": "Using the principle of inclusion\u2013exclusion for finite sets, what inequality involving |D_train \u222a D_test| signals the presence of exact contamination between D_train and D_test?",
    "answer": "|D_train \u222a D_test| < |D_train| + |D_test|",
    "explanation": "If there is at least one shared element then |D_train \u222a D_test| = |D_train| + |D_test| \u2212 |D_train \u2229 D_test|, so a nonempty intersection implies |D_train \u222a D_test| < |D_train| + |D_test|.",
    "question_token_count": 36,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 18
  },
  {
    "context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n",
    "topic": "Critically assess KIEval\u2019s methodology for deriving follow-up questions from static benchmarks and its contribution to interactive evaluation dynamics.",
    "response": "<document_analysis>\nThe context outlines various interactive evaluation frameworks, highlighting KIEval\u2019s specific strategy: it generates follow-up questions by analyzing a model\u2019s responses to initial static-benchmark prompts. This introduces dynamic probing but remains tethered to the fixed question source. The methodology\u2019s nuance lies in its response-conditioned follow-up generation versus fully generative or multi-agent systems.\nDifficulty rating: 8/10\u2014requires understanding interactive evaluation dynamics, response-driven question generation, and limitations of static benchmarks.\n</document_analysis>\n\n<output_format>\nQuestion: How does KIEval\u2019s response-conditioned follow-up generation enhance the depth of interactive evaluation, and what core limitation does its dependence on static benchmarks impose on this dynamics?\n\nExplanation: KIEval tailors subsequent questions to the model\u2019s prior answers, deepening the assessment of reasoning, but remains confined by the original static prompt set, limiting question diversity and adaptability.\n\nCorrect Answer: It adapts follow-ups to the model\u2019s answers for deeper probing but is constrained by the static initial benchmark, restricting scope and flexibility.\n</output_format>",
    "question": "How does KIEval\u2019s response-conditioned follow-up generation enhance the depth of interactive evaluation, and what core limitation does its dependence on static benchmarks impose on this dynamics?",
    "answer": "It adapts follow-ups to the model\u2019s answers for deeper probing but is constrained by the static initial benchmark, restricting scope and flexibility.",
    "explanation": "KIEval tailors subsequent questions to the model\u2019s prior answers, deepening the assessment of reasoning, but remains confined by the original static prompt set, limiting question diversity and adaptability.",
    "question_token_count": 34,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 28
  },
  {
    "context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n",
    "topic": "Cross-method evaluation contrasting rule-based and template-based generation methods in terms of diversity of instances, human involvement, and contamination risk.",
    "response": "<document_analysis>\nThe context compares rule-based synthesis, highlighting its extremely low collision probability, against template-based generation, which uses placeholders for dynamic, diverse problem instances but requires manual template creation. It also notes human effort requirements and contamination risks in benchmark creation. This provides sufficient material to contrast both methods on collision probability, diversity, human involvement, and contamination risk.\n</document_analysis>\n\n<output_format>\nQuestion: In designing evaluation benchmarks, what trade-off arises between collision probability, instance diversity, and human labor when choosing rule-based generation versus template-based generation?\n\nExplanation: Rule-based generation minimizes collisions and contamination risk through predefined rules but demands substantial upfront rule engineering, whereas template-based generation enhances diversity via dynamic variable substitution and shuffling yet requires continuous human template design and bears higher contamination risk.\n\nCorrect Answer: Rule-based generation offers minimal collision probability and contamination risk at the expense of heavy upfront rule crafting, while template-based generation provides greater instance diversity through dynamic templates but necessitates ongoing human curation and carries higher contamination risk.\n</output_format>",
    "question": "In designing evaluation benchmarks, what trade-off arises between collision probability, instance diversity, and human labor when choosing rule-based generation versus template-based generation?",
    "answer": "Rule-based generation offers minimal collision probability and contamination risk at the expense of heavy upfront rule crafting, while template-based generation provides greater instance diversity through dynamic templates but necessitates ongoing human curation and carries higher contamination risk.",
    "explanation": "Rule-based generation minimizes collisions and contamination risk through predefined rules but demands substantial upfront rule engineering, whereas template-based generation enhances diversity via dynamic variable substitution and shuffling yet requires continuous human template design and bears higher contamination risk.",
    "question_token_count": 30,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 44
  },
  {
    "context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n",
    "topic": "How label protection maintains evaluation integrity and mitigates risks of data contamination and answer memorization.",
    "response": "<document_analysis>\nThe context contrasts encryption and label protection as defenses against data leakage in model evaluation. Label protection hides test answers to prevent models from memorizing or training on them, preserving benchmark integrity. It is commonly used in GLUE, SuperGLUE, and HumanEval. Underlying this is the assumption that models have no indirect access to true labels; if labels leak or correlate with public metadata, this protection fails.\n</document_analysis>\n\n<output_format>\nQuestion: What fundamental assumption underlies the effectiveness of label protection in preventing data contamination, and under what scenario does this assumption break down?\n\nExplanation: The answer pinpoints that hiding labels works only if models cannot access or infer them, and it breaks when labels leak or are deduced via side channels.\n\nCorrect Answer: It assumes models lack any direct or indirect access to true labels, which fails if labels are leaked or inferred through side channels or correlated metadata.\n</output_format>",
    "question": "What fundamental assumption underlies the effectiveness of label protection in preventing data contamination, and under what scenario does this assumption break down?",
    "answer": "It assumes models lack any direct or indirect access to true labels, which fails if labels are leaked or inferred through side channels or correlated metadata.",
    "explanation": "The answer pinpoints that hiding labels works only if models cannot access or infer them, and it breaks when labels leak or are deduced via side channels.",
    "question_token_count": 26,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 2,
    "avg_answer_token_count": 29
  },
  {
    "context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n",
    "topic": "Critical evaluation of how static benchmarking methods\u2019 vulnerability to data contamination escalates with the expansion of LLM training datasets.",
    "response": "<document_analysis>\nThe context summarizes findings on static vs dynamic benchmarking of LLMs, noting that static methods remain consistent but grow more vulnerable to data contamination as training datasets expand. This vulnerability stems from increasing overlap between test benchmarks and training corpora, which undermines novelty and inflates performance metrics. A probing question should focus on this mechanism, requiring experts to articulate how dataset scaling intrinsically threatens static evaluation integrity.\n</document_analysis>\n\n<output_format>\nQuestion: What systemic effect inherent to static evaluation sets causes their vulnerability to data contamination to escalate as LLM training corpora continue to scale?\n\nExplanation: As training datasets grow, the chance that static benchmark examples appear in the training data increases, eroding test novelty and inflating performance.\n\nCorrect Answer: Increased overlap between training data and static benchmarks\n</output_format>",
    "question": "What systemic effect inherent to static evaluation sets causes their vulnerability to data contamination to escalate as LLM training corpora continue to scale?",
    "answer": "Increased overlap between training data and static benchmarks",
    "explanation": "As training datasets grow, the chance that static benchmark examples appear in the training data increases, eroding test novelty and inflating performance.",
    "question_token_count": 27,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 9
  },
  {
    "context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n",
    "topic": "Describing the CONSTAT framework for detecting contamination by comparing model performance across multiple benchmarks.",
    "response": "<document_analysis>\nThe provided context describes various post-hoc detection methods for data contamination: n-gram/token overlaps, embedding-based similarity, behavioral tests (masked inputs, partial completions, preference for originals), and finally introduces CONSTAT as a framework that detects contamination by comparing model performance across benchmarks. Irrelevant details (citations, specific technical minutiae beyond detection categories) have been noted but not extraneous. The key insight is CONSTAT\u2019s unique performance-comparison approach versus direct data overlap or behavior under perturbations.\nDifficulty level: 8 (requires synthesis of detection paradigms and understanding of CONSTAT\u2019s distinct mechanism).\n</document_analysis>\n\n<output_format>\nQuestion: Compared to overlap-based and embedding-similarity methods, what unique strategy does the CONSTAT framework employ to detect data contamination, and which assumption about model performance underpins this strategy?\n\nExplanation: CONSTAT identifies contamination by measuring discrepancies in model performance across benchmarks, assuming that data leakage will manifest as unusually better results on contaminated tasks relative to clean ones.\nCorrect Answer: It compares model performance across benchmarks, assuming contamination causes anomalously higher performance on contaminated benchmarks.\n</output_format>",
    "question": "Compared to overlap-based and embedding-similarity methods, what unique strategy does the CONSTAT framework employ to detect data contamination, and which assumption about model performance underpins this strategy?",
    "answer": "It compares model performance across benchmarks, assuming contamination causes anomalously higher performance on contaminated benchmarks.",
    "explanation": "CONSTAT identifies contamination by measuring discrepancies in model performance across benchmarks, assuming that data leakage will manifest as unusually better results on contaminated tasks relative to clean ones.",
    "question_token_count": 36,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 19
  },
  {
    "context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n",
    "topic": "The role and importance of the Correctness criterion in ensuring benchmark reliability and validity.",
    "response": "<document_analysis>\nThe context defines the Correctness criterion for dynamic benchmarking, highlighting the use of an oracle function \ud835\udca2 for ground truth and a scoring function \ud835\udcae to compute expected alignment between generated outputs and true outputs. Central themes include the necessity of reliable ground truth, quantification via expectation, and implications for benchmark reliability. A probing question should examine deeper implications, such as how aggregating over transformations might conceal systematic errors or biases and affect validity.\n</document_analysis>\n\n<output_format>\nQuestion: How can measuring correctness as an expected alignment over all transformations mask systematic errors in specific subsets of the data, and why does this pose a risk to the validity of a dynamic benchmark?\nExplanation: Aggregating correctness via expectation can average out consistent errors in particular transformations, hiding biases and thus undermining the benchmark\u2019s trustworthiness.\nCorrect Answer: It can average out consistent transformation errors, masking biases that compromise validity.\n</output_format>",
    "question": "How can measuring correctness as an expected alignment over all transformations mask systematic errors in specific subsets of the data, and why does this pose a risk to the validity of a dynamic benchmark?",
    "answer": "It can average out consistent transformation errors, masking biases that compromise validity.",
    "explanation": "Aggregating correctness via expectation can average out consistent errors in particular transformations, hiding biases and thus undermining the benchmark\u2019s trustworthiness.",
    "question_token_count": 37,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 15
  },
  {
    "context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n",
    "topic": "Analyze how the sheer scale and diversity of LLM training corpora challenge existing evaluation methodologies and propose scalable solutions.",
    "response": "<document_analysis>\nThe context discusses how large-scale, diverse pretraining and fine-tuning datasets for LLMs increase the risk of evaluation set contamination. It highlights the limitations of retrieval-based detection due to corpus scale, complexity, and proprietary opacity, and emphasizes the need for fair, reliable benchmarks. Relevant themes include data provenance, auditing, and scalable governance to ensure evaluation integrity.\n</document_analysis>\n\n<output_format>\nQuestion: Given that retrieval-based methods struggle with the sheer volume and proprietary opacity of LLM training corpora, what scalable mechanism can be implemented to guarantee evaluation-set confidentiality and integrity?\n\nExplanation: Retrieval-based detection cannot handle massive, opaque datasets; embedding cryptographic provenance provides a verifiable audit trail to prevent contamination.\n\nCorrect Answer: Cryptographic data provenance tracking\n</output_format>",
    "question": "Given that retrieval-based methods struggle with the sheer volume and proprietary opacity of LLM training corpora, what scalable mechanism can be implemented to guarantee evaluation-set confidentiality and integrity?",
    "answer": "Cryptographic data provenance tracking",
    "explanation": "Retrieval-based detection cannot handle massive, opaque datasets; embedding cryptographic provenance provides a verifiable audit trail to prevent contamination.",
    "question_token_count": 34,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 7,
    "question_difficulty_score": 9,
    "question_groundedness_score": 8,
    "avg_answer_token_count": 6
  },
  {
    "context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Describe the construction of the sequence of dynamic datasets \ud835\udcd3_t = T_t(\ud835\udcd3) for t \u2208 {1,\u2026,N} and analyze the implications of varying N.",
    "response": "<document_analysis>\nThe context defines a dynamic benchmarking framework where an initial static dataset \ud835\udc9f is repeatedly transformed by a sequence of functions T\u2081,\u2026,T\u2099 to yield time\u2010stamped evaluation sets \ud835\udc9f\u209c = T\u209c(\ud835\udc9f). A key parameter is N, the number of timestamps, which may be finite\u2014ensuring a bounded, reproducible sequence\u2014or infinite\u2014enabling continuous data renewal but complicating transparency and repeatability. Core ideas involve balancing contamination avoidance, dataset freshness, and evaluation reproducibility.  \nThis informs a question probing the trade\u2010off introduced by choosing finite versus infinite N, a nuanced topic requiring expertise in benchmarking design, data generation, and model evaluation.\nDifficulty: 8/10  \n</document_analysis>\n\n<output_format>\nQuestion: How does varying the total timestamp count N influence the balance between reproducibility and data freshness in the sequence of dynamic datasets \ud835\udcd3\u209c = T\u209c(\ud835\udcd3)?\n\nExplanation: A finite N yields a fixed set of transformed datasets that can be fully disclosed and re\u2010used for consistent comparisons, whereas an infinite N continuously introduces new data transformations to keep evaluations fresh but prevents rerunning the entire sequence identically.\n\nCorrect Answer: Finite N enhances reproducibility but limits data freshness; infinite N ensures continuous freshness but undermines reproducibility.\n</output_format>",
    "question": "How does varying the total timestamp count N influence the balance between reproducibility and data freshness in the sequence of dynamic datasets \ud835\udcd3\u209c = T\u209c(\ud835\udcd3)?",
    "answer": "Finite N enhances reproducibility but limits data freshness; infinite N ensures continuous freshness but undermines reproducibility.",
    "explanation": "A finite N yields a fixed set of transformed datasets that can be fully disclosed and re\u2010used for consistent comparisons, whereas an infinite N continuously introduces new data transformations to keep evaluations fresh but prevents rerunning the entire sequence identically.",
    "question_token_count": 37,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 20
  },
  {
    "context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n",
    "topic": "Compare and contrast the four rewriting methods (Auto-Dataset, StructEval, ITD, VarBench) in terms of contamination mitigation, resource requirements, and sample difficulty preservation.",
    "response": "<document_analysis>\nThe context outlines four LLM-based benchmark rewriting methods\u2014Auto-Dataset, StructEval, ITD, and VarBench\u2014highlighting each one\u2019s contamination strategy (if any), reliance on external resources, and approach to preserving sample difficulty. Only ITD explicitly uses a contamination detector and preserves difficulty; StructEval requires knowledge graphs to expand concepts; Auto-Dataset varies cognitive levels without explicit contamination checks; VarBench swaps variables with minimal overhead. The rich diversity of these designs supports a comparative question about trade\u2011offs between contamination mitigation, difficulty retention, and resource demands.\n</document_analysis>\n\n<output_format>\nQuestion: Which rewriting method most effectively balances contamination mitigation and sample difficulty preservation with minimal additional resource dependencies, and how does it compare in resource requirements to the approach that leverages knowledge graphs for conceptual expansion?\n\nExplanation: ITD uniquely employs a contamination detector and prompt\u2011based rewriting to preserve difficulty without external tools, whereas StructEval relies heavily on integrating knowledge graphs, incurring greater computational and integration overhead.\n\nCorrect Answer: ITD vs. StructEval: ITD uses only LLM prompts and a contamination detector, while StructEval\u2019s reliance on knowledge graphs demands higher resource investment.\n</output_format>",
    "question": "Which rewriting method most effectively balances contamination mitigation and sample difficulty preservation with minimal additional resource dependencies, and how does it compare in resource requirements to the approach that leverages knowledge graphs for conceptual expansion?",
    "answer": "ITD vs. StructEval: ITD uses only LLM prompts and a contamination detector, while StructEval\u2019s reliance on knowledge graphs demands higher resource investment.",
    "explanation": "ITD uniquely employs a contamination detector and prompt\u2011based rewriting to preserve difficulty without external tools, whereas StructEval relies heavily on integrating knowledge graphs, incurring greater computational and integration overhead.",
    "question_token_count": 38,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 4,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 32
  },
  {
    "context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n",
    "topic": "Fundamental differences between table-based and graph-based reasoning evaluations for LLMs",
    "response": "<document_analysis>\nThe context outlines two categories for evaluating LLM reasoning: \n1. Table-Based: S3Eval uses random SQL tables and queries, focusing on relational data manipulation. \n2. Graph-Based: Methods like DyVal use DAGs to test root-node value inference, NPHardEval uses random graphs for TSP, and Xie et al. create reasoning graphs for puzzles.\n\nKey distinctions:\n- Underlying data structure: fixed relational schema vs dynamic node-edge graphs.\n- Reasoning paradigm: relational algebra operations vs topological/combinatorial inference.\n- Task complexity: SQL query execution vs DAG traversal and NP-hard problem solving.\n\nThis informs a deep question contrasting these paradigms and their cognitive demands on LLMs. Selected difficulty: 8/10.\n</document_analysis>\n\n<output_format>\nQuestion: How does the reliance on fixed-schema relational queries in S3Eval fundamentally differ from the topological and combinatorial challenges posed by graph-based evaluations such as DyVal and NPHardEval, and what does this reveal about the distinct reasoning paradigms each approach targets?\n\nExplanation: Table-based evaluations center on relational algebra within a static schema, while graph-based tasks introduce dynamic topological structures and NP-hard combinatorial reasoning.\n\nCorrect Answer: S3Eval emphasizes relational algebraic manipulation on fixed schemas, whereas graph-based evaluations test topological traversal and combinatorial problem-solving.\n</output_format>",
    "question": "How does the reliance on fixed-schema relational queries in S3Eval fundamentally differ from the topological and combinatorial challenges posed by graph-based evaluations such as DyVal and NPHardEval, and what does this reveal about the distinct reasoning paradigms each approach targets?",
    "answer": "S3Eval emphasizes relational algebraic manipulation on fixed schemas, whereas graph-based evaluations test topological traversal and combinatorial problem-solving.",
    "explanation": "Table-based evaluations center on relational algebra within a static schema, while graph-based tasks introduce dynamic topological structures and NP-hard combinatorial reasoning.",
    "question_token_count": 51,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 9,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 24
  },
  {
    "context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Compare and contrast static and dynamic benchmarking schemes in terms of adaptability, contamination risk, and evaluation fidelity.",
    "response": "<document_analysis>\nThe context contrasts static benchmarking\u2014which relies on a fixed dataset\u2014with dynamic benchmarking, where a transformation function T(\u00b7) is repeatedly applied over timestamps to produce new evaluation sets (\ud835\udc9f\u209c = T\u209c(\ud835\udc9f)). Static schemes are vulnerable to data contamination, limited adaptability, and stale evaluations. Dynamic schemes mitigate overlap with training data, adapt to evolving evaluation needs, and enhance fidelity by continually reshaping the test set. The core mechanism is the timestamped transformation T\u209c, which prevents memorization biases and maintains transparency and faithfulness in evaluation.\n</document_analysis>\n\n<output_format>\nQuestion: How do static and dynamic benchmarking schemes differ in their adaptability, contamination risk, and evaluation fidelity, and what specific role does the timestamped transformation function T\u209c(\u00b7) play in dynamic benchmarking?\n\nExplanation: Static benchmarks use a fixed dataset prone to overlap and stale evaluations, whereas dynamic benchmarks apply T\u209c(\u00b7) at each timestamp to regenerate fresh test sets, thus reducing contamination risk, adapting to new evaluation challenges, and ensuring more faithful assessments.\n\nCorrect Answer: Static schemes are fixed and risk contamination with limited adaptability and fidelity; dynamic schemes use T\u209c(\u00b7) to continuously produce fresh datasets, mitigating overlap, adapting tests, and improving evaluation fidelity.\n</output_format>",
    "question": "How do static and dynamic benchmarking schemes differ in their adaptability, contamination risk, and evaluation fidelity, and what specific role does the timestamped transformation function T\u209c(\u00b7) play in dynamic benchmarking?",
    "answer": "Static schemes are fixed and risk contamination with limited adaptability and fidelity; dynamic schemes use T\u209c(\u00b7) to continuously produce fresh datasets, mitigating overlap, adapting tests, and improving evaluation fidelity.",
    "explanation": "Static benchmarks use a fixed dataset prone to overlap and stale evaluations, whereas dynamic benchmarks apply T\u209c(\u00b7) at each timestamp to regenerate fresh test sets, thus reducing contamination risk, adapting to new evaluation challenges, and ensuring more faithful assessments.",
    "question_token_count": 40,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 39
  },
  {
    "context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n",
    "topic": "Evaluate TreeEval\u2019s two-stage process of initial question generation and adaptive subtopic follow-up, and its impact on depth of LLM assessment.",
    "response": "<document_analysis>\nThe context describes TreeEval\u2019s two-stage evaluation: generating an initial question on a given topic and then, based on the LLM\u2019s response, creating follow-up subtopic questions to further assess the model. This dynamic approach contrasts with static benchmarks by adapting queries to the model\u2019s outputs, potentially uncovering deeper reasoning flaws and latent weaknesses. For experts, the core insight lies in how the adaptive branching probes hierarchical concept spaces and mitigates superficial breadth. This analysis informs a question that examines the mechanism by which follow-up generation enhances depth of assessment.\n</document_analysis>\n\n<output_format>\nQuestion: In what way does TreeEval\u2019s adaptive subtopic generation, informed by an LLM\u2019s answer to an initial question, systematically deepen the evaluation of its reasoning capabilities beyond static one-off prompts?\n\nExplanation: Follow-up questions derived from the model\u2019s own responses guide targeted exploration of conceptual branches, revealing subtle reasoning failures that static benchmarks overlook.\n\nCorrect Answer: By recursively creating context-sensitive follow-ups based on the model\u2019s answers, it directs probes into latent conceptual gaps and nuanced reasoning failures.\n</output_format>",
    "question": "In what way does TreeEval\u2019s adaptive subtopic generation, informed by an LLM\u2019s answer to an initial question, systematically deepen the evaluation of its reasoning capabilities beyond static one-off prompts?",
    "answer": "By recursively creating context-sensitive follow-ups based on the model\u2019s answers, it directs probes into latent conceptual gaps and nuanced reasoning failures.",
    "explanation": "Follow-up questions derived from the model\u2019s own responses guide targeted exploration of conceptual branches, revealing subtle reasoning failures that static benchmarks overlook.",
    "question_token_count": 38,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 7,
    "question_difficulty_score": 9,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 27
  },
  {
    "context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n",
    "topic": "Formulating ethical guidelines for responsible data usage in AI benchmarking.",
    "response": "<document_analysis>\nThe context discusses ethical considerations for AI benchmarking, highlighting tensions between transparency, fairness, and privacy\u2014particularly in dynamic benchmarks that continuously collect data. It emphasizes the need for mechanisms that prevent privacy violations while maintaining accountability and unbiased evaluation. The question formulated probes the application of a specific privacy-preserving technique within this trade-off, requiring domain experts to connect benchmark design principles with advanced statistical methods.\n</document_analysis>\n\n<output_format>\nQuestion: How does differential privacy reconcile the need for transparency and fairness with user privacy in dynamic AI benchmarking frameworks?  \nExplanation: Differential privacy injects calibrated noise into benchmark data, obscuring individual contributions while preserving accurate aggregate metrics for transparent and fair evaluation.  \nCorrect Answer: It injects calibrated noise into benchmark data, ensuring individual contributions remain unidentifiable while preserving aggregate performance metrics.  \n</output_format>",
    "question": "How does differential privacy reconcile the need for transparency and fairness with user privacy in dynamic AI benchmarking frameworks?",
    "answer": "It injects calibrated noise into benchmark data, ensuring individual contributions remain unidentifiable while preserving aggregate performance metrics.",
    "explanation": "Differential privacy injects calibrated noise into benchmark data, obscuring individual contributions while preserving accurate aggregate metrics for transparent and fair evaluation.",
    "question_token_count": 21,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 4,
    "question_difficulty_score": 8,
    "question_groundedness_score": 2,
    "avg_answer_token_count": 23
  },
  {
    "context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Formally define a dynamic benchmark (\u212c_dynamic) and describe the roles of its components \ud835\udcd3 and T(\u00b7).",
    "response": "<document_analysis>\nThe context defines a dynamic benchmark \u212c_dynamic formally as a pair (\ud835\udcd3,\u00a0T(\u00b7)), where \ud835\udcd3 is a static dataset tuple (\ud835\udce7,\u00a0\ud835\udce8,\u00a0\ud835\udce2(\u00b7)) and T(\u00b7) is a transformation function that produces time\u2010indexed evaluation sets \ud835\udcd3\u209c=T\u209c(\ud835\udcd3). This provides enough substance for a self\u2010contained, technically precise question probing the mathematical structure and roles of these components.\n</document_analysis>\n\n<output_format>\nQuestion: Provide the formal mathematical definition of a dynamic benchmark \u212c_dynamic and describe the roles of its components \ud835\udcd3 and T(\u00b7).\n\nExplanation: This answer matches the given formalism, identifying \u212c_dynamic as the tuple (\ud835\udcd3,\u00a0T(\u00b7)), with \ud835\udcd3 specifying the static example space and sampling and T(\u00b7) creating the time\u2010varying evaluation sets.\n\nCorrect Answer: \u212c_dynamic = (\ud835\udcd3,\u00a0T(\u00b7)), where \ud835\udcd3 = (\ud835\udce7,\u00a0\ud835\udce8,\u00a0\ud835\udce2(\u00b7)) is the static benchmark dataset of input\u2013output pairs and sampling mechanism, and T(\u00b7) is the transformation that generates each timestamped evaluation set from \ud835\udcd3.\n</output_format>",
    "question": "Provide the formal mathematical definition of a dynamic benchmark \u212c_dynamic and describe the roles of its components \ud835\udcd3 and T(\u00b7).",
    "answer": "\u212c_dynamic = (\ud835\udcd3,\u00a0T(\u00b7)), where \ud835\udcd3 = (\ud835\udce7,\u00a0\ud835\udce8,\u00a0\ud835\udce2(\u00b7)) is the static benchmark dataset of input\u2013output pairs and sampling mechanism, and T(\u00b7) is the transformation that generates each timestamped evaluation set from \ud835\udcd3.",
    "explanation": "This answer matches the given formalism, identifying \u212c_dynamic as the tuple (\ud835\udcd3,\u00a0T(\u00b7)), with \ud835\udcd3 specifying the static example space and sampling and T(\u00b7) creating the time\u2010varying evaluation sets.",
    "question_token_count": 27,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 66
  },
  {
    "context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n",
    "topic": "Ensuring model transparency in LLM evaluations to foster societal trust and understanding.",
    "response": "<document_analysis>\nThe context addresses ethical considerations in LLM benchmarking, contrasting static and dynamic benchmarks, and highlighting transparency, fairness, accountability, and privacy concerns. It notes that dynamic benchmarks offer adaptability but raise ongoing data collection and privacy issues, while misuse of results can undermine trust. This directly intersects with ensuring model transparency to foster societal trust. A deep question should probe the reconciliation of transparency with privacy in dynamic evaluations, challenging experts to balance adaptability and ethical safeguards.\n</document_analysis>\n<output_format>\nQuestion: What mechanism can be implemented within dynamic benchmarking to both protect user privacy and foster transparency for societal trust?\nExplanation: Dynamic benchmarks require continual data updates but risk privacy breaches; embedding transparent, privacy-preserving audit trails ensures accountability and visibility into benchmark evolution.\nCorrect Answer: Transparent, privacy-preserving audit trails for dynamic benchmark updates.\n</output_format>",
    "question": "What mechanism can be implemented within dynamic benchmarking to both protect user privacy and foster transparency for societal trust?",
    "answer": "Transparent, privacy-preserving audit trails for dynamic benchmark updates.",
    "explanation": "Dynamic benchmarks require continual data updates but risk privacy breaches; embedding transparent, privacy-preserving audit trails ensures accountability and visibility into benchmark evolution.",
    "question_token_count": 21,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 12
  },
  {
    "context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n",
    "topic": "Discuss the implications of linguistic and cultural specificity in benchmark design, particularly comparing Chinese\u2011focused datasets (C\u2011Eval, C\u2011SimpleQA) with analogous English benchmarks.",
    "response": "<document_analysis>\nThe context outlines various NLP benchmark categories\u2014coding, instruction following, reasoning\u2014and cites Chinese-specific datasets (C\u2011Eval, C\u2011SimpleQA) alongside English analogs. Key themes include linguistic variation (e.g., instruction formats in Chinese vs English), cultural specificity in question content, and comparative evaluation validity across languages. This prompts deep analysis of cross-lingual generalization, tokenization issues, cultural biases, and mitigation strategies in benchmark design.\n</document_analysis>\n\n<output_format>\nQuestion: What unique challenges do linguistic typology differences and culturally embedded content in Chinese-focused benchmarks like C\u2011Eval and C\u2011SimpleQA pose for equitable cross-lingual performance comparison with English benchmarks, and what strategies can benchmark designers employ to mitigate these issues?\nExplanation: The answer identifies how language-specific tokenization, cultural context biases, and annotation artifacts distort direct comparisons, and suggests standardization, cultural balancing, and calibration to restore fairness.\nCorrect Answer: They create tokenization inconsistencies, culturally driven reasoning biases, and annotation artifacts that hinder fair comparison, which can be mitigated by standardized preprocessing, culturally balanced item design, and cross-lingual calibration.\n</output_format>",
    "question": "What unique challenges do linguistic typology differences and culturally embedded content in Chinese-focused benchmarks like C\u2011Eval and C\u2011SimpleQA pose for equitable cross-lingual performance comparison with English benchmarks, and what strategies can benchmark designers employ to mitigate these issues?",
    "answer": "They create tokenization inconsistencies, culturally driven reasoning biases, and annotation artifacts that hinder fair comparison, which can be mitigated by standardized preprocessing, culturally balanced item design, and cross-lingual calibration.",
    "explanation": "The answer identifies how language-specific tokenization, cultural context biases, and annotation artifacts distort direct comparisons, and suggests standardization, cultural balancing, and calibration to restore fairness.",
    "question_token_count": 50,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 7,
    "question_difficulty_score": 9,
    "question_groundedness_score": 4,
    "avg_answer_token_count": 40
  },
  {
    "context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n",
    "topic": "Critical examination of existing dynamic benchmarks and their specific shortcomings.",
    "response": "<document_analysis>\nThe abstract centers on the shift from static to dynamic benchmarking to mitigate LLM data contamination, identifies inherent limitations in both approaches, and pinpoints a crucial gap\u2014the absence of standardized evaluation criteria for dynamic benchmarks. This deficiency is highlighted as undermining consistent, fair comparisons and impeding methodological progress, which motivates the authors\u2019 proposal of optimal design principles. Given the abstract\u2019s focus, a question exploring how this missing standardization specifically weakens dynamic benchmark efficacy and shapes the need for design principles will probe a nuanced, expert-level understanding.\n</document_analysis>\n\n<output_format>\nQuestion: How does the absence of standardized evaluation criteria in current dynamic LLM benchmarks undermine their effectiveness in mitigating data contamination, and what implication does this have for the development of robust benchmarking frameworks?\nExplanation: Without standardized criteria, dynamic benchmarks cannot be compared or reproduced reliably, which directly leads to the necessity of well-defined design principles for consistent and trustworthy evaluations.\nCorrect Answer: It prevents objective comparison and reproducibility, driving the need for standardized design principles to ensure consistent, reliable benchmarking.\n</output_format>",
    "question": "How does the absence of standardized evaluation criteria in current dynamic LLM benchmarks undermine their effectiveness in mitigating data contamination, and what implication does this have for the development of robust benchmarking frameworks?",
    "answer": "It prevents objective comparison and reproducibility, driving the need for standardized design principles to ensure consistent, reliable benchmarking.",
    "explanation": "Without standardized criteria, dynamic benchmarks cannot be compared or reproduced reliably, which directly leads to the necessity of well-defined design principles for consistent and trustworthy evaluations.",
    "question_token_count": 36,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 22
  },
  {
    "context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n",
    "topic": "Analyze the effect of predefined rule-based generation on sample diversity and the risk of in-distribution contamination in LLM training data.",
    "response": "<document_analysis>\nThe context identifies a key limitation: rule-based generation reduces sample diversity and increases in-distribution contamination risk. It explains that fixed templates and rule sets lead to repetitive outputs that may mirror benchmark items. These points provide sufficient material for a question probing the dual impact and underlying mechanism of contamination due to diversity constraints.\n</document_analysis>\n\n<output_format>\nQuestion: How do predefined rule-based generation methods amplify the risk of in-distribution contamination in LLM training datasets, and what underlying mechanism drives this effect?\nExplanation: Rule-based approaches constrain sample diversity, leading to repetitive patterns that closely mirror existing benchmark data, thus causing inadvertent overlap and contamination.\nCorrect Answer: By limiting variability through rigid templates, they create repetitive patterns that overlap with real benchmarks, driving contamination via inadvertent sample replication.\n</output_format>",
    "question": "How do predefined rule-based generation methods amplify the risk of in-distribution contamination in LLM training datasets, and what underlying mechanism drives this effect?",
    "answer": "By limiting variability through rigid templates, they create repetitive patterns that overlap with real benchmarks, driving contamination via inadvertent sample replication.",
    "explanation": "Rule-based approaches constrain sample diversity, leading to repetitive patterns that closely mirror existing benchmark data, thus causing inadvertent overlap and contamination.",
    "question_token_count": 28,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 7,
    "question_difficulty_score": 9,
    "question_groundedness_score": 8,
    "avg_answer_token_count": 26
  },
  {
    "context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n",
    "topic": "Examination of multi\u2011domain knowledge benchmarks MMLU, BBH, and AGI\u00a0Eval to evaluate breadth and depth of LLM subject-matter competencies.",
    "response": "<document_analysis>\nThe context outlines three multi-domain benchmarks\u2014MMLU, BBH, and AGI Eval\u2014used to measure an LLM\u2019s internal knowledge across diverse subjects. MMLU uses multiple-choice questions spanning academic domains to gauge breadth; BBH employs scenario-based, complexity-graded problems to probe depth of reasoning; AGI Eval combines multi-step reasoning tasks with broad coverage to bridge breadth and depth. These distinctions support a comparative question that requires domain expertise and deep understanding of benchmark design.\n</document_analysis>\n\n<output_format>\nQuestion: How do BBH\u2019s scenario-based, open-ended tasks contrast with MMLU\u2019s multiple-choice format in evaluating an LLM\u2019s reasoning capabilities, and what specific evaluation gap does AGI Eval address between them?\n\nExplanation: BBH uses narrative scenarios to probe complex reasoning depth, MMLU relies on domain-spanning multiple-choice for breadth, and AGI Eval integrates multi-step reasoning with wide subject coverage to bridge breadth and depth.\n\nCorrect Answer: BBH uses scenario-driven open-ended problems vs. MMLU\u2019s domain-wide multiple-choice, while AGI Eval fills the gap by combining multi-step reasoning tasks with broad coverage.\n</output_format>",
    "question": "How do BBH\u2019s scenario-based, open-ended tasks contrast with MMLU\u2019s multiple-choice format in evaluating an LLM\u2019s reasoning capabilities, and what specific evaluation gap does AGI Eval address between them?",
    "answer": "BBH uses scenario-driven open-ended problems vs. MMLU\u2019s domain-wide multiple-choice, while AGI Eval fills the gap by combining multi-step reasoning tasks with broad coverage.",
    "explanation": "BBH uses narrative scenarios to probe complex reasoning depth, MMLU relies on domain-spanning multiple-choice for breadth, and AGI Eval integrates multi-step reasoning with wide subject coverage to bridge breadth and depth.",
    "question_token_count": 41,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 7,
    "avg_answer_token_count": 36
  },
  {
    "context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n",
    "topic": "Formal definition of data contamination and its effect on the validity of LLM benchmark performance measurements.",
    "response": "<document_analysis>\nThe context defines two types of data contamination\u2014exact and syntactic\u2014in LLM benchmarks, formalizes them via duplicate presence or via syntactic transformations (\u2131_syntactic), and explains that contamination undermines validity of performance measurements by introducing memorization rather than true generalization. Key nuanced point: syntactic contamination can evade simple duplicate filters, making it subtler and more insidious than exact duplication. This interplay is critical for domain experts to understand the depth of contamination risks.\nDifficulty rating: 8/10\n</document_analysis>\n\n<output_format>\nQuestion: Why does syntactic contamination\u2014where a syntactic transformation from \u2131_syntactic maps a test example into the training set\u2014present a subtler yet more insidious threat to the validity of LLM benchmark performance than exact duplication?\n\nExplanation: Syntactic transformations allow overlap to escape exact\u2010match detection, causing hidden leakage that inflates evaluation metrics and masks true model generalization.\n\nCorrect Answer: Because transformations like paraphrasing or synonym substitution evade exact\u2010match filters, creating covert overlap that artificially boosts benchmark scores.\n</output_format>",
    "question": "Why does syntactic contamination\u2014where a syntactic transformation from \u2131_syntactic maps a test example into the training set\u2014present a subtler yet more insidious threat to the validity of LLM benchmark performance than exact duplication?",
    "answer": "Because transformations like paraphrasing or synonym substitution evade exact\u2010match filters, creating covert overlap that artificially boosts benchmark scores.",
    "explanation": "Syntactic transformations allow overlap to escape exact\u2010match detection, causing hidden leakage that inflates evaluation metrics and masks true model generalization.",
    "question_token_count": 46,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 26
  },
  {
    "context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n",
    "topic": "Explore potential biases introduced by selecting specific data sources (e.g., arXiv, coding competitions, math forums) and suggest robust mitigation strategies.",
    "response": "<document_analysis>\nThe context describes the use of post-cutoff data sources (arXiv papers, coding competitions, math forums, forecasting markets) to construct benchmarks that avoid contamination. It highlights varied domain emphases: theoretical research (arXiv), algorithmic problem-solving (coding contests), community-vetted math problems (AoPS), and real-world forecasting. Potential biases include overrepresentation of certain research paradigms, task formats, problem complexities, and community preferences. A deep expert question should probe awareness of these source-specific distortions and require a unified, robust mitigation approach.\nDifficulty rating: 8/10\n</document_analysis>\n\n<output_format>\nQuestion: When assembling temporal benchmarks from sources like arXiv, coding competitions, and math forums, what selection biases uniquely affect each source, and what unified mitigation strategy can balance these skewed distributions?\n\nExplanation: Different platforms overrepresent theoretical work, algorithmic puzzles, or popular community problems; stratified sampling across sources with topic-normalized quotas addresses all biases.\n\nCorrect Answer: Each source\u2019s skew (theoretical, algorithmic, community-vetted) is mitigated by stratified, topic\u2011normalized cross\u2011source sampling.\n</output_format>",
    "question": "When assembling temporal benchmarks from sources like arXiv, coding competitions, and math forums, what selection biases uniquely affect each source, and what unified mitigation strategy can balance these skewed distributions?",
    "answer": "Each source\u2019s skew (theoretical, algorithmic, community-vetted) is mitigated by stratified, topic\u2011normalized cross\u2011source sampling.",
    "explanation": "Different platforms overrepresent theoretical work, algorithmic puzzles, or popular community problems; stratified sampling across sources with topic-normalized quotas addresses all biases.",
    "question_token_count": 37,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 4,
    "avg_answer_token_count": 30
  },
  {
    "context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n",
    "topic": "Ethical and scientific implications of reporting LLM performance metrics in the presence of potential data contamination.",
    "response": "<document_analysis>\nThe context discusses syntactic contamination\u2014where test data are merely rephrased training examples\u2014and its impact on LLM benchmarking validity. It highlights how such contamination inflates performance metrics, obscures true generalization, and misleads stakeholders, with ethical implications for AI deployment and policy. A probing question should link contamination to both scientific validity and ethical integrity.\n</document_analysis>\n\n<output_format>\nQuestion: What ethical and scientific threat arises when test benchmarks include syntactically transformed training data without safeguards?\nExplanation: Reusing rephrased training inputs as test cases artificially boosts performance metrics, invalidating generalization claims and leading to potentially harmful deployment and policy decisions.\nCorrect Answer: Artificially inflated performance metrics that compromise generalization assessments and mislead stakeholders.\n</output_format>",
    "question": "What ethical and scientific threat arises when test benchmarks include syntactically transformed training data without safeguards?",
    "answer": "Artificially inflated performance metrics that compromise generalization assessments and mislead stakeholders.",
    "explanation": "Reusing rephrased training inputs as test cases artificially boosts performance metrics, invalidating generalization claims and leading to potentially harmful deployment and policy decisions.",
    "question_token_count": 20,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 15
  },
  {
    "context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n",
    "topic": "Assess the ethical and legal considerations of using live data streams from public platforms for constructing LLM evaluation benchmarks.",
    "response": "<document_analysis>\nThe provided context details the use of temporal cutoffs in LLM benchmark construction to avoid data contamination, listing multiple \u201clive\u201d benchmarks that continuously ingest new content. Although the text does not explicitly discuss ethics or legalities, it introduces the core mechanism (temporal cutoff) that can also address privacy and intellectual property concerns. By focusing on how temporal buffers enable additional vetting, we connect the technical practice with ethical (user privacy) and legal (licensing, Terms of Service) considerations.\n</document_analysis>\n\n<output_format>\nQuestion: How can implementing a temporal cutoff policy for live data streams from public coding platforms help address both user privacy and intellectual property compliance when creating LLM evaluation benchmarks?\n\nExplanation: A deliberate delay gives time to remove or anonymize sensitive user data and to verify that newly collected content conforms to platform licensing and Terms of Service.\n\nCorrect Answer: Impose a temporal buffer between content publication and dataset inclusion for privacy screening and license verification.\n</output_format>",
    "question": "How can implementing a temporal cutoff policy for live data streams from public coding platforms help address both user privacy and intellectual property compliance when creating LLM evaluation benchmarks?",
    "answer": "Impose a temporal buffer between content publication and dataset inclusion for privacy screening and license verification.",
    "explanation": "A deliberate delay gives time to remove or anonymize sensitive user data and to verify that newly collected content conforms to platform licensing and Terms of Service.",
    "question_token_count": 31,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 8,
    "avg_answer_token_count": 19
  },
  {
    "context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n",
    "topic": "Impact of documentation leaks on overlap between training corpora and benchmark test data.",
    "response": "<document_analysis>\nThe context defines data contamination in LLMs, distinguishing exact duplicates and syntactic contamination via transformations (e.g., punctuation normalization, paraphrasing). It specifically mentions \u201cdocumentation leaks\u201d as a source of exact contamination. To challenge experts, the question will probe how documentation leaks can also induce syntactic contamination and undermine benchmark validity, requiring synthesis of both contamination types and implications for evaluation reliability.\n</document_analysis>\n\n<output_format>\nQuestion: How do documentation leaks facilitate syntactic contamination in LLM training corpora, and why does this critically undermine the validity of benchmark evaluations?\n\nExplanation: Syntactic transformations of leaked documentation produce near-duplicate test examples in training data, violating the assumption of test\u2013train separation and artificially inflating performance metrics.\n\nCorrect Answer: They introduce paraphrased or minimally altered benchmark examples into the training set, breaching data exclusivity and compromising evaluation integrity.\n</output_format>",
    "question": "How do documentation leaks facilitate syntactic contamination in LLM training corpora, and why does this critically undermine the validity of benchmark evaluations?",
    "answer": "They introduce paraphrased or minimally altered benchmark examples into the training set, breaching data exclusivity and compromising evaluation integrity.",
    "explanation": "Syntactic transformations of leaked documentation produce near-duplicate test examples in training data, violating the assumption of test\u2013train separation and artificially inflating performance metrics.",
    "question_token_count": 26,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 26
  },
  {
    "context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n",
    "topic": "The mathematical interpretation of stability as the variance in measured complexity across multiple dynamic transformation trials.",
    "response": "<document_analysis>\nThe provided context defines stability of dynamic benchmarks as the variance of complexity measurements \u03a8 across multiple transformation trials. It emphasizes the need to quantify fluctuations in complexity to distinguish true performance drops from increased task complexity. The key mathematical insight is that stability = Var[\u03a8(trial_i)], where high variance implies instability. This concept is central to understanding reliability of dynamic benchmarks.\n</document_analysis>\n\n<output_format>\nQuestion: Formally, if \u03a8\u2081,\u2026,\u03a8\u2096 are the complexity values measured across k dynamic transformation trials, how is the stability of the benchmark computed, and what does a high value of this measure indicate?\nExplanation: Stability is defined as the statistical variance of the complexity measurements; a large variance shows that the benchmark\u2019s complexity fluctuates greatly, signaling instability.\nCorrect Answer: The variance of \u03a8\u2081,\u2026,\u03a8\u2096; a high variance indicates low stability.\n</output_format>",
    "question": "Formally, if \u03a8\u2081,\u2026,\u03a8\u2096 are the complexity values measured across k dynamic transformation trials, how is the stability of the benchmark computed, and what does a high value of this measure indicate?",
    "answer": "The variance of \u03a8\u2081,\u2026,\u03a8\u2096; a high variance indicates low stability.",
    "explanation": "Stability is defined as the statistical variance of the complexity measurements; a large variance shows that the benchmark\u2019s complexity fluctuates greatly, signaling instability.",
    "question_token_count": 42,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 18
  },
  {
    "context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n",
    "topic": "Suggest possible extensions or alternative metrics to capture additional aspects of scalability beyond data-per-cost.",
    "response": "<document_analysis>\nThe context defines a scalability metric for dynamic benchmarks as the expected size of transformed data per unit cost. It focuses solely on data volume relative to cost. A natural extension is to capture additional dimensions of scalability, such as data diversity or informativeness, by replacing raw counts with measures like entropy or information gain. This leads to alternative metrics that challenge domain experts to integrate information-theoretic concepts into cost analyses.\n</document_analysis>\n\n<output_format>\nQuestion: Propose a scalability metric that accounts for the diversity of generated examples by measuring the expected increase in dataset entropy normalized by transformation cost.\nExplanation: By replacing raw data count with entropy gain, this metric reflects the information richness of new examples per unit cost.\nCorrect Answer: Expected entropy gain per unit cost\n</output_format>",
    "question": "Propose a scalability metric that accounts for the diversity of generated examples by measuring the expected increase in dataset entropy normalized by transformation cost.",
    "answer": "Expected entropy gain per unit cost",
    "explanation": "By replacing raw data count with entropy gain, this metric reflects the information richness of new examples per unit cost.",
    "question_token_count": 27,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 2,
    "avg_answer_token_count": 7
  },
  {
    "context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n",
    "topic": "The design and security guarantees of Jacovi et\u00a0al.\u2019s (2023) public key encryption combined with a \u201cNo Derivatives\u201d license to block automated test data crawling and reuse.",
    "response": "<document_analysis>\nThe context describes Jacovi et al.\u2019s method of encrypting test data with a public key, supplemented by a \u201cNo Derivatives\u201d license, to block automated crawling and reuse. Key points: (1) Public\u2011key encryption restricts data access to private\u2011key holders; (2) The \u201cNo Derivatives\u201d clause adds a legal barrier against transforming or publishing the data; (3) Together they defend against adversaries who can fetch ciphertext but cannot decrypt or legally derive new versions. This core mechanism underpins strong access control and legal enforceability, pending secure key management.\nDifficulty rating: 8\n</document_analysis>\n\n<output_format>\nQuestion: In Jacovi et al.\u2019s framework for protecting test data, how does the combination of public key encryption and a \u201cNo Derivatives\u201d license jointly prevent automated crawling and reuse, and what security guarantee does this offer against an adversary who has obtained only the ciphertext?\n\nExplanation: Encryption restricts decryption to private\u2011key holders while the \u201cNo Derivatives\u201d license legally forbids any creation or distribution of modified versions, so attackers with only the ciphertext cannot access or legally repurpose the data.\n\nCorrect Answer: Only private key holders can decrypt the data, and the \u201cNo Derivatives\u201d license legally forbids any transformations or distributions of the content, thus blocking unauthorized reuse.\n</output_format>",
    "question": "In Jacovi et al.\u2019s framework for protecting test data, how does the combination of public key encryption and a \u201cNo Derivatives\u201d license jointly prevent automated crawling and reuse, and what security guarantee does this offer against an adversary who has obtained only the ciphertext?",
    "answer": "Only private key holders can decrypt the data, and the \u201cNo Derivatives\u201d license legally forbids any transformations or distributions of the content, thus blocking unauthorized reuse.",
    "explanation": "Encryption restricts decryption to private\u2011key holders while the \u201cNo Derivatives\u201d license legally forbids any creation or distribution of modified versions, so attackers with only the ciphertext cannot access or legally repurpose the data.",
    "question_token_count": 53,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 33
  },
  {
    "context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n",
    "topic": "Strategies to minimize collision effects in dynamic dataset transformations",
    "response": "<document_analysis>\nThe context defines \"collision\" in dynamic benchmarking as overlap between independently transformed dataset versions, and introduces \"Collision Rate\" and \"Repeat Trials\" as metrics for dataset novelty. It does not explicitly detail mitigation strategies, but the concept implies expanding the transformation space to minimize overlap and increase repeat trials. Thus, a probing question can ask for designing transformation strategies to reduce collision, requiring synthesis of the given metrics and general knowledge of dataset transformations.\n</document_analysis>\n\n<output_format>\nQuestion: How can transformation functions be designed to minimize collision rate and maximize the expected number of repeat trials in a dynamic benchmarking dataset?\nExplanation: Employing high-entropy, compositional transformations parameterized by cryptographically secure random seeds and combining diverse syntactic, semantic, and lexical perturbations exponentially expands the transformation space, reducing overlap and increasing the number of unique trials before a repeat occurs.\nCorrect Answer: Use high-entropy, parameterized transformations that combine diverse syntactic, semantic, and lexical perturbations seeded with cryptographically secure randomness.\n</output_format>",
    "question": "How can transformation functions be designed to minimize collision rate and maximize the expected number of repeat trials in a dynamic benchmarking dataset?",
    "answer": "Use high-entropy, parameterized transformations that combine diverse syntactic, semantic, and lexical perturbations seeded with cryptographically secure randomness.",
    "explanation": "Employing high-entropy, compositional transformations parameterized by cryptographically secure random seeds and combining diverse syntactic, semantic, and lexical perturbations exponentially expands the transformation space, reducing overlap and increasing the number of unique trials before a repeat occurs.",
    "question_token_count": 25,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 4,
    "avg_answer_token_count": 25
  },
  {
    "context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n",
    "topic": "The computational overhead and performance trade\u2010offs introduced by encryption mechanisms in private benchmarking systems.",
    "response": "<document_analysis>\nThe provided context outlines encryption methods (public-key encryption with \u201cNo Derivatives\u201d licensing, robust text decontamination, and TRUCE\u2019s use of confidential computing and secure multi-party computation) to protect evaluation data in private benchmarking. It highlights a core trade-off: while these methods secure data confidentiality, they introduce significant computational overhead and depend on rigorous key management. For experts, the most nuanced challenge is quantifying and understanding how specific cryptographic techniques (especially interactive secure multi-party computation protocols) impact system performance (latency, throughput, resource utilization) in private benchmarking environments.\nDifficulty: 8/10 \u2013 requires in-depth knowledge of cryptographic protocols, secure multi-party computation, and performance trade-offs in benchmarking systems.\n</document_analysis>\n\n<output_format>\nQuestion: In a private benchmarking system employing TRUCE's secure multi-party computation, which aspect of the cryptographic protocol most significantly elevates computational overhead, and how does this trade-off impact evaluation performance?\nExplanation: Secure multi-party computation\u2019s interactive cryptographic rounds dominate overhead by increasing per-query latency and resource usage, directly reducing throughput to maintain confidentiality.\nCorrect Answer: The interactive rounds of secure multi-party computation, which raise per-query latency and resource consumption, trading off throughput for confidentiality.\n</output_format>",
    "question": "In a private benchmarking system employing TRUCE's secure multi-party computation, which aspect of the cryptographic protocol most significantly elevates computational overhead, and how does this trade-off impact evaluation performance?",
    "answer": "The interactive rounds of secure multi-party computation, which raise per-query latency and resource consumption, trading off throughput for confidentiality.",
    "explanation": "Secure multi-party computation\u2019s interactive cryptographic rounds dominate overhead by increasing per-query latency and resource usage, directly reducing throughput to maintain confidentiality.",
    "question_token_count": 38,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 4,
    "question_difficulty_score": 8,
    "question_groundedness_score": 6,
    "avg_answer_token_count": 25
  },
  {
    "context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n",
    "topic": "Critically evaluate the compounding effects of post-training fine-tuning on human-annotated versus synthetic datasets in exacerbating contamination risks in LLM benchmarking.",
    "response": "<document_analysis>\nThe context highlights how LLMs undergo massive web-scale pre-training followed by post-training fine-tuning on both human-annotated and synthetic datasets, each introducing additional overlap with evaluation benchmarks. Human-annotated data, while curated, can still reflect test distributions, whereas synthetic data\u2014often engineered to mirror evaluation tasks\u2014poses an even greater contamination risk by embedding nearly identical examples. Retrieval-based detection methods struggle due to scale and lack of transparency, especially when synthetic content is tailored to benchmark formats. A probing question will ask experts to compare and contrast how fine-tuning on these two data types differentially exacerbates contamination, focusing on the inherent properties of synthetic data that make overlap more direct and detection more challenging.\n</document_analysis>\n\n<output_format>\nQuestion: How does post-training fine-tuning on synthetic datasets compound contamination risks more severely than fine-tuning on human-annotated datasets in LLM benchmarking?\n\nExplanation: Synthetic datasets are often generated to mimic the structure and content of evaluation tasks, causing direct overlap with benchmarks that is both more extensive and harder to detect than with human-annotated data.\n\nCorrect Answer: Because synthetic fine-tuning data replicates benchmark-like examples, increasing direct overlap and evading detection more than human-annotated data.\n</output_format>",
    "question": "How does post-training fine-tuning on synthetic datasets compound contamination risks more severely than fine-tuning on human-annotated datasets in LLM benchmarking?",
    "answer": "Because synthetic fine-tuning data replicates benchmark-like examples, increasing direct overlap and evading detection more than human-annotated data.",
    "explanation": "Synthetic datasets are often generated to mimic the structure and content of evaluation tasks, causing direct overlap with benchmarks that is both more extensive and harder to detect than with human-annotated data.",
    "question_token_count": 28,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 8,
    "avg_answer_token_count": 26
  },
  {
    "context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n",
    "topic": "Recommended future directions for formalizing evaluation standards to guide the development of contamination\u2010resistant dynamic benchmarks",
    "response": "<document_analysis>\nThe context discusses limitations of static benchmarks, introduces dynamic benchmarks, and highlights a lack of standardized evaluation criteria for dynamic benchmarks. It identifies key challenges\u2014contamination risk, transparency issues, balancing correctness with scalability, and neglected complexity control\u2014that future work must address. These points directly inform which dimensions standardized criteria should cover to guide contamination\u2010resistant dynamic benchmark development.\n</document_analysis>\n\n<output_format>\nQuestion: To formalize evaluation standards that effectively guide the development of contamination\u2011resistant dynamic benchmarks, which core dimensions should the standardized criteria encompass, and how do they address the main challenges identified with static and dynamic benchmarking methods?\n\nExplanation: The four dimensions directly respond to the highlighted issues\u2014contamination in static benchmarks, transparency deficits, correctness\u2011scalability trade\u2011offs, and lack of complexity control in dynamic benchmarks.\n\nCorrect Answer: Contamination mitigation, transparency and label protection, scalability\u2011correctness balance, and complexity control\n</output_format>",
    "question": "To formalize evaluation standards that effectively guide the development of contamination\u2011resistant dynamic benchmarks, which core dimensions should the standardized criteria encompass, and how do they address the main challenges identified with static and dynamic benchmarking methods?",
    "answer": "Contamination mitigation, transparency and label protection, scalability\u2011correctness balance, and complexity control",
    "explanation": "The four dimensions directly respond to the highlighted issues\u2014contamination in static benchmarks, transparency deficits, correctness\u2011scalability trade\u2011offs, and lack of complexity control in dynamic benchmarks.",
    "question_token_count": 44,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 19
  },
  {
    "context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Evaluate the criteria summarized in Table\u00a03 for existing dynamic benchmarks, interpreting the meaning of \u25cf\u25cf, \u25d0\u25d0, and \u25cb\u25cb markings.",
    "response": "<document_analysis>\nThe context defines the legend for Table\u00a03: \u25cf\u25cf indicates full support for a given criterion, \u25d0\u25d0 partial support, and \u25cb\u25cb no support. It further mentions dynamic benchmarks involve dataset transformations (T(\u00b7)) over timestamps to avoid contamination. The question should probe the expert\u2019s understanding of how these markings guide the assessment of a benchmark\u2019s strengths and weaknesses in terms of transformation capabilities versus contamination safeguards.\n</document_analysis>\n\n<output_format>\nQuestion: If a dynamic benchmark in Table\u00a03 is marked \u25cf\u25cf for transformation support but \u25cb\u25cb for contamination control, what does this combination reveal about its evaluation strengths and vulnerabilities?\n\nExplanation: The symbols \u25cf\u25cf and \u25cb\u25cb denote full and no support respectively; full transformation support without contamination safeguards implies robust temporal variation testing but susceptibility to data leakage.\n\nCorrect Answer: It fully supports dataset transformation but lacks any contamination controls, making it strong on temporal variations yet vulnerable to data leaks.\n</output_format>",
    "question": "If a dynamic benchmark in Table\u00a03 is marked \u25cf\u25cf for transformation support but \u25cb\u25cb for contamination control, what does this combination reveal about its evaluation strengths and vulnerabilities?",
    "answer": "It fully supports dataset transformation but lacks any contamination controls, making it strong on temporal variations yet vulnerable to data leaks.",
    "explanation": "The symbols \u25cf\u25cf and \u25cb\u25cb denote full and no support respectively; full transformation support without contamination safeguards implies robust temporal variation testing but susceptibility to data leakage.",
    "question_token_count": 35,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 7,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 24
  },
  {
    "context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n",
    "topic": "Explain how StructEval integrates knowledge graphs with LLM prompting to expand original benchmark concepts into a broader series of extended questions.",
    "response": "<document_analysis>\nThe context describes StructEval as a methodology that expands on concepts from original benchmarks by using both LLMs and knowledge graphs to develop extended questions. The key mechanism involves mapping benchmark concepts onto a knowledge graph to discover related entities and relations, then prompting an LLM to generate questions exploring those new semantic paths. Content is coherent and directly addresses the question topic.\n</document_analysis>\n\n<output_format>\nQuestion: How does StructEval utilize knowledge graphs together with LLM prompting to generate a broader series of extended questions from original benchmark concepts?\nExplanation: It pinpoints the two-step process of mapping concepts onto a knowledge graph to find related entities/relations, then uses LLM prompts to craft new questions along those connections.\nCorrect Answer: It maps benchmark concepts onto a knowledge graph to extract related nodes and relations, then prompts the LLM to generate extended questions based on those graph-derived connections.\n</output_format>",
    "question": "How does StructEval utilize knowledge graphs together with LLM prompting to generate a broader series of extended questions from original benchmark concepts?",
    "answer": "It maps benchmark concepts onto a knowledge graph to extract related nodes and relations, then prompts the LLM to generate extended questions based on those graph-derived connections.",
    "explanation": "It pinpoints the two-step process of mapping concepts onto a knowledge graph to find related entities/relations, then uses LLM prompts to craft new questions along those connections.",
    "question_token_count": 25,
    "answer_correctness_score": 8,
    "explanation_validity_score": 8,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 6,
    "avg_answer_token_count": 31
  },
  {
    "context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n",
    "topic": "Interpret how the forecasting tasks in Forecastbench conceptually differ from the knowledge retrieval and problem\u2010solving tasks in the other benchmarks.",
    "response": "<document_analysis>\nThe context describes various benchmarks that use data collected after an LLM\u2019s training cutoff to avoid contamination. Most benchmarks (LiveBench, AntiLeak-Bench, AcademicEval, LiveCodeBench, LiveAoPSBench) focus on static tasks: retrieving new facts, writing academic content, solving math or coding problems. In contrast, Forecastbench updates forecasting questions daily from real-time sources, emphasizing prediction under uncertainty. This reflects a conceptual shift from deterministic knowledge retrieval and problem-solving to dynamic, probabilistic forecasting using live data streams.\n</document_analysis>\n\n<output_format>\nQuestion: In evaluating language models, what key capability does Forecastbench\u2019s daily-updated forecasting tasks assess that static knowledge retrieval and problem-solving benchmarks do not?\nExplanation: Forecastbench requires models to perform dynamic, probabilistic predictions based on real-time data, rather than retrieving facts or solving predetermined problems.\nCorrect Answer: Real-time probabilistic forecasting from live data streams\n</output_format>",
    "question": "In evaluating language models, what key capability does Forecastbench\u2019s daily-updated forecasting tasks assess that static knowledge retrieval and problem-solving benchmarks do not?",
    "answer": "Real-time probabilistic forecasting from live data streams",
    "explanation": "Forecastbench requires models to perform dynamic, probabilistic predictions based on real-time data, rather than retrieving facts or solving predetermined problems.",
    "question_token_count": 30,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 9
  },
  {
    "context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Challenges associated with tracing and auditing the training data of proprietary LLMs for contamination detection.",
    "response": "<document_analysis>\nThe context discusses the difficulty of evaluating LLMs due to potential data contamination, specifically noting that proprietary models often scrape vast Internet data and that privacy and commercial concerns make it nearly impossible to trace exact training corpora. This lack of transparency prevents auditors from verifying whether benchmark items were seen during training, undermining contamination detection and leading to inflated performance metrics. For the topic of tracing and auditing proprietary LLM training data, a probing question should target why these constraints exist and their direct impact on contamination detection and benchmark validity.\nDifficulty Rating: 8/10\n</document_analysis>\n\n<output_format>\nQuestion: How do privacy and commercial constraints in proprietary LLMs hinder tracing the provenance of training data, and what is the impact on contamination detection?  \nExplanation: Restricting access to data sources and metadata prevents verifying overlap between training and benchmark sets, so contamination remains undetected and skews performance results.  \nCorrect Answer: They obscure data provenance by withholding source and metadata disclosures, making contamination detection infeasible and inflating benchmark scores.  \n</output_format>",
    "question": "How do privacy and commercial constraints in proprietary LLMs hinder tracing the provenance of training data, and what is the impact on contamination detection?",
    "answer": "They obscure data provenance by withholding source and metadata disclosures, making contamination detection infeasible and inflating benchmark scores.",
    "explanation": "Restricting access to data sources and metadata prevents verifying overlap between training and benchmark sets, so contamination remains undetected and skews performance results.",
    "question_token_count": 28,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 23
  },
  {
    "context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n",
    "topic": "Evaluate the importance and challenges of interpretability in dynamic benchmarking transformation processes to ensure data correctness and cost efficiency.",
    "response": "<document_analysis>\nThe context emphasizes the critical role of interpretability in dynamic benchmarking transformations to ensure data correctness and cost efficiency. It contrasts inherently interpretable rule-based transformations with opaque LLM-assisted ones, highlighting the need for explainability tools and human-in-the-loop validation to maintain reliability. Key themes include trade-offs between interpretability, transparency, and scalability, and methods for reconciling rule-based clarity with the flexibility of generative models.\nDifficulty rating: 8 (requires synthesis of interpretability theory, cost considerations, and hybrid pipeline design).\n</document_analysis>\n\n<output_format>\nQuestion: In a dynamic benchmarking pipeline that employs both rule-based and LLM-assisted data transformations, which combined strategy optimally preserves interpretability for cost-efficient correctness verification?\nExplanation: It merges rule-based logs for inherent transparency with explainability tools and targeted human-in-the-loop checks to ensure reliable, scalable transformations.\nCorrect Answer: Integrate explainability tools with human-in-the-loop validation alongside rule-based transformation logging\n</output_format>",
    "question": "In a dynamic benchmarking pipeline that employs both rule-based and LLM-assisted data transformations, which combined strategy optimally preserves interpretability for cost-efficient correctness verification?",
    "answer": "Integrate explainability tools with human-in-the-loop validation alongside rule-based transformation logging",
    "explanation": "It merges rule-based logs for inherent transparency with explainability tools and targeted human-in-the-loop checks to ensure reliable, scalable transformations.",
    "question_token_count": 30,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 17
  },
  {
    "context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "The taxonomy of LLM benchmarking research as presented in the paper\u2019s Figure\u00a02.",
    "response": "<document_analysis>\nThe context introduces a taxonomy (Fig.\u00a02) dividing LLM benchmarking research into static and dynamic approaches, with specific methods like data encryption and post-hoc contamination detection under static, and timestamp\u2010aware updates and benchmark regeneration under dynamic. A probing question can explore the conceptual distinction between two representative methods\u2014post-hoc detection (static) and timestamp-based updates (dynamic)\u2014highlighting the reactive vs. proactive strategies for contamination mitigation.\n</document_analysis>\n\n<output_format>\nQuestion: What fundamental difference does the taxonomy draw between post-hoc contamination detection methods and timestamp-aware dataset update methods in LLM benchmarking?\n\nExplanation: This distinction highlights that post-hoc detection reacts to contamination after benchmarks are released, while timestamp-aware updates proactively prevent contamination by synchronizing benchmarks with model training timelines.\n\nCorrect Answer: Post-hoc detection is reactive, addressing contamination after evaluation, whereas timestamp-aware updates are proactive, aligning benchmark refreshes with training periods.\n</output_format>",
    "question": "What fundamental difference does the taxonomy draw between post-hoc contamination detection methods and timestamp-aware dataset update methods in LLM benchmarking?",
    "answer": "Post-hoc detection is reactive, addressing contamination after evaluation, whereas timestamp-aware updates are proactive, aligning benchmark refreshes with training periods.",
    "explanation": "This distinction highlights that post-hoc detection reacts to contamination after benchmarks are released, while timestamp-aware updates proactively prevent contamination by synchronizing benchmarks with model training timelines.",
    "question_token_count": 24,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 27
  },
  {
    "context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n",
    "topic": "Strategies for combining encryption and label protection to achieve comprehensive confidentiality in model evaluation workflows.",
    "response": "<document_analysis>\nThe context describes two confidentiality-preserving techniques for model evaluation: encryption of test data (public\u2010key encryption, \u201cNo Derivatives\u201d license, confidential computing, secure MPC) and label protection (withholding answers behind an authorized evaluation service). Both mitigate data leakage but have trade-offs: key management complexity, computational overhead, vulnerability to key compromise. Combining them requires an architecture that encrypts inputs, hides labels, runs evaluation within a secure enclave or MPC, and uses robust key management (e.g., threshold secret sharing, key rotation) to address vulnerabilities and performance costs. A deep question would ask for an integrated workflow employing these primitives and addressing their limitations.\nDifficulty: 8/10\n</document_analysis>\n\n<output_format>\nQuestion: Design an integrated model evaluation workflow that ensures both test inputs and labels remain confidential, specifying the cryptographic primitives and system components you would use to mitigate key compromise risks and manage computational overhead.\nExplanation: The answer outlines a hybrid pipeline combining public-key encryption for data, secure MPC/TEE for private benchmarking, and threshold key-sharing for labels with key rotation to address both confidentiality and practical limitations.\nCorrect Answer: Employ public\u2010key encryption for test prompts, execute evaluation inside a TEE-backed MPC, store labels in a threshold\u2010shared encrypted vault with scheduled key rotations, and batch process evaluations to amortize overhead.\n</output_format>",
    "question": "Design an integrated model evaluation workflow that ensures both test inputs and labels remain confidential, specifying the cryptographic primitives and system components you would use to mitigate key compromise risks and manage computational overhead.",
    "answer": "Employ public\u2010key encryption for test prompts, execute evaluation inside a TEE-backed MPC, store labels in a threshold\u2010shared encrypted vault with scheduled key rotations, and batch process evaluations to amortize overhead.",
    "explanation": "The answer outlines a hybrid pipeline combining public-key encryption for data, secure MPC/TEE for private benchmarking, and threshold key-sharing for labels with key rotation to address both confidentiality and practical limitations.",
    "question_token_count": 37,
    "answer_correctness_score": 9,
    "explanation_validity_score": 8,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 8,
    "avg_answer_token_count": 42
  },
  {
    "context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n",
    "topic": "Design a novel framework that extends temporal cutoff principles to an emerging domain (e.g., social media trends or biomedical research) not covered in the text.",
    "response": "<document_analysis>\nThe context describes the principle of using a temporal cutoff to construct benchmarks by collecting data post\u2011model cutoff date, thereby preventing contamination. It lists existing implementations in math competitions, coding competitions, academic writing, and forecasting. There is no coverage of social media or biomedical domains. To generate a challenging, application\u2011based question, I will ask for the core architectural components required to extend temporal cutoff to an emerging biomedical research benchmark. This demands expert insight into data pipelines, contamination mitigation, and evaluation processes specific to biomedical research. Difficulty is calibrated at 8/10 to ensure deep domain expertise is required.\n</document_analysis>\n\n<output_format>\nQuestion: What combination of mechanisms constitutes an effective temporal\u2011cutoff benchmarking framework for emerging biomedical research that integrates post\u2011cutoff discoveries while eliminating knowledge contamination?\n\nExplanation: By combining automated post\u2011cutoff literature ingestion, temporal dataset partitioning, contamination filters, domain\u2011specific metadata tagging, and periodic expert review, the framework ensures freshness, reproducibility, and contamination control.\n\nCorrect Answer: A pipeline integrating automated literature ingestion post\u2011cutoff, version\u2011controlled temporal splits, contamination detection filters, domain\u2011specific metadata tagging, and periodic peer\u2011review updates.\n</output_format>",
    "question": "What combination of mechanisms constitutes an effective temporal\u2011cutoff benchmarking framework for emerging biomedical research that integrates post\u2011cutoff discoveries while eliminating knowledge contamination?",
    "answer": "A pipeline integrating automated literature ingestion post\u2011cutoff, version\u2011controlled temporal splits, contamination detection filters, domain\u2011specific metadata tagging, and periodic peer\u2011review updates.",
    "explanation": "By combining automated post\u2011cutoff literature ingestion, temporal dataset partitioning, contamination filters, domain\u2011specific metadata tagging, and periodic expert review, the framework ensures freshness, reproducibility, and contamination control.",
    "question_token_count": 30,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 9,
    "question_groundedness_score": 8,
    "avg_answer_token_count": 35
  },
  {
    "context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Assess the role of human effort in constructing comprehensive benchmarks and its implications for scalability and repeatability.",
    "response": "<document_analysis>\nThe context discusses the extensive human labor involved in constructing LLM benchmarks (instruction-following, coding tasks), highlights the static nature of these benchmarks as models evolve, and notes challenges around scalability, repeatability, and data contamination. The core idea is that while human-curated benchmarks offer comprehensive evaluation, their static and labor-intensive nature hinders scalability and repeatable assessment as LLM capabilities rapidly improve. This trade-off is subtle but fundamental to benchmark design.\n</document_analysis>\n\n<output_format>\nQuestion: What fundamental trade-off does the extensive human effort in constructing LLM benchmarks introduce, and how does this affect their scalability and repeatability?\nExplanation: Manually crafted benchmarks impose a high maintenance burden, making them static, hard to update, and difficult to replicate at scale.\nCorrect Answer: A high maintenance burden limits scalability and repeatability.\n</output_format>",
    "question": "What fundamental trade-off does the extensive human effort in constructing LLM benchmarks introduce, and how does this affect their scalability and repeatability?",
    "answer": "A high maintenance burden limits scalability and repeatability.",
    "explanation": "Manually crafted benchmarks impose a high maintenance burden, making them static, hard to update, and difficult to replicate at scale.",
    "question_token_count": 27,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 8,
    "avg_answer_token_count": 11
  },
  {
    "context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n",
    "topic": "Synthesize strategies for mitigating evaluation data contamination, including transparent data curation, provenance tracking, and community-driven verification.",
    "response": "<document_analysis>\nThe context discusses risks of evaluation data contamination in LLMs due to massive pre-training corpora, proprietary data, fine-tuning on human/synthetic datasets, and challenges of retrieval-based detection. To address this, strategies like transparent data curation, provenance tracking, and community-driven verification are proposed. A high-level, system-oriented question combining these elements challenges experts to synthesize a cohesive framework.\n</document_analysis>\n<output_format>\nQuestion: What system-level framework integrates transparent dataset curation, automated provenance tracking, and community-driven verification to prevent evaluation data contamination in large language models?\n\nExplanation: It combines open, versioned data management with detailed lineage metadata and collaborative audits to detect and exclude overlaps between training and evaluation sets.\n\nCorrect Answer: An open, versioned data registry with fine-grained provenance metadata and community-led auditing mechanisms.\n</output_format>",
    "question": "What system-level framework integrates transparent dataset curation, automated provenance tracking, and community-driven verification to prevent evaluation data contamination in large language models?",
    "answer": "An open, versioned data registry with fine-grained provenance metadata and community-led auditing mechanisms.",
    "explanation": "It combines open, versioned data management with detailed lineage metadata and collaborative audits to detect and exclude overlaps between training and evaluation sets.",
    "question_token_count": 29,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 2,
    "avg_answer_token_count": 19
  },
  {
    "context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n",
    "topic": "Debate the trade-offs between high update frequency and benchmark stability when implementing continuous evaluation protocols.",
    "response": "<document_analysis>\nThe provided context discusses temporal cutoff strategies in continuous evaluation benchmarks (e.g., LiveBench, AntiLeak-Bench, LiveCodeBench, Forecastbench) emphasizing frequent updates to incorporate new data post-LLM knowledge cutoffs. Key themes include freshness of test data versus risk of data contamination and the need for stability in performance metrics. The tension lies in updating benchmarks often (for relevance) which can introduce statistical variance and drift, undermining longitudinal comparability. A high-difficulty expert question should probe this tension and ask for a method that balances update frequency with stable benchmark measurement.\n</document_analysis>\n\n<output_format>\nQuestion: What is the primary trade-off introduced by very high update frequency in continuous evaluation benchmarks, and which methodological strategy best reconciles the need for freshness with the requirement for stable longitudinal comparability?\n\nExplanation: Continuously refreshing a benchmark improves relevance but destabilizes performance measurements over time; maintaining an immutable core test set alongside rolling updates preserves comparability while still incorporating new data.\n\nCorrect Answer: Use a fixed core test set combined with rolling fresh data.\n</output_format>",
    "question": "What is the primary trade-off introduced by very high update frequency in continuous evaluation benchmarks, and which methodological strategy best reconciles the need for freshness with the requirement for stable longitudinal comparability?",
    "answer": "Use a fixed core test set combined with rolling fresh data.",
    "explanation": "Continuously refreshing a benchmark improves relevance but destabilizes performance measurements over time; maintaining an immutable core test set alongside rolling updates preserves comparability while still incorporating new data.",
    "question_token_count": 38,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 4,
    "avg_answer_token_count": 13
  },
  {
    "context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "The distinction between static and dynamic benchmarking methods for evaluating large language models.",
    "response": "<document_analysis>\nThe context introduces static versus dynamic benchmarking for LLMs, emphasizing that static benchmarks (e.g., HumanEval) are vulnerable to data contamination because they are publicly released and get scraped into training data. Dynamic methods\u2014specifically timestamp-based continuous updates and benchmark data regeneration\u2014are presented as strategies to mitigate contamination by ensuring novelty of test items or reconstructing them to avoid overlap. A deep technical question comparing the two dynamic strategies will test expert understanding of their distinct operational mechanisms and contamination prevention rationale.\n</document_analysis>\n\n<output_format>\nQuestion: How do timestamp-based continuous updates and benchmark data regeneration each functionally mitigate data contamination in dynamic benchmarking of large language models?\n\nExplanation: This question probes the expert's understanding of the two core dynamic strategies, requiring precise differentiation of their mechanisms for preventing overlap between training data and test benchmarks.\n\nCorrect Answer: Timestamp-based updates enforce a temporal cutoff so only data created after the model\u2019s training period are included, ensuring novelty, while benchmark data regeneration reconstructs or transforms original test items to eliminate any that may already reside in the training corpus.\n</output_format>",
    "question": "How do timestamp-based continuous updates and benchmark data regeneration each functionally mitigate data contamination in dynamic benchmarking of large language models?",
    "answer": "Timestamp-based updates enforce a temporal cutoff so only data created after the model\u2019s training period are included, ensuring novelty, while benchmark data regeneration reconstructs or transforms original test items to eliminate any that may already reside in the training corpus.",
    "explanation": "This question probes the expert's understanding of the two core dynamic strategies, requiring precise differentiation of their mechanisms for preventing overlap between training data and test benchmarks.",
    "question_token_count": 24,
    "answer_correctness_score": 9,
    "explanation_validity_score": 8,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 47
  },
  {
    "context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n",
    "topic": "The design principles and objectives of S3Eval in evaluating LLMs using randomly generated SQL tables and queries",
    "response": "<document_analysis>\nThe context describes S3Eval\u2019s methodology: it evaluates LLM reasoning by testing SQL execution accuracy on randomly generated tables and queries. The core design principle is randomness to prevent memorization and ensure genuine relational reasoning. The question will probe the rationale behind this randomness and what reasoning capability it reveals, requiring expert knowledge of evaluation design.\n</document_analysis>\n\n<output_format>\nQuestion: Why is randomness in both SQL table and query generation central to S3Eval\u2019s evaluation design, and what aspect of reasoning does this randomness aim to reveal in LLMs?\n\nExplanation: Random generation prevents reliance on memorized patterns, forcing models to dynamically interpret novel schemas and query logic, thereby assessing true relational reasoning.\n\nCorrect Answer: It prevents memorization through randomization, revealing the model\u2019s ability to generalize relational reasoning and accurately execute novel SQL queries.\n</output_format>",
    "question": "Why is randomness in both SQL table and query generation central to S3Eval\u2019s evaluation design, and what aspect of reasoning does this randomness aim to reveal in LLMs?",
    "answer": "It prevents memorization through randomization, revealing the model\u2019s ability to generalize relational reasoning and accurately execute novel SQL queries.",
    "explanation": "Random generation prevents reliance on memorized patterns, forcing models to dynamically interpret novel schemas and query logic, thereby assessing true relational reasoning.",
    "question_token_count": 35,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 24
  },
  {
    "context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n",
    "topic": "Motivations for dynamic benchmarking and its potential to mitigate contamination versus its introduced trade\u2010offs",
    "response": "<document_analysis>\nThe context discusses the limitations of static benchmarks\u2014specifically their increasing contamination risk as training data scales\u2014and introduces dynamic benchmarks as a solution. Dynamic benchmarks generate fresh test items (through temporal cutoffs, LLM-based generation, graph perturbations, bilingual methods) to minimize overlap with training corpora, thereby reducing contamination probability. However, they introduce trade-offs: ensuring correctness while maintaining scalability and controlling sample complexity to avoid inefficiency. This dichotomy directly informs the question topic.\n</document_analysis>\n\n<output_format>\nQuestion: How do dynamic benchmarks reduce data contamination in LLM evaluation, and what primary trade-offs do they introduce?\n\nExplanation: Dynamic benchmarks continuously generate new test data to prevent overlap with training corpora\u2014thus lowering contamination risk\u2014but this approach demands balancing evaluation correctness against scalability and often suffers from insufficient complexity control.\n\nCorrect Answer: They mitigate contamination by generating fresh test items, at the cost of increased scalability demands and potential inefficiencies from poor complexity control.\n</output_format>",
    "question": "How do dynamic benchmarks reduce data contamination in LLM evaluation, and what primary trade-offs do they introduce?",
    "answer": "They mitigate contamination by generating fresh test items, at the cost of increased scalability demands and potential inefficiencies from poor complexity control.",
    "explanation": "Dynamic benchmarks continuously generate new test data to prevent overlap with training corpora\u2014thus lowering contamination risk\u2014but this approach demands balancing evaluation correctness against scalability and often suffers from insufficient complexity control.",
    "question_token_count": 21,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 26
  },
  {
    "context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n",
    "topic": "Investigating improved mapping metrics and how they enhance overlap detection efficacy.",
    "response": "<document_analysis>\nThe context outlines limitations of exact n-gram matching for data contamination detection and introduces embedding-based similarity and improved mapping metrics as robust alternatives. The key idea is that improved mapping metrics refine semantic overlap detection by going beyond basic token-level or embedding-based comparisons to reduce false negatives. This invites a deep technical question on how these metrics build upon embedding techniques to enhance detection efficacy.\n</document_analysis>\n\n<output_format>\nQuestion: How do improved mapping metrics build upon embedding-based similarity techniques to further mitigate false negatives in post-hoc overlap detection?\n\nExplanation: Improved mapping metrics enhance semantic alignment by integrating optimized mapping and context-aware thresholding, capturing paraphrastic overlaps missed by basic embedding similarity.\n\nCorrect Answer: They incorporate optimized alignment mechanisms and context-aware scoring thresholds to map semantically related content beyond direct embeddings, thus capturing subtle paraphrastic overlaps missed by basic similarity measures.\n</output_format>",
    "question": "How do improved mapping metrics build upon embedding-based similarity techniques to further mitigate false negatives in post-hoc overlap detection?",
    "answer": "They incorporate optimized alignment mechanisms and context-aware scoring thresholds to map semantically related content beyond direct embeddings, thus capturing subtle paraphrastic overlaps missed by basic similarity measures.",
    "explanation": "Improved mapping metrics enhance semantic alignment by integrating optimized mapping and context-aware thresholding, capturing paraphrastic overlaps missed by basic embedding similarity.",
    "question_token_count": 23,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 34
  },
  {
    "context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n",
    "topic": "Synthesize the evolution of reasoning benchmarks from early commonsense tests to complex multi-step inference tasks, highlighting shifts in difficulty and domain coverage.",
    "response": "<document_analysis>\nThe context outlines the evolution of reasoning benchmarks: early intuitive reasoning sets like PIQA, SIQA, HellaSwag, WinoGrande, and later academic challenge sets (ARC, OpenBookQA, CommonsenseQA) requiring background knowledge integration. It also notes a recent shift into non\u2010English factual evaluations (C\u2011SimpleQA). Key dimensions include reasoning style (intuitive vs. knowledge\u2010based), multi\u2010step inference, and domain coverage. To probe this evolution, a comparative question between an intuitive benchmark (HellaSwag) and a knowledge\u2010intensive one (OpenBookQA) will require nuanced domain\u2010expert understanding of both their aims and complexities.\nDifficulty: 7/10\n</document_analysis>\n\n<output_format>\nQuestion: How does the reasoning challenge in OpenBookQA fundamentally differ from that in HellaSwag, illustrating the shift from intuitive next-event prediction to inference requiring structured domain knowledge integration?\nExplanation: OpenBookQA requires models to perform multi-step inference using external science facts, whereas HellaSwag centers on predicting the most plausible continuation of everyday scenarios without needing external knowledge.\nCorrect Answer: OpenBookQA demands multi-step reasoning with external science facts, while HellaSwag focuses on intuitive context-based next-event prediction without external background knowledge.\n</output_format>",
    "question": "How does the reasoning challenge in OpenBookQA fundamentally differ from that in HellaSwag, illustrating the shift from intuitive next-event prediction to inference requiring structured domain knowledge integration?",
    "answer": "OpenBookQA demands multi-step reasoning with external science facts, while HellaSwag focuses on intuitive context-based next-event prediction without external background knowledge.",
    "explanation": "OpenBookQA requires models to perform multi-step inference using external science facts, whereas HellaSwag centers on predicting the most plausible continuation of everyday scenarios without needing external knowledge.",
    "question_token_count": 36,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 6,
    "avg_answer_token_count": 31
  },
  {
    "context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n",
    "topic": "Influence of undetected data contamination on evaluations of generalization performance in LLMs.",
    "response": "<document_analysis>\nThe context discusses two main ideas: examples of contamination (particularly syntactic contamination via rephrasing with added prefixes) and the significance of contamination in LLM benchmarks (leading to overestimated capabilities, undermined validity, and misleading conclusions about generalization). For the topic\u2014impact of undetected data contamination on generalization performance evaluations\u2014the core insight is that such contamination inflates performance metrics by conflating memorization with true generalization. This question probes an expert's understanding of how minor rephrasings can mask a model's weakness on novel data.\nDifficulty rating: 8/10 (targets nuanced understanding of contamination effects on benchmarking).\n</document_analysis>\n\n<output_format>\nQuestion: How does undetected syntactic contamination\u2014where test inputs are simple rephrasings of training examples\u2014specifically distort evaluations of an LLM\u2019s generalization ability?\n\nExplanation: Such contamination causes performance metrics to reflect memorization of seen data rather than the model\u2019s capacity to handle truly novel inputs, thus inflating its apparent generalization.\n\nCorrect Answer: By artificially boosting generalization scores through memorized responses to rephrased training data.\n</output_format>",
    "question": "How does undetected syntactic contamination\u2014where test inputs are simple rephrasings of training examples\u2014specifically distort evaluations of an LLM\u2019s generalization ability?",
    "answer": "By artificially boosting generalization scores through memorized responses to rephrased training data.",
    "explanation": "Such contamination causes performance metrics to reflect memorization of seen data rather than the model\u2019s capacity to handle truly novel inputs, thus inflating its apparent generalization.",
    "question_token_count": 32,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 18
  },
  {
    "context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n",
    "topic": "Interpretation and structure of Table\u00a02\u2019s overview of static benchmarks, task categories, and key references",
    "response": "<document_analysis>\nThe context outlines the formalism for static benchmarks (\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))) and describes Table\u00a02 as an overview mapping benchmarks to task categories and references. Central ideas include the triplet\u2019s role in defining a benchmark, the need for fixed inputs, expected outputs, and a scoring function, and the application across diverse tasks. A high\u2010difficulty question can probe implicit constraints this formulation imposes on benchmark selection in Table\u00a02.\n</document_analysis>\n\n<output_format>\nQuestion: What inherent limitation does the static benchmark definition D=(X,Y,S(.)) impose on the types of tasks included in Table\u00a02\u2019s overview, and how might this shape the selection of benchmarks?\nExplanation: Since D requires fixed inputs, outputs, and a scoring function, any task without these deterministic elements\u2014like interactive dialogues or adaptive learning tasks\u2014cannot be framed as a static benchmark, thus biasing Table\u00a02 toward only fully deterministic evaluations.\nCorrect Answer: It excludes tasks lacking fixed inputs, outputs, and scoring, biasing selection toward deterministic evaluations.\n</output_format>",
    "question": "What inherent limitation does the static benchmark definition D=(X,Y,S(.)) impose on the types of tasks included in Table\u00a02\u2019s overview, and how might this shape the selection of benchmarks?",
    "answer": "It excludes tasks lacking fixed inputs, outputs, and scoring, biasing selection toward deterministic evaluations.",
    "explanation": "Since D requires fixed inputs, outputs, and a scoring function, any task without these deterministic elements\u2014like interactive dialogues or adaptive learning tasks\u2014cannot be framed as a static benchmark, thus biasing Table\u00a02 toward only fully deterministic evaluations.",
    "question_token_count": 40,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 20
  },
  {
    "context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n",
    "topic": "The approach of NPHardEval in synthesizing random graphs for P and NP problems like the Traveling Salesman Problem",
    "response": "<document_analysis>\n1. Thoughtful Content Examination:\n   - The context describes NPHardEval\u2019s methodology: it evaluates LLM reasoning on NP-hard problems (e.g., TSP) by synthesizing random graphs of varying sizes.\n   - Central idea: varying graph size is the key lever to control problem difficulty and test model performance.\n\n2. Concept Exploration:\n   - Implicit assumption: larger graphs increase NP-hard complexity, requiring more advanced heuristic or exact reasoning.\n   - Underlying theory: computational complexity scales with graph size in NP-hard problems.\n   - Application: assessing an LLM\u2019s ability to generalize and scale reasoning strategies.\n\n3. Strategic Complexity Calibration:\n   - Difficulty level: 7/10. The question probes understanding of why size variation matters for NP-hard evaluation, beyond mere recall.\n\n4. Intentional Question Planning:\n   - The question invites reflection on the rationale behind random instance generation and complexity scaling.\n   - It targets domain experts\u2019 insight into complexity theory and evaluation design.\n   - Ensures answer is a concise statement summarizing this rationale.\n\n</document_analysis>\n\n<output_format>\nQuestion: Why does NPHardEval vary the size of randomly synthesized graphs when evaluating large language models on the Traveling Salesman Problem?  \nExplanation: It assesses how well the model scales its reasoning and heuristic strategies as problem complexity increases.  \nCorrect Answer: To systematically modulate problem complexity and test the model\u2019s scalability and heuristic reasoning across difficulty levels.  \n</output_format>",
    "question": "Why does NPHardEval vary the size of randomly synthesized graphs when evaluating large language models on the Traveling Salesman Problem?",
    "answer": "To systematically modulate problem complexity and test the model\u2019s scalability and heuristic reasoning across difficulty levels.",
    "explanation": "It assesses how well the model scales its reasoning and heuristic strategies as problem complexity increases.",
    "question_token_count": 26,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 9,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 19
  },
  {
    "context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n",
    "topic": "Analysis of practical mitigation tools designed to prevent or detect data contamination in LLM benchmarking and their real-world applicability.",
    "response": "<document_analysis>\nThe context discusses vulnerabilities of static benchmarking methods and the challenges of dynamic approaches, emphasizing the need for practical mitigation tools in real-world LLM benchmarking. Central themes include detecting and preventing data contamination, ensuring reproducibility, and maintaining operational efficiency in evolving datasets. A probing question should target the core mechanism enabling subtle overlap detection between training and test sets\u2014a key requirement for any practical tool\u2014and challenge domain experts to identify the most effective technique for large-scale, dynamic environments.\n</document_analysis>\n\n<output_format>\nQuestion: Which core mechanism should a mitigation tool employ to detect subtle overlaps between training and evaluation data in dynamic LLM benchmarking while maintaining operational efficiency?\nExplanation: Employing fuzzy hash\u2013based fingerprinting allows approximate matching of near-duplicate text at scale, enabling continuous contamination detection with minimal computational overhead.\nCorrect Answer: Fuzzy hash\u2013based dataset fingerprinting\n</output_format>",
    "question": "Which core mechanism should a mitigation tool employ to detect subtle overlaps between training and evaluation data in dynamic LLM benchmarking while maintaining operational efficiency?",
    "answer": "Fuzzy hash\u2013based dataset fingerprinting",
    "explanation": "Employing fuzzy hash\u2013based fingerprinting allows approximate matching of near-duplicate text at scale, enabling continuous contamination detection with minimal computational overhead.",
    "question_token_count": 27,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 2,
    "avg_answer_token_count": 8
  },
  {
    "context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n",
    "topic": "Methods for generating random SQL tables and queries and their impact on reasoning difficulty",
    "response": "<document_analysis>\nThe context describes S3Eval\u2019s methodology of evaluating LLM reasoning by executing random SQL queries on randomly generated tables but does not detail exact parameters. The key lever for increasing reasoning difficulty in SQL tasks is the complexity of query structure\u2014primarily join operations across multiple tables. This parameter directly governs the combinatorial search space and thus the cognitive load on the model.\nDifficulty rating: 6/10\n</document_analysis>\n\n<output_format>\nQuestion: Identify the primary feature of random SQL query generation in S3Eval that directly amplifies combinatorial reasoning complexity for LLMs.\nExplanation: Increasing join complexity across multiple randomly generated tables exponentially expands the search space and reasoning branches the model must consider.\nCorrect Answer: Multi-table join complexity\n</output_format>",
    "question": "Identify the primary feature of random SQL query generation in S3Eval that directly amplifies combinatorial reasoning complexity for LLMs.",
    "answer": "Multi-table join complexity",
    "explanation": "Increasing join complexity across multiple randomly generated tables exponentially expands the search space and reasoning branches the model must consider.",
    "question_token_count": 25,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 7,
    "avg_answer_token_count": 5
  },
  {
    "context": "###### Safety\nSafety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content. Datasets such as RealToxicityPrompts\u00a0(Gehman et\u00a0al., 2020) and ToxiGen\u00a0(Hartvigsen et\u00a0al., 2022) assess resilience against producing harmful outputs.\n\nBy providing a controlled environment to measure these aspects, safety benchmarks play a critical role in guiding the development of models that are not only powerful outputs but also responsible and trustworthy for real-world applications.\n\n\n###### Language\nLanguage benchmarks assess the LLMs\u2019 proficiency in specific languages.\n\nGLUE\u00a0(Wang, 2018) and SuperGLUE\u00a0(Wang et\u00a0al., 2019) cover tasks from sentiment analysis to language inference, while CLUE\u00a0(Xu et\u00a0al., 2020) targets Chinese language.\n\nTypo-fixing\u00a0(Suzgun et\u00a0al., 2022) is also widely used.\n\n\n###### Reading Comprehension\nReading comprehension tasks test a model\u2019s ability to extract and infer information from text. Benchmarks like SQuAD\u00a0(Rajpurkar et\u00a0al., 2018), QuAC\u00a0(Choi et\u00a0al., 2018), and BoolQ\u00a0(Clark et\u00a0al., 2019) challenge models to understand passages and draw logical conclusions.\n",
    "topic": "Design challenges and performance considerations in developing language\u2011specific benchmarks such as CLUE for Chinese",
    "response": "<document_analysis>\nThe context outlines the role of language benchmarks, including CLUE for Chinese, highlighting the need to assess language-specific proficiency. Key challenge: Chinese lacks explicit word boundaries, affecting tokenization and thus benchmark fairness and performance comparability across models. A question probing how this linguistic feature drives CLUE\u2019s design and what mitigation is needed will require domain expertise in Chinese NLP and benchmark construction.\n</document_analysis>\n\n<output_format>\nQuestion: In designing a Chinese\u2010specific benchmark like CLUE, how does the lack of explicit word boundaries in Chinese influence task evaluation, and what strategy ensures consistent performance comparisons across different models?\nExplanation: Because Chinese text has no spaces, inconsistent tokenization can skew results; applying a uniform segmentation scheme ensures fair, comparable evaluation.\nCorrect Answer: Use of a standardized word\u2010segmentation scheme for uniform tokenization.\n</output_format>",
    "question": "In designing a Chinese\u2010specific benchmark like CLUE, how does the lack of explicit word boundaries in Chinese influence task evaluation, and what strategy ensures consistent performance comparisons across different models?",
    "answer": "Use of a standardized word\u2010segmentation scheme for uniform tokenization.",
    "explanation": "Because Chinese text has no spaces, inconsistent tokenization can skew results; applying a uniform segmentation scheme ensures fair, comparable evaluation.",
    "question_token_count": 37,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 4,
    "avg_answer_token_count": 15
  },
  {
    "context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n",
    "topic": "The design considerations and potential metrics for the scoring function \ud835\udcae(\u22c5) to measure alignment between generated outputs and ground truth.",
    "response": "<document_analysis>\nThe context outlines the \u201cCorrectness\u201d criterion for dynamic benchmarking, defining a scoring function S(\u22c5) that measures alignment between generated outputs and ground truth. Key points include the need for robust, objective measures of alignment and possible use of domain-specific annotators. For a high-level technical question, focusing on the design of S\u2014specifically integrating both surface-level (n\u2011gram) and deep semantic metrics\u2014would challenge experts to balance trade\u2011offs and formalize a composite metric. This demands knowledge of BLEU, BERTScore, and weighting strategies.\n</document_analysis>\n\n<output_format>\nQuestion: To measure both lexical precision and semantic fidelity in the correctness score S(\u22c5) for dynamic benchmarks, which hybrid metric can be employed, and how is it formally defined?\nExplanation: This composite metric balances n\u2011gram overlap via BLEU with deep semantic similarity via BERTScore, addressing both surface accuracy and meaning alignment.\nCorrect Answer: A weighted sum S = \u03bb\u00b7BLEU + (1\u2212\u03bb)\u00b7BERTScore, where \u03bb\u2208[0,1] trades off lexical precision against semantic similarity.\n</output_format>",
    "question": "To measure both lexical precision and semantic fidelity in the correctness score S(\u22c5) for dynamic benchmarks, which hybrid metric can be employed, and how is it formally defined?",
    "answer": "A weighted sum S = \u03bb\u00b7BLEU + (1\u2212\u03bb)\u00b7BERTScore, where \u03bb\u2208[0,1] trades off lexical precision against semantic similarity.",
    "explanation": "This composite metric balances n\u2011gram overlap via BLEU with deep semantic similarity via BERTScore, addressing both surface accuracy and meaning alignment.",
    "question_token_count": 35,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 1,
    "avg_answer_token_count": 35
  },
  {
    "context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Dynamic benchmarking schema based on continuously updating datasets with respect to model training timestamps.",
    "response": "<document_analysis>\nThe context introduces dynamic benchmarking methods to mitigate data contamination in LLM evaluation, with one key approach being continuously updating benchmark datasets based on the model\u2019s training timestamps. This addresses the contamination of static benchmarks but hinges on knowing the precise training cutoff time, which is often unavailable due to opaque, large-scale, internet-scraped corpora. The analysis identifies the underlying assumption (knowledge of training timestamps) and its practical vulnerability (opacity of data sources), which offers a nuanced, expert-level question.\nDifficulty rating: 8/10\n</document_analysis>\n\n<output_format>\nQuestion: What key assumption underlies timestamp-based dynamic benchmarking\u2019s ability to prevent data contamination, and why is this assumption difficult to satisfy given the nature of LLM training corpora?\nExplanation: Timestamp-based updating only excludes examples prior to a known training cutoff, but the opaque, continuously scraped nature of LLM training data makes identifying that cutoff infeasible.\nCorrect Answer: It assumes precise knowledge of the model\u2019s training cutoff timestamp, which is unattainable due to the black\u2011box, large\u2011scale scraping of training corpora.\n</output_format>",
    "question": "What key assumption underlies timestamp-based dynamic benchmarking\u2019s ability to prevent data contamination, and why is this assumption difficult to satisfy given the nature of LLM training corpora?",
    "answer": "It assumes precise knowledge of the model\u2019s training cutoff timestamp, which is unattainable due to the black\u2011box, large\u2011scale scraping of training corpora.",
    "explanation": "Timestamp-based updating only excludes examples prior to a known training cutoff, but the opaque, continuously scraped nature of LLM training data makes identifying that cutoff infeasible.",
    "question_token_count": 34,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 34
  },
  {
    "context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "The set of proposed criteria for evaluating the quality and reliability of dynamic benchmarks.",
    "response": "<document_analysis>\nThe introduction outlines the evolution from static to dynamic benchmarking, specifically highlighting the use of timestamp-based continuous updates as a method to minimize data contamination in LLM evaluation. While the full set of criteria is not enumerated here, the text clearly presents timestamp alignment as a pivotal dynamic benchmarking criterion due to its direct impact on preventing training-test set leakage. This provides sufficient substance to formulate a deep, domain-expert-level question probing the rationale and mechanism by which timestamp-based updates mitigate contamination.\n</document_analysis>\n\n<output_format>\nQuestion: Explain why aligning benchmark data updates with LLM training timestamps is a critical criterion for dynamic benchmarking and how it directly addresses the risk of data contamination.  \nExplanation: By tying benchmark refreshes to a model\u2019s training cutoff, any newly generated test instances are guaranteed to be unseen during training, preventing information leakage.  \nCorrect Answer: Because updating benchmarks based on training timestamps ensures that test data remains unseen by the model during training, directly mitigating data contamination.  \n</output_format>",
    "question": "Explain why aligning benchmark data updates with LLM training timestamps is a critical criterion for dynamic benchmarking and how it directly addresses the risk of data contamination.",
    "answer": "Because updating benchmarks based on training timestamps ensures that test data remains unseen by the model during training, directly mitigating data contamination.",
    "explanation": "By tying benchmark refreshes to a model\u2019s training cutoff, any newly generated test instances are guaranteed to be unseen during training, preventing information leakage.",
    "question_token_count": 29,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 9,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 25
  },
  {
    "context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n",
    "topic": "The impact of low correctness measurements on downstream LLM benchmarking outcomes.",
    "response": "<document_analysis>\nThe context defines \u201cCorrectness\u201d as the alignment between generated dataset outputs and an oracle-provided ground truth, quantified by a scoring function. It stresses that without sufficient correctness, dynamic benchmarks may give a false sense of reliability and produce misleading evaluations of LLMs. The question topic asks about the downstream impact of low correctness measurements. A high-level, domain-expert question will probe how insufficient correctness distorts LLM performance interpretation.\n</document_analysis>\n\n<output_format>\nQuestion: What primary risk arises when a dynamic benchmark exhibits low correctness, particularly regarding the interpretability of downstream LLM benchmarking results?\nExplanation: Low correctness means the transformed outputs diverge from ground-truth references, causing performance signals to be misleading.\nCorrect Answer: It undermines the reliability of LLM evaluations by generating misleading performance signals misaligned with ground truth.\n</output_format>",
    "question": "What primary risk arises when a dynamic benchmark exhibits low correctness, particularly regarding the interpretability of downstream LLM benchmarking results?",
    "answer": "It undermines the reliability of LLM evaluations by generating misleading performance signals misaligned with ground truth.",
    "explanation": "Low correctness means the transformed outputs diverge from ground-truth references, causing performance signals to be misleading.",
    "question_token_count": 24,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 20
  },
  {
    "context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n",
    "topic": "Critically evaluate how high external diversity but low internal diversity (and vice versa) would impact downstream model training and generalization.",
    "response": "<document_analysis>\nThe context defines external diversity as the expected distance between each transformed dataset and the original seed, and internal diversity as the expected distance between pairs of transformed datasets. The question topic asks for a critical evaluation of two extremes: high external but low internal diversity versus low external but high internal diversity. A deep analysis must connect these diversity metrics to model bias, overfitting, and generalization. This invites expert reflection on domain shift, robustness to variations, and exposure to novel patterns. Difficulty is calibrated high (\u22488/10), demanding integration of bias-variance trade-offs with dataset transformation characteristics.\n</document_analysis>\n\n<output_format>\nQuestion: Contrast the expected impacts on model bias, overfitting, and generalization when augmentations exhibit (a) high external but low internal diversity versus (b) low external but high internal diversity.\n\nExplanation: High external/low internal diversity means all augmentations shift far from the seed in the same narrow way (inducing domain misalignment and overfitting to that shift), whereas low external/high internal diversity provides varied perturbations around the seed (boosting robustness locally but limiting exposure to genuinely new patterns).\n\nCorrect Answer: High external/low internal diversity leads to overfitting on a consistent but shifted style and poor generalization, while low external/high internal diversity enhances local robustness yet constrains broader extrapolation.\n</output_format>",
    "question": "Contrast the expected impacts on model bias, overfitting, and generalization when augmentations exhibit (a) high external but low internal diversity versus (b) low external but high internal diversity.",
    "answer": "High external/low internal diversity leads to overfitting on a consistent but shifted style and poor generalization, while low external/high internal diversity enhances local robustness yet constrains broader extrapolation.",
    "explanation": "High external/low internal diversity means all augmentations shift far from the seed in the same narrow way (inducing domain misalignment and overfitting to that shift), whereas low external/high internal diversity provides varied perturbations around the seed (boosting robustness locally but limiting exposure to genuinely new patterns).",
    "question_token_count": 38,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 6,
    "avg_answer_token_count": 35
  },
  {
    "context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Analyze the strengths and limitations of instruction\u2010following tasks as a metric for LLM comprehension and execution abilities.",
    "response": "<document_analysis>\nThe context outlines the role of instruction-following tasks in LLM benchmarking, noting their utility in evaluating command interpretation and execution, but also their limitations as static benchmarks (e.g., becoming too easy, data contamination). There is sufficient meaningful content to craft a question that probes these strengths and weaknesses deeply.\nDifficulty rating: 8/10 (requires understanding of benchmarking trade-offs and dynamic vs static benchmark issues).\n</document_analysis>\n\n<output_format>\nQuestion: What fundamental trade-off do instruction-following benchmarks embody when evaluating comprehension and execution abilities of rapidly evolving LLMs?\n\nExplanation: They precisely assess a model\u2019s ability to follow directives but, being static, become easier for stronger models and susceptible to data contamination.\n\nCorrect Answer: They reliably measure directive execution yet degrade over time as model strength and data contamination erode their discriminative power.\n</output_format>",
    "question": "What fundamental trade-off do instruction-following benchmarks embody when evaluating comprehension and execution abilities of rapidly evolving LLMs?",
    "answer": "They reliably measure directive execution yet degrade over time as model strength and data contamination erode their discriminative power.",
    "explanation": "They precisely assess a model\u2019s ability to follow directives but, being static, become easier for stronger models and susceptible to data contamination.",
    "question_token_count": 23,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 7,
    "question_difficulty_score": 9,
    "question_groundedness_score": 6,
    "avg_answer_token_count": 23
  },
  {
    "context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n",
    "topic": "Investigate the strengths and weaknesses of LLM-based generation approaches for creating dynamic benchmarking datasets, focusing on model transparency and control.",
    "response": "<document_analysis>\nThe context details the interpretability challenges of LLM-assisted transformations in dynamic benchmarking, contrasting them with inherently transparent rule-based methods and suggesting explainability tools and human-in-the-loop validation to ensure correctness. Key themes include model transparency, traceability, and hybrid mechanisms. A probing question can ask for a detailed pipeline design that balances LLM generative power with interpretability requirements, demanding expert synthesis of these concepts.\n</document_analysis>\n\n<output_format>\nQuestion: Design a hybrid dynamic benchmarking pipeline that harnesses LLM-based data generation for maximal diversity while ensuring each transformation remains transparent, traceable, and verifiable\u2014what critical components must this pipeline include?\nExplanation: Incorporating rule-based schema constraints, detailed prompt\u2013response logging, explainability tool integration, and human-in-the-loop validation collectively reconcile the LLM\u2019s generative strengths with interpretability and correctness.\nCorrect Answer: It must integrate LLM-driven generation with rule-based schema validation, comprehensive prompt and response logging, explainability tool integration, and human-in-the-loop validation.\n</output_format>",
    "question": "Design a hybrid dynamic benchmarking pipeline that harnesses LLM-based data generation for maximal diversity while ensuring each transformation remains transparent, traceable, and verifiable\u2014what critical components must this pipeline include?",
    "answer": "It must integrate LLM-driven generation with rule-based schema validation, comprehensive prompt and response logging, explainability tool integration, and human-in-the-loop validation.",
    "explanation": "Incorporating rule-based schema constraints, detailed prompt\u2013response logging, explainability tool integration, and human-in-the-loop validation collectively reconcile the LLM\u2019s generative strengths with interpretability and correctness.",
    "question_token_count": 40,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 31
  },
  {
    "context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n",
    "topic": "The role of complexity assessment in distinguishing performance drops caused by data contamination from those caused by increased task difficulty in dynamic benchmarking.",
    "response": "<document_analysis>\nThe provided context discusses the necessity of quantifying dataset complexity in dynamic benchmarks to distinguish whether LLM performance drops stem from data contamination or from increases in task difficulty. It introduces a complexity measurement function \u03a8(\u00b7) and formalizes stability as the variance of \u03a8 across multiple transformed trials. High variance implies unstable complexity, complicating attribution of performance changes. The text also highlights that existing complexity metrics are often domain-specific and lack generalizability. This interplay between metric reliability and attribution clarity is central to understanding the role and limitations of complexity assessment in dynamic benchmarking.\n</document_analysis>\n\n<output_format>\nQuestion: Why might the variance of a complexity measurement \u03a8 across dynamic benchmark transformations fail to clearly distinguish LLM performance drops due to data contamination from those due to increased task complexity?\n\nExplanation: Domain-specific biases and insensitivities in \u03a8 can distort the measured variance, conflating metric limitations with actual shifts in difficulty and thereby obscuring the true cause of performance degradation.\n\nCorrect Answer: Because \u03a8 can be biased or incomplete, its variance may reflect measurement shortcomings rather than genuine complexity changes.\n</output_format>",
    "question": "Why might the variance of a complexity measurement \u03a8 across dynamic benchmark transformations fail to clearly distinguish LLM performance drops due to data contamination from those due to increased task complexity?",
    "answer": "Because \u03a8 can be biased or incomplete, its variance may reflect measurement shortcomings rather than genuine complexity changes.",
    "explanation": "Domain-specific biases and insensitivities in \u03a8 can distort the measured variance, conflating metric limitations with actual shifts in difficulty and thereby obscuring the true cause of performance degradation.",
    "question_token_count": 33,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 21
  },
  {
    "context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n",
    "topic": "Investigate how temporal cutoff benchmarks can be adapted to evaluate multimodal LLMs that incorporate images, audio, or other non-text modalities.",
    "response": "<document_analysis>\nThe context discusses the use of temporal cutoff benchmarks to prevent data contamination in LLM evaluation, listing examples for text-based tasks. To adapt this concept for multimodal LLMs, one must address novel challenges like verifying novelty across non-text streams (images, audio) and aligning different update schedules. A core technical requirement is ensuring post-cutoff multimodal data were not seen during training, which implies robust deduplication methods across modalities.\nDifficulty rating: 8/10 (requires specialized knowledge of multimodal data fingerprinting).\n</document_analysis>\n\n<output_format>\nQuestion: What unified deduplication method can ensure both images and audio collected after the cutoff date are novel relative to a multimodal model\u2019s pre-cutoff training data?\n\nExplanation: Preventing cross-modal contamination requires content-based fingerprinting using hashes tailored to each modality (e.g., perceptual hashes for images, acoustic fingerprints for audio).\n\nCorrect Answer: Content-based fingerprinting with modality-specific hashes\n</output_format>",
    "question": "What unified deduplication method can ensure both images and audio collected after the cutoff date are novel relative to a multimodal model\u2019s pre-cutoff training data?",
    "answer": "Content-based fingerprinting with modality-specific hashes",
    "explanation": "Preventing cross-modal contamination requires content-based fingerprinting using hashes tailored to each modality (e.g., perceptual hashes for images, acoustic fingerprints for audio).",
    "question_token_count": 31,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 8,
    "question_groundedness_score": 2,
    "avg_answer_token_count": 9
  },
  {
    "context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n",
    "topic": "Development of hybrid synthetic data generation frameworks that integrate rule-based and template-based approaches to address existing limitations.",
    "response": "<document_analysis>\nThe context outlines limitations of human-curated benchmarks (high labor, data contamination) and describes two synthetic generation paradigms: rule-based (low collision) and template-based (dynamic variability). It mentions specific systems (GSM-Symbolic, Mathador-LM, MMLU-CF) that exemplify template-driven instantiation. The prompt requests a probing question on architecting a hybrid framework combining rule-based and template-based methods to overcome these issues. \n</document_analysis>\n\n<output_format>\nQuestion: What architectural pipeline can a hybrid synthetic data generation framework employ to combine the low collision guarantees of rule-based methods with the dynamic diversity of template-based approaches, while automating verification and preventing future benchmark contamination?\nExplanation: The answer describes a layered system where a rule-engine ensures structural validity and uniqueness, template modules inject variability, provenance hashing tracks instance origin to avoid reuse, and an automated solver verifies correctness.\nCorrect Answer: A layered pipeline with a rule-based constraint engine feeding template-driven instantiation, augmented by provenance-based hashing and automated solver validation.\n</output_format>",
    "question": "What architectural pipeline can a hybrid synthetic data generation framework employ to combine the low collision guarantees of rule-based methods with the dynamic diversity of template-based approaches, while automating verification and preventing future benchmark contamination?",
    "answer": "A layered pipeline with a rule-based constraint engine feeding template-driven instantiation, augmented by provenance-based hashing and automated solver validation.",
    "explanation": "The answer describes a layered system where a rule-engine ensures structural validity and uniqueness, template modules inject variability, provenance hashing tracks instance origin to avoid reuse, and an automated solver verifies correctness.",
    "question_token_count": 41,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 9,
    "question_groundedness_score": 7,
    "avg_answer_token_count": 25
  },
  {
    "context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n",
    "topic": "Analyzing the reliance on centralized evaluation systems and its effect on detailed error analysis and reproducibility.",
    "response": "<document_analysis>\nThe context highlights that hiding labels (label protection) reduces transparency and prevents independent verification by making researchers use centralized evaluation services. This dependency obstructs granular error inspection and hinders reproducibility. No irrelevant content detected.\n</document_analysis>\n\n<output_format>\nQuestion: How does hiding ground-truth labels and mandating centralized evaluation systems obstruct detailed error analysis and reproducibility in model research?\nExplanation: Without access to labels or local scoring, researchers cannot perform fine-grained failure diagnostics or independently rerun evaluation pipelines.\nCorrect Answer: By denying label access and forcing black-box scoring, it prevents in-depth failure inspection and independent replication of evaluation procedures.\n</output_format>",
    "question": "How does hiding ground-truth labels and mandating centralized evaluation systems obstruct detailed error analysis and reproducibility in model research?",
    "answer": "By denying label access and forcing black-box scoring, it prevents in-depth failure inspection and independent replication of evaluation procedures.",
    "explanation": "Without access to labels or local scoring, researchers cannot perform fine-grained failure diagnostics or independently rerun evaluation pipelines.",
    "question_token_count": 24,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 9,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 24
  },
  {
    "context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Limitations of static benchmarks and motivations for adopting dynamic benchmarking approaches.",
    "response": "<document_analysis>\nThe context discusses the limitations of static benchmarks due to inevitable data contamination when LLMs are trained on internet-sourced data that may include the benchmark itself. It introduces dynamic benchmarking approaches, notably timestamp-based continuous updates, to enforce separation between training and evaluation data. However, it also highlights residual challenges, such as opaque training corpora and retrospective data scraping that can still lead to contamination. This interplay between maintaining the fundamental ML principle of train-test separation and the persistent difficulty of completely preventing contamination underpins a nuanced, expert-level question.\n</document_analysis>\n\n<output_format>\nQuestion: How do timestamp-based continuous updates in dynamic benchmarking enforce the machine learning principle of training-test separation, and why can they still fail to completely prevent data contamination in large language model evaluations?\n\nExplanation: Timestamp-based updates create evaluation data after the model's training cutoff to avoid overlap, but hidden archival scraping and opaque training corpora can still introduce unseen overlaps.\n\nCorrect Answer: They generate benchmarks post-training cutoff to avoid overlap, yet retrospective scraping into opaque training data can still cause contamination.\n</output_format>",
    "question": "How do timestamp-based continuous updates in dynamic benchmarking enforce the machine learning principle of training-test separation, and why can they still fail to completely prevent data contamination in large language model evaluations?",
    "answer": "They generate benchmarks post-training cutoff to avoid overlap, yet retrospective scraping into opaque training data can still cause contamination.",
    "explanation": "Timestamp-based updates create evaluation data after the model's training cutoff to avoid overlap, but hidden archival scraping and opaque training corpora can still introduce unseen overlaps.",
    "question_token_count": 37,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 4,
    "question_difficulty_score": 9,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 23
  },
  {
    "context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Evaluate the necessity of holistic benchmark design for assessing the evolving performance of general-purpose LLMs.",
    "response": "<document_analysis>\n1. The context emphasizes the need for benchmarks that offer a holistic assessment of general-purpose LLMs, covering diverse capabilities such as instruction following and code generation.\n2. It highlights the limitations of static benchmarks\u2014namely that they become outdated as LLMs grow stronger and risk data contamination\u2014and introduces dynamic benchmarks and contamination detectors as remedies.\n3. Implicitly, the text assumes that continuous evolution of both model capabilities and training data necessitates adaptive evaluation frameworks.\n4. Difficulty level: 8/10, since the question probes the interplay between benchmarking strategies and contamination control to preserve evaluation fidelity over time.\n5. Planned question: Ask how dynamic benchmark generation and contamination detection work together to sustain the rigour and novelty of LLM evaluations across different task domains as models continuously absorb more data.\n</document_analysis>\n\n<output_format>\nQuestion: How does the synergy between dynamic benchmark generation and contamination detection mechanisms contribute to maintaining the sensitivity of LLM evaluations across instruction-following and coding tasks as models continually train on expanding datasets?\nExplanation: Dynamic benchmarks refresh tasks to stay ahead of model familiarity, while contamination detectors guard against performance inflation from leaked data, together ensuring evaluations remain rigorous and novel.\nCorrect Answer: By continuously updating task distributions to outpace model exposure and employing contamination detection to prevent inflated scores from pre-training data leaks.\n</output_format>",
    "question": "How does the synergy between dynamic benchmark generation and contamination detection mechanisms contribute to maintaining the sensitivity of LLM evaluations across instruction-following and coding tasks as models continually train on expanding datasets?",
    "answer": "By continuously updating task distributions to outpace model exposure and employing contamination detection to prevent inflated scores from pre-training data leaks.",
    "explanation": "Dynamic benchmarks refresh tasks to stay ahead of model familiarity, while contamination detectors guard against performance inflation from leaked data, together ensuring evaluations remain rigorous and novel.",
    "question_token_count": 36,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 9,
    "question_groundedness_score": 6,
    "avg_answer_token_count": 25
  },
  {
    "context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n",
    "topic": "Interpretation of collision metrics in the context of iterative benchmark updates",
    "response": "<document_analysis>\nThe context introduces two metrics\u2014Collision Rate and Repeat Trials\u2014to evaluate dynamic benchmarks\u2019 robustness against data contamination. Collision Rate quantifies overlap between independent transformations, while Repeat Trials measures how many attempts are needed to regenerate an existing transformed dataset. Together, they reveal the size and diversity of the transformation space and its ability to produce novel test cases over iterations. For iterative updates, one must interpret their interplay to decide when transformation novelty is saturating and when to adjust the transformation algorithm.\n</document_analysis>\n<output_format>\nQuestion: In a dynamic benchmarking cycle where repeated transformations maintain a low collision rate (under 5%) but the repeat trials metric rises exponentially, what does this dual pattern indicate about the underlying transformation space, and what strategic change should be made to preserve benchmark novelty?\nExplanation: Minimal overlap alongside exploding attempts to find new variations signals a large yet finite transformation space reaching saturation, necessitating the introduction of new or varied transformation functions.\nCorrect Answer: The transformation space is vast but bounded, so one must diversify or expand the transformation rules.\n</output_format>",
    "question": "In a dynamic benchmarking cycle where repeated transformations maintain a low collision rate (under 5%) but the repeat trials metric rises exponentially, what does this dual pattern indicate about the underlying transformation space, and what strategic change should be made to preserve benchmark novelty?",
    "answer": "The transformation space is vast but bounded, so one must diversify or expand the transformation rules.",
    "explanation": "Minimal overlap alongside exploding attempts to find new variations signals a large yet finite transformation space reaching saturation, necessitating the introduction of new or varied transformation functions.",
    "question_token_count": 51,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 20
  },
  {
    "context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n",
    "topic": "Commonsense reasoning and factual knowledge retrieval benchmarks and the measurement of reasoning quality",
    "response": "<document_analysis>\nThe context defines a static benchmark as D=(X,Y,S(.)), highlighting that S is the scoring function comparing model outputs to expected outputs Y. It notes benchmarks span diverse tasks, including commonsense reasoning and factual knowledge retrieval, each requiring different evaluation metrics. A profound question probes how the scoring function S must adapt to capture the nuances of reasoning quality versus factual precision. This invites critical engagement on tailored evaluation criteria, challenging even domain experts to distinguish inference-coherence metrics from fact-matching metrics.\n</document_analysis>\n\n<output_format>\nQuestion: In a static benchmark D=(X,Y,S), what distinct scoring strategies should S employ to accurately evaluate commonsense reasoning versus factual knowledge retrieval tasks?\nExplanation: Because commonsense reasoning requires measuring inference validity and logical coherence, while factual retrieval demands precise matching of factual content to reference answers.\nCorrect Answer: Use entailment- or coherence-based metrics for commonsense reasoning and precision/recall\u2013based fact-matching metrics for factual retrieval.\n</output_format>",
    "question": "In a static benchmark D=(X,Y,S), what distinct scoring strategies should S employ to accurately evaluate commonsense reasoning versus factual knowledge retrieval tasks?",
    "answer": "Use entailment- or coherence-based metrics for commonsense reasoning and precision/recall\u2013based fact-matching metrics for factual retrieval.",
    "explanation": "Because commonsense reasoning requires measuring inference validity and logical coherence, while factual retrieval demands precise matching of factual content to reference answers.",
    "question_token_count": 30,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 4,
    "avg_answer_token_count": 26
  },
  {
    "context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n",
    "topic": "Impact of dynamic benchmarking on long-term reliability and fairness of LLM assessments.",
    "response": "<document_analysis>\nThe context discusses the shift from static to dynamic benchmarking for LLMs, highlighting limitations of static benchmarks, absence of standard criteria for dynamic ones, and proposing optimal design principles. It emphasizes continuous updates and standardization to mitigate data contamination risks. This directly relates to how dynamic benchmarking impacts long-term reliability and fairness.\n</document_analysis>\n\n<output_format>\nQuestion: How does dynamically refreshing evaluation datasets and enforcing standardized assessment criteria safeguard both the reliability and fairness of LLM benchmarks over time?\nExplanation: Continuously introducing unseen data prevents models from overfitting to stale benchmarks, while uniform metrics ensure equitable comparison and bias mitigation across model iterations.\nCorrect Answer: Regularly refreshing evaluation datasets with unseen data and applying standardized metrics.\n</output_format>",
    "question": "How does dynamically refreshing evaluation datasets and enforcing standardized assessment criteria safeguard both the reliability and fairness of LLM benchmarks over time?",
    "answer": "Regularly refreshing evaluation datasets with unseen data and applying standardized metrics.",
    "explanation": "Continuously introducing unseen data prevents models from overfitting to stale benchmarks, while uniform metrics ensure equitable comparison and bias mitigation across model iterations.",
    "question_token_count": 24,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 14
  },
  {
    "context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n",
    "topic": "Proposing integrated strategies combining multiple post-hoc methods to achieve comprehensive contamination detection.",
    "response": "<document_analysis>\nThe context outlines various post-hoc contamination detection techniques\u2014exact n\u2011gram matching, embedding\u2011based similarity, and behavioral analyses (masked inputs, partial completions, paraphrase preference). It highlights the trade\u2011offs (false negatives with exact matching, robustness of embeddings, behavior\u2011based validation). For an expert\u2011level question on integrated strategies, the challenge is synthesizing these methods into a coherent pipeline that addresses their limitations and calibrates thresholds to control error rates.\nDifficulty rating: 8/10\n</document_analysis>\n\n<output_format>\nQuestion: How would you design an integrated post\u2011hoc contamination detection pipeline that combines exact n\u2011gram matching, embedding\u2011based similarity, and behavioral analyses (e.g., masked\u2011input memorization checks and paraphrase preference tests) to minimize both false negatives and false positives?\n\nExplanation: The answer outlines a staged detection workflow that leverages the precision of n\u2011gram filters, the coverage of calibrated embedding similarity, and the validation power of behavioral probes to comprehensively catch contamination while balancing error rates.\n\nCorrect Answer: Implement a three\u2011stage pipeline: initial high\u2011precision n\u2011gram filtering, followed by thresholded embedding\u2011based similarity detection, and finalized with behavioral probes (masked\u2011input memorization and paraphrase preference tests) to cross\u2011validate and adjust thresholds, thereby reducing false negatives and false positives.\n</output_format>",
    "question": "How would you design an integrated post\u2011hoc contamination detection pipeline that combines exact n\u2011gram matching, embedding\u2011based similarity, and behavioral analyses (e.g., masked\u2011input memorization checks and paraphrase preference tests) to minimize both false negatives and false positives?",
    "answer": "Implement a three\u2011stage pipeline: initial high\u2011precision n\u2011gram filtering, followed by thresholded embedding\u2011based similarity detection, and finalized with behavioral probes (masked\u2011input memorization and paraphrase preference tests) to cross\u2011validate and adjust thresholds, thereby reducing false negatives and false positives.",
    "explanation": "The answer outlines a staged detection workflow that leverages the precision of n\u2011gram filters, the coverage of calibrated embedding similarity, and the validation power of behavioral probes to comprehensively catch contamination while balancing error rates.",
    "question_token_count": 54,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 9,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 60
  },
  {
    "context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n",
    "topic": "Discussion on potential contamination and reuse issues in synthetic benchmarks despite the use of rule/template-based methodologies.",
    "response": "<document_analysis>\nThe context delineates two primary approaches to synthetic benchmark creation\u2014rule-based and template-based\u2014and highlights concerns about data contamination due to reuse of problems across evaluations. Although template-based methods claim low collision probability by randomizing placeholders, the underlying template skeletons remain constant, enabling LLMs to memorize structural patterns. This nuance underpins the question\u2019s focus: probing how reuse of templates can still foster contamination and reduce benchmark novelty.\nDifficulty rating: 8/10 (requires understanding of data contamination mechanics in synthetic benchmarks)\n</document_analysis>\n\n<output_format>\nQuestion: How can the repeated use of structured templates in synthetic benchmark generation inadvertently permit contamination and reduce evaluation novelty?\nExplanation: Reusing the same template skeletons, even with varied placeholders, allows models to learn and recall familiar problem formats rather than tackling truly unseen instances.\nCorrect Answer: By repeatedly employing identical template structures, enabling models to memorize and exploit familiar problem formats despite surface-level variations.\n</output_format>",
    "question": "How can the repeated use of structured templates in synthetic benchmark generation inadvertently permit contamination and reduce evaluation novelty?",
    "answer": "By repeatedly employing identical template structures, enabling models to memorize and exploit familiar problem formats despite surface-level variations.",
    "explanation": "Reusing the same template skeletons, even with varied placeholders, allows models to learn and recall familiar problem formats rather than tackling truly unseen instances.",
    "question_token_count": 21,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 9,
    "question_difficulty_score": 7,
    "question_groundedness_score": 8,
    "avg_answer_token_count": 22
  },
  {
    "context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n",
    "topic": "Exploration of the proposed criteria for standardized dynamic evaluation and their potential to improve benchmarking consistency.",
    "response": "<document_analysis>\nThe conclusion contrasts static and dynamic benchmarking methods, highlighting the increasing contamination vulnerability of static approaches as LLM training data grows, and the promise but reproducibility and reliability challenges of dynamic approaches. The text calls for \u201cstandardized dynamic evaluation\u201d as a means to improve benchmarking consistency. A probing question should target the core reproducibility hurdle in dynamic evaluation and how standardization directly addresses it. This invites critical reflection on the link between fragmented protocols and inconsistent results, and the role of unified criteria in mitigating these issues. Difficulty: 8/10.\n</document_analysis>\n\n<output_format>\nQuestion: What primary reproducibility challenge do dynamic LLM evaluation methods face, and how would standardized dynamic evaluation criteria directly address it?\nExplanation: Dynamic evaluations suffer from fragmented protocols and data versioning, causing inconsistent results; standardization enforces uniform procedures and dataset version control, ensuring reproducible benchmarks.\nCorrect Answer: Fragmented evaluation protocols; standardized criteria enforce consistent procedures and dataset versioning.\n</output_format>",
    "question": "What primary reproducibility challenge do dynamic LLM evaluation methods face, and how would standardized dynamic evaluation criteria directly address it?",
    "answer": "Fragmented evaluation protocols; standardized criteria enforce consistent procedures and dataset versioning.",
    "explanation": "Dynamic evaluations suffer from fragmented protocols and data versioning, causing inconsistent results; standardization enforces uniform procedures and dataset version control, ensuring reproducible benchmarks.",
    "question_token_count": 23,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 16
  },
  {
    "context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n",
    "topic": "The architecture and workflow of DyVal, including DAG construction, node/edge variation, and difficulty control",
    "response": "<document_analysis>\nThe context describes DyVal\u2019s workflow: generating directed acyclic graphs (DAGs) with varying numbers of nodes and edges to calibrate reasoning difficulty, converting these DAGs into natural language via rule-based conversion, then querying an LLM for the root node\u2019s value. The key technical element for difficulty control is the dual variation of node and edge counts. Expert-level probing can focus on why edge variation adds a distinct dimension of complexity beyond simply scaling node count, testing understanding of graph structural complexity and its impact on reasoning tasks.\nDifficulty: 8/10\n</document_analysis>\n\n<output_format>\nQuestion: In DyVal\u2019s DAG generation, why does adjusting edge count in addition to node count enable finer control over reasoning difficulty?\nExplanation: Varying edge count changes the graph\u2019s branching complexity and interdependencies independently of its size, allowing more nuanced scaling of difficulty.\nCorrect Answer: It modulates branching complexity.\n</output_format>",
    "question": "In DyVal\u2019s DAG generation, why does adjusting edge count in addition to node count enable finer control over reasoning difficulty?",
    "answer": "It modulates branching complexity.",
    "explanation": "Varying edge count changes the graph\u2019s branching complexity and interdependencies independently of its size, allowing more nuanced scaling of difficulty.",
    "question_token_count": 25,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 7
  },
  {
    "context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n",
    "topic": "Risks associated with static benchmarks and how outdated or biased data sources perpetuate unfair evaluation.",
    "response": "<document_analysis>\nThe context discusses ethical concerns of static versus dynamic benchmarks, focusing especially on the risk that static benchmarks relying on outdated or biased data can perpetuate biases and unfairly evaluate AI models. It highlights how such benchmarks embed historical prejudices into evaluation metrics, disadvantaging certain user groups. The content is coherent, relevant, and educationally valuable for generating a high-level question on the specified topic.\n</document_analysis>\n\n<output_format>\nQuestion: How does reliance on outdated or biased data sources in static benchmarks embed and perpetuate systemic biases within AI model evaluations?\n\nExplanation: Static benchmarks built on skewed historical data encode existing prejudices into the criteria, leading performance metrics to favor biased patterns and disadvantage fairer models.\n\nCorrect Answer: By encoding historical prejudices into evaluation metrics, thereby skewing performance to favor biased patterns over fair assessments.\n</output_format>",
    "question": "How does reliance on outdated or biased data sources in static benchmarks embed and perpetuate systemic biases within AI model evaluations?",
    "answer": "By encoding historical prejudices into evaluation metrics, thereby skewing performance to favor biased patterns over fair assessments.",
    "explanation": "Static benchmarks built on skewed historical data encode existing prejudices into the criteria, leading performance metrics to favor biased patterns and disadvantage fairer models.",
    "question_token_count": 24,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 22
  },
  {
    "context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n",
    "topic": "Interpret the significance of dynamic benchmarks summarized in recent surveys (e.g., Table\u00a02) for LLM evaluation and their role in minimizing data contamination.",
    "response": "<document_analysis>\nThe context outlines the need for interpretability in dynamic benchmarking transformations and categorizes dynamic benchmarks into four types: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches. Key themes include contamination minimization by using fresh or generated data, transparency via rule-based methods, and the interpretability challenges posed by LLM-assisted processes. An expert-level question can probe how each category addresses data contamination and the interpretability trade-offs, requiring synthesis of these concepts.\n</document_analysis>\n\n<output_format>\nQuestion: How do temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches in dynamic benchmarks each mitigate training data contamination, and what unique interpretability challenges arise in the LLM-based and hybrid methods?\nExplanation: This answer specifies how each category prevents contamination (fresh data, transparent rules, model generation, combination) and highlights the need for explainability in opaque LLM-driven processes.\nCorrect Answer: Temporal cutoff uses only post\u2010cutoff data, rule\u2010based generation applies transparent handcrafted rules, LLM\u2010based generation synthesizes novel points but risks opaque transformations requiring explainability tools, and hybrid methods blend fresh data and rules yet inherit LLM traceability challenges.\n</output_format>",
    "question": "How do temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches in dynamic benchmarks each mitigate training data contamination, and what unique interpretability challenges arise in the LLM-based and hybrid methods?",
    "answer": "Temporal cutoff uses only post\u2010cutoff data, rule\u2010based generation applies transparent handcrafted rules, LLM\u2010based generation synthesizes novel points but risks opaque transformations requiring explainability tools, and hybrid methods blend fresh data and rules yet inherit LLM traceability challenges.",
    "explanation": "This answer specifies how each category prevents contamination (fresh data, transparent rules, model generation, combination) and highlights the need for explainability in opaque LLM-driven processes.",
    "question_token_count": 41,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 54
  },
  {
    "context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n",
    "topic": "The role of transparency in LLM evaluation frameworks and its effect on bias mitigation.",
    "response": "<document_analysis>\nThe context discusses ethical considerations in LLM evaluation regarding transparency, fairness, static vs. dynamic benchmarks, and the risks of bias and misuse. Key themes include how transparency helps identify and correct biases and the need to combine it with governance to prevent manipulation. These relationships inform a question on the role of transparency in mitigating bias while balancing risks from both static and dynamic benchmarks.\n</document_analysis>\n\n<output_format>\nQuestion: What role does transparency play in mitigating bias within LLM evaluation frameworks, and how can it address both the perpetuation of static benchmark biases and the potential misuse of dynamic benchmarks?\nExplanation: Openly documenting data sources, criteria, and update mechanisms enables bias detection and correction, while governance measures like access controls and audit trails prevent manipulation.\nCorrect Answer: Transparency ensures that data sources, benchmark criteria, and update processes are openly documented and audited, facilitating bias detection and correction while using access controls and audit logs to guard against manipulation.\n</output_format>",
    "question": "What role does transparency play in mitigating bias within LLM evaluation frameworks, and how can it address both the perpetuation of static benchmark biases and the potential misuse of dynamic benchmarks?",
    "answer": "Transparency ensures that data sources, benchmark criteria, and update processes are openly documented and audited, facilitating bias detection and correction while using access controls and audit logs to guard against manipulation.",
    "explanation": "Openly documenting data sources, criteria, and update mechanisms enables bias detection and correction, while governance measures like access controls and audit trails prevent manipulation.",
    "question_token_count": 35,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 7,
    "question_difficulty_score": 9,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 37
  },
  {
    "context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n",
    "topic": "Best practices for curating novel and unseen evaluation data to accurately measure LLM performance.",
    "response": "<document_analysis>\nThe context discusses the types and significance of data contamination in LLM benchmarks, emphasizing the need for rigorous contamination detection and prevention to preserve evaluation validity. Key themes include syntactic transformations as contamination, semantic overlap, provenance, and the risk of overestimating model capabilities. The question topic asks for best practices to curate novel, uncontaminated benchmarks. To generate a challenging expert-level question, I\u2019ll require identification and justification of specific, multi-stage methods that address both syntactic and semantic contamination while ensuring representative, diverse test coverage.\nDifficulty rating: 8/10 (requires synthesis of domain best practices)\n</document_analysis>\n\n<output_format>\nQuestion: Which four critical steps should a benchmark curator implement to mitigate data contamination and preserve evaluation validity when constructing novel LLM test sets?\n\nExplanation: These steps address both syntactic and semantic overlap with training data, ensure data provenance, introduce challenging new examples, and maintain diverse, representative coverage.\n\nCorrect Answer: Data provenance tracking, automated syntactic and semantic duplicate detection, adversarial example generation, and diversity-driven sampling across domains.\n</output_format>",
    "question": "Which four critical steps should a benchmark curator implement to mitigate data contamination and preserve evaluation validity when constructing novel LLM test sets?",
    "answer": "Data provenance tracking, automated syntactic and semantic duplicate detection, adversarial example generation, and diversity-driven sampling across domains.",
    "explanation": "These steps address both syntactic and semantic overlap with training data, ensure data provenance, introduce challenging new examples, and maintain diverse, representative coverage.",
    "question_token_count": 25,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 8,
    "question_groundedness_score": 4,
    "avg_answer_token_count": 23
  },
  {
    "context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n",
    "topic": "Methods for continuously auditing fairness in static and dynamic benchmarking systems.",
    "response": "<document_analysis>\nThe context discusses ethical considerations in static and dynamic LLM benchmarking, highlighting risks of bias perpetuation, privacy concerns, and the need for transparency, fairness, and accountability. Key themes: static benchmarks\u2019 potential to embed outdated biases, dynamic benchmarks\u2019 privacy/security tradeoffs, and the necessity for ongoing ethical frameworks. The topic focuses on methods for continuously auditing fairness across both benchmark types, requiring solutions that address bias drift, demographic fairness, and privacy. \n</document_analysis>\n\n<output_format>\nQuestion: In a unified framework for continuously auditing fairness in both static and dynamic LLM benchmarks, which technical mechanism enables real-time detection of performance disparity across demographic groups while guaranteeing user privacy?\n\nExplanation: Differentially private streaming metrics allow ongoing measurement of performance gaps without revealing individual data, addressing both fairness and privacy.\n\nCorrect Answer: Differentially private disparity monitoring via streaming metrics\n</output_format>",
    "question": "In a unified framework for continuously auditing fairness in both static and dynamic LLM benchmarks, which technical mechanism enables real-time detection of performance disparity across demographic groups while guaranteeing user privacy?",
    "answer": "Differentially private disparity monitoring via streaming metrics",
    "explanation": "Differentially private streaming metrics allow ongoing measurement of performance gaps without revealing individual data, addressing both fairness and privacy.",
    "question_token_count": 35,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 2,
    "question_difficulty_score": 9,
    "question_groundedness_score": 1,
    "avg_answer_token_count": 9
  },
  {
    "context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n",
    "topic": "Establishing privacy-preserving techniques within dynamic benchmark data pipelines.",
    "response": "<document_analysis>\nThe context discusses ethical considerations in dynamic benchmarks, emphasizing privacy risks of continual data updates. Key issues include privacy-preserving techniques for dynamic benchmarking pipelines, such as differential privacy, and their trade-offs between privacy guarantees and data utility. A high-difficulty question can probe understanding of the privacy budget concept within differential privacy and its implications in iterative data release scenarios. This challenges experts on nuanced trade-offs in privacy-preserving dynamic data pipelines.\nDifficulty: 8/10\n</document_analysis>\n\n<output_format>\nQuestion: In dynamic benchmark pipelines employing differential privacy, what is the privacy budget, and how does its consumption through successive data releases affect both the strength of privacy guarantees and the utility of the data?\n\nExplanation: The privacy budget \u03b5 quantifies the total allowable privacy loss, and as it is consumed across multiple releases, remaining budget limits noise injection, forcing a trade-off that either weakens privacy or degrades data utility.\n\nCorrect Answer: The privacy budget \u03b5 is the cumulative privacy-loss parameter, and as successive releases expend \u03b5, reduced noise leads to weaker privacy guarantees or lower data utility.\n</output_format>",
    "question": "In dynamic benchmark pipelines employing differential privacy, what is the privacy budget, and how does its consumption through successive data releases affect both the strength of privacy guarantees and the utility of the data?",
    "answer": "The privacy budget \u03b5 is the cumulative privacy-loss parameter, and as successive releases expend \u03b5, reduced noise leads to weaker privacy guarantees or lower data utility.",
    "explanation": "The privacy budget \u03b5 quantifies the total allowable privacy loss, and as it is consumed across multiple releases, remaining budget limits noise injection, forcing a trade-off that either weakens privacy or degrades data utility.",
    "question_token_count": 38,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 1,
    "avg_answer_token_count": 31
  },
  {
    "context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Post-hoc contamination detection methods and their effectiveness in LLM evaluation.",
    "response": "<document_analysis>\nThe context outlines the transition from static to dynamic benchmarking of LLMs and notes that static approaches, including post-hoc contamination detection, have inherent limitations due to opaque and large-scale scraped training data, making retroactive detection unreliable. It contrasts these with dynamic benchmarks that proactively regenerate or update datasets. For an expert-level question on post-hoc detection effectiveness, the key nuance is that post-hoc methods only identify contamination after model training and rely on incomplete data provenance, thus unable to guarantee uncontaminated evaluations.\n</document_analysis>\n\n<output_format>\nQuestion: What is the fundamental limitation of relying solely on post-hoc contamination detection methods for evaluating large language models on static benchmarks?\n\nExplanation: Post-hoc methods can only flag contamination after the fact and require full knowledge of the model\u2019s training corpus, which is typically unavailable, so they cannot guarantee the integrity of the evaluation data.\n\nCorrect Answer: They cannot reliably detect contamination without complete provenance of the model\u2019s training data.\n</output_format>",
    "question": "What is the fundamental limitation of relying solely on post-hoc contamination detection methods for evaluating large language models on static benchmarks?",
    "answer": "They cannot reliably detect contamination without complete provenance of the model\u2019s training data.",
    "explanation": "Post-hoc methods can only flag contamination after the fact and require full knowledge of the model\u2019s training corpus, which is typically unavailable, so they cannot guarantee the integrity of the evaluation data.",
    "question_token_count": 24,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 16
  },
  {
    "context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Analyze the legal and privacy constraints that limit access to LLM training datasets and their impact on post-hoc contamination detection.",
    "response": "<document_analysis>\nThe context describes limitations in post-hoc contamination detection for LLMs due to legal and privacy restrictions that prevent full access to training datasets. It highlights that without this access, overlap detection is challenging and reliance shifts to behavioral heuristics (e.g., memorization rates, perplexity) that are not uniformly valid across models or tasks. This interplay between data governance and evaluation reliability is key for advanced analysis. A high-level analytical question can probe how these constraints concretely degrade the effectiveness of contamination detection.\nDifficulty rating: 8/10\n</document_analysis>\n\n<output_format>\nQuestion: How do legal and privacy restrictions on accessing training data undermine the effectiveness of overlap-based and behavioral-heuristic contamination detection methods in LLM evaluation?\nExplanation: Without full dataset access due to these restrictions, direct overlap checks become impossible and evaluators must rely on model-specific heuristics like memorization or perplexity that vary across architectures and tasks, reducing detection reliability.\nCorrect Answer: They block full access to the original data, forcing reliance on inconsistent heuristics and preventing robust overlap checks.\n</output_format>",
    "question": "How do legal and privacy restrictions on accessing training data undermine the effectiveness of overlap-based and behavioral-heuristic contamination detection methods in LLM evaluation?",
    "answer": "They block full access to the original data, forcing reliance on inconsistent heuristics and preventing robust overlap checks.",
    "explanation": "Without full dataset access due to these restrictions, direct overlap checks become impossible and evaluators must rely on model-specific heuristics like memorization or perplexity that vary across architectures and tasks, reducing detection reliability.",
    "question_token_count": 28,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 21
  },
  {
    "context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n",
    "topic": "Recommendations for future research to refine, validate, and operationalize dynamic benchmarking criteria in live applications.",
    "response": "<document_analysis>\nThe context summarizes conclusions and limitations of a survey on LLM benchmarking methods, highlighting that static approaches, while consistent, suffer increasing contamination vulnerability, and dynamic approaches, though promising, lack reliability and reproducibility. It explicitly calls for future research to establish standardized dynamic evaluation and develop practical mitigation tools, pointing to a clear two\u2010pronged recommendation. This offers a focused basis for a question that probes the expert\u2019s understanding of these interrelated research priorities for operationalizing dynamic benchmarking in real\u2010world settings.\nDifficulty: 8/10 \u2013 requires synthesizing the survey\u2019s future directions into precise, actionable research avenues.\n</document_analysis>\n\n<output_format>\nQuestion: Which two critical research directions should be pursued to refine and operationalize dynamic benchmarking criteria for live large language model deployments?\n\nExplanation: The survey concludes that dynamic methods need both standardized evaluation frameworks to ensure reliability and reproducibility, and practical tools to mitigate contamination risks in real-world applications.\n\nCorrect Answer: Standardizing dynamic evaluation protocols and developing practical mitigation tools\n</output_format>",
    "question": "Which two critical research directions should be pursued to refine and operationalize dynamic benchmarking criteria for live large language model deployments?",
    "answer": "Standardizing dynamic evaluation protocols and developing practical mitigation tools",
    "explanation": "The survey concludes that dynamic methods need both standardized evaluation frameworks to ensure reliability and reproducibility, and practical tools to mitigate contamination risks in real-world applications.",
    "question_token_count": 24,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 11
  }
]