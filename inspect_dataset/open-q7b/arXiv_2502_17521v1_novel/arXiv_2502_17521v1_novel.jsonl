[
  {
    "context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n",
    "topic": "Challenging a model\u2019s understanding of everyday knowledge through diverse reasoning tasks such as those found in HellaSwag and WinoGrande.",
    "response": "<output_format>\nQuestion: How would a model's performance on HellaSwag and WinoGrande benchmarks reflect its capability to integrate background knowledge with logical reasoning, and what aspects of everyday reasoning might still be challenging for current models?\nExplanation: These benchmarks require models to understand and apply commonsense knowledge in conjunction with logical inference, which is a complex task. Answering this question requires insight into the nature of these benchmarks and an understanding of the types of reasoning they test, including common sense and logical deduction.\nCorrect Answer: A model's performance on HellaSwag and WinoGrande would reflect its ability to integrate background knowledge and logical reasoning, but challenges might include understanding subtle implications, handling unexpected scenarios, and maintaining coherence across different narrative contexts.\n</output_format>",
    "question": "How would a model's performance on HellaSwag and WinoGrande benchmarks reflect its capability to integrate background knowledge with logical reasoning, and what aspects of everyday reasoning might still be challenging for current models?",
    "answer": "A model's performance on HellaSwag and WinoGrande would reflect its ability to integrate background knowledge and logical reasoning, but challenges might include understanding subtle implications, handling unexpected scenarios, and maintaining coherence across different narrative contexts.",
    "explanation": "These benchmarks require models to understand and apply commonsense knowledge in conjunction with logical inference, which is a complex task. Answering this question requires insight into the nature of these benchmarks and an understanding of the types of reasoning they test, including common sense and logical deduction.",
    "question_token_count": 42,
    "answer_correctness_score": 8,
    "explanation_validity_score": 8,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 46
  },
  {
    "context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n",
    "topic": "The interpretation of variance in complexity measurements across trials as an indicator of benchmarking stability.",
    "response": "<document_analysis>\nThe context provides a detailed discussion on the importance of complexity measurement in dynamic benchmarking for language models. It explains that variance in complexity across trials can indicate the stability of the benchmarking method. The key points include:\n1. The need to distinguish between performance drops due to data contamination and increased task complexity.\n2. The challenge in accurately measuring complexity, with existing metrics being domain-specific and not generalizing well.\n3. The use of graph complexity as one proposed metric by DyVal.\nThe core idea is to interpret variance in complexity measurements as an indicator of the stability of the benchmarking process. This requires understanding the relationship between complexity variance and benchmark stability, which is the crux of the question to be generated.\n</document_analysis>\n\n<output_format>\nQuestion: How does variance in the measured complexity across multiple trials of a dynamic benchmark impact our assessment of the stability of the benchmarking method, and what implications does this have for interpreting performance drops in language models?\nExplanation: This question probes the candidate's ability to understand the nuanced relationship between complexity variance and benchmark stability. It requires the candidate to consider both the technical aspects of complexity measurement and the practical implications for evaluating model performance.\nCorrect Answer: Variance in measured complexity across trials indicates instability in the benchmarking method, suggesting that observed performance drops may be due to changes in task complexity rather than data contamination. A stable benchmark should yield consistent complexity measurements, highlighting the importance of reliable complexity metrics.\n</output_format>",
    "question": "How does variance in the measured complexity across multiple trials of a dynamic benchmark impact our assessment of the stability of the benchmarking method, and what implications does this have for interpreting performance drops in language models?",
    "answer": "Variance in measured complexity across trials indicates instability in the benchmarking method, suggesting that observed performance drops may be due to changes in task complexity rather than data contamination. A stable benchmark should yield consistent complexity measurements, highlighting the importance of reliable complexity metrics.",
    "explanation": "This question probes the candidate's ability to understand the nuanced relationship between complexity variance and benchmark stability. It requires the candidate to consider both the technical aspects of complexity measurement and the practical implications for evaluating model performance.",
    "question_token_count": 40,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 49
  },
  {
    "context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n",
    "topic": "Need for Mitigating Data Contamination in LLM Evaluations",
    "response": "<output_format>\nQuestion: Why is it important to mitigate data contamination in benchmarking large language models (LLMs)?\nExplanation: The question requires a deep understanding of the significance of data contamination in evaluating LLMs, touching on the impact on benchmarks' validity, the assessment of generalization, robustness, and real-world applicability, as well as the broader implications for model comparisons, deployment decisions, and policy-making.\nCorrect Answer: Mitigating data contamination in benchmarking LLMs is essential because it ensures that evaluations accurately reflect the model's true capabilities on novel and unseen data, avoiding overestimation due to exposure to training data. This is crucial for validating generalization, robustness, and real-world applicability, and for making informed decisions about model deployment and policy.\n</output_format>",
    "question": "Why is it important to mitigate data contamination in benchmarking large language models (LLMs)?",
    "answer": "Mitigating data contamination in benchmarking LLMs is essential because it ensures that evaluations accurately reflect the model's true capabilities on novel and unseen data, avoiding overestimation due to exposure to training data. This is crucial for validating generalization, robustness, and real-world applicability, and for making informed decisions about model deployment and policy.",
    "explanation": "The question requires a deep understanding of the significance of data contamination in evaluating LLMs, touching on the impact on benchmarks' validity, the assessment of generalization, robustness, and real-world applicability, as well as the broader implications for model comparisons, deployment decisions, and policy-making.",
    "question_token_count": 18,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 9,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 64
  },
  {
    "context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n",
    "topic": "Methods for Rewriting Samples from Existing Benchmarks to Enhance Sample Diversity",
    "response": "<output_format>\nQuestion: How might an advanced language model, such as those used in benchmark rewriting, address the challenge of preserving both the stylistic elements and the core knowledge of original samples while generating new, diverse samples?\nExplanation: This question requires a deep understanding of the complexities involved in rewriting samples from existing benchmarks while maintaining both stylistic consistency and essential knowledge. It probes the ability to conceptualize sophisticated methods that balance these dual requirements, which is crucial for enhancing sample diversity without losing the integrity of the original data.\nCorrect Answer: Advanced models could employ a combination of techniques such as style transfer algorithms, knowledge retention mechanisms, and context-aware generation strategies to ensure both stylistic and substantive fidelity.\n</output_format>",
    "question": "How might an advanced language model, such as those used in benchmark rewriting, address the challenge of preserving both the stylistic elements and the core knowledge of original samples while generating new, diverse samples?",
    "answer": "Advanced models could employ a combination of techniques such as style transfer algorithms, knowledge retention mechanisms, and context-aware generation strategies to ensure both stylistic and substantive fidelity.",
    "explanation": "This question requires a deep understanding of the complexities involved in rewriting samples from existing benchmarks while maintaining both stylistic consistency and essential knowledge. It probes the ability to conceptualize sophisticated methods that balance these dual requirements, which is crucial for enhancing sample diversity without losing the integrity of the original data.",
    "question_token_count": 40,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 33
  },
  {
    "context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Exploring the challenges and limitations of current dynamic benchmarking approaches.",
    "response": "<output_format>\nQuestion: How does the reliance on dynamic benchmarking methods to mitigate data contamination in LLMs inherently limit their effectiveness, and what specific criteria can be proposed to evaluate the robustness and reliability of these dynamic benchmarks?\nExplanation: This question probes the fundamental limitations of dynamic benchmarking approaches and requires a deep understanding of both the challenges posed by data contamination and the criteria needed to assess the efficacy of dynamic benchmarks. It encourages reflection on the trade-offs between dynamic and static benchmarking methods and the necessity of developing rigorous evaluation standards for dynamic benchmarks.\nCorrect Answer: Dynamic benchmarking methods face inherent limitations due to the continuous and unpredictable nature of LLM training data, making it difficult to ensure thorough coverage and consistency over time. Proposed criteria for evaluating dynamic benchmarks might include: real-time adaptability, transparency in data sources and update mechanisms, robustness against adversarial attacks, and comprehensive validation through multiple independent evaluations.\n</output_format>",
    "question": "How does the reliance on dynamic benchmarking methods to mitigate data contamination in LLMs inherently limit their effectiveness, and what specific criteria can be proposed to evaluate the robustness and reliability of these dynamic benchmarks?",
    "answer": "Dynamic benchmarking methods face inherent limitations due to the continuous and unpredictable nature of LLM training data, making it difficult to ensure thorough coverage and consistency over time. Proposed criteria for evaluating dynamic benchmarks might include: real-time adaptability, transparency in data sources and update mechanisms, robustness against adversarial attacks, and comprehensive validation through multiple independent evaluations.",
    "explanation": "This question probes the fundamental limitations of dynamic benchmarking approaches and requires a deep understanding of both the challenges posed by data contamination and the criteria needed to assess the efficacy of dynamic benchmarks. It encourages reflection on the trade-offs between dynamic and static benchmarking methods and the necessity of developing rigorous evaluation standards for dynamic benchmarks.",
    "question_token_count": 39,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 65
  },
  {
    "context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n",
    "topic": "The relationship between complexity measurement functions and the stability of dynamic benchmarking methods.",
    "response": "<document_analysis>\nThe context discusses the importance of complexity measurement in dynamic benchmarking methods, highlighting the challenge of accurately measuring complexity across different applications. It introduces the concept of using complexity metrics like graph complexity and formulates stability in terms of variance across different trials. The key points are:\n1. Complexity measurement is crucial for distinguishing between performance drops due to data contamination and increased task complexity.\n2. Existing complexity metrics are often domain-specific and don't generalize well.\n3. Stability is defined as the variance in complexity across trials, with high variance indicating instability.\n\nGiven these points, the question should probe a deep understanding of the relationship between complexity measurement functions and the stability of dynamic benchmarking methods.\n</document_analysis>\n\n<output_format>\nQuestion: How would the stability of a dynamic benchmarking method change if the complexity measurement function \u03a8(\u22c5) were replaced with a new function that more accurately captures the inherent complexity across diverse datasets, and what does this imply about the choice of complexity metric?\nExplanation: This question requires the respondent to understand the role of the complexity measurement function in determining stability and to consider the implications of using a more accurate metric. It challenges the respondent to think about the practical impact of choosing the right complexity metric and its effect on the reliability of the benchmarking method.\nCorrect Answer: The stability would likely improve because a more accurate complexity measurement function would better capture the true complexity variations, leading to more consistent performance evaluations and reducing the variance observed in different trials.\n</output_format>",
    "question": "How would the stability of a dynamic benchmarking method change if the complexity measurement function \u03a8(\u22c5) were replaced with a new function that more accurately captures the inherent complexity across diverse datasets, and what does this imply about the choice of complexity metric?",
    "answer": "The stability would likely improve because a more accurate complexity measurement function would better capture the true complexity variations, leading to more consistent performance evaluations and reducing the variance observed in different trials.",
    "explanation": "This question requires the respondent to understand the role of the complexity measurement function in determining stability and to consider the implications of using a more accurate metric. It challenges the respondent to think about the practical impact of choosing the right complexity metric and its effect on the reliability of the benchmarking method.",
    "question_token_count": 49,
    "answer_correctness_score": 9,
    "explanation_validity_score": 8,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 36
  },
  {
    "context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n",
    "topic": "Define and explain the concept of exact data contamination in the context of LLM training and evaluation.",
    "response": "<output_format>\nQuestion: Define and explain the concept of exact data contamination in the context of LLM training and evaluation.\nExplanation: Exact contamination occurs when there is any exact duplicate in the benchmark dataset. This means that there exists a data point \\(d\\) that appears in both the training data \\(\\mathcal{D}_{\\text{train}}\\) and the test data \\(\\mathcal{D}_{\\text{test}}\\). Such contamination can compromise the validity of performance measurements because the model might perform well simply by recognizing these duplicates rather than truly understanding the task.\nCorrect Answer: There exists a data point \\(d\\) that both in \\(\\mathcal{D}_{\\text{train}}\\) and \\(\\mathcal{D}_{\\text{test}}\\).\n</output_format>",
    "question": "Define and explain the concept of exact data contamination in the context of LLM training and evaluation.",
    "answer": "There exists a data point \\(d\\) that both in \\(\\mathcal{D}_{\\text{train}}\\) and \\(\\mathcal{D}_{\\text{test}}\\).",
    "explanation": "Exact contamination occurs when there is any exact duplicate in the benchmark dataset. This means that there exists a data point \\(d\\) that appears in both the training data \\(\\mathcal{D}_{\\text{train}}\\) and the test data \\(\\mathcal{D}_{\\text{test}}\\). Such contamination can compromise the validity of performance measurements because the model might perform well simply by recognizing these duplicates rather than truly understanding the task.",
    "question_token_count": 19,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 30
  },
  {
    "context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n",
    "topic": "Evaluation Reliability of LLM-generated Samples",
    "response": "<output_format>\nQuestion: How does the increasing size of training corpora impact the effectiveness of static benchmarks in LLM evaluation, and what mathematical relationship describes this impact?\nExplanation: The question requires a deep understanding of the relationship between the size of training corpora and the effectiveness of static benchmarks, as described in the context. It probes the domain expert's ability to interpret and articulate the mathematical relationship provided.\nCorrect Answer: As the size of training corpora grows, the effectiveness of static benchmarks decreases, with the probability of contamination increasing proportional to the product of the sizes of the training and test sets, inversely.\n</output_format>",
    "question": "How does the increasing size of training corpora impact the effectiveness of static benchmarks in LLM evaluation, and what mathematical relationship describes this impact?",
    "answer": "As the size of training corpora grows, the effectiveness of static benchmarks decreases, with the probability of contamination increasing proportional to the product of the sizes of the training and test sets, inversely.",
    "explanation": "The question requires a deep understanding of the relationship between the size of training corpora and the effectiveness of static benchmarks, as described in the context. It probes the domain expert's ability to interpret and articulate the mathematical relationship provided.",
    "question_token_count": 28,
    "answer_correctness_score": 8,
    "explanation_validity_score": 7,
    "question_clarity_score": 9,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 39
  },
  {
    "context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Understanding the process of regenerating benchmark data to reduce contamination.",
    "response": "<output_format>\nQuestion: How can the process of regenerating benchmark data be leveraged to ensure the integrity of LLM evaluations, and what are the potential limitations of this approach?\nExplanation: This question requires a deep understanding of the concept of benchmark data regeneration and its role in mitigating data contamination. It probes the ability to synthesize knowledge about the process, the benefits, and the challenges associated with this method.\nCorrect Answer: Regenerating benchmark data can ensure the integrity of LLM evaluations by creating fresh datasets that are not contaminated by the same issues that affected previous benchmarks. However, this approach may have limitations such as increased complexity in maintaining consistent standards, potential biases in the new data generation process, and the difficulty in completely avoiding all forms of contamination.\n</output_format>",
    "question": "How can the process of regenerating benchmark data be leveraged to ensure the integrity of LLM evaluations, and what are the potential limitations of this approach?",
    "answer": "Regenerating benchmark data can ensure the integrity of LLM evaluations by creating fresh datasets that are not contaminated by the same issues that affected previous benchmarks. However, this approach may have limitations such as increased complexity in maintaining consistent standards, potential biases in the new data generation process, and the difficulty in completely avoiding all forms of contamination.",
    "explanation": "This question requires a deep understanding of the concept of benchmark data regeneration and its role in mitigating data contamination. It probes the ability to synthesize knowledge about the process, the benefits, and the challenges associated with this method.",
    "question_token_count": 30,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 66
  },
  {
    "context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n",
    "topic": "Assessing the Effectiveness of Dynamic Benchmarks in Evaluating LLM Capabilities Amid Data Contamination",
    "response": "<output_format>\nQuestion: How does the introduction of collision metrics contribute to understanding the resilience of dynamic benchmarks against data contamination, and what does a high collision rate imply about a benchmark's capability to generate novel and diverse test cases?\nExplanation: This question requires a deep understanding of how the collision metrics, including Collision Rate and Repeat Trials, work to assess the robustness of dynamic benchmarks. It prompts reflection on the implications of a high collision rate and challenges the respondent to consider both the quantitative and qualitative aspects of benchmark effectiveness.\nCorrect Answer: A high collision rate implies a significant overlap between transformed versions of the benchmark dataset, suggesting that the benchmark may struggle to generate truly novel and diverse test cases, thereby reducing its resilience against data contamination. Conversely, low collision rates indicate a higher potential for the benchmark to produce unique and varied test scenarios, enhancing its ability to evaluate LLM capabilities accurately.\n</output_format>",
    "question": "How does the introduction of collision metrics contribute to understanding the resilience of dynamic benchmarks against data contamination, and what does a high collision rate imply about a benchmark's capability to generate novel and diverse test cases?",
    "answer": "A high collision rate implies a significant overlap between transformed versions of the benchmark dataset, suggesting that the benchmark may struggle to generate truly novel and diverse test cases, thereby reducing its resilience against data contamination. Conversely, low collision rates indicate a higher potential for the benchmark to produce unique and varied test scenarios, enhancing its ability to evaluate LLM capabilities accurately.",
    "explanation": "This question requires a deep understanding of how the collision metrics, including Collision Rate and Repeat Trials, work to assess the robustness of dynamic benchmarks. It prompts reflection on the implications of a high collision rate and challenges the respondent to consider both the quantitative and qualitative aspects of benchmark effectiveness.",
    "question_token_count": 41,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 69
  },
  {
    "context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n",
    "topic": "Human Effort and Data Contamination in Benchmark Updates",
    "response": "<output_format>\nQuestion: Given the limitations mentioned, how would you design a novel benchmark update strategy that minimizes human effort while ensuring data purity and robustness against reuse, and what theoretical guarantees can be provided for its effectiveness?\nExplanation: This question probes the ability to synthesize a comprehensive strategy that addresses multiple aspects of benchmark updating\u2014human effort reduction, data purity, and resistance to reuse\u2014while requiring a deep understanding of the current methods and their limitations. It also necessitates the application of theoretical knowledge to propose guarantees.\nCorrect Answer: A strategy involving automated rule-based generation with dynamic templates and verification mechanisms could minimize human effort. Ensuring data purity can be achieved through cryptographic techniques to detect reuse, and theoretical guarantees might include probabilistic bounds on collision and contamination rates.\n</output_format>",
    "question": "Given the limitations mentioned, how would you design a novel benchmark update strategy that minimizes human effort while ensuring data purity and robustness against reuse, and what theoretical guarantees can be provided for its effectiveness?",
    "answer": "A strategy involving automated rule-based generation with dynamic templates and verification mechanisms could minimize human effort. Ensuring data purity can be achieved through cryptographic techniques to detect reuse, and theoretical guarantees might include probabilistic bounds on collision and contamination rates.",
    "explanation": "This question probes the ability to synthesize a comprehensive strategy that addresses multiple aspects of benchmark updating\u2014human effort reduction, data purity, and resistance to reuse\u2014while requiring a deep understanding of the current methods and their limitations. It also necessitates the application of theoretical knowledge to propose guarantees.",
    "question_token_count": 39,
    "answer_correctness_score": 8,
    "explanation_validity_score": 8,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 7,
    "avg_answer_token_count": 46
  },
  {
    "context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n",
    "topic": "Evaluating the Robustness of Dynamic Benchmarks Against Data Contamination Through Collision Metrics",
    "response": "<output_format>\nQuestion: How does the proposed collision rate and repeat trials metric jointly contribute to assessing the robustness of a dynamic benchmark against data contamination, and what implications might their values have for the reliability of such benchmarks in evaluating LLM capabilities?\nExplanation: This question requires a deep understanding of how the collision metrics work and their implications. It encourages the respondent to consider both quantitative measures and their qualitative effects on the benchmark's reliability. A thorough answer would need to demonstrate knowledge of the metrics' definitions and how they interrelate to provide a comprehensive assessment of a benchmark's resistance to data contamination.\nCorrect Answer: By using the collision rate to measure overlap between transformed datasets and repeat trials to estimate the number of trials needed to regenerate a dataset, these metrics collectively indicate the extent of potential contamination and the benchmark's ability to generate new, diverse test cases. High collision rates and low repeat trial counts suggest higher risks of contamination and reduced robustness, whereas low collision rates and high repeat trial counts indicate better protection against contamination and more reliable evaluation of LLM capabilities.\n</output_format>",
    "question": "How does the proposed collision rate and repeat trials metric jointly contribute to assessing the robustness of a dynamic benchmark against data contamination, and what implications might their values have for the reliability of such benchmarks in evaluating LLM capabilities?",
    "answer": "By using the collision rate to measure overlap between transformed datasets and repeat trials to estimate the number of trials needed to regenerate a dataset, these metrics collectively indicate the extent of potential contamination and the benchmark's ability to generate new, diverse test cases. High collision rates and low repeat trial counts suggest higher risks of contamination and reduced robustness, whereas low collision rates and high repeat trial counts indicate better protection against contamination and more reliable evaluation of LLM capabilities.",
    "explanation": "This question requires a deep understanding of how the collision metrics work and their implications. It encourages the respondent to consider both quantitative measures and their qualitative effects on the benchmark's reliability. A thorough answer would need to demonstrate knowledge of the metrics' definitions and how they interrelate to provide a comprehensive assessment of a benchmark's resistance to data contamination.",
    "question_token_count": 43,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 9,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 88
  },
  {
    "context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n",
    "topic": "Importance of Continuous Collection of Benchmarking Methods Through GitHub Repository",
    "response": "<output_format>\nQuestion: What is the significance of maintaining a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs, and how does this contribute to the ongoing effort to mitigate data contamination risks?\nExplanation: This question probes the deeper implications of maintaining a GitHub repository for collecting benchmarking methods. It requires the respondent to consider the practical benefits of such a repository in terms of accessibility, collaboration, and continuous improvement of benchmarking techniques, all of which are crucial for addressing data contamination risks in LLMs.\nCorrect Answer: The repository facilitates easy access and sharing of benchmarking methods, fosters collaborative improvements, and ensures a transparent and systematic approach to evaluating and enhancing the robustness of LLMs against data contamination.\n</output_format>",
    "question": "What is the significance of maintaining a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs, and how does this contribute to the ongoing effort to mitigate data contamination risks?",
    "answer": "The repository facilitates easy access and sharing of benchmarking methods, fosters collaborative improvements, and ensures a transparent and systematic approach to evaluating and enhancing the robustness of LLMs against data contamination.",
    "explanation": "This question probes the deeper implications of maintaining a GitHub repository for collecting benchmarking methods. It requires the respondent to consider the practical benefits of such a repository in terms of accessibility, collaboration, and continuous improvement of benchmarking techniques, all of which are crucial for addressing data contamination risks in LLMs.",
    "question_token_count": 39,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 36
  },
  {
    "context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n",
    "topic": "Mathematical formulation and interpretation of the correctness metric in the context of dynamic benchmarking.",
    "response": "<document_analysis>\nThe provided context discusses the mathematical formulation and interpretation of the correctness metric for dynamic benchmarking algorithms used to evaluate LLMs. It introduces a specific formula to quantify the correctness of such benchmarks. The equation involves an input-output transformation, a ground truth oracle, and a scoring function. The relevance of the provided content to the topic is strong, and there are no identified irrelevant or promotional elements.\n</document_analysis>\n\n<output_format>\nQuestion: Given the correctness metric defined by the expected alignment between the transformed dataset's outputs and their corresponding ground truth values, how would you design a practical method to estimate the correctness of a dynamic benchmarking algorithm without access to a perfect ground truth oracle, and what are the implications of approximation errors in such an estimation?\n\nExplanation: This question encourages a deep understanding of the concept of correctness in dynamic benchmarking, requiring the respondent to think about practical implementation and the limitations involved in real-world scenarios.\nCorrect Answer: One practical method could involve using multiple annotators to create ground truth data and applying statistical techniques to aggregate and refine the annotations. Approximation errors could lead to biased or inaccurate evaluations, highlighting the importance of robust error handling and validation in benchmarking processes.\n</output_format>",
    "question": "Given the correctness metric defined by the expected alignment between the transformed dataset's outputs and their corresponding ground truth values, how would you design a practical method to estimate the correctness of a dynamic benchmarking algorithm without access to a perfect ground truth oracle, and what are the implications of approximation errors in such an estimation?",
    "answer": "One practical method could involve using multiple annotators to create ground truth data and applying statistical techniques to aggregate and refine the annotations. Approximation errors could lead to biased or inaccurate evaluations, highlighting the importance of robust error handling and validation in benchmarking processes.",
    "explanation": "This question encourages a deep understanding of the concept of correctness in dynamic benchmarking, requiring the respondent to think about practical implementation and the limitations involved in real-world scenarios.",
    "question_token_count": 61,
    "answer_correctness_score": 8,
    "explanation_validity_score": 7,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 49
  },
  {
    "context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n",
    "topic": "Template-Based Generation Techniques for Creating Diverse Problem Instances",
    "response": "<document_analysis>\nThe provided context discusses various techniques for generating diverse problem instances, with a focus on template-based methods. The context mentions GSM-Symbolic, Mathador-LM, and MMLU-CF. These systems use query templates with placeholder variables and random fillings to create varied problem instances. The context emphasizes the importance of diverse and novel problem sets in benchmarking large language models (LLMs).\n\nGiven the detailed nature of the context and its focus on template-based generation techniques, the question should probe the understanding of how these techniques ensure diversity and novelty in problem instances while also addressing the limitations mentioned, such as potential data contamination and verification oversight.\n</document_analysis>\n\n<output_format>\nQuestion: How does the GSM-Symbolic system ensure diversity and novelty in the problem instances it generates, and what potential risks associated with template-based generation methods does it mitigate?\nExplanation: This question requires a deep understanding of the GSM-Symbolic approach, including its mechanism of using query templates and placeholder variables, as well as the ability to reflect on the broader implications of template-based generation techniques.\nCorrect Answer: GSM-Symbolic ensures diversity and novelty by using query templates with placeholder variables that are randomly filled, creating a wide range of problem instances. It mitigates the risk of data contamination and verification oversight by relying on predefined rules and randomization, which reduces the likelihood of reusing existing problems in future competitions.\n</output_format>",
    "question": "How does the GSM-Symbolic system ensure diversity and novelty in the problem instances it generates, and what potential risks associated with template-based generation methods does it mitigate?",
    "answer": "GSM-Symbolic ensures diversity and novelty by using query templates with placeholder variables that are randomly filled, creating a wide range of problem instances. It mitigates the risk of data contamination and verification oversight by relying on predefined rules and randomization, which reduces the likelihood of reusing existing problems in future competitions.",
    "explanation": "This question requires a deep understanding of the GSM-Symbolic approach, including its mechanism of using query templates and placeholder variables, as well as the ability to reflect on the broader implications of template-based generation techniques.",
    "question_token_count": 34,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 62
  },
  {
    "context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n",
    "topic": "Comprehensive Assessment of Language Models' Internal Knowledge through Diverse Benchmark Datasets (NaturalQuestions, TriviaQA).",
    "response": "<document_analysis>\nThe context primarily discusses various benchmark datasets used to evaluate language models' capabilities in different domains, including mathematical problem-solving and internal knowledge retrieval. It mentions specific datasets such as NaturalQuestions and TriviaQA for evaluating internal knowledge. These benchmarks are designed to test the model's ability to retrieve and apply real-world information effectively. Given the focus on NaturalQuestions and TriviaQA, the question should probe the nuances of how these benchmarks assess a language model's knowledge and its practical application.\n</document_analysis>\n\n<output_format>\nQuestion: How does NaturalQuestions benchmark specifically challenge a language model's internal knowledge compared to TriviaQA, and what aspects of real-world information retrieval does each dataset emphasize?\nExplanation: This question requires a deep understanding of the specific characteristics of NaturalQuestions and TriviaQA, their design goals, and how they differ in assessing real-world information retrieval. It also invites a comparison between the two datasets to highlight unique strengths and areas of focus.\nCorrect Answer: NaturalQuestions focuses on the model's ability to comprehend and answer questions grounded in common sense and everyday scenarios, emphasizing the integration of contextual understanding and logical reasoning. TriviaQA, on the other hand, emphasizes factual recall of specific pieces of information, often from less structured and more varied sources.\n</output_format>",
    "question": "How does NaturalQuestions benchmark specifically challenge a language model's internal knowledge compared to TriviaQA, and what aspects of real-world information retrieval does each dataset emphasize?",
    "answer": "NaturalQuestions focuses on the model's ability to comprehend and answer questions grounded in common sense and everyday scenarios, emphasizing the integration of contextual understanding and logical reasoning. TriviaQA, on the other hand, emphasizes factual recall of specific pieces of information, often from less structured and more varied sources.",
    "explanation": "This question requires a deep understanding of the specific characteristics of NaturalQuestions and TriviaQA, their design goals, and how they differ in assessing real-world information retrieval. It also invites a comparison between the two datasets to highlight unique strengths and areas of focus.",
    "question_token_count": 32,
    "answer_correctness_score": 8,
    "explanation_validity_score": 8,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 6,
    "avg_answer_token_count": 58
  },
  {
    "context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n",
    "topic": "The effectiveness and limitations of using canary strings in mitigating data contamination.",
    "response": "<document_analysis>\nThe context provides a detailed explanation of canary strings and their role in detecting data contamination in large language models (LLMs). The text highlights both the benefits and limitations of using canary strings. The key points include their purpose as markers for data memorization and their dependency on model trainers' awareness and responsiveness. Additionally, the text mentions a limitation related to potential data leakage by developers aiming to boost scores. This information forms the basis for generating a highly insightful and probing question.\n</document_analysis>\n\n<output_format>\nQuestion: How might an astute model developer exploit the presence of canary strings to manipulate benchmarking outcomes, and what countermeasures could be implemented to prevent such exploitation?\n\nExplanation: This question challenges the domain expert to think about potential strategic uses of canary strings by malicious actors and to consider robust methods to ensure the integrity of benchmarking processes. It requires a deep understanding of both the mechanism of canary strings and the psychological aspects of model development.\n\nCorrect Answer: An astute model developer could intentionally include canary strings in the test set to inflate performance metrics. To prevent this, developers could implement stricter validation protocols, use more sophisticated string patterns that are harder to predict, or incorporate dynamic elements in the canary strings that change periodically.\n</output_format>",
    "question": "How might an astute model developer exploit the presence of canary strings to manipulate benchmarking outcomes, and what countermeasures could be implemented to prevent such exploitation?",
    "answer": "An astute model developer could intentionally include canary strings in the test set to inflate performance metrics. To prevent this, developers could implement stricter validation protocols, use more sophisticated string patterns that are harder to predict, or incorporate dynamic elements in the canary strings that change periodically.",
    "explanation": "This question challenges the domain expert to think about potential strategic uses of canary strings by malicious actors and to consider robust methods to ensure the integrity of benchmarking processes. It requires a deep understanding of both the mechanism of canary strings and the psychological aspects of model development.",
    "question_token_count": 32,
    "answer_correctness_score": 9,
    "explanation_validity_score": 8,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 57
  },
  {
    "context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n",
    "topic": "Challenges in Dynamic Benchmarking and Potential Solutions",
    "response": "<output_format>\nQuestion: Given the increasing reliance on dynamic benchmarks to address contamination in LLM evaluation, what specific challenges arise from the lack of standardized criteria, and how might these challenges impact the reliability and validity of benchmark results?\nExplanation: This question requires a deep understanding of the challenges identified in dynamic benchmarking and the importance of standardized criteria. It probes the respondent's ability to reflect on the implications of current limitations in dynamic benchmarking methods.\nCorrect Answer: The lack of standardized criteria in dynamic benchmarks leads to issues such as inconsistency in evaluation procedures, varying degrees of complexity control, and potential biases in sample generation. These challenges can significantly impact the reliability and validity of benchmark results, as different evaluators may interpret and implement the same criteria differently, leading to inconsistent outcomes.\n</output_format>",
    "question": "Given the increasing reliance on dynamic benchmarks to address contamination in LLM evaluation, what specific challenges arise from the lack of standardized criteria, and how might these challenges impact the reliability and validity of benchmark results?",
    "answer": "The lack of standardized criteria in dynamic benchmarks leads to issues such as inconsistency in evaluation procedures, varying degrees of complexity control, and potential biases in sample generation. These challenges can significantly impact the reliability and validity of benchmark results, as different evaluators may interpret and implement the same criteria differently, leading to inconsistent outcomes.",
    "explanation": "This question requires a deep understanding of the challenges identified in dynamic benchmarking and the importance of standardized criteria. It probes the respondent's ability to reflect on the implications of current limitations in dynamic benchmarking methods.",
    "question_token_count": 40,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 63
  },
  {
    "context": "###### Safety\nSafety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content. Datasets such as RealToxicityPrompts\u00a0(Gehman et\u00a0al., 2020) and ToxiGen\u00a0(Hartvigsen et\u00a0al., 2022) assess resilience against producing harmful outputs.\n\nBy providing a controlled environment to measure these aspects, safety benchmarks play a critical role in guiding the development of models that are not only powerful outputs but also responsible and trustworthy for real-world applications.\n\n\n###### Language\nLanguage benchmarks assess the LLMs\u2019 proficiency in specific languages.\n\nGLUE\u00a0(Wang, 2018) and SuperGLUE\u00a0(Wang et\u00a0al., 2019) cover tasks from sentiment analysis to language inference, while CLUE\u00a0(Xu et\u00a0al., 2020) targets Chinese language.\n\nTypo-fixing\u00a0(Suzgun et\u00a0al., 2022) is also widely used.\n\n\n###### Reading Comprehension\nReading comprehension tasks test a model\u2019s ability to extract and infer information from text. Benchmarks like SQuAD\u00a0(Rajpurkar et\u00a0al., 2018), QuAC\u00a0(Choi et\u00a0al., 2018), and BoolQ\u00a0(Clark et\u00a0al., 2019) challenge models to understand passages and draw logical conclusions.\n",
    "topic": "The importance and methodology of safety benchmarks in evaluating LLMs, including the use of datasets like RealToxicityPrompts and ToxiGen.",
    "response": "<document_analysis>\nThe context focuses on the importance of safety benchmarks in evaluating LLMs, particularly highlighting the role of specific datasets like RealToxicityPrompts and ToxiGen. It mentions that these benchmarks provide a controlled environment to measure the LLM's ability to generate non-toxic and ethically aligned content. The context does not delve deeply into the methodologies used within these benchmarks or provide extensive details about the datasets themselves. Therefore, the question should challenge the expert's understanding of the implications and application of these benchmarks rather than just restating their definitions.\n</document_analysis>\n\n<output_format>\nQuestion: How do safety benchmarks like RealToxicityPrompts and ToxiGen ensure that LLMs are not only powerful but also responsible and trustworthy in real-world applications, and what specific techniques or metrics might these benchmarks employ to achieve this goal?\nExplanation: This question probes the expert's understanding of the broader impact of safety benchmarks on the ethical deployment of LLMs and requires them to reflect on the specific methodologies used in these benchmarks.\nCorrect Answer: Safety benchmarks ensure responsibility and trustworthiness by measuring LLMs' resilience against producing harmful outputs through controlled environments. Techniques might include adversarial testing, toxic response generation, and ethical alignment training metrics to evaluate the model's behavior across a range of scenarios.\n</output_format>",
    "question": "How do safety benchmarks like RealToxicityPrompts and ToxiGen ensure that LLMs are not only powerful but also responsible and trustworthy in real-world applications, and what specific techniques or metrics might these benchmarks employ to achieve this goal?",
    "answer": "Safety benchmarks ensure responsibility and trustworthiness by measuring LLMs' resilience against producing harmful outputs through controlled environments. Techniques might include adversarial testing, toxic response generation, and ethical alignment training metrics to evaluate the model's behavior across a range of scenarios.",
    "explanation": "This question probes the expert's understanding of the broader impact of safety benchmarks on the ethical deployment of LLMs and requires them to reflect on the specific methodologies used in these benchmarks.",
    "question_token_count": 49,
    "answer_correctness_score": 8,
    "explanation_validity_score": 8,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 48
  },
  {
    "context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n",
    "topic": "Understanding and application of the correctness criterion in dynamic benchmarking for LLMs.",
    "response": "<output_format>\nQuestion: How would you design a dynamic benchmarking algorithm that ensures high correctness, and what specific challenges might arise in ensuring the ground truth alignment during the transformation process?\nExplanation: This question requires a deep understanding of the correctness criterion and its practical application. It asks the respondent to think about the design considerations and potential pitfalls, which would challenge even a knowledgeable domain expert.\nCorrect Answer: Designing such an algorithm involves creating a robust transformation process and an effective ground truth oracle. Challenges include ensuring the oracle's accuracy, dealing with complex or ambiguous inputs, and maintaining the integrity of the ground truth across different transformations.\n</output_format>\n<document_analysis>\nThe context focuses on the correctness criterion for dynamic benchmarking algorithms used to evaluate language models (LLMs). It explains that correctness is crucial as incorrect benchmarks can lead to misleading evaluations. The mathematical formulation provided quantifies correctness, involving an oracle to ensure ground truth alignment. The question should probe the respondent's understanding of this concept and ability to apply it in practice, hence the detailed response provided.\n</document_analysis>",
    "question": "How would you design a dynamic benchmarking algorithm that ensures high correctness, and what specific challenges might arise in ensuring the ground truth alignment during the transformation process?",
    "answer": "Designing such an algorithm involves creating a robust transformation process and an effective ground truth oracle. Challenges include ensuring the oracle's accuracy, dealing with complex or ambiguous inputs, and maintaining the integrity of the ground truth across different transformations.",
    "explanation": "This question requires a deep understanding of the correctness criterion and its practical application. It asks the respondent to think about the design considerations and potential pitfalls, which would challenge even a knowledgeable domain expert.",
    "question_token_count": 31,
    "answer_correctness_score": 8,
    "explanation_validity_score": 7,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 46
  },
  {
    "context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n",
    "topic": "Explaining the factors influencing the cost of data transformation in dynamic benchmarking.",
    "response": "<output_format>\nQuestion: What are the key components that influence the cost function Cost(\u22c5) in the context of dynamic benchmarking, and how do they interact to affect the overall scalability of a dynamic benchmark method?\nExplanation: The question probes the multifaceted nature of the cost function in dynamic benchmarking, requiring an understanding of both the quantitative aspects (like monetary cost, time, and effort) and the transformation process itself. It also challenges the respondent to consider how these factors collectively impact the scalability of the benchmarking method.\nCorrect Answer: The cost function Cost(\u22c5) is influenced by monetary cost, time spent, and manual effort required for the transformation process. These factors interact such that reducing one might increase another, impacting the efficiency and scalability of generating large-scale benchmark datasets.\n</output_format>",
    "question": "What are the key components that influence the cost function Cost(\u22c5) in the context of dynamic benchmarking, and how do they interact to affect the overall scalability of a dynamic benchmark method?",
    "answer": "The cost function Cost(\u22c5) is influenced by monetary cost, time spent, and manual effort required for the transformation process. These factors interact such that reducing one might increase another, impacting the efficiency and scalability of generating large-scale benchmark datasets.",
    "explanation": "The question probes the multifaceted nature of the cost function in dynamic benchmarking, requiring an understanding of both the quantitative aspects (like monetary cost, time, and effort) and the transformation process itself. It also challenges the respondent to consider how these factors collectively impact the scalability of the benchmarking method.",
    "question_token_count": 38,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 4,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 49
  },
  {
    "context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n",
    "topic": "Application of Recent Challenges (AIME 2024 and CNMO 2024) in Assessing Model Competence in Complex Mathematical Tasks.",
    "response": "<output_format>\nQuestion: How might the AIME 2024 and CNMO 2024 challenges be designed to effectively assess a language model's capability to handle complex mathematical tasks compared to existing datasets like GSM8K and MATH?\n\nExplanation: This question requires an understanding of the characteristics of recent challenges like AIME 2024 and CNMO 2024, and how they differ from established benchmarks such as GSM8K and MATH. It probes the ability to critically analyze the nature of these new challenges and their implications for evaluating a model's competence in complex mathematical tasks.\n\nCorrect Answer: The AIME 2024 and CNMO 2024 challenges are likely designed to include more varied and intricate problem structures, real-world application scenarios, and potentially higher cognitive demands, such as strategic planning and multi-step reasoning, which go beyond the typical problem-solving tasks found in GSM8K and MATH. These challenges may also incorporate a broader range of mathematical concepts and their interconnections, pushing models to demonstrate a deeper and more flexible understanding of mathematics.\n</output_format>",
    "question": "How might the AIME 2024 and CNMO 2024 challenges be designed to effectively assess a language model's capability to handle complex mathematical tasks compared to existing datasets like GSM8K and MATH?",
    "answer": "The AIME 2024 and CNMO 2024 challenges are likely designed to include more varied and intricate problem structures, real-world application scenarios, and potentially higher cognitive demands, such as strategic planning and multi-step reasoning, which go beyond the typical problem-solving tasks found in GSM8K and MATH. These challenges may also incorporate a broader range of mathematical concepts and their interconnections, pushing models to demonstrate a deeper and more flexible understanding of mathematics.",
    "explanation": "This question requires an understanding of the characteristics of recent challenges like AIME 2024 and CNMO 2024, and how they differ from established benchmarks such as GSM8K and MATH. It probes the ability to critically analyze the nature of these new challenges and their implications for evaluating a model's competence in complex mathematical tasks.",
    "question_token_count": 41,
    "answer_correctness_score": 8,
    "explanation_validity_score": 9,
    "question_clarity_score": 7,
    "question_difficulty_score": 6,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 91
  },
  {
    "context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n",
    "topic": "Provide examples of common cases of exact contamination in LLM datasets.",
    "response": "<document_analysis>\nThe context provides a detailed explanation of data contamination in LLMs, distinguishing between exact and syntactic contamination. For exact contamination, the context mentions verbatim test examples appearing in training corpora, code snippets from benchmark implementations, and documentation leaks as common cases. These points are specific and directly relevant to the topic of exact contamination. The context does not contain any irrelevant or promotional information that would distract from the core educational value.\n</document_analysis>\n\n<output_format>\nQuestion: What are some examples of common cases of exact contamination in LLM datasets?\nExplanation: The question requires the respondent to identify and articulate specific instances of exact contamination as described in the context, demonstrating a deep understanding of the nuances in data contamination in LLMs.\nCorrect Answer: Verbatim test examples appearing in training corpora, code snippets from benchmark implementations, and documentation leaks.\n</output_format>",
    "question": "What are some examples of common cases of exact contamination in LLM datasets?",
    "answer": "Verbatim test examples appearing in training corpora, code snippets from benchmark implementations, and documentation leaks.",
    "explanation": "The question requires the respondent to identify and articulate specific instances of exact contamination as described in the context, demonstrating a deep understanding of the nuances in data contamination in LLMs.",
    "question_token_count": 15,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 21
  },
  {
    "context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Evaluating the effectiveness of static benchmarking methods in mitigating data contamination.",
    "response": "<output_format>\nQuestion: How does the reliance on static benchmarking methods exacerbate the risk of data contamination in Large Language Models (LLMs), and what specific measures can be taken to reduce this risk?\nExplanation: This question requires a deep understanding of the limitations of static benchmarking methods, particularly their susceptibility to data contamination, and the necessity of adopting dynamic benchmarking strategies. It encourages reflection on the fundamental issues with static benchmarks and prompts a discussion on practical solutions to mitigate contamination.\nCorrect Answer: Static benchmarking methods exacerbate the risk of data contamination because they are prone to unintentional inclusion of benchmark data in the training phase of LLMs. To reduce this risk, measures such as continuous dataset updates based on LLM training timestamps, data encryption, and post-hoc contamination detection can be employed.\n</output_format>",
    "question": "How does the reliance on static benchmarking methods exacerbate the risk of data contamination in Large Language Models (LLMs), and what specific measures can be taken to reduce this risk?",
    "answer": "Static benchmarking methods exacerbate the risk of data contamination because they are prone to unintentional inclusion of benchmark data in the training phase of LLMs. To reduce this risk, measures such as continuous dataset updates based on LLM training timestamps, data encryption, and post-hoc contamination detection can be employed.",
    "explanation": "This question requires a deep understanding of the limitations of static benchmarking methods, particularly their susceptibility to data contamination, and the necessity of adopting dynamic benchmarking strategies. It encourages reflection on the fundamental issues with static benchmarks and prompts a discussion on practical solutions to mitigate contamination.",
    "question_token_count": 36,
    "answer_correctness_score": 9,
    "explanation_validity_score": 8,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 59
  },
  {
    "context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n",
    "topic": "Evaluating the trade-off between dataset size and associated costs in optimizing dynamic benchmarks.",
    "response": "<output_format>\nQuestion: Given the formula for scalability in dynamic benchmarking, what strategic insights can a domain expert derive to optimize the balance between dataset size and associated costs, particularly when considering the cost function Cost(\u22c5)?\nExplanation: This question requires a deep understanding of the formula provided and encourages reflection on how to practically apply the concept of scalability to real-world scenarios. It asks for strategic insights, not just a definition of the formula, pushing the expert to consider practical optimizations and trade-offs.\nCorrect Answer: A strategic insight would be to identify transformations that maximize the ratio of dataset size increase to cost, thus efficiently generating larger datasets with minimal additional costs. Another insight is to focus on cost functions that can be optimized through automation or parallel processing to reduce time and manual effort.\n</output_format>",
    "question": "Given the formula for scalability in dynamic benchmarking, what strategic insights can a domain expert derive to optimize the balance between dataset size and associated costs, particularly when considering the cost function Cost(\u22c5)?",
    "answer": "A strategic insight would be to identify transformations that maximize the ratio of dataset size increase to cost, thus efficiently generating larger datasets with minimal additional costs. Another insight is to focus on cost functions that can be optimized through automation or parallel processing to reduce time and manual effort.",
    "explanation": "This question requires a deep understanding of the formula provided and encourages reflection on how to practically apply the concept of scalability to real-world scenarios. It asks for strategic insights, not just a definition of the formula, pushing the expert to consider practical optimizations and trade-offs.",
    "question_token_count": 39,
    "answer_correctness_score": 9,
    "explanation_validity_score": 8,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 54
  },
  {
    "context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n",
    "topic": "Enhancing Static Benchmarks and Their Limitations",
    "response": "<document_analysis>\nThe context focuses on the evolution of benchmarking methods from static to dynamic for addressing data contamination in large language models (LLMs). It highlights the limitations of enhancing static benchmarks and the need for standardized criteria for dynamic benchmarks. The proposed focus on static benchmarks is crucial as it sets the stage for understanding the challenges faced by current methodologies before proposing improvements.\n\nGiven the emphasis on the limitations of static benchmark enhancements, a highly insightful question would probe into the specific shortcomings and their implications for the field of LLM benchmarking.\n</document_analysis>\n\n<output_format>\nQuestion: What are the key limitations identified in the enhancement of static benchmarks for mitigating data contamination in LLMs, and how do these limitations impact the effectiveness of current benchmarking practices?\n\nExplanation: This question requires a deep understanding of the context and the ability to synthesize information about the limitations of static benchmark enhancements. It encourages a critical reflection on the practical implications of these limitations for the broader field of LLM benchmarking.\nCorrect Answer: The key limitations identified include the inability to capture real-time changes in data quality, difficulty in detecting subtle forms of data contamination, and a lack of adaptability to evolving data sources. These limitations significantly impact the effectiveness of current benchmarking practices by potentially leading to outdated and inaccurate assessments of model performance.\n</output_format>",
    "question": "What are the key limitations identified in the enhancement of static benchmarks for mitigating data contamination in LLMs, and how do these limitations impact the effectiveness of current benchmarking practices?",
    "answer": "The key limitations identified include the inability to capture real-time changes in data quality, difficulty in detecting subtle forms of data contamination, and a lack of adaptability to evolving data sources. These limitations significantly impact the effectiveness of current benchmarking practices by potentially leading to outdated and inaccurate assessments of model performance.",
    "explanation": "This question requires a deep understanding of the context and the ability to synthesize information about the limitations of static benchmark enhancements. It encourages a critical reflection on the practical implications of these limitations for the broader field of LLM benchmarking.",
    "question_token_count": 34,
    "answer_correctness_score": 8,
    "explanation_validity_score": 8,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 58
  },
  {
    "context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n",
    "topic": "Dynamic nature of benchmarking and the importance of regular updates in model evaluation.",
    "response": "<output_format>\nQuestion: How does the dynamic nature of benchmarking methods, such as LiveBench and Forecastbench, reflect the evolving challenges in evaluating LLMs over time, and what are the key implications for maintaining model reliability?\n\nExplanation: This question probes the expert's understanding of the evolving landscape of benchmarking methods and their importance in ensuring model reliability. It requires an in-depth analysis of how continuous updates in benchmarking methods address the limitations of static datasets and the challenges posed by the temporal cutoff of LLMs' knowledge.\n\nCorrect Answer: The dynamic nature of benchmarking methods ensures that LLMs are evaluated against the most current and relevant data, thus mitigating the effects of outdated knowledge cutoffs and providing a more accurate assessment of the model's performance. Regular updates in benchmarking methods, such as the inclusion of recent data, emerging knowledge, and forecasting questions, highlight the need for ongoing refinement in model evaluation techniques to maintain reliability and relevance.\n</output_format>",
    "question": "How does the dynamic nature of benchmarking methods, such as LiveBench and Forecastbench, reflect the evolving challenges in evaluating LLMs over time, and what are the key implications for maintaining model reliability?",
    "answer": "The dynamic nature of benchmarking methods ensures that LLMs are evaluated against the most current and relevant data, thus mitigating the effects of outdated knowledge cutoffs and providing a more accurate assessment of the model's performance. Regular updates in benchmarking methods, such as the inclusion of recent data, emerging knowledge, and forecasting questions, highlight the need for ongoing refinement in model evaluation techniques to maintain reliability and relevance.",
    "explanation": "This question probes the expert's understanding of the evolving landscape of benchmarking methods and their importance in ensuring model reliability. It requires an in-depth analysis of how continuous updates in benchmarking methods address the limitations of static datasets and the challenges posed by the temporal cutoff of LLMs' knowledge.",
    "question_token_count": 40,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 78
  },
  {
    "context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n",
    "topic": "Limitations of Static Benchmarks and Their Ongoing Relevance",
    "response": "<output_format>\nQuestion: How does the increasing size of the training corpus affect the effectiveness of static benchmarks in evaluating large language models, and what implication does this have for the development of future evaluation methodologies?\nExplanation: The question probes the expert's understanding of the relationship between the size of training data and the efficacy of static benchmarks, encouraging reflection on the implications for benchmarking practices.\nCorrect Answer: As the training corpus size grows, the probability of contamination in static benchmarks increases, rendering them less effective; this necessitates the development of more dynamic and robust benchmarking approaches.\n</output_format>",
    "question": "How does the increasing size of the training corpus affect the effectiveness of static benchmarks in evaluating large language models, and what implication does this have for the development of future evaluation methodologies?",
    "answer": "As the training corpus size grows, the probability of contamination in static benchmarks increases, rendering them less effective; this necessitates the development of more dynamic and robust benchmarking approaches.",
    "explanation": "The question probes the expert's understanding of the relationship between the size of training data and the efficacy of static benchmarks, encouraging reflection on the implications for benchmarking practices.",
    "question_token_count": 36,
    "answer_correctness_score": 9,
    "explanation_validity_score": 8,
    "question_clarity_score": 8,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 35
  },
  {
    "context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n",
    "topic": "Impact of Syntactic Transformations on LLM Evaluation",
    "response": "<output_format>\nQuestion: How does the distinction between syntactic recall and reasoning capability impact the evaluation of LLMs, particularly in the context of syntactic contamination?\nExplanation: This question probes the deeper implications of treating syntactic transformations as contamination, forcing the respondent to consider both the technical nuances and the broader consequences of such a classification on LLM evaluations. It requires a sophisticated understanding of the relationship between memory and reasoning in language models.\nCorrect Answer: The distinction is crucial because conflating syntactic recall with reasoning capability can lead to overestimating an LLM's ability to generalize and reason effectively, potentially skewing benchmark results and misguiding research and deployment decisions.\n</output_format>",
    "question": "How does the distinction between syntactic recall and reasoning capability impact the evaluation of LLMs, particularly in the context of syntactic contamination?",
    "answer": "The distinction is crucial because conflating syntactic recall with reasoning capability can lead to overestimating an LLM's ability to generalize and reason effectively, potentially skewing benchmark results and misguiding research and deployment decisions.",
    "explanation": "This question probes the deeper implications of treating syntactic transformations as contamination, forcing the respondent to consider both the technical nuances and the broader consequences of such a classification on LLM evaluations. It requires a sophisticated understanding of the relationship between memory and reasoning in language models.",
    "question_token_count": 26,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 43
  },
  {
    "context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n",
    "topic": "Definition and Calculation of Internal Diversity",
    "response": "<document_analysis>\nThe provided context discusses the concept of diversity in transformed datasets, particularly focusing on external and internal diversity. The context includes mathematical formulations for both types of diversity, with a focus on internal diversity defined as the average of the diversity metric \u0398(\u22c5) applied to all pairs of transformed datasets (i.e., \\(\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}\\Theta(\\mathcal{D}_i,\\mathcal{D}_j)\\)).\n\nThe context provides a definition but does not delve deeply into the nuances or implications of using different diversity metrics or the practical considerations involved in calculating internal diversity. Given this, a suitable question would test the deeper understanding of internal diversity beyond just the formula.\n</document_analysis>\n\n<output_format>\nQuestion: How might the choice of the diversity metric \\(\\Theta(\u22c5)\\) impact the calculation and interpretation of internal diversity in transformed datasets? Provide an example scenario where a specific choice of \\(\\Theta(\u22c5)\\) would significantly influence the assessment of internal diversity.\nExplanation: The question requires the respondent to reflect on the role of the diversity metric \\(\\Theta(\u22c5)\\) in defining internal diversity and consider how different choices of this metric can lead to varying assessments of dataset transformations. A deep understanding of the implications of the chosen metric is essential to provide a comprehensive answer.\nCorrect Answer: The choice of \\(\\Theta(\u22c5)\\) can greatly affect the internal diversity score. For instance, if \\(\\Theta(\u22c5)\\) is based on N-gram metrics, it will capture syntactic similarity, whereas using BLEU scores will also consider semantic aspects. In a scenario where the transformed datasets are nearly identical in terms of syntax but differ significantly in semantics, using N-gram metrics might yield a low internal diversity score, suggesting high internal consistency, while BLEU scores could indicate higher internal diversity due to the consideration of semantic differences.\n</output_format>",
    "question": "How might the choice of the diversity metric \\(\\Theta(\u22c5)\\) impact the calculation and interpretation of internal diversity in transformed datasets? Provide an example scenario where a specific choice of \\(\\Theta(\u22c5)\\) would significantly influence the assessment of internal diversity.",
    "answer": "The choice of \\(\\Theta(\u22c5)\\) can greatly affect the internal diversity score. For instance, if \\(\\Theta(\u22c5)\\) is based on N-gram metrics, it will capture syntactic similarity, whereas using BLEU scores will also consider semantic aspects. In a scenario where the transformed datasets are nearly identical in terms of syntax but differ significantly in semantics, using N-gram metrics might yield a low internal diversity score, suggesting high internal consistency, while BLEU scores could indicate higher internal diversity due to the consideration of semantic differences.",
    "explanation": "The question requires the respondent to reflect on the role of the diversity metric \\(\\Theta(\u22c5)\\) in defining internal diversity and consider how different choices of this metric can lead to varying assessments of dataset transformations. A deep understanding of the implications of the chosen metric is essential to provide a comprehensive answer.",
    "question_token_count": 49,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 104
  },
  {
    "context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "The concept and importance of contamination detectors in benchmarking LLMs.",
    "response": "<document_analysis>\nThe context discusses the evolution of LLM benchmarks and highlights the challenges posed by static benchmarks as LLMs improve over time. It mentions the development of contamination detectors to address data contamination issues. The focus on contamination detectors is important but the context is primarily descriptive rather than providing deep insights into the topic. Given this, the question aims to probe a deeper understanding of the implications and nuances of using contamination detectors in benchmarking LLMs.\n</document_analysis>\n\n<output_format>\nQuestion: How do contamination detectors contribute to the robustness of LLM benchmarking, and what are the potential limitations or considerations that domain experts must keep in mind when relying on these detectors?\nExplanation: This question requires a deep understanding of the concept of contamination detectors and their role in benchmarking LLMs. It also prompts the respondent to consider the broader implications and potential limitations of these tools.\nCorrect Answer: Contamination detectors help identify and quantify contamination risks in datasets used for benchmarking, ensuring more accurate performance assessments. However, they may not capture all forms of contamination, and their effectiveness depends on the quality and diversity of the contamination detection methods employed. Domain experts must consider the detector's sensitivity, specificity, and the potential for false positives or negatives.\n</output_format>",
    "question": "How do contamination detectors contribute to the robustness of LLM benchmarking, and what are the potential limitations or considerations that domain experts must keep in mind when relying on these detectors?",
    "answer": "Contamination detectors help identify and quantify contamination risks in datasets used for benchmarking, ensuring more accurate performance assessments. However, they may not capture all forms of contamination, and their effectiveness depends on the quality and diversity of the contamination detection methods employed. Domain experts must consider the detector's sensitivity, specificity, and the potential for false positives or negatives.",
    "explanation": "This question requires a deep understanding of the concept of contamination detectors and their role in benchmarking LLMs. It also prompts the respondent to consider the broader implications and potential limitations of these tools.",
    "question_token_count": 34,
    "answer_correctness_score": 9,
    "explanation_validity_score": 8,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 6,
    "avg_answer_token_count": 69
  },
  {
    "context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n",
    "topic": "Importance and impact of using up-to-date data sources in evaluating language models.",
    "response": "<output_format>\nQuestion: How does the continuous use of up-to-date data sources in benchmarking language models (such as LiveBench, AntiLeak-Bench, AcademicEval, LiveCodeBench, LiveAoPSBench, and Forecastbench) fundamentally alter the evaluation process and what are the implications for detecting and mitigating data contamination?\nExplanation: This question requires an expert to deeply understand the nuances of using current data sources in benchmarking, recognize the various methods employed (like collecting recent math competitions, updating academic writing tasks, and continuously gathering new coding problems), and reflect on how these approaches impact the reliability and integrity of the model evaluations.\nCorrect Answer: By ensuring that the models are evaluated on data they have not seen during training, these up-to-date data sources significantly reduce the risk of data contamination, allowing for a more accurate assessment of the model's true capabilities and understanding of recent developments.\n</output_format>",
    "question": "How does the continuous use of up-to-date data sources in benchmarking language models (such as LiveBench, AntiLeak-Bench, AcademicEval, LiveCodeBench, LiveAoPSBench, and Forecastbench) fundamentally alter the evaluation process and what are the implications for detecting and mitigating data contamination?",
    "answer": "By ensuring that the models are evaluated on data they have not seen during training, these up-to-date data sources significantly reduce the risk of data contamination, allowing for a more accurate assessment of the model's true capabilities and understanding of recent developments.",
    "explanation": "This question requires an expert to deeply understand the nuances of using current data sources in benchmarking, recognize the various methods employed (like collecting recent math competitions, updating academic writing tasks, and continuously gathering new coding problems), and reflect on how these approaches impact the reliability and integrity of the model evaluations.",
    "question_token_count": 61,
    "answer_correctness_score": 9,
    "explanation_validity_score": 8,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 48
  },
  {
    "context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n",
    "topic": "The necessity and benefits of interpretable transformations in ensuring the correctness and reliability of dynamic benchmarking, particularly in LLM evaluation.",
    "response": "<output_format>\nQuestion: How does the interpretability of the transformation process impact the cost-effectiveness and reliability of dynamic benchmarking for evaluating LLMs, and what measures can be taken to ensure interpretability in LLM-assisted transformations?\nExplanation: This question probes the deeper implications of interpretability in the context of dynamic benchmarking, encouraging the respondent to consider both economic and technical aspects of the transformation process. It requires a nuanced understanding of the challenges associated with LLM-assisted transformations and the importance of ensuring reliability through interpretable methods.\nCorrect Answer: By enabling easier verification, reducing the need for extensive manual validation, and facilitating the use of explainability tools, interpretable transformations enhance the cost-effectiveness and reliability of dynamic benchmarking for LLM evaluation. For LLM-assisted transformations, additional measures such as human-in-the-loop validation or employing transparent models can ensure interpretability.\n</output_format>",
    "question": "How does the interpretability of the transformation process impact the cost-effectiveness and reliability of dynamic benchmarking for evaluating LLMs, and what measures can be taken to ensure interpretability in LLM-assisted transformations?",
    "answer": "By enabling easier verification, reducing the need for extensive manual validation, and facilitating the use of explainability tools, interpretable transformations enhance the cost-effectiveness and reliability of dynamic benchmarking for LLM evaluation. For LLM-assisted transformations, additional measures such as human-in-the-loop validation or employing transparent models can ensure interpretability.",
    "explanation": "This question probes the deeper implications of interpretability in the context of dynamic benchmarking, encouraging the respondent to consider both economic and technical aspects of the transformation process. It requires a nuanced understanding of the challenges associated with LLM-assisted transformations and the importance of ensuring reliability through interpretable methods.",
    "question_token_count": 39,
    "answer_correctness_score": 9,
    "explanation_validity_score": 8,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 62
  },
  {
    "context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n",
    "topic": "Variable Replacement Techniques in Generating New Samples from Existing Benchmarks",
    "response": "<output_format>\nQuestion: How does VarBench ensure that the replacement of variables in existing benchmark samples does not compromise the integrity or difficulty of the generated new samples?\nExplanation: This question requires a deep understanding of the variable replacement process and its impact on the overall quality and challenge level of the new samples created from existing benchmarks. It probes whether the technique preserves the original sample's characteristics and difficulty, which is crucial for maintaining the benchmark's utility.\nCorrect Answer: VarBench employs a systematic approach to identify and replace variables while ensuring that the difficulty level is maintained through a combination of automatic detection and manual verification processes.\n</output_format>",
    "question": "How does VarBench ensure that the replacement of variables in existing benchmark samples does not compromise the integrity or difficulty of the generated new samples?",
    "answer": "VarBench employs a systematic approach to identify and replace variables while ensuring that the difficulty level is maintained through a combination of automatic detection and manual verification processes.",
    "explanation": "This question requires a deep understanding of the variable replacement process and its impact on the overall quality and challenge level of the new samples created from existing benchmarks. It probes whether the technique preserves the original sample's characteristics and difficulty, which is crucial for maintaining the benchmark's utility.",
    "question_token_count": 28,
    "answer_correctness_score": 8,
    "explanation_validity_score": 8,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 4,
    "avg_answer_token_count": 31
  },
  {
    "context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n",
    "topic": "Debate on True Data Contamination",
    "response": "<document_analysis>\nThe provided context discusses the debate on whether syntactic transformations in data constitute true data contamination and highlights the significance of addressing this issue in benchmarking large language models (LLMs). The context emphasizes the challenge in distinguishing between a model's ability to recall memorized information versus its reasoning capability. The significance of this distinction lies in the potential for contaminated benchmarks to overestimate a model's capabilities and undermine the validity of evaluations.\n\nGiven the complexity and importance of the topic, a highly insightful question would challenge the domain expert to reflect on the nuances of this debate and its implications.\n</document_analysis>\n\n<output_format>\nQuestion: Given the ongoing debate on whether syntactic transformations in data constitute true data contamination, what is the primary risk of conflating a model's ability to recall memorized information with its reasoning capability in the context of benchmarking LLMs, and how might this affect the validity of evaluations?\nExplanation: This question requires the expert to delve into the subtleties of the debate and articulate the risks associated with conflating memory recall with reasoning. It challenges the expert to consider the implications of such confusion on the accuracy and reliability of LLM benchmarks.\nCorrect Answer: Conflating a model's ability to recall memorized information with its reasoning capability can lead to overestimating the model's true capabilities, undermining the validity of benchmarks and potentially misguiding model comparisons, deployment decisions, and policy-making.\n</output_format>",
    "question": "Given the ongoing debate on whether syntactic transformations in data constitute true data contamination, what is the primary risk of conflating a model's ability to recall memorized information with its reasoning capability in the context of benchmarking LLMs, and how might this affect the validity of evaluations?",
    "answer": "Conflating a model's ability to recall memorized information with its reasoning capability can lead to overestimating the model's true capabilities, undermining the validity of benchmarks and potentially misguiding model comparisons, deployment decisions, and policy-making.",
    "explanation": "This question requires the expert to delve into the subtleties of the debate and articulate the risks associated with conflating memory recall with reasoning. It challenges the expert to consider the implications of such confusion on the accuracy and reliability of LLM benchmarks.",
    "question_token_count": 54,
    "answer_correctness_score": 9,
    "explanation_validity_score": 8,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 48
  },
  {
    "context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n",
    "topic": "Role of Syntactic Information in NLP Applications",
    "response": "<document_analysis>\nThe context focuses on the role of syntactic information in NLP applications and the significance of data contamination in evaluating LLMs. While it mentions syntactic contamination and the debate surrounding its nature, the primary emphasis is on the broader implications of contamination for benchmarking and LLM development. Therefore, the context is highly relevant to the topic of the role of syntactic information in NLP applications but does not provide detailed insights into specific nuances or challenges related to syntactic information itself.\n</document_analysis>\n\n<output_format>\nQuestion: How does the reliance on syntactic information in NLP applications impact the evaluation of LLMs, and what are the potential pitfalls of not accounting for syntactic contamination in benchmarking?\nExplanation: This question probes the domain expert's understanding of the interplay between syntactic information and NLP application requirements, while also assessing their awareness of the broader implications of contamination in benchmarking. It requires a nuanced understanding of both the technical aspects and the practical consequences.\nCorrect Answer: The reliance on syntactic information in NLP applications can lead to misleading evaluations of LLMs if contamination is not accounted for, as LLMs might perform well due to their ability to recall memorized syntactic structures rather than true reasoning capabilities. This can result in overestimating the model's generalization and robustness.\n</output_format>",
    "question": "How does the reliance on syntactic information in NLP applications impact the evaluation of LLMs, and what are the potential pitfalls of not accounting for syntactic contamination in benchmarking?",
    "answer": "The reliance on syntactic information in NLP applications can lead to misleading evaluations of LLMs if contamination is not accounted for, as LLMs might perform well due to their ability to recall memorized syntactic structures rather than true reasoning capabilities. This can result in overestimating the model's generalization and robustness.",
    "explanation": "This question probes the domain expert's understanding of the interplay between syntactic information and NLP application requirements, while also assessing their awareness of the broader implications of contamination in benchmarking. It requires a nuanced understanding of both the technical aspects and the practical consequences.",
    "question_token_count": 33,
    "answer_correctness_score": 9,
    "explanation_validity_score": 8,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 59
  },
  {
    "context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n",
    "topic": "Importance of Robust Benchmarking in LLM Research",
    "response": "<output_format>\nQuestion: What are the potential long-term consequences of failing to address data contamination in LLM benchmarking, and how might these affect the field of NLP?\nExplanation: This question requires a deep understanding of the implications of data contamination in LLM benchmarking. It asks the respondent to reflect on the broader impacts of such issues, including their effect on model comparisons, deployment decisions, and policy-making, as well as the importance of ensuring robust benchmarks.\nCorrect Answer: Failing to address data contamination in LLM benchmarking can lead to misleading conclusions about progress in LLM research, influence model comparisons inaccurately, affect deployment decisions, and impact policy-making based on unreliable data. It undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability, thereby hindering the advancement of NLP technologies.\n</output_format>",
    "question": "What are the potential long-term consequences of failing to address data contamination in LLM benchmarking, and how might these affect the field of NLP?",
    "answer": "Failing to address data contamination in LLM benchmarking can lead to misleading conclusions about progress in LLM research, influence model comparisons inaccurately, affect deployment decisions, and impact policy-making based on unreliable data. It undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability, thereby hindering the advancement of NLP technologies.",
    "explanation": "This question requires a deep understanding of the implications of data contamination in LLM benchmarking. It asks the respondent to reflect on the broader impacts of such issues, including their effect on model comparisons, deployment decisions, and policy-making, as well as the importance of ensuring robust benchmarks.",
    "question_token_count": 28,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 72
  },
  {
    "context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n",
    "topic": "Role of initial questions in guiding the evaluation process in LLM evaluation techniques",
    "response": "<output_format>\nQuestion: How does the formulation of the initial question impact the subsequent evaluation process in TreeEval, and what aspect of the evaluated LLM's response does it primarily influence?\nExplanation: The initial question sets the direction for the entire evaluation process in TreeEval. It determines the focus and scope of the topics that will be explored through follow-up questions. The effectiveness of the initial question can significantly influence the depth and breadth of insights gained from the LLM's responses.\nCorrect Answer: The initial question primarily influences the specific areas of knowledge the LLM demonstrates and the types of follow-up questions that are generated.\n</output_format>",
    "question": "How does the formulation of the initial question impact the subsequent evaluation process in TreeEval, and what aspect of the evaluated LLM's response does it primarily influence?",
    "answer": "The initial question primarily influences the specific areas of knowledge the LLM demonstrates and the types of follow-up questions that are generated.",
    "explanation": "The initial question sets the direction for the entire evaluation process in TreeEval. It determines the focus and scope of the topics that will be explored through follow-up questions. The effectiveness of the initial question can significantly influence the depth and breadth of insights gained from the LLM's responses.",
    "question_token_count": 32,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 25
  },
  {
    "context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Understanding the impact of data contamination on LLM benchmarking.",
    "response": "<output_format>\nQuestion: How does the continuous update strategy in dynamic benchmarking help mitigate data contamination, and what are its potential limitations?\nExplanation: This question requires a deep understanding of the dynamic benchmarking approach and its mechanism to prevent data contamination. It also probes the candidate's ability to identify potential drawbacks and limitations, which is crucial for a thorough evaluation of the method's effectiveness.\nCorrect Answer: The continuous update strategy helps mitigate data contamination by ensuring that the benchmark data used for evaluation is updated frequently to reflect the most recent training data of the LLM. However, it may introduce new challenges such as increased complexity in maintaining the benchmark, potential delays in reflecting the latest training data, and the possibility of introducing new biases or inaccuracies through frequent updates.\n</output_format>\n<document_analysis>\nThe context emphasizes the issue of data contamination in LLM benchmarking, particularly how static benchmarks can lead to inflated assessments due to inadvertent inclusion of training data. Dynamic benchmarking strategies, such as continuous updates, are discussed as a solution to this problem. However, the context does not explicitly detail all potential limitations of this approach, making the question relevant and probing.\n\nThe question integrates key elements from the provided context, such as the continuous update strategy and its role in mitigating data contamination, while encouraging reflection on the broader implications and potential drawbacks. This aligns with the requirement for a highly insightful and challenging question.\n</document_analysis>",
    "question": "How does the continuous update strategy in dynamic benchmarking help mitigate data contamination, and what are its potential limitations?",
    "answer": "The continuous update strategy helps mitigate data contamination by ensuring that the benchmark data used for evaluation is updated frequently to reflect the most recent training data of the LLM. However, it may introduce new challenges such as increased complexity in maintaining the benchmark, potential delays in reflecting the latest training data, and the possibility of introducing new biases or inaccuracies through frequent updates.",
    "explanation": "This question requires a deep understanding of the dynamic benchmarking approach and its mechanism to prevent data contamination. It also probes the candidate's ability to identify potential drawbacks and limitations, which is crucial for a thorough evaluation of the method's effectiveness.",
    "question_token_count": 22,
    "answer_correctness_score": 8,
    "explanation_validity_score": 8,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 71
  },
  {
    "context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n",
    "topic": "Recent Enhancements in Knowledge Benchmarking with MMLU-Redux and MMLU-Pro.",
    "response": "<output_format>\nQuestion: How do MMLU-Redux and MMLU-Pro refine the original MMLU benchmark to better assess an LLM's comprehensive knowledge across various domains?\n\nExplanation: MMLU-Redux and MMLU-Pro address limitations of the original MMLU benchmark by refining its scope and depth of knowledge assessment. This question requires an expert to reflect on the specific enhancements made and their implications for evaluating LLMs.\n\nCorrect Answer: MMLU-Redux and MMLU-Pro likely introduce more rigorous filtering, include more diverse and challenging questions, and possibly incorporate new domains to ensure a broader and more comprehensive evaluation of an LLM's knowledge.\n</output_format>",
    "question": "How do MMLU-Redux and MMLU-Pro refine the original MMLU benchmark to better assess an LLM's comprehensive knowledge across various domains?",
    "answer": "MMLU-Redux and MMLU-Pro likely introduce more rigorous filtering, include more diverse and challenging questions, and possibly incorporate new domains to ensure a broader and more comprehensive evaluation of an LLM's knowledge.",
    "explanation": "MMLU-Redux and MMLU-Pro address limitations of the original MMLU benchmark by refining its scope and depth of knowledge assessment. This question requires an expert to reflect on the specific enhancements made and their implications for evaluating LLMs.",
    "question_token_count": 32,
    "answer_correctness_score": 8,
    "explanation_validity_score": 8,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 8,
    "avg_answer_token_count": 43
  },
  {
    "context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n",
    "topic": "Discuss the implications of data contamination on the validity of LLM performance measurements.",
    "response": "<output_format>\nQuestion: How does data contamination affect the reliability of LLM performance metrics, and what are the key differences between exact and syntactic contamination in terms of their impact?\nExplanation: This question requires a deep understanding of the mechanisms and implications of data contamination, as well as the ability to compare and contrast exact and syntactic contamination types. It probes the nuances of how contamination can introduce biases and invalidate performance benchmarks, encouraging a critical analysis of the core issues.\nCorrect Answer: Data contamination can severely undermine the validity of LLM performance measurements because it introduces artificial overlap between training and test sets, leading to overly optimistic performance estimates. Exact contamination involves direct duplicates, which can result in perfect recall and precision scores but do not necessarily reflect the model's true capabilities on unseen data. Syntactic contamination, on the other hand, relies on more subtle transformations that preserve the essence of the data points. While it may not achieve exact matches, it can still lead to higher-than-expected performance metrics, as the model may recognize and exploit these transformations, thus failing to generalize properly to new, unaltered instances.\n</output_format>",
    "question": "How does data contamination affect the reliability of LLM performance metrics, and what are the key differences between exact and syntactic contamination in terms of their impact?",
    "answer": "Data contamination can severely undermine the validity of LLM performance measurements because it introduces artificial overlap between training and test sets, leading to overly optimistic performance estimates. Exact contamination involves direct duplicates, which can result in perfect recall and precision scores but do not necessarily reflect the model's true capabilities on unseen data. Syntactic contamination, on the other hand, relies on more subtle transformations that preserve the essence of the data points. While it may not achieve exact matches, it can still lead to higher-than-expected performance metrics, as the model may recognize and exploit these transformations, thus failing to generalize properly to new, unaltered instances.",
    "explanation": "This question requires a deep understanding of the mechanisms and implications of data contamination, as well as the ability to compare and contrast exact and syntactic contamination types. It probes the nuances of how contamination can introduce biases and invalidate performance benchmarks, encouraging a critical analysis of the core issues.",
    "question_token_count": 30,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 9,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 125
  },
  {
    "context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n",
    "topic": "Pushing a model's factual and logical reasoning abilities with academic challenge sets like ARC and OpenBookQA.",
    "response": "<output_format>\nQuestion: How might a language model's performance on the ARC challenge set differ from its performance on the OpenBookQA challenge set, and what does this difference suggest about the nature of the logical reasoning tasks involved?\nExplanation: The ARC (A Large Scale Challenge Set for Complex Reasoning) and OpenBookQA datasets require different types of logical reasoning, with ARC focusing on complex, multi-step reasoning tasks involving abstract concepts and external knowledge, while OpenBookQA emphasizes the integration of commonsense knowledge and logical inference to resolve ambiguities in everyday scenarios.\nCorrect Answer: ARC challenges require more sophisticated, abstract reasoning involving complex problem-solving and integration of diverse knowledge, whereas OpenBookQA focuses on applying common sense and resolving ambiguity in practical situations, indicating distinct cognitive skills.\n</output_format>",
    "question": "How might a language model's performance on the ARC challenge set differ from its performance on the OpenBookQA challenge set, and what does this difference suggest about the nature of the logical reasoning tasks involved?",
    "answer": "ARC challenges require more sophisticated, abstract reasoning involving complex problem-solving and integration of diverse knowledge, whereas OpenBookQA focuses on applying common sense and resolving ambiguity in practical situations, indicating distinct cognitive skills.",
    "explanation": "The ARC (A Large Scale Challenge Set for Complex Reasoning) and OpenBookQA datasets require different types of logical reasoning, with ARC focusing on complex, multi-step reasoning tasks involving abstract concepts and external knowledge, while OpenBookQA emphasizes the integration of commonsense knowledge and logical inference to resolve ambiguities in everyday scenarios.",
    "question_token_count": 40,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 9,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 40
  },
  {
    "context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n",
    "topic": "Exploring ethical guidelines for data usage and model transparency in the context of AI benchmarking.",
    "response": "<document_analysis>\nThe context discusses the ethical considerations involved in using both static and dynamic benchmarks for evaluating language models (LLMs). It highlights potential issues with static benchmarks, such as perpetuating biases if based on outdated or biased data. Dynamic benchmarks are noted for their adaptability but raise concerns about privacy and security. Additionally, there are worries about transparency and misuse of benchmarking results. The importance of designing benchmarking frameworks with fairness, accountability, and privacy in mind is emphasized. Finally, the need for further exploration of ethical guidelines concerning data usage, model transparency, and societal impact is highlighted.\n\nGiven the complexity and multifaceted nature of these ethical concerns, the question should probe a deeper understanding of how these guidelines are interrelated and applied in practice. A highly technical and reflective question would encourage a domain expert to consider the nuances and implications of these guidelines.\n</document_analysis>\n\n<output_format>\nQuestion: How might the principles of fairness, accountability, and privacy be integrated into the design of a benchmarking framework for LLMs, and what are the potential trade-offs and challenges in doing so?\nExplanation: This question requires the respondent to synthesize the various ethical guidelines discussed and apply them to the practical challenge of designing a benchmarking framework. It encourages reflection on the complexities and trade-offs involved in balancing these principles.\nCorrect Answer: A comprehensive benchmarking framework for LLMs should include mechanisms to ensure representative and unbiased data collection, transparent and auditable evaluation processes, and robust privacy protections for participants. However, achieving these goals may involve trade-offs, such as increased complexity in data preprocessing and evaluation metrics, potential delays in benchmark updates, and challenges in obtaining fully anonymized datasets.\n</output_format>",
    "question": "How might the principles of fairness, accountability, and privacy be integrated into the design of a benchmarking framework for LLMs, and what are the potential trade-offs and challenges in doing so?",
    "answer": "A comprehensive benchmarking framework for LLMs should include mechanisms to ensure representative and unbiased data collection, transparent and auditable evaluation processes, and robust privacy protections for participants. However, achieving these goals may involve trade-offs, such as increased complexity in data preprocessing and evaluation metrics, potential delays in benchmark updates, and challenges in obtaining fully anonymized datasets.",
    "explanation": "This question requires the respondent to synthesize the various ethical guidelines discussed and apply them to the practical challenge of designing a benchmarking framework. It encourages reflection on the complexities and trade-offs involved in balancing these principles.",
    "question_token_count": 38,
    "answer_correctness_score": 9,
    "explanation_validity_score": 8,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 69
  },
  {
    "context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n",
    "topic": "Strategies for mitigating data contamination in LLM training and evaluation processes, considering both technical and transparency aspects.",
    "response": "<output_format>\nQuestion: How can the opacity of proprietary training data in LLMs impact the development and assessment of effective mitigation strategies for data contamination, and what alternative approaches might circumvent this issue?\nExplanation: The question requires an expert to reflect on the challenges posed by proprietary training data and explore innovative solutions beyond current detection methods. It encourages a deep understanding of both technical and transparency issues in LLM training and evaluation.\nCorrect Answer: Addressing the opacity issue could involve developing open-source training datasets, enhancing model architecture to better generalize from diverse sources, implementing more robust and transparent evaluation metrics, and fostering a community-driven effort to establish fair benchmarks. These strategies would help mitigate data contamination without relying on proprietary data.\n</output_format>",
    "question": "How can the opacity of proprietary training data in LLMs impact the development and assessment of effective mitigation strategies for data contamination, and what alternative approaches might circumvent this issue?",
    "answer": "Addressing the opacity issue could involve developing open-source training datasets, enhancing model architecture to better generalize from diverse sources, implementing more robust and transparent evaluation metrics, and fostering a community-driven effort to establish fair benchmarks. These strategies would help mitigate data contamination without relying on proprietary data.",
    "explanation": "The question requires an expert to reflect on the challenges posed by proprietary training data and explore innovative solutions beyond current detection methods. It encourages a deep understanding of both technical and transparency issues in LLM training and evaluation.",
    "question_token_count": 34,
    "answer_correctness_score": 9,
    "explanation_validity_score": 8,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 56
  },
  {
    "context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n",
    "topic": "The limitations of the current survey in covering recent developments and emerging benchmarking techniques in LLM research.",
    "response": "<output_format>\nQuestion: What key limitation does the survey acknowledge in terms of its coverage of recent advancements and emerging techniques in LLM benchmarking, and what does this imply for future research directions?\nExplanation: The question probes the author's recognition of the evolving nature of LLM benchmarking and the necessity for ongoing research and updates. It requires a deep understanding of the survey's limitations and the implications for future work.\nCorrect Answer: The survey acknowledges that due to the rapid evolution of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered, implying a need for continuous updates and further research in this area.\n</output_format>",
    "question": "What key limitation does the survey acknowledge in terms of its coverage of recent advancements and emerging techniques in LLM benchmarking, and what does this imply for future research directions?",
    "answer": "The survey acknowledges that due to the rapid evolution of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered, implying a need for continuous updates and further research in this area.",
    "explanation": "The question probes the author's recognition of the evolving nature of LLM benchmarking and the necessity for ongoing research and updates. It requires a deep understanding of the survey's limitations and the implications for future work.",
    "question_token_count": 33,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 42
  },
  {
    "context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Identifying gaps in the current literature regarding dynamic benchmarking methods.",
    "response": "<output_format>\nQuestion: Given the rapid development of dynamic benchmarking methods for LLMs, what specific criteria must be met for a dynamic benchmark to be considered effective, and why are existing benchmarks falling short in meeting these criteria?\nExplanation: The question requires a deep understanding of the criteria outlined in the paper for evaluating dynamic benchmarks and the ability to critically assess the current state of dynamic benchmarking methods. It encourages the respondent to reflect on the strengths and limitations discussed in the literature and identify the gaps that need to be addressed.\nCorrect Answer: A dynamic benchmark must meet the following criteria: (1) It should be able to update benchmark datasets based on the timestamps of LLM training to minimize contamination; (2) It should regenerate benchmark data to reconstruct original benchmarks, thereby reducing the likelihood of contamination; (3) It should be transparent and reproducible, allowing for consistent evaluation over time; (4) It should consider and mitigate potential biases introduced by dynamic updates; (5) It should be scalable and applicable to large-scale models. Existing benchmarks fall short in meeting these criteria due to limited transparency, lack of comprehensive evaluation, and insufficient consideration of biases and scalability.\n</output_format>",
    "question": "Given the rapid development of dynamic benchmarking methods for LLMs, what specific criteria must be met for a dynamic benchmark to be considered effective, and why are existing benchmarks falling short in meeting these criteria?",
    "answer": "A dynamic benchmark must meet the following criteria: (1) It should be able to update benchmark datasets based on the timestamps of LLM training to minimize contamination; (2) It should regenerate benchmark data to reconstruct original benchmarks, thereby reducing the likelihood of contamination; (3) It should be transparent and reproducible, allowing for consistent evaluation over time; (4) It should consider and mitigate potential biases introduced by dynamic updates; (5) It should be scalable and applicable to large-scale models. Existing benchmarks fall short in meeting these criteria due to limited transparency, lack of comprehensive evaluation, and insufficient consideration of biases and scalability.",
    "explanation": "The question requires a deep understanding of the criteria outlined in the paper for evaluating dynamic benchmarks and the ability to critically assess the current state of dynamic benchmarking methods. It encourages the respondent to reflect on the strengths and limitations discussed in the literature and identify the gaps that need to be addressed.",
    "question_token_count": 40,
    "answer_correctness_score": 9,
    "explanation_validity_score": 8,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 124
  },
  {
    "context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n",
    "topic": "Advanced decontamination methods and their vulnerabilities.",
    "response": "<output_format>\nQuestion: How does the vulnerability of encryption methods to minor text variations relate to the broader challenges in maintaining data integrity during machine learning model evaluation?\nExplanation: This question probes the domain expert's understanding of the interplay between encryption methods and the robustness of machine learning systems against subtle data manipulations. It requires a deep understanding of both the technical details of encryption methods and the broader implications for data security and integrity in model evaluation.\nCorrect Answer: Minor text variations can bypass advanced decontamination methods, highlighting the necessity for robust encryption and strong key management to prevent unauthorized data access and ensure the integrity of evaluation data.\n</output_format>",
    "question": "How does the vulnerability of encryption methods to minor text variations relate to the broader challenges in maintaining data integrity during machine learning model evaluation?",
    "answer": "Minor text variations can bypass advanced decontamination methods, highlighting the necessity for robust encryption and strong key management to prevent unauthorized data access and ensure the integrity of evaluation data.",
    "explanation": "This question probes the domain expert's understanding of the interplay between encryption methods and the robustness of machine learning systems against subtle data manipulations. It requires a deep understanding of both the technical details of encryption methods and the broader implications for data security and integrity in model evaluation.",
    "question_token_count": 27,
    "answer_correctness_score": 8,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 35
  },
  {
    "context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n",
    "topic": "Hybrid Generation Techniques in LLM Benchmarking",
    "response": "<output_format>\nQuestion: How does the integration of LLM-based and graph-based generation techniques in hybrid systems like DARG impact the process of creating new samples for LLM evaluation compared to using LLMs alone?\nExplanation: This question requires a deep understanding of the strengths and limitations of both LLM-based and graph-based approaches. It encourages reflection on how the combination of these methods enhances the generation of new samples while addressing the challenges of contamination and scale. The answer would need to consider the extraction of reasoning graphs, the application of predefined rules, and the overall effectiveness of hybrid systems in comparison to LLMs alone.\nCorrect Answer: By extracting reasoning graphs from existing benchmarks and perturbing them with predefined rules, hybrid systems like DARG can generate new samples that are more varied and less prone to contamination, offering a more robust alternative to LLM-based generation alone.\n</output_format>",
    "question": "How does the integration of LLM-based and graph-based generation techniques in hybrid systems like DARG impact the process of creating new samples for LLM evaluation compared to using LLMs alone?",
    "answer": "By extracting reasoning graphs from existing benchmarks and perturbing them with predefined rules, hybrid systems like DARG can generate new samples that are more varied and less prone to contamination, offering a more robust alternative to LLM-based generation alone.",
    "explanation": "This question requires a deep understanding of the strengths and limitations of both LLM-based and graph-based approaches. It encourages reflection on how the combination of these methods enhances the generation of new samples while addressing the challenges of contamination and scale. The answer would need to consider the extraction of reasoning graphs, the application of predefined rules, and the overall effectiveness of hybrid systems in comparison to LLMs alone.",
    "question_token_count": 36,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 46
  },
  {
    "context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n",
    "topic": "Interpreting the mathematical formula for quantifying scalability in dynamic benchmarking methods.",
    "response": "<output_format>\nQuestion: Given the mathematical representation of scalability in dynamic benchmarking methods, explain how the formula captures the trade-off between the cost of data transformation and the size of the transformed dataset relative to the original dataset. What does the ratio \u2225Ti\u2062(\ud835\udc9f)\u2225/\u2225\ud835\udc9f\u2225 represent in practical terms, and how does it inform the optimization goals for dynamic benchmarking?\nExplanation: The formula quantifies the efficiency of data transformation processes in generating scalable benchmark datasets. The ratio \u2225Ti\u2062(\ud835\udc9f)\u2225/\u2225\ud835\udc9f\u2225 represents the amplification factor or the number of instances added to the dataset per unit of the original dataset, which directly impacts the accuracy and reliability of the benchmarking process. By optimizing this ratio, one aims to maximize the dataset size while keeping the associated costs at a minimum.\nCorrect Answer: The ratio \u2225Ti\u2062(\ud835\udc9f)\u2225/\u2225\ud835\udc9f\u2225 represents the amplification factor of the transformation process, indicating the number of new instances generated per unit of the original dataset. This informs the optimization goal of balancing data size and cost.\n</output_format>",
    "question": "Given the mathematical representation of scalability in dynamic benchmarking methods, explain how the formula captures the trade-off between the cost of data transformation and the size of the transformed dataset relative to the original dataset. What does the ratio \u2225Ti\u2062(\ud835\udc9f)\u2225/\u2225\ud835\udc9f\u2225 represent in practical terms, and how does it inform the optimization goals for dynamic benchmarking?",
    "answer": "The ratio \u2225Ti\u2062(\ud835\udc9f)\u2225/\u2225\ud835\udc9f\u2225 represents the amplification factor of the transformation process, indicating the number of new instances generated per unit of the original dataset. This informs the optimization goal of balancing data size and cost.",
    "explanation": "The formula quantifies the efficiency of data transformation processes in generating scalable benchmark datasets. The ratio \u2225Ti\u2062(\ud835\udc9f)\u2225/\u2225\ud835\udc9f\u2225 represents the amplification factor or the number of instances added to the dataset per unit of the original dataset, which directly impacts the accuracy and reliability of the benchmarking process. By optimizing this ratio, one aims to maximize the dataset size while keeping the associated costs at a minimum.",
    "question_token_count": 73,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 51
  },
  {
    "context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n",
    "topic": "Consequences of Contaminated Benchmarks on Model Comparisons and Deployment",
    "response": "<output_format>\nQuestion: How might contaminated benchmarks influence the accuracy of model comparisons and the reliability of deployment decisions in NLP applications, and what are the implications for policy-making?\nExplanation: This question requires a deep understanding of the impact of contamination on benchmarking, as well as the broader implications for research, deployment, and policy. It encourages reflection on the consequences of misleading evaluations and the need for robust methods to ensure benchmarks are valid and reliable.\nCorrect Answer: Contaminated benchmarks can lead to overestimating a model's true capabilities, which can result in flawed model comparisons, unreliable deployment decisions, and potentially harmful policy-making based on inaccurate assessments of an LLM's performance.\n</output_format>",
    "question": "How might contaminated benchmarks influence the accuracy of model comparisons and the reliability of deployment decisions in NLP applications, and what are the implications for policy-making?",
    "answer": "Contaminated benchmarks can lead to overestimating a model's true capabilities, which can result in flawed model comparisons, unreliable deployment decisions, and potentially harmful policy-making based on inaccurate assessments of an LLM's performance.",
    "explanation": "This question requires a deep understanding of the impact of contamination on benchmarking, as well as the broader implications for research, deployment, and policy. It encourages reflection on the consequences of misleading evaluations and the need for robust methods to ensure benchmarks are valid and reliable.",
    "question_token_count": 30,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 43
  },
  {
    "context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n",
    "topic": "Evaluating a model\u2019s performance on complex reasoning tasks requiring the application of both factual and conceptual knowledge.",
    "response": "<output_format>\nQuestion: How would a model's performance on complex reasoning tasks, such as those involving both factual and conceptual knowledge, be assessed using existing benchmarks, and what are some examples of such benchmarks?\nExplanation: This question requires the respondent to synthesize knowledge across different benchmark categories\u2014coding, instruction following, and reasoning\u2014to identify how various benchmarks address complex reasoning tasks that involve factual and conceptual knowledge. It encourages a deep understanding of the nature and scope of each benchmark category and their interconnections.\nCorrect Answer: Complex reasoning tasks involving both factual and conceptual knowledge would be assessed through benchmarks like PIQA, SIQA, HellaSwag, WinoGrande, ARC, OpenBookQA, and CommonsenseQA, which evaluate a model\u2019s ability to apply everyday knowledge and logical reasoning. For example, PIQA and SIQA test a model\u2019s ability to infer from given stories, while ARC and OpenBookQA require the integration of background knowledge and logical reasoning to provide plausible answers.\n</output_format>",
    "question": "How would a model's performance on complex reasoning tasks, such as those involving both factual and conceptual knowledge, be assessed using existing benchmarks, and what are some examples of such benchmarks?",
    "answer": "Complex reasoning tasks involving both factual and conceptual knowledge would be assessed through benchmarks like PIQA, SIQA, HellaSwag, WinoGrande, ARC, OpenBookQA, and CommonsenseQA, which evaluate a model\u2019s ability to apply everyday knowledge and logical reasoning. For example, PIQA and SIQA test a model\u2019s ability to infer from given stories, while ARC and OpenBookQA require the integration of background knowledge and logical reasoning to provide plausible answers.",
    "explanation": "This question requires the respondent to synthesize knowledge across different benchmark categories\u2014coding, instruction following, and reasoning\u2014to identify how various benchmarks address complex reasoning tasks that involve factual and conceptual knowledge. It encourages a deep understanding of the nature and scope of each benchmark category and their interconnections.",
    "question_token_count": 36,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 9,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 96
  },
  {
    "context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n",
    "topic": "Analysis of model behavior under various conditions to identify contamination.",
    "response": "<output_format>\nQuestion: How might a researcher design a post-hoc detection method to uncover contamination that goes beyond simple token or word-level overlaps but still leverages model behavior under controlled conditions?\nExplanation: This question requires the respondent to synthesize knowledge of advanced post-hoc detection techniques and consider novel approaches that go beyond traditional methods. It encourages reflection on the limitations of current techniques and prompts thinking about innovative ways to detect contamination that may not be easily captured by simple overlap checks.\nCorrect Answer: A researcher could develop a method that evaluates model performance on specific masked input scenarios or partial completions, compares outcomes with benchmarks, and assesses model preferences for original versus paraphrased test cases, thereby leveraging model behavior under controlled conditions to identify contamination.\n</output_format>",
    "question": "How might a researcher design a post-hoc detection method to uncover contamination that goes beyond simple token or word-level overlaps but still leverages model behavior under controlled conditions?",
    "answer": "A researcher could develop a method that evaluates model performance on specific masked input scenarios or partial completions, compares outcomes with benchmarks, and assesses model preferences for original versus paraphrased test cases, thereby leveraging model behavior under controlled conditions to identify contamination.",
    "explanation": "This question requires the respondent to synthesize knowledge of advanced post-hoc detection techniques and consider novel approaches that go beyond traditional methods. It encourages reflection on the limitations of current techniques and prompts thinking about innovative ways to detect contamination that may not be easily captured by simple overlap checks.",
    "question_token_count": 32,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 50
  },
  {
    "context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n",
    "topic": "Definition and Examples of Syntactic Contamination",
    "response": "<output_format>\nQuestion: How does the distinction between syntactic recall and reasoning capability complicate the evaluation of language models, particularly concerning syntactic contamination?\nExplanation: The question probes the nuance between what a language model might recall syntactically versus what it understands conceptually. It requires a deep understanding of the nature of syntactic contamination and its implications for model evaluation.\nCorrect Answer: The distinction complicates evaluation because syntactic transformations in test data derived from training data can trigger a model's recall of memorized information rather than its reasoning capability, leading to inflated performance metrics that do not accurately reflect the model's true reasoning abilities.\n</output_format>",
    "question": "How does the distinction between syntactic recall and reasoning capability complicate the evaluation of language models, particularly concerning syntactic contamination?",
    "answer": "The distinction complicates evaluation because syntactic transformations in test data derived from training data can trigger a model's recall of memorized information rather than its reasoning capability, leading to inflated performance metrics that do not accurately reflect the model's true reasoning abilities.",
    "explanation": "The question probes the nuance between what a language model might recall syntactically versus what it understands conceptually. It requires a deep understanding of the nature of syntactic contamination and its implications for model evaluation.",
    "question_token_count": 23,
    "answer_correctness_score": 9,
    "explanation_validity_score": 8,
    "question_clarity_score": 9,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 47
  },
  {
    "context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n",
    "topic": "Examining a model's capability to integrate background knowledge and logical reasoning through tasks like CommonsenseQA.",
    "response": "<output_format>\nQuestion: How would a model's performance on tasks like CommonsenseQA differ when incorporating extensive pre-training on diverse datasets compared to fine-tuning on task-specific data alone? Provide a brief example to illustrate your point.\nExplanation: This question requires the respondent to reflect on the nuances between pre-training and fine-tuning approaches and their impact on a model's ability to handle commonsense reasoning tasks. It also invites a practical example, demonstrating a deeper understanding of the mechanisms involved.\nCorrect Answer: Models pre-trained on diverse datasets are likely to have a broader base of common sense knowledge, whereas fine-tuning on specific tasks may enhance task-specific reasoning but at the cost of less generalizable knowledge. For instance, a model pre-trained on a wide variety of texts might better grasp the context of idiomatic expressions, while a model fine-tuned on a narrow dataset might struggle with these.\n</output_format>",
    "question": "How would a model's performance on tasks like CommonsenseQA differ when incorporating extensive pre-training on diverse datasets compared to fine-tuning on task-specific data alone? Provide a brief example to illustrate your point.",
    "answer": "Models pre-trained on diverse datasets are likely to have a broader base of common sense knowledge, whereas fine-tuning on specific tasks may enhance task-specific reasoning but at the cost of less generalizable knowledge. For instance, a model pre-trained on a wide variety of texts might better grasp the context of idiomatic expressions, while a model fine-tuned on a narrow dataset might struggle with these.",
    "explanation": "This question requires the respondent to reflect on the nuances between pre-training and fine-tuning approaches and their impact on a model's ability to handle commonsense reasoning tasks. It also invites a practical example, demonstrating a deeper understanding of the mechanisms involved.",
    "question_token_count": 40,
    "answer_correctness_score": 8,
    "explanation_validity_score": 8,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 4,
    "avg_answer_token_count": 78
  },
  {
    "context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n",
    "topic": "Evaluating a model's ability to generate and debug code through benchmarks like HumanEval and MBPP.",
    "response": "<output_format>\nQuestion: How would a model's performance on HumanEval and MBPP benchmarks differ in terms of the types of challenges they present, and what specific skills does each benchmark primarily test?\nExplanation: This question requires a deep understanding of the nature and focus of both benchmarks. HumanEval and MBPP both test code synthesis and debugging, but they may differ in the complexity and variety of problems they present. The answer should highlight the nuances between the two benchmarks and explain the specific skills required for each.\nCorrect Answer: HumanEval tests a broader range of coding challenges, including synthesis and debugging tasks, while MBPP focuses on more advanced coding challenges. Both benchmarks primarily test a model's ability to generate correct and efficient code, but HumanEval also evaluates a model's ability to handle a wider variety of coding tasks and edge cases, whereas MBPP emphasizes complex problem-solving and robustness.\n</output_format>",
    "question": "How would a model's performance on HumanEval and MBPP benchmarks differ in terms of the types of challenges they present, and what specific skills does each benchmark primarily test?",
    "answer": "HumanEval tests a broader range of coding challenges, including synthesis and debugging tasks, while MBPP focuses on more advanced coding challenges. Both benchmarks primarily test a model's ability to generate correct and efficient code, but HumanEval also evaluates a model's ability to handle a wider variety of coding tasks and edge cases, whereas MBPP emphasizes complex problem-solving and robustness.",
    "explanation": "This question requires a deep understanding of the nature and focus of both benchmarks. HumanEval and MBPP both test code synthesis and debugging, but they may differ in the complexity and variety of problems they present. The answer should highlight the nuances between the two benchmarks and explain the specific skills required for each.",
    "question_token_count": 34,
    "answer_correctness_score": 7,
    "explanation_validity_score": 8,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 71
  },
  {
    "context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n",
    "topic": "Overview of static benchmarks used to evaluate model performance across various tasks such as arithmetic, natural language understanding, program synthesis, commonsense reasoning, factual knowledge, and toxicity detection.",
    "response": "<output_format>\nQuestion: How does the scoring function \\( \\mathcal{S}(\\cdot) \\) in a static benchmark contribute to evaluating the model's performance across diverse tasks, and what are the implications of using such benchmarks for ensuring comprehensive model evaluation?\nExplanation: The question requires a deep understanding of the role of the scoring function in assessing model performance across various tasks, and it probes the domain expert's ability to consider the broader implications of using static benchmarks.\nCorrect Answer: The scoring function \\( \\mathcal{S}(\\cdot) \\) evaluates the quality of an LLM's outputs by comparing them against the expected outputs \\( \\mathcal{Y} \\). This ensures that models are assessed based on their ability to solve arithmetic problems, understand natural language, synthesize programs, reason about common sense, retrieve factual knowledge, and detect toxic content. Using such benchmarks comprehensively evaluates models, but it also highlights limitations like potential biases and the need for diverse and representative data.\n</output_format>",
    "question": "How does the scoring function \\( \\mathcal{S}(\\cdot) \\) in a static benchmark contribute to evaluating the model's performance across diverse tasks, and what are the implications of using such benchmarks for ensuring comprehensive model evaluation?",
    "answer": "The scoring function \\( \\mathcal{S}(\\cdot) \\) evaluates the quality of an LLM's outputs by comparing them against the expected outputs \\( \\mathcal{Y} \\). This ensures that models are assessed based on their ability to solve arithmetic problems, understand natural language, synthesize programs, reason about common sense, retrieve factual knowledge, and detect toxic content. Using such benchmarks comprehensively evaluates models, but it also highlights limitations like potential biases and the need for diverse and representative data.",
    "explanation": "The question requires a deep understanding of the role of the scoring function in assessing model performance across various tasks, and it probes the domain expert's ability to consider the broader implications of using static benchmarks.",
    "question_token_count": 43,
    "answer_correctness_score": 9,
    "explanation_validity_score": 8,
    "question_clarity_score": 8,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 94
  },
  {
    "context": "###### Safety\nSafety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content. Datasets such as RealToxicityPrompts\u00a0(Gehman et\u00a0al., 2020) and ToxiGen\u00a0(Hartvigsen et\u00a0al., 2022) assess resilience against producing harmful outputs.\n\nBy providing a controlled environment to measure these aspects, safety benchmarks play a critical role in guiding the development of models that are not only powerful outputs but also responsible and trustworthy for real-world applications.\n\n\n###### Language\nLanguage benchmarks assess the LLMs\u2019 proficiency in specific languages.\n\nGLUE\u00a0(Wang, 2018) and SuperGLUE\u00a0(Wang et\u00a0al., 2019) cover tasks from sentiment analysis to language inference, while CLUE\u00a0(Xu et\u00a0al., 2020) targets Chinese language.\n\nTypo-fixing\u00a0(Suzgun et\u00a0al., 2022) is also widely used.\n\n\n###### Reading Comprehension\nReading comprehension tasks test a model\u2019s ability to extract and infer information from text. Benchmarks like SQuAD\u00a0(Rajpurkar et\u00a0al., 2018), QuAC\u00a0(Choi et\u00a0al., 2018), and BoolQ\u00a0(Clark et\u00a0al., 2019) challenge models to understand passages and draw logical conclusions.\n",
    "topic": "The comprehensive nature of language benchmarks, encompassing GLUE, SuperGLUE, CLUE, and Typo-fixing, and their respective roles in testing linguistic proficiency.",
    "response": "<output_format>\nQuestion: How does the diversity of language benchmarks like GLUE, SuperGLUE, CLUE, and Typo-fixing collectively contribute to evaluating the multifaceted linguistic capabilities of LLMs, and what unique challenges do each of these benchmarks present to model developers?\nExplanation: This question requires a deep understanding of the various aspects of language proficiency and the specific tasks each benchmark aims to evaluate. It encourages reflection on the comprehensive nature of language benchmarks and their unique contributions to assessing different linguistic skills.\nCorrect Answer: GLUE tests a broad range of natural language understanding and reasoning tasks, including sentiment analysis and language inference; SuperGLUE focuses on more complex tasks requiring higher-level reasoning and comprehension; CLUE specifically targets Chinese language proficiency; and Typo-fixing evaluates the ability to correct typographical errors, thus collectively covering a wide spectrum of linguistic capabilities.\n</output_format>",
    "question": "How does the diversity of language benchmarks like GLUE, SuperGLUE, CLUE, and Typo-fixing collectively contribute to evaluating the multifaceted linguistic capabilities of LLMs, and what unique challenges do each of these benchmarks present to model developers?",
    "answer": "GLUE tests a broad range of natural language understanding and reasoning tasks, including sentiment analysis and language inference; SuperGLUE focuses on more complex tasks requiring higher-level reasoning and comprehension; CLUE specifically targets Chinese language proficiency; and Typo-fixing evaluates the ability to correct typographical errors, thus collectively covering a wide spectrum of linguistic capabilities.",
    "explanation": "This question requires a deep understanding of the various aspects of language proficiency and the specific tasks each benchmark aims to evaluate. It encourages reflection on the comprehensive nature of language benchmarks and their unique contributions to assessing different linguistic skills.",
    "question_token_count": 51,
    "answer_correctness_score": 9,
    "explanation_validity_score": 8,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 69
  },
  {
    "context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n",
    "topic": "Multi-round interaction techniques for evaluating LLMs (LLM-as-an-Interviewer, TreeEval, KIEval)",
    "response": "<output_format>\nQuestion: How might an LLM-as-an-Interviewer system adapt its follow-up questions in real-time based on the nuanced responses of the evaluated LLM, and what theoretical framework underpins such adaptive questioning strategies?\nExplanation: This question encourages a deep understanding of the dynamic nature of the LLM-as-an-Interviewer technique, requiring the respondent to consider both the practical application of real-time adaptation and the underlying theoretical principles guiding such an approach.\nCorrect Answer: An LLM-as-an-Interviewer system might adapt its follow-up questions based on the evaluated LLM's response patterns and nuances, potentially using natural language processing and machine learning models to predict effective follow-ups. The theoretical framework could be grounded in cognitive psychology, educational theory, and dialogue systems.\n</output_format>",
    "question": "How might an LLM-as-an-Interviewer system adapt its follow-up questions in real-time based on the nuanced responses of the evaluated LLM, and what theoretical framework underpins such adaptive questioning strategies?",
    "answer": "An LLM-as-an-Interviewer system might adapt its follow-up questions based on the evaluated LLM's response patterns and nuances, potentially using natural language processing and machine learning models to predict effective follow-ups. The theoretical framework could be grounded in cognitive psychology, educational theory, and dialogue systems.",
    "explanation": "This question encourages a deep understanding of the dynamic nature of the LLM-as-an-Interviewer technique, requiring the respondent to consider both the practical application of real-time adaptation and the underlying theoretical principles guiding such an approach.",
    "question_token_count": 40,
    "answer_correctness_score": 8,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 6,
    "avg_answer_token_count": 58
  },
  {
    "context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n",
    "topic": "The unique challenges of data contamination in Large Language Models, including the use of massive, diverse training corpora and the risks associated with fine-tuning on similar datasets.",
    "response": "<output_format>\nQuestion: Given the proprietary nature of LLM training data and the difficulty in detecting evaluation data overlap, what critical implications does this have for the development of reliable benchmarks and the overall trustworthiness of LLM performance assessments?\nExplanation: This question probes the domain expert's understanding of the complex interplay between proprietary training data, the risk of data contamination, and its impact on benchmarking and trust in LLMs. It requires a deep reflection on the ethical and practical challenges of ensuring transparency and fairness in model evaluation.\nCorrect Answer: It undermines the reliability of benchmarks, reduces trust in LLM performance assessments, and highlights the need for more transparent and accessible evaluation processes.\n</output_format>",
    "question": "Given the proprietary nature of LLM training data and the difficulty in detecting evaluation data overlap, what critical implications does this have for the development of reliable benchmarks and the overall trustworthiness of LLM performance assessments?",
    "answer": "It undermines the reliability of benchmarks, reduces trust in LLM performance assessments, and highlights the need for more transparent and accessible evaluation processes.",
    "explanation": "This question probes the domain expert's understanding of the complex interplay between proprietary training data, the risk of data contamination, and its impact on benchmarking and trust in LLMs. It requires a deep reflection on the ethical and practical challenges of ensuring transparency and fairness in model evaluation.",
    "question_token_count": 40,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 28
  },
  {
    "context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n",
    "topic": "Tasks covered by the benchmarks including arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and reading comprehension.",
    "response": "<document_analysis>\nThe context discusses a collection of static benchmarks used to evaluate model performance across various tasks. It mentions a general form of a static benchmark, defined by a seed dataset containing input prompts, expected outputs, and a scoring function. The context lists several types of tasks covered by these benchmarks, including arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and reading comprehension. The question should probe the expert's understanding of the breadth and depth of these tasks and their interconnections.\n\nGiven the complexity and the need for a deep understanding of these tasks, the difficulty is set to 8 out of 10. The question should encourage reflection on how these tasks interrelate and contribute to overall model evaluation.\n</document_analysis>\n\n<output_format>\nQuestion: How do the tasks of arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and reading comprehension interrelate in evaluating a model's overall performance, and what unique insights does each task provide into different facets of a model's capabilities?\nExplanation: This question encourages reflection on the comprehensive nature of the benchmarks and requires a deep understanding of how each task contributes to evaluating a model's performance. It probes the ability to synthesize knowledge across multiple domains.\nCorrect Answer: Arithmetic problem-solving and natural language understanding provide insights into numerical reasoning and linguistic capabilities; program synthesis and commonsense reasoning assess a model's ability to generate executable code and apply common-sense knowledge; factual knowledge retrieval and toxicity detection measure the model's accuracy in factual recall and ethical reasoning; reading comprehension gauges the model's ability to understand and summarize complex texts.\n</output_format>",
    "question": "How do the tasks of arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and reading comprehension interrelate in evaluating a model's overall performance, and what unique insights does each task provide into different facets of a model's capabilities?",
    "answer": "Arithmetic problem-solving and natural language understanding provide insights into numerical reasoning and linguistic capabilities; program synthesis and commonsense reasoning assess a model's ability to generate executable code and apply common-sense knowledge; factual knowledge retrieval and toxicity detection measure the model's accuracy in factual recall and ethical reasoning; reading comprehension gauges the model's ability to understand and summarize complex texts.",
    "explanation": "This question encourages reflection on the comprehensive nature of the benchmarks and requires a deep understanding of how each task contributes to evaluating a model's performance. It probes the ability to synthesize knowledge across multiple domains.",
    "question_token_count": 57,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 8,
    "avg_answer_token_count": 68
  },
  {
    "context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n",
    "topic": "Techniques to Address Sample Diversity and Contamination in LLMs",
    "response": "<output_format>\nQuestion: How might an advanced contamination detection algorithm identify and mitigate the issue of in-distribution contamination in rule-generated data, and what specific strategies can be employed to ensure the generated samples maintain their original difficulty levels while addressing such contamination?\nExplanation: This question probes the domain expert's understanding of both the nature of in-distribution contamination and the sophisticated methods required to address it. It requires knowledge of contamination detection algorithms and the ability to think critically about maintaining sample difficulty while ensuring diversity and reducing contamination.\nCorrect Answer: An advanced contamination detection algorithm would first use statistical and machine learning techniques to identify patterns indicative of contamination, followed by prompting LLMs to rephrase or rewrite identified samples while preserving their original cognitive complexity and essential knowledge. Strategies might include using knowledge graphs to expand on concepts, employing variable substitution methods to diversify samples, and leveraging extended questioning techniques to explore related topics at various cognitive levels.\n</output_format>",
    "question": "How might an advanced contamination detection algorithm identify and mitigate the issue of in-distribution contamination in rule-generated data, and what specific strategies can be employed to ensure the generated samples maintain their original difficulty levels while addressing such contamination?",
    "answer": "An advanced contamination detection algorithm would first use statistical and machine learning techniques to identify patterns indicative of contamination, followed by prompting LLMs to rephrase or rewrite identified samples while preserving their original cognitive complexity and essential knowledge. Strategies might include using knowledge graphs to expand on concepts, employing variable substitution methods to diversify samples, and leveraging extended questioning techniques to explore related topics at various cognitive levels.",
    "explanation": "This question probes the domain expert's understanding of both the nature of in-distribution contamination and the sophisticated methods required to address it. It requires knowledge of contamination detection algorithms and the ability to think critically about maintaining sample difficulty while ensuring diversity and reducing contamination.",
    "question_token_count": 44,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 7,
    "question_difficulty_score": 9,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 77
  },
  {
    "context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n",
    "topic": "The role of model trainers and developers in responding to canary string markers.",
    "response": "<output_format>\nQuestion: How might a model developer's lack of awareness or responsiveness to canary string markers compromise the integrity of a dataset used for benchmarking?\nExplanation: This question probes the importance of model trainers' vigilance in identifying and mitigating the risk of data contamination. It challenges the respondent to consider the potential consequences of negligence in monitoring for canary strings.\nCorrect Answer: A model developer's lack of awareness or responsiveness to canary string markers can allow memorization of training data to pass undetected, leading to unreliable benchmarking results.\n</output_format>",
    "question": "How might a model developer's lack of awareness or responsiveness to canary string markers compromise the integrity of a dataset used for benchmarking?",
    "answer": "A model developer's lack of awareness or responsiveness to canary string markers can allow memorization of training data to pass undetected, leading to unreliable benchmarking results.",
    "explanation": "This question probes the importance of model trainers' vigilance in identifying and mitigating the risk of data contamination. It challenges the respondent to consider the potential consequences of negligence in monitoring for canary strings.",
    "question_token_count": 27,
    "answer_correctness_score": 9,
    "explanation_validity_score": 8,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 33
  },
  {
    "context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n",
    "topic": "Formal definition of a static benchmark, including its components: seed dataset, input prompts, expected outputs, and scoring function.",
    "response": "<document_analysis>\nThe context provides a detailed description of static benchmarks and their formal definition. It outlines the components of a static benchmark, which include the seed dataset, input prompts, expected outputs, and a scoring function. The context is relevant and directly addresses the question topic, making it suitable for generating a challenging question.\n</document_analysis>\n\n<output_format>\nQuestion: What is the formal definition of a static benchmark, and what are the four key components that define it? How would you describe each component in the context of evaluating an LLM's performance?\nExplanation: The question requires a deep understanding of the static benchmark concept and its components. It asks for a comprehensive description of each part, encouraging reflection on how they collectively contribute to the evaluation of LLMs.\nCorrect Answer: A static benchmark is defined by four key components: (1) the seed dataset (\\(\\mathcal{D}\\)), (2) input prompts (\\(\\mathcal{X}\\)), (3) expected outputs (\\(\\mathcal{Y}\\)), and (4) a scoring function (\\(\\mathcal{S}(\\cdot)\\)). The seed dataset is the collection of training data used to form the benchmark. Input prompts are the problems or tasks presented to the LLM. Expected outputs are the correct responses or solutions that the LLM should aim to match. The scoring function evaluates the accuracy of the LLM's outputs by comparing them to the expected outputs.\n</output_format>",
    "question": "What is the formal definition of a static benchmark, and what are the four key components that define it? How would you describe each component in the context of evaluating an LLM's performance?",
    "answer": "A static benchmark is defined by four key components: (1) the seed dataset (\\(\\mathcal{D}\\)), (2) input prompts (\\(\\mathcal{X}\\)), (3) expected outputs (\\(\\mathcal{Y}\\)), and (4) a scoring function (\\(\\mathcal{S}(\\cdot)\\)). The seed dataset is the collection of training data used to form the benchmark. Input prompts are the problems or tasks presented to the LLM. Expected outputs are the correct responses or solutions that the LLM should aim to match. The scoring function evaluates the accuracy of the LLM's outputs by comparing them to the expected outputs.",
    "explanation": "The question requires a deep understanding of the static benchmark concept and its components. It asks for a comprehensive description of each part, encouraging reflection on how they collectively contribute to the evaluation of LLMs.",
    "question_token_count": 38,
    "answer_correctness_score": 8,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 118
  },
  {
    "context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n",
    "topic": "Dynamic benchmark creation using multi-agent systems (Benchmark Self-Evolving, BENCHAGENTS)",
    "response": "<output_format>\nQuestion: How does the multi-agent framework in BENCHAGENTS dynamically extend static benchmarks, and what is the role of human-in-the-loop feedback in this process?\nExplanation: This question requires a deep understanding of the BENCHAGENTS system's architecture and the unique contribution of human-in-the-loop feedback in enhancing its dynamic benchmark creation capabilities. It probes the respondent's ability to explain the intricacies of a multi-agent framework and the importance of human oversight in generating high-quality benchmarks.\nCorrect Answer: BENCHAGENTS uses a multi-agent framework to split the benchmark creation process into planning, generation, verification, and evaluation stages, each managed by a specialized LLM agent. Human-in-the-loop feedback is crucial for providing qualitative assessments and guiding the iterative refinement of the generated benchmarks.\n</output_format>",
    "question": "How does the multi-agent framework in BENCHAGENTS dynamically extend static benchmarks, and what is the role of human-in-the-loop feedback in this process?",
    "answer": "BENCHAGENTS uses a multi-agent framework to split the benchmark creation process into planning, generation, verification, and evaluation stages, each managed by a specialized LLM agent. Human-in-the-loop feedback is crucial for providing qualitative assessments and guiding the iterative refinement of the generated benchmarks.",
    "explanation": "This question requires a deep understanding of the BENCHAGENTS system's architecture and the unique contribution of human-in-the-loop feedback in enhancing its dynamic benchmark creation capabilities. It probes the respondent's ability to explain the intricacies of a multi-agent framework and the importance of human oversight in generating high-quality benchmarks.",
    "question_token_count": 32,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 56
  },
  {
    "context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Critically assessing the criteria for designing effective dynamic benchmarks.",
    "response": "<document_analysis>\nThe context provides a detailed overview of the challenges and advancements in evaluating large language models (LLMs) through both static and dynamic benchmarking methods. The core of the document focuses on the critical issue of data contamination and proposes the need for robust criteria to evaluate dynamic benchmarks. Given this, the question should delve into the nuances of these criteria and their importance in the field of NLP.\n\nThe text mentions the need for a systematic survey of benchmarking methods and the introduction of criteria for evaluating dynamic benchmarks. It highlights the importance of these criteria in addressing the limitations of existing methods and ensuring transparency in LLM evaluation. The challenge lies in formulating a question that probes the domain expert's understanding of these criteria and their implications.\n\nKey points to consider:\n1. The definition and importance of dynamic benchmarks.\n2. The criteria proposed for evaluating dynamic benchmarks.\n3. The limitations of existing static benchmarks and how dynamic benchmarks aim to address these.\n4. The need for transparency and fairness in benchmarking practices.\n\nThe question should be crafted to encourage a deep reflection on the proposed criteria and their application in practice.\n</document_analysis>\n\n<output_format>\nQuestion: What are the key criteria proposed for evaluating dynamic benchmarks in the context of mitigating data contamination in large language models (LLMs), and how do they address the limitations of traditional static benchmarks?\nExplanation: This question requires a deep understanding of the proposed criteria and their significance. It encourages the respondent to reflect on the core issues of data contamination and the need for more transparent and effective benchmarking methods.\nCorrect Answer: Key criteria for evaluating dynamic benchmarks include timeliness of benchmark updates, data provenance, reusability of benchmark data, and fairness in the evaluation process. These criteria address the limitations of static benchmarks by ensuring that the benchmarks remain relevant, traceable, and free from contamination, thus providing a fair and transparent evaluation of LLMs.\n</output_format>",
    "question": "What are the key criteria proposed for evaluating dynamic benchmarks in the context of mitigating data contamination in large language models (LLMs), and how do they address the limitations of traditional static benchmarks?",
    "answer": "Key criteria for evaluating dynamic benchmarks include timeliness of benchmark updates, data provenance, reusability of benchmark data, and fairness in the evaluation process. These criteria address the limitations of static benchmarks by ensuring that the benchmarks remain relevant, traceable, and free from contamination, thus providing a fair and transparent evaluation of LLMs.",
    "explanation": "This question requires a deep understanding of the proposed criteria and their significance. It encourages the respondent to reflect on the core issues of data contamination and the need for more transparent and effective benchmarking methods.",
    "question_token_count": 38,
    "answer_correctness_score": 9,
    "explanation_validity_score": 8,
    "question_clarity_score": 9,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 65
  },
  {
    "context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n",
    "topic": "The role of \"No Derivatives\" licenses in encryption for evaluation data.",
    "response": "<output_format>\nQuestion: How does the \"No Derivatives\" license component in Jacovi et al.'s (2023) encryption method contribute to the security of evaluation data, and what alternative measures might still be necessary to fully mitigate the risk of unauthorized data usage?\nExplanation: While the \"No Derivatives\" license aims to prevent automated crawling and reuse of the encrypted data, it does not address computational vulnerabilities or the risk of manual exploitation. A comprehensive security strategy would need to consider both technical and legal protections.\nCorrect Answer: The \"No Derivatives\" license prevents automated reuse but does not fully secure against computational attacks; strong key management and robust encryption techniques are essential.\n</output_format>",
    "question": "How does the \"No Derivatives\" license component in Jacovi et al.'s (2023) encryption method contribute to the security of evaluation data, and what alternative measures might still be necessary to fully mitigate the risk of unauthorized data usage?",
    "answer": "The \"No Derivatives\" license prevents automated reuse but does not fully secure against computational attacks; strong key management and robust encryption techniques are essential.",
    "explanation": "While the \"No Derivatives\" license aims to prevent automated crawling and reuse of the encrypted data, it does not address computational vulnerabilities or the risk of manual exploitation. A comprehensive security strategy would need to consider both technical and legal protections.",
    "question_token_count": 49,
    "answer_correctness_score": 9,
    "explanation_validity_score": 8,
    "question_clarity_score": 7,
    "question_difficulty_score": 6,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 29
  },
  {
    "context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n",
    "topic": "The importance of robust key management in encryption systems.",
    "response": "<output_format>\nQuestion: How does the compromise of encryption keys impact the security of evaluation data, and what measures can be taken to mitigate this risk?\nExplanation: This question requires the respondent to understand the critical role of key management in securing evaluation data through encryption. It probes their ability to recognize vulnerabilities and suggest mitigation strategies, which is essential for maintaining the integrity and confidentiality of the data.\nCorrect Answer: Compromise of encryption keys can render the entire encryption system ineffective, allowing unauthorized access to protected data. Mitigation measures include using strong key management practices, regular key rotation, and secure key storage.\n</output_format>",
    "question": "How does the compromise of encryption keys impact the security of evaluation data, and what measures can be taken to mitigate this risk?",
    "answer": "Compromise of encryption keys can render the entire encryption system ineffective, allowing unauthorized access to protected data. Mitigation measures include using strong key management practices, regular key rotation, and secure key storage.",
    "explanation": "This question requires the respondent to understand the critical role of key management in securing evaluation data through encryption. It probes their ability to recognize vulnerabilities and suggest mitigation strategies, which is essential for maintaining the integrity and confidentiality of the data.",
    "question_token_count": 26,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 9,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 40
  },
  {
    "context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n",
    "topic": "Evaluation of Multi-step Math Problem Solving Abilities using GSM8K and MATH Datasets.",
    "response": "<output_format>\nQuestion: Given that GSM8K and MATH datasets require models to solve complex multi-step math problems, how would you design a new dataset that builds upon these benchmarks to specifically test a model's ability to handle problems involving both algebraic manipulation and geometric reasoning, and what unique features would this dataset include?\n\nExplanation: This question requires a deep understanding of the current benchmarks and the ability to synthesize knowledge about different types of mathematical problems. It also encourages the respondent to think creatively about designing a new dataset that combines elements from existing benchmarks, thereby testing a broader range of mathematical skills.\nCorrect Answer: A new dataset combining algebraic manipulation and geometric reasoning could include problems that require solving equations to find geometric properties, or vice versa. Unique features might include:\n- Problems that start with an algebraic equation and require finding the geometric representation.\n- Problems that involve interpreting geometric figures to formulate algebraic expressions.\n- Real-world scenarios that necessitate applying both algebraic and geometric concepts simultaneously.\n</output_format>",
    "question": "Given that GSM8K and MATH datasets require models to solve complex multi-step math problems, how would you design a new dataset that builds upon these benchmarks to specifically test a model's ability to handle problems involving both algebraic manipulation and geometric reasoning, and what unique features would this dataset include?",
    "answer": "A new dataset combining algebraic manipulation and geometric reasoning could include problems that require solving equations to find geometric properties, or vice versa. Unique features might include:",
    "explanation": "This question requires a deep understanding of the current benchmarks and the ability to synthesize knowledge about different types of mathematical problems. It also encourages the respondent to think creatively about designing a new dataset that combines elements from existing benchmarks, thereby testing a broader range of mathematical skills.",
    "question_token_count": 57,
    "answer_correctness_score": 6,
    "explanation_validity_score": 7,
    "question_clarity_score": 9,
    "question_difficulty_score": 8,
    "question_groundedness_score": 8,
    "avg_answer_token_count": 31
  },
  {
    "context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n",
    "topic": "Label protection techniques and their implementation in benchmarks.",
    "response": "<output_format>\nQuestion: How does the principle of label protection differ from encryption methods in maintaining evaluation integrity, and what are the specific challenges each faces in practical implementation?\nExplanation: The question encourages a deep comparison between label protection and encryption methods, highlighting the distinct approaches to maintaining evaluation integrity. It prompts an analysis of the advantages and limitations of both techniques, fostering a nuanced understanding of their respective roles and challenges.\nCorrect Answer: Label protection ensures that true answers are hidden from public access, making it difficult for models to learn or memorize them, whereas encryption methods secure data by making it inaccessible to unauthorized parties. However, label protection relies on strict access control and confidentiality agreements, while encryption methods face challenges related to key management and computational overhead.\n</output_format>",
    "question": "How does the principle of label protection differ from encryption methods in maintaining evaluation integrity, and what are the specific challenges each faces in practical implementation?",
    "answer": "Label protection ensures that true answers are hidden from public access, making it difficult for models to learn or memorize them, whereas encryption methods secure data by making it inaccessible to unauthorized parties. However, label protection relies on strict access control and confidentiality agreements, while encryption methods face challenges related to key management and computational overhead.",
    "explanation": "The question encourages a deep comparison between label protection and encryption methods, highlighting the distinct approaches to maintaining evaluation integrity. It prompts an analysis of the advantages and limitations of both techniques, fostering a nuanced understanding of their respective roles and challenges.",
    "question_token_count": 29,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 63
  },
  {
    "context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n",
    "topic": "How graph complexity can be used as a specific metric to evaluate reasoning problem complexity in dynamic benchmarks.",
    "response": "<document_analysis>\nThe context discusses the importance of accounting for complexity in dynamic benchmarks and mentions DyVal\u00a0(Zhu et\u00a0al., 2024a) as an example of a method that uses graph complexity to evaluate reasoning problem complexity. The key points are:\n1. The need to measure complexity to distinguish between performance drops due to data contamination and increased task complexity.\n2. Graph complexity as a specific metric for evaluating reasoning problems.\n3. The formulation of stability as the variance in complexity across different trials.\n\nTo ensure the question is highly insightful and challenging, we will focus on the application and implications of using graph complexity as a metric. This will encourage reflection on the nuances and limitations of such a metric.\n</document_analysis>\n\n<output_format>\nQuestion: How does the use of graph complexity as a metric for evaluating reasoning problem complexity in dynamic benchmarks address the challenges of measuring task complexity, and what are its limitations?\nExplanation: The question invites a deep reflection on the specificities of graph complexity as a metric, its applicability, and the limitations inherent in its use, requiring a sophisticated understanding of both the methodology and the broader context of benchmarking.\nCorrect Answer: Graph complexity addresses the challenge of measuring task complexity by providing a structured approach to quantify the intricacy of reasoning problems. However, it may not generalize well across different domains and lacks a standardized method for all types of reasoning tasks.\n</output_format>",
    "question": "How does the use of graph complexity as a metric for evaluating reasoning problem complexity in dynamic benchmarks address the challenges of measuring task complexity, and what are its limitations?",
    "answer": "Graph complexity addresses the challenge of measuring task complexity by providing a structured approach to quantify the intricacy of reasoning problems. However, it may not generalize well across different domains and lacks a standardized method for all types of reasoning tasks.",
    "explanation": "The question invites a deep reflection on the specificities of graph complexity as a metric, its applicability, and the limitations inherent in its use, requiring a sophisticated understanding of both the methodology and the broader context of benchmarking.",
    "question_token_count": 33,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 46
  },
  {
    "context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n",
    "topic": "Lack of Standardized Criteria for Evaluating Dynamic Benchmarks",
    "response": "<output_format>\nQuestion: Given the identified gap in the lack of standardized criteria for evaluating dynamic benchmarks, propose a set of key dimensions or metrics that a domain expert would consider essential for designing such a standard. How would these dimensions ensure a more rigorous evaluation of dynamic benchmarks in the context of mitigating data contamination in large language models?\nExplanation: This question requires a deep understanding of the challenges in evaluating dynamic benchmarks and the ability to synthesize a framework that addresses these challenges. It encourages reflection on the nuances of benchmarking methodologies and the criteria necessary for robust evaluation.\nCorrect Answer: A set of key dimensions could include: (1) Representativeness of test cases, (2) Robustness against various types of data contamination, (3) Scalability and computational efficiency, (4) Transparency and reproducibility, (5) Comprehensive coverage of linguistic and contextual aspects, and (6) Integration with current and emerging evaluation frameworks.\n</output_format>",
    "question": "Given the identified gap in the lack of standardized criteria for evaluating dynamic benchmarks, propose a set of key dimensions or metrics that a domain expert would consider essential for designing such a standard. How would these dimensions ensure a more rigorous evaluation of dynamic benchmarks in the context of mitigating data contamination in large language models?",
    "answer": "A set of key dimensions could include: (1) Representativeness of test cases, (2) Robustness against various types of data contamination, (3) Scalability and computational efficiency, (4) Transparency and reproducibility, (5) Comprehensive coverage of linguistic and contextual aspects, and (6) Integration with current and emerging evaluation frameworks.",
    "explanation": "This question requires a deep understanding of the challenges in evaluating dynamic benchmarks and the ability to synthesize a framework that addresses these challenges. It encourages reflection on the nuances of benchmarking methodologies and the criteria necessary for robust evaluation.",
    "question_token_count": 61,
    "answer_correctness_score": 9,
    "explanation_validity_score": 8,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 70
  },
  {
    "context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n",
    "topic": "Optimal Design Principles for Dynamic Benchmarking",
    "response": "<output_format>\nQuestion: What are the key challenges in transitioning from static to dynamic benchmarking for mitigating data contamination in LLMs, and how do these challenges affect the design of optimal benchmarking principles?\nExplanation: This question requires a deep understanding of the limitations and gaps identified in existing static benchmarks and the proposed optimal design principles for dynamic benchmarks. It probes the domain expert's ability to critically analyze the transition process and recognize the implications for designing effective benchmarking methods.\nCorrect Answer: Key challenges include variability in data sources, dynamic nature of internet content, and the need for real-time evaluation mechanisms. These challenges necessitate robust, adaptable, and scalable benchmarking principles that can effectively evaluate and mitigate data contamination risks in dynamic environments.\n</output_format>",
    "question": "What are the key challenges in transitioning from static to dynamic benchmarking for mitigating data contamination in LLMs, and how do these challenges affect the design of optimal benchmarking principles?",
    "answer": "Key challenges include variability in data sources, dynamic nature of internet content, and the need for real-time evaluation mechanisms. These challenges necessitate robust, adaptable, and scalable benchmarking principles that can effectively evaluate and mitigate data contamination risks in dynamic environments.",
    "explanation": "This question requires a deep understanding of the limitations and gaps identified in existing static benchmarks and the proposed optimal design principles for dynamic benchmarks. It probes the domain expert's ability to critically analyze the transition process and recognize the implications for designing effective benchmarking methods.",
    "question_token_count": 34,
    "answer_correctness_score": 8,
    "explanation_validity_score": 8,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 6,
    "avg_answer_token_count": 49
  },
  {
    "context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n",
    "topic": "Impact of Training Corpus Size on Benchmark Contamination",
    "response": "<output_format>\nQuestion: Given the relationship \\( \\text{Pr}_{\\text{contam}} \\propto \\frac{|\\mathcal{D}_{\\text{train}}|}{|\\mathcal{D}_{\\text{test}}|} \\) described in the context, how would increasing the size of the training corpus (\\(\\mathcal{D}_{\\text{train}}\\)) impact the likelihood of contamination in LLM benchmarks, and what implications does this have for the design of future benchmarks?\n\nExplanation: The relationship between the probability of contamination (\\(\\text{Pr}_{\\text{contam}}\\)) and the sizes of the training and testing datasets indicates that as the training dataset grows, the likelihood of contamination increases relative to the size of the test dataset. This implies that larger training corpora pose a greater risk of introducing contamination, which has significant implications for the design of benchmarks, especially in the context of web-scale data where contamination becomes more probable.\n\nCorrect Answer: Increasing the size of the training corpus (\\(\\mathcal{D}_{\\text{train}}\\)) would increase the likelihood of contamination in LLM benchmarks, making traditional static benchmarks less effective and highlighting the need for more robust and dynamic benchmarking strategies.\n</output_format>",
    "question": "Given the relationship \\( \\text{Pr}_{\\text{contam}} \\propto \\frac{|\\mathcal{D}_{\\text{train}}|}{|\\mathcal{D}_{\\text{test}}|} \\) described in the context, how would increasing the size of the training corpus (\\(\\mathcal{D}_{\\text{train}}\\)) impact the likelihood of contamination in LLM benchmarks, and what implications does this have for the design of future benchmarks?",
    "answer": "Increasing the size of the training corpus (\\(\\mathcal{D}_{\\text{train}}\\)) would increase the likelihood of contamination in LLM benchmarks, making traditional static benchmarks less effective and highlighting the need for more robust and dynamic benchmarking strategies.",
    "explanation": "The relationship between the probability of contamination (\\(\\text{Pr}_{\\text{contam}}\\)) and the sizes of the training and testing datasets indicates that as the training dataset grows, the likelihood of contamination increases relative to the size of the test dataset. This implies that larger training corpora pose a greater risk of introducing contamination, which has significant implications for the design of benchmarks, especially in the context of web-scale data where contamination becomes more probable.",
    "question_token_count": 85,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 45
  },
  {
    "context": "###### Safety\nSafety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content. Datasets such as RealToxicityPrompts\u00a0(Gehman et\u00a0al., 2020) and ToxiGen\u00a0(Hartvigsen et\u00a0al., 2022) assess resilience against producing harmful outputs.\n\nBy providing a controlled environment to measure these aspects, safety benchmarks play a critical role in guiding the development of models that are not only powerful outputs but also responsible and trustworthy for real-world applications.\n\n\n###### Language\nLanguage benchmarks assess the LLMs\u2019 proficiency in specific languages.\n\nGLUE\u00a0(Wang, 2018) and SuperGLUE\u00a0(Wang et\u00a0al., 2019) cover tasks from sentiment analysis to language inference, while CLUE\u00a0(Xu et\u00a0al., 2020) targets Chinese language.\n\nTypo-fixing\u00a0(Suzgun et\u00a0al., 2022) is also widely used.\n\n\n###### Reading Comprehension\nReading comprehension tasks test a model\u2019s ability to extract and infer information from text. Benchmarks like SQuAD\u00a0(Rajpurkar et\u00a0al., 2018), QuAC\u00a0(Choi et\u00a0al., 2018), and BoolQ\u00a0(Clark et\u00a0al., 2019) challenge models to understand passages and draw logical conclusions.\n",
    "topic": "The evaluation criteria for reading comprehension benchmarks, exemplified by SQuAD, QuAC, and BoolQ, focusing on the extraction and inference of information from text.",
    "response": "<document_analysis>\nThe provided context discusses various benchmarks used to evaluate language models, emphasizing their importance in assessing a model's robustness, language proficiency, and reading comprehension skills. The context does not contain any hyperlinks, advertisements, or irrelevant information that would detract from the main discussion. Therefore, the content is deemed relevant and suitable for generating a challenging question.\n\nFor the reading comprehension benchmarks (SQuAD, QuAC, and BoolQ), the focus should be on the extraction and inference of information from text. A highly challenging question would require the respondent to deeply understand the nature of these benchmarks and apply this understanding to a scenario that goes beyond the specifics of the examples given.\n</document_analysis>\n\n<output_format>\nQuestion: How would you design a new reading comprehension benchmark that challenges LLMs to infer information that requires synthesizing multiple sentences across a passage, considering the nuances of implicit meanings and contextual dependencies, and what specific tasks or questions would you include to ensure that the model demonstrates a deep understanding of the text?\n\nExplanation: This question requires the respondent to think critically about the design of reading comprehension benchmarks, considering the complexities of inferring nuanced information from text. It probes the ability to create tasks that go beyond surface-level comprehension and require a sophisticated understanding of context and implication.\nCorrect Answer: A new benchmark would include multi-sentence inference tasks where the model must synthesize information across different parts of the passage. Specific tasks might involve questions that require understanding implicit meanings, drawing logical conclusions from contextual clues, and making inferences based on the nuances of the text.\n</output_format>",
    "question": "How would you design a new reading comprehension benchmark that challenges LLMs to infer information that requires synthesizing multiple sentences across a passage, considering the nuances of implicit meanings and contextual dependencies, and what specific tasks or questions would you include to ensure that the model demonstrates a deep understanding of the text?",
    "answer": "A new benchmark would include multi-sentence inference tasks where the model must synthesize information across different parts of the passage. Specific tasks might involve questions that require understanding implicit meanings, drawing logical conclusions from contextual clues, and making inferences based on the nuances of the text.",
    "explanation": "This question requires the respondent to think critically about the design of reading comprehension benchmarks, considering the complexities of inferring nuanced information from text. It probes the ability to create tasks that go beyond surface-level comprehension and require a sophisticated understanding of context and implication.",
    "question_token_count": 59,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 9,
    "question_difficulty_score": 8,
    "question_groundedness_score": 6,
    "avg_answer_token_count": 53
  },
  {
    "context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "The definition and structure of a dynamic benchmark dataset, including the role of the transformation function T(\u22c5).",
    "response": "<output_format>\nQuestion: How does the transformation function \\( T(\\cdot) \\) impact the creation and evaluation of a dynamic benchmark dataset in the context of evaluating language model performance?\nExplanation: The transformation function \\( T(\\cdot) \\) plays a crucial role in avoiding data contamination by modifying the dataset dynamically. This ensures that the evaluation process remains faithful and transparent, addressing the limitations of static benchmarking methods.\nCorrect Answer: The transformation function \\( T(\\cdot) \\) modifies the dataset during benchmarking to prevent data contamination, thereby ensuring the integrity of the evaluation process.\n</output_format>",
    "question": "How does the transformation function \\( T(\\cdot) \\) impact the creation and evaluation of a dynamic benchmark dataset in the context of evaluating language model performance?",
    "answer": "The transformation function \\( T(\\cdot) \\) modifies the dataset during benchmarking to prevent data contamination, thereby ensuring the integrity of the evaluation process.",
    "explanation": "The transformation function \\( T(\\cdot) \\) plays a crucial role in avoiding data contamination by modifying the dataset dynamically. This ensures that the evaluation process remains faithful and transparent, addressing the limitations of static benchmarking methods.",
    "question_token_count": 31,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 5,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 29
  },
  {
    "context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n",
    "topic": "Managing the transparency and potential misuse of benchmarking results in LLM evaluations.",
    "response": "<output_format>\nQuestion: How can an ethical framework ensure that the use of LLM benchmarking results does not lead to artificial inflation of model performance or the selection of biased evaluation criteria, thereby maintaining fairness and transparency?\nExplanation: This question requires a deep understanding of the potential misuses of benchmarking results and the need for robust ethical guidelines to prevent such issues. It challenges the respondent to consider both the technical and ethical dimensions of LLM evaluations.\nCorrect Answer: An ethical framework must include mechanisms for independent validation of benchmark results, clear documentation of the evaluation process, and regular audits to detect and mitigate biases. Additionally, it should establish guidelines for transparent reporting and discourage practices that artificially inflate performance metrics or rely on biased datasets.\n</output_format>",
    "question": "How can an ethical framework ensure that the use of LLM benchmarking results does not lead to artificial inflation of model performance or the selection of biased evaluation criteria, thereby maintaining fairness and transparency?",
    "answer": "An ethical framework must include mechanisms for independent validation of benchmark results, clear documentation of the evaluation process, and regular audits to detect and mitigate biases. Additionally, it should establish guidelines for transparent reporting and discourage practices that artificially inflate performance metrics or rely on biased datasets.",
    "explanation": "This question requires a deep understanding of the potential misuses of benchmarking results and the need for robust ethical guidelines to prevent such issues. It challenges the respondent to consider both the technical and ethical dimensions of LLM evaluations.",
    "question_token_count": 37,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 53
  },
  {
    "context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n",
    "topic": "Assessing a model's capacity to follow detailed instructions through datasets such as IFEval and InfoBench.",
    "response": "<output_format>\nQuestion: How would a model performing well on the IFEval benchmark differ from one excelling at the InfoBench benchmark in terms of required skills and problem-solving strategies?\nExplanation: This question requires a deep understanding of both IFEval and InfoBench, their specific requirements, and the nuances between them. It prompts a reflection on the distinct skill sets and strategies that each benchmark tests, encouraging a thorough comparison and highlighting the unique aspects of instruction-following versus detailed directive execution.\nCorrect Answer: A model excelling on IFEval would primarily demonstrate the ability to understand and execute step-by-step, detailed instructions in a variety of contexts, possibly including complex tasks and multi-step processes. In contrast, a model performing well on InfoBench would show proficiency in comprehending and executing instructions that simulate real-world scenarios, emphasizing clarity and precision in following directives.\n</output_format>",
    "question": "How would a model performing well on the IFEval benchmark differ from one excelling at the InfoBench benchmark in terms of required skills and problem-solving strategies?",
    "answer": "A model excelling on IFEval would primarily demonstrate the ability to understand and execute step-by-step, detailed instructions in a variety of contexts, possibly including complex tasks and multi-step processes. In contrast, a model performing well on InfoBench would show proficiency in comprehending and executing instructions that simulate real-world scenarios, emphasizing clarity and precision in following directives.",
    "explanation": "This question requires a deep understanding of both IFEval and InfoBench, their specific requirements, and the nuances between them. It prompts a reflection on the distinct skill sets and strategies that each benchmark tests, encouraging a thorough comparison and highlighting the unique aspects of instruction-following versus detailed directive execution.",
    "question_token_count": 32,
    "answer_correctness_score": 8,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 6,
    "avg_answer_token_count": 71
  },
  {
    "context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n",
    "topic": "Automated Construction of Knights and Knaves Puzzles for Reasoning Evaluation",
    "response": "<document_analysis>\nThe context discusses various frameworks for evaluating the reasoning abilities of LLMs, including S3Eval, DyVal, NPHardEval, and Xie et al.'s work on Knights and Knaves puzzles. Xie et al.'s approach stands out as it focuses specifically on constructing puzzles with random reasoning graphs. The key aspect here is the automated construction of these puzzles, which involves generating random reasoning graphs to evaluate logical reasoning skills.\n\nGiven this detailed focus, a highly insightful question would probe the underlying methodology and implications of this approach. It should encourage reflection on the challenges and benefits of automating puzzle construction and its impact on evaluating reasoning capabilities.\n</document_analysis>\n\n<output_format>\nQuestion: How does the automated construction of Knights and Knaves puzzles with random reasoning graphs by Xie et al. (2024) enhance the evaluation of LLMs' reasoning abilities compared to other graph-based methods, and what are the potential limitations of this approach?\n\nExplanation: This question requires a deep understanding of the specific methodology used by Xie et al. and its implications. It asks for a comparison with other graph-based methods and highlights the unique aspects of automating puzzle construction, encouraging the respondent to reflect on both the strengths and weaknesses of the approach.\n\nCorrect Answer: Xie et al.'s method enhances the evaluation of LLMs by providing a dynamic and varied set of reasoning tasks through randomized graph structures, which can better simulate real-world complexity. However, the limitation lies in the potential lack of standardization in puzzle generation and the difficulty in controlling for all possible logical scenarios, which might introduce biases or oversights.\n</output_format>",
    "question": "How does the automated construction of Knights and Knaves puzzles with random reasoning graphs by Xie et al. (2024) enhance the evaluation of LLMs' reasoning abilities compared to other graph-based methods, and what are the potential limitations of this approach?",
    "answer": "Xie et al.'s method enhances the evaluation of LLMs by providing a dynamic and varied set of reasoning tasks through randomized graph structures, which can better simulate real-world complexity. However, the limitation lies in the potential lack of standardization in puzzle generation and the difficulty in controlling for all possible logical scenarios, which might introduce biases or oversights.",
    "explanation": "This question requires a deep understanding of the specific methodology used by Xie et al. and its implications. It asks for a comparison with other graph-based methods and highlights the unique aspects of automating puzzle construction, encouraging the respondent to reflect on both the strengths and weaknesses of the approach.",
    "question_token_count": 51,
    "answer_correctness_score": 9,
    "explanation_validity_score": 8,
    "question_clarity_score": 2,
    "question_difficulty_score": 8,
    "question_groundedness_score": 4,
    "avg_answer_token_count": 70
  },
  {
    "context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n",
    "topic": "Application of multi-round interactions in assessing LLM responses to static benchmarks",
    "response": "<output_format>\nQuestion: How does KIEval generate follow-up questions, and what is its primary objective in the context of evaluating LLM responses to static benchmarks?\nExplanation: This question requires a deep understanding of the KIEval process, including how it uses the evaluated model's response to generate new questions and its main goal in the evaluation framework.\nCorrect Answer: KIEval generates follow-up questions based on the evaluated model's response to an initial question from a static benchmark, aiming to further assess the model's depth of knowledge and capability to handle complex queries.\n</output_format>",
    "question": "How does KIEval generate follow-up questions, and what is its primary objective in the context of evaluating LLM responses to static benchmarks?",
    "answer": "KIEval generates follow-up questions based on the evaluated model's response to an initial question from a static benchmark, aiming to further assess the model's depth of knowledge and capability to handle complex queries.",
    "explanation": "This question requires a deep understanding of the KIEval process, including how it uses the evaluated model's response to generate new questions and its main goal in the evaluation framework.",
    "question_token_count": 28,
    "answer_correctness_score": 10,
    "explanation_validity_score": 8,
    "question_clarity_score": 9,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 39
  },
  {
    "context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n",
    "topic": "Open-Domain Evaluations of Language Models via AlpacaEval and ArenaHard.",
    "response": "<document_analysis>\nThe context provided discusses various benchmarks used to evaluate language models' capabilities, including mathematical problem-solving and knowledge retrieval. It mentions AlpacaEval and ArenaHard as open-domain evaluation tools, focusing on practical applications rather than theoretical knowledge. The information provided is relevant to the topic of open-domain evaluations but does not contain enough specific details about AlpacaEval and ArenaHard to formulate a highly technical or insightful question directly. Thus, we will focus on the broader implications and application aspects of these tools.\n\nSince the context does not provide detailed specifics about AlpacaEval and ArenaHard, the question should still encourage deep reflection on the nature and importance of open-domain evaluations for language models.\n</document_analysis>\n\n<output_format>\nQuestion: How do AlpacaEval and ArenaHard uniquely contribute to assessing the practical capabilities of large language models in real-world scenarios compared to traditional benchmarks, and what implications do their methodologies have for the development of more robust AI systems?\nExplanation: This question requires a deep understanding of the specific attributes of AlpacaEval and ArenaHard, their methodologies, and how they differ from traditional benchmarks. It also demands insight into how these tools can influence the development and deployment of AI systems in practical applications.\nCorrect Answer: AlpacaEval and ArenaHard contribute to assessing practical capabilities by focusing on real-world tasks and scenarios, which are often more complex and varied than the structured problems in traditional benchmarks. Their methodologies involve evaluating models in diverse, open-domain settings that simulate real-world usage, thereby highlighting strengths and weaknesses that are crucial for developing robust AI systems capable of handling unpredictable and complex situations.\n</output_format>",
    "question": "How do AlpacaEval and ArenaHard uniquely contribute to assessing the practical capabilities of large language models in real-world scenarios compared to traditional benchmarks, and what implications do their methodologies have for the development of more robust AI systems?",
    "answer": "AlpacaEval and ArenaHard contribute to assessing practical capabilities by focusing on real-world tasks and scenarios, which are often more complex and varied than the structured problems in traditional benchmarks. Their methodologies involve evaluating models in diverse, open-domain settings that simulate real-world usage, thereby highlighting strengths and weaknesses that are crucial for developing robust AI systems capable of handling unpredictable and complex situations.",
    "explanation": "This question requires a deep understanding of the specific attributes of AlpacaEval and ArenaHard, their methodologies, and how they differ from traditional benchmarks. It also demands insight into how these tools can influence the development and deployment of AI systems in practical applications.",
    "question_token_count": 45,
    "answer_correctness_score": 8,
    "explanation_validity_score": 8,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 6,
    "avg_answer_token_count": 75
  },
  {
    "context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n",
    "topic": "Robust methods for detecting overlaps between training and testing datasets.",
    "response": "<output_format>\nQuestion: How can robust post-hoc detection methods address the limitations of exact matching in identifying data contamination between the training and testing datasets, and what alternative techniques are employed to enhance detection accuracy?\nExplanation: The question probes the candidate's ability to understand the limitations of exact matching and their knowledge of alternative methods used to improve detection accuracy, such as embedding-based similarity and improved mapping metrics. It also assesses the candidate's comprehension of broader post-hoc detection techniques beyond simple overlap detection.\nCorrect Answer: Robust post-hoc detection methods address the limitations of exact matching by employing techniques like embedding-based similarity and improved mapping metrics. These methods help in detecting more nuanced overlaps that exact matching might miss. Alternative techniques include analyzing model behavior under different conditions, such as masked inputs, partial completions, and comparing model performance across benchmarks.\n</output_format>",
    "question": "How can robust post-hoc detection methods address the limitations of exact matching in identifying data contamination between the training and testing datasets, and what alternative techniques are employed to enhance detection accuracy?",
    "answer": "Robust post-hoc detection methods address the limitations of exact matching by employing techniques like embedding-based similarity and improved mapping metrics. These methods help in detecting more nuanced overlaps that exact matching might miss. Alternative techniques include analyzing model behavior under different conditions, such as masked inputs, partial completions, and comparing model performance across benchmarks.",
    "explanation": "The question probes the candidate's ability to understand the limitations of exact matching and their knowledge of alternative methods used to improve detection accuracy, such as embedding-based similarity and improved mapping metrics. It also assesses the candidate's comprehension of broader post-hoc detection techniques beyond simple overlap detection.",
    "question_token_count": 36,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 64
  },
  {
    "context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "The need for dynamic benchmarking due to limitations in static benchmarking schemes.",
    "response": "<output_format>\nQuestion: How does dynamic benchmarking address the key limitations of static benchmarking schemes in evaluating Large Language Models (LLMs)?\nExplanation: The question requires an understanding of the limitations of static benchmarking and how dynamic benchmarking overcomes them. It probes the ability to explain the transformation process and its impact on avoiding data contamination.\nCorrect Answer: Dynamic benchmarking addresses the limitations of static benchmarking by continuously modifying the dataset through a transformation function \\( T(\\cdot) \\) to prevent data contamination, thereby providing a more transparent and faithful evaluation of LLMs.\n</output_format>\n\n<document_analysis>\nThe context discusses the limitations of static benchmarking and introduces dynamic benchmarking as a solution. Key points include:\n1. **Static Benchmarking Limitations**: Legal and privacy constraints restrict full access to the training dataset, making overlap detection challenging.\n2. **Dynamic Benchmarking Definition**: Defined as \\(\\mathcal{B}_{\\text{dynamic}} = (\\mathcal{D}, T(\\cdot))\\) where \\(\\mathcal{D} = (\\mathcal{X}, \\mathcal{Y}, \\mathcal{S}(\\cdot))\\).\n3. **Transformation Function**: The transformation function \\(T(\\cdot)\\) modifies the data set to avoid possible data contamination.\n4. **Evaluation Data Sets**: The dynamic dataset for evaluation is expressed as \\(\\mathcal{D}_t = T_t(\\mathcal{D})\\), where \\(\\mathcal{D}_t\\) is the evaluation data set at timestamp \\(t\\).\n\nThe question focuses on how dynamic benchmarking addresses the key limitations, requiring a deep understanding of both the limitations of static benchmarking and how dynamic benchmarking mitigates these issues.\n</document_analysis>",
    "question": "How does dynamic benchmarking address the key limitations of static benchmarking schemes in evaluating Large Language Models (LLMs)?",
    "answer": "Dynamic benchmarking addresses the limitations of static benchmarking by continuously modifying the dataset through a transformation function \\( T(\\cdot) \\) to prevent data contamination, thereby providing a more transparent and faithful evaluation of LLMs.",
    "explanation": "The question requires an understanding of the limitations of static benchmarking and how dynamic benchmarking overcomes them. It probes the ability to explain the transformation process and its impact on avoiding data contamination.",
    "question_token_count": 22,
    "answer_correctness_score": 10,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 41
  },
  {
    "context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n",
    "topic": "The scope and depth of the survey in providing detailed technical implementation guidelines for practitioners.",
    "response": "<output_format>\nQuestion: Given the survey's focus, what specific limitation might a practitioner face when attempting to implement the dynamic benchmarking methods discussed for LLMs?\nExplanation: The question requires the respondent to identify a gap in the survey's coverage that would be crucial for a practitioner trying to apply the methods discussed. This probes the extent to which the survey provides actionable, detailed implementation guidelines.\nCorrect Answer: The survey may not provide fine-grained technical details necessary for practitioners to implement dynamic benchmarking methods effectively.\n</output_format>\n\n<document_analysis>\nThe provided context discusses the survey's limitations and does not offer detailed technical implementation guidelines for practitioners. It mentions that the survey focuses on high-level concepts and may not cover all fine-grained technical details, which is a critical gap for someone looking to implement the methods discussed. This question directly addresses this limitation, encouraging reflection on the practical applicability of the survey's findings.\n</document_analysis>",
    "question": "Given the survey's focus, what specific limitation might a practitioner face when attempting to implement the dynamic benchmarking methods discussed for LLMs?",
    "answer": "The survey may not provide fine-grained technical details necessary for practitioners to implement dynamic benchmarking methods effectively.",
    "explanation": "The question requires the respondent to identify a gap in the survey's coverage that would be crucial for a practitioner trying to apply the methods discussed. This probes the extent to which the survey provides actionable, detailed implementation guidelines.",
    "question_token_count": 27,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 20
  },
  {
    "context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n",
    "topic": "Critical Gap in Current Benchmarking Practices",
    "response": "<document_analysis>\nThe context discusses the evolution of benchmarking methods for large language models (LLMs) from static to dynamic approaches, emphasizing the need to reduce data contamination risks. A critical gap highlighted is the lack of standardized criteria for evaluating dynamic benchmarks. This gap suggests that while there have been advancements, there is no uniform method to assess the effectiveness of these dynamic benchmarks.\n\nThe analysis reveals that existing methods focus more on enhancing static benchmarks but fail to address the evaluation of dynamic benchmarks adequately. This indicates a significant area for improvement in the field, necessitating the establishment of standardized criteria.\n</document_analysis>\n\n<output_format>\nQuestion: What is the primary limitation of current benchmarking practices as highlighted in the work, and how does it affect the assessment of dynamic benchmarks?\nExplanation: The work identifies a critical gap in the absence of standardized criteria for evaluating dynamic benchmarks. Without such criteria, it becomes challenging to objectively assess the performance and effectiveness of dynamic benchmarking methods.\nCorrect Answer: The lack of standardized criteria for evaluating dynamic benchmarks.\n</output_format>",
    "question": "What is the primary limitation of current benchmarking practices as highlighted in the work, and how does it affect the assessment of dynamic benchmarks?",
    "answer": "The lack of standardized criteria for evaluating dynamic benchmarks.",
    "explanation": "The work identifies a critical gap in the absence of standardized criteria for evaluating dynamic benchmarks. Without such criteria, it becomes challenging to objectively assess the performance and effectiveness of dynamic benchmarking methods.",
    "question_token_count": 27,
    "answer_correctness_score": 10,
    "explanation_validity_score": 10,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 11
  },
  {
    "context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n",
    "topic": "Understanding the definition and importance of scalability in dynamic benchmarking methods.",
    "response": "<document_analysis>\nThe provided context focuses on the concept of scalability in dynamic benchmarking methods. It describes how scalability is quantified using a mathematical formula that involves the size of the transformed and original datasets along with a cost measure. This information is crucial for understanding the trade-offs between dataset size, computational resources, and associated costs. The central idea revolves around optimizing the benchmarking process by generating larger datasets while keeping costs low. This requires a deep understanding of the underlying mathematical representation and its practical implications.\n</document_analysis>\n\n<output_format>\nQuestion: What does the scalability metric in dynamic benchmarking methods fundamentally represent, and how is it mathematically expressed in terms of the transformed dataset size, original dataset size, and the cost function?\nExplanation: The question seeks to probe the candidate's ability to explain the core concept of scalability and the mathematical formulation used to quantify it. It requires a deep understanding of the relationship between dataset size, transformation costs, and the overall efficiency of the benchmarking method.\nCorrect Answer: The scalability metric represents the ratio of the size of the transformed dataset to the original dataset, adjusted for the cost of transformation. Mathematically, it is expressed as E[\u2225Ti(\ud835\udc9f)\u2225] / \u2225\ud835\udc9f\u2225 * Cost(\u22c5), where E[] denotes the expectation over the transformation space.\n</output_format>",
    "question": "What does the scalability metric in dynamic benchmarking methods fundamentally represent, and how is it mathematically expressed in terms of the transformed dataset size, original dataset size, and the cost function?",
    "answer": "The scalability metric represents the ratio of the size of the transformed dataset to the original dataset, adjusted for the cost of transformation. Mathematically, it is expressed as E[\u2225Ti(\ud835\udc9f)\u2225] / \u2225\ud835\udc9f\u2225 * Cost(\u22c5), where E[] denotes the expectation over the transformation space.",
    "explanation": "The question seeks to probe the candidate's ability to explain the core concept of scalability and the mathematical formulation used to quantify it. It requires a deep understanding of the relationship between dataset size, transformation costs, and the overall efficiency of the benchmarking method.",
    "question_token_count": 36,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 64
  },
  {
    "context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n",
    "topic": "Evaluating LLMs on P and NP Problems Such as the Traveling Salesman Problem",
    "response": "<document_analysis>\nThe context discusses various frameworks for evaluating the reasoning abilities of LLMs, focusing on their performance on P and NP problems, particularly the Traveling Salesman Problem (TSP). It mentions that NPHardEval synthesizes random graphs for TSP to assess LLMs. This suggests that LLMs are tested on complex optimization problems with varying graph sizes. The key aspect here is the evaluation method's adaptability to different problem instances, which challenges the LLMs' ability to handle complexity and optimization effectively.\n\nThe question should probe the nuances of how LLMs are evaluated on such problems and the implications of these evaluations for the LLMs' reasoning capabilities. A suitable question would require a deep understanding of the evaluation methods and their implications for LLM performance.\n</document_analysis>\n\n<output_format>\nQuestion: How does the adaptability of the graph size in NPHardEval impact the evaluation of LLMs on the Traveling Salesman Problem (TSP), and what does this suggest about the robustness of LLMs in handling varying levels of problem complexity?\n\nExplanation: This question requires the respondent to understand the role of varying graph sizes in the evaluation process and its implications for LLM performance. It encourages reflection on the robustness of LLMs and their ability to handle different levels of problem complexity, which is crucial for assessing their true reasoning capabilities.\n\nCorrect Answer: Varying graph sizes in NPHardEval allow for a comprehensive assessment of LLMs' ability to handle increasing levels of computational complexity, suggesting robustness in solving TSP instances of different sizes. This adaptability tests the LLMs' capacity to scale and maintain performance across diverse problem instances.\n</output_format>",
    "question": "How does the adaptability of the graph size in NPHardEval impact the evaluation of LLMs on the Traveling Salesman Problem (TSP), and what does this suggest about the robustness of LLMs in handling varying levels of problem complexity?",
    "answer": "Varying graph sizes in NPHardEval allow for a comprehensive assessment of LLMs' ability to handle increasing levels of computational complexity, suggesting robustness in solving TSP instances of different sizes. This adaptability tests the LLMs' capacity to scale and maintain performance across diverse problem instances.",
    "explanation": "This question requires the respondent to understand the role of varying graph sizes in the evaluation process and its implications for LLM performance. It encourages reflection on the robustness of LLMs and their ability to handle different levels of problem complexity, which is crucial for assessing their true reasoning capabilities.",
    "question_token_count": 49,
    "answer_correctness_score": 9,
    "explanation_validity_score": 8,
    "question_clarity_score": 7,
    "question_difficulty_score": 6,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 56
  },
  {
    "context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n",
    "topic": "The role of complexity in dynamic benchmarking for LLMs and its impact on performance evaluation.",
    "response": "<document_analysis>\nThe context discusses the importance of accounting for complexity in dynamic benchmarking for LLMs. It highlights the challenges in accurately measuring complexity and introduces the concept of using complexity metrics like graph complexity (DyVal) to evaluate reasoning problems. The context also mentions that high variance in complexity across trials indicates instability in the dynamic benchmarking method.\n\nGiven this, a highly insightful and challenging question should probe the domain expert's understanding of how complexity affects performance evaluation in dynamic benchmarking and their ability to apply theoretical concepts to practical scenarios.\n</document_analysis>\n\n<output_format>\nQuestion: How would you design a complexity metric that could be applied universally across different domains and tasks to assess the stability of a dynamic benchmarking method for LLMs, and what specific challenges might arise in applying such a metric in practice?\n\nExplanation: This question requires the respondent to synthesize the understanding of complexity measurement and its role in benchmarking, while also considering the challenges of universal applicability. It probes deeper understanding of the topic and the ability to apply theoretical concepts practically.\n\nCorrect Answer: A complexity metric that could be applied universally across different domains and tasks would ideally capture the inherent difficulty of tasks in a way that is independent of domain-specific nuances. Challenges might include ensuring the metric is sensitive enough to detect variations in complexity, yet robust to variations in data characteristics; maintaining interpretability across diverse tasks; and addressing the issue of overfitting to specific task types.\n</output_format>",
    "question": "How would you design a complexity metric that could be applied universally across different domains and tasks to assess the stability of a dynamic benchmarking method for LLMs, and what specific challenges might arise in applying such a metric in practice?",
    "answer": "A complexity metric that could be applied universally across different domains and tasks would ideally capture the inherent difficulty of tasks in a way that is independent of domain-specific nuances. Challenges might include ensuring the metric is sensitive enough to detect variations in complexity, yet robust to variations in data characteristics; maintaining interpretability across diverse tasks; and addressing the issue of overfitting to specific task types.",
    "explanation": "This question requires the respondent to synthesize the understanding of complexity measurement and its role in benchmarking, while also considering the challenges of universal applicability. It probes deeper understanding of the topic and the ability to apply theoretical concepts practically.",
    "question_token_count": 45,
    "answer_correctness_score": 8,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 75
  },
  {
    "context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n",
    "topic": "Evaluation of Technical and Long-Context Skills through Benchmarks like ControlBench and FRAMES.",
    "response": "<output_format>\nQuestion: How does the ControlBench benchmark differ from traditional knowledge benchmarks like MMLU in evaluating long-context and technical skills, and what specific aspects of a model's performance does it primarily assess?\n\nExplanation: This question requires a deep understanding of the nature of ControlBench compared to other benchmarks. It prompts reflection on the unique aspects of ControlBench that make it suitable for evaluating long-context and technical skills, and challenges the respondent to articulate the specific competencies being tested.\n\nCorrect Answer: ControlBench primarily assesses a model's ability to handle complex, multi-step reasoning tasks and maintain coherence across extended contexts, unlike MMLU which focuses more on discrete factual knowledge retrieval.\n</output_format>",
    "question": "How does the ControlBench benchmark differ from traditional knowledge benchmarks like MMLU in evaluating long-context and technical skills, and what specific aspects of a model's performance does it primarily assess?",
    "answer": "ControlBench primarily assesses a model's ability to handle complex, multi-step reasoning tasks and maintain coherence across extended contexts, unlike MMLU which focuses more on discrete factual knowledge retrieval.",
    "explanation": "This question requires a deep understanding of the nature of ControlBench compared to other benchmarks. It prompts reflection on the unique aspects of ControlBench that make it suitable for evaluating long-context and technical skills, and challenges the respondent to articulate the specific competencies being tested.",
    "question_token_count": 37,
    "answer_correctness_score": 9,
    "explanation_validity_score": 8,
    "question_clarity_score": 9,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 36
  },
  {
    "context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n",
    "topic": "Application of Reference-Based Metrics in Internal Diversity Measurement",
    "response": "<output_format>\nQuestion: How would you design an internal diversity measure using reference-based metrics like BLEU scores for comparing two transformation trials of a dataset?\nExplanation: This question requires a deep understanding of the concept of internal diversity and the ability to apply reference-based metrics, such as BLEU scores, to measure the differences between two transformation trials. It challenges the domain expert to think about the practical implementation of such a measure, considering the nuances and complexities involved.\nCorrect Answer: An internal diversity measure using reference-based metrics like BLEU scores would involve calculating the similarity between each pair of transformed datasets (D_i and D_j) for i \u2260 j, and averaging these similarities across all pairs. The specific implementation would require defining a set of reference translations (e.g., ground truth translations) and computing the BLEU score for each pair of transformed datasets against these references.\n</output_format>\n\n<document_analysis>\nThe provided context discusses the concepts of external and internal diversity in transformed datasets and introduces a mathematical formulation for both types of diversity. The focus here is on the application of reference-based metrics, particularly BLEU scores, in measuring internal diversity. By asking about designing an internal diversity measure using reference-based metrics, the question encourages a deep understanding of the process and practical application, ensuring that the response goes beyond mere recitation of definitions.\n</document_analysis>",
    "question": "How would you design an internal diversity measure using reference-based metrics like BLEU scores for comparing two transformation trials of a dataset?",
    "answer": "An internal diversity measure using reference-based metrics like BLEU scores would involve calculating the similarity between each pair of transformed datasets (D_i and D_j) for i \u2260 j, and averaging these similarities across all pairs. The specific implementation would require defining a set of reference translations (e.g., ground truth translations) and computing the BLEU score for each pair of transformed datasets against these references.",
    "explanation": "This question requires a deep understanding of the concept of internal diversity and the ability to apply reference-based metrics, such as BLEU scores, to measure the differences between two transformation trials. It challenges the domain expert to think about the practical implementation of such a measure, considering the nuances and complexities involved.",
    "question_token_count": 26,
    "answer_correctness_score": 8,
    "explanation_validity_score": 8,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 79
  },
  {
    "context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Analyzing the role of dynamic benchmarking techniques in reducing data contamination risks.",
    "response": "<output_format>\nQuestion: How can the continuous adaptation of dynamic benchmarking techniques be leveraged to proactively identify and mitigate data contamination risks in Large Language Model (LLM) training, and what specific criteria should be considered in the evaluation of such dynamic benchmarks?\nExplanation: This question requires a deep understanding of the evolving nature of dynamic benchmarking techniques and their potential to prevent data contamination. It prompts reflection on the proactive measures needed to address the issue and challenges the respondent to think critically about the evaluation criteria for dynamic benchmarks.\nCorrect Answer: Dynamic benchmarking techniques can be adapted to proactively identify and mitigate data contamination risks by continuously updating benchmark datasets based on the timestamps of LLM training, thereby minimizing contamination. Specific criteria for evaluating dynamic benchmarks include: (1) timeliness and relevance of the benchmark updates, (2) robustness against potential data contamination, (3) transparency and reproducibility of the benchmarking process, and (4) effectiveness in enhancing model performance without introducing biases.\n</output_format>",
    "question": "How can the continuous adaptation of dynamic benchmarking techniques be leveraged to proactively identify and mitigate data contamination risks in Large Language Model (LLM) training, and what specific criteria should be considered in the evaluation of such dynamic benchmarks?",
    "answer": "Dynamic benchmarking techniques can be adapted to proactively identify and mitigate data contamination risks by continuously updating benchmark datasets based on the timestamps of LLM training, thereby minimizing contamination. Specific criteria for evaluating dynamic benchmarks include: (1) timeliness and relevance of the benchmark updates, (2) robustness against potential data contamination, (3) transparency and reproducibility of the benchmarking process, and (4) effectiveness in enhancing model performance without introducing biases.",
    "explanation": "This question requires a deep understanding of the evolving nature of dynamic benchmarking techniques and their potential to prevent data contamination. It prompts reflection on the proactive measures needed to address the issue and challenges the respondent to think critically about the evaluation criteria for dynamic benchmarks.",
    "question_token_count": 46,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 86
  },
  {
    "context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n",
    "topic": "Investigating the implications of data contamination on the reliability of LLM performance assessments.",
    "response": "<output_format>\nQuestion: Given that LLMs are trained on vast amounts of Internet-sourced data, how does the practice of dynamically updating benchmark datasets based on the timestamps of LLM training data potentially fail to fully mitigate data contamination, and what specific criteria should be considered when evaluating the effectiveness of such dynamic benchmarks?\nExplanation: This question delves into the complexities of data contamination in LLM benchmarking and requires a deep understanding of the limitations of dynamic benchmarking methods. It challenges the respondent to think critically about the nuances of data collection and the potential for contamination, even when benchmark updates are timestamp-based. Additionally, it prompts the respondent to consider the criteria needed for evaluating these benchmarks, which goes beyond surface-level considerations of contamination.\nCorrect Answer: Dynamic updates based on timestamps may still fail to fully mitigate data contamination because the initial training data might already contain contaminated examples. Furthermore, the criteria for evaluating dynamic benchmarks should include: 1) transparency in dataset composition, 2) robustness against pre-existing contamination, 3) effectiveness in reducing contamination over time, and 4) fairness in comparison across different LLMs.\n</output_format>",
    "question": "Given that LLMs are trained on vast amounts of Internet-sourced data, how does the practice of dynamically updating benchmark datasets based on the timestamps of LLM training data potentially fail to fully mitigate data contamination, and what specific criteria should be considered when evaluating the effectiveness of such dynamic benchmarks?",
    "answer": "Dynamic updates based on timestamps may still fail to fully mitigate data contamination because the initial training data might already contain contaminated examples. Furthermore, the criteria for evaluating dynamic benchmarks should include: 1) transparency in dataset composition, 2) robustness against pre-existing contamination, 3) effectiveness in reducing contamination over time, and 4) fairness in comparison across different LLMs.",
    "explanation": "This question delves into the complexities of data contamination in LLM benchmarking and requires a deep understanding of the limitations of dynamic benchmarking methods. It challenges the respondent to think critically about the nuances of data collection and the potential for contamination, even when benchmark updates are timestamp-based. Additionally, it prompts the respondent to consider the criteria needed for evaluating these benchmarks, which goes beyond surface-level considerations of contamination.",
    "question_token_count": 57,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 75
  },
  {
    "context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n",
    "topic": "Functionality of \u0398(\u22c5)\\\u0398(\u22c5)roman_\u0398 ( \u22c5 ) in Measuring Diversity",
    "response": "<output_format>\nQuestion: What is the role of the function \\( \\Theta(\\cdot) \\) in quantifying the diversity between datasets, and how does its application impact the evaluation of external and internal diversity metrics?\n\nExplanation: The question requires a deep understanding of how the diversity measurement function \\( \\Theta(\\cdot) \\) operates within the framework of external and internal diversity. It probes the expert's ability to explain the functionality of \\( \\Theta(\\cdot) \\) and its implications on the evaluation of different types of diversity. The answer would need to cover the conceptual basis of \\( \\Theta(\\cdot) \\) and its practical application in calculating both external and internal diversity metrics, demonstrating a comprehensive grasp of the topic.\n\nCorrect Answer: The function \\( \\Theta(\\cdot) \\) serves as a metric to quantify the dissimilarity between two datasets. For external diversity, it measures the difference between the transformed dataset and the original seed dataset, while for internal diversity, it assesses the differences between different transformation trials. Its choice impacts the accuracy and reliability of the diversity evaluation.\n</output_format>",
    "question": "What is the role of the function \\( \\Theta(\\cdot) \\) in quantifying the diversity between datasets, and how does its application impact the evaluation of external and internal diversity metrics?",
    "answer": "The function \\( \\Theta(\\cdot) \\) serves as a metric to quantify the dissimilarity between two datasets. For external diversity, it measures the difference between the transformed dataset and the original seed dataset, while for internal diversity, it assesses the differences between different transformation trials. Its choice impacts the accuracy and reliability of the diversity evaluation.",
    "explanation": "The question requires a deep understanding of how the diversity measurement function \\( \\Theta(\\cdot) \\) operates within the framework of external and internal diversity. It probes the expert's ability to explain the functionality of \\( \\Theta(\\cdot) \\) and its implications on the evaluation of different types of diversity. The answer would need to cover the conceptual basis of \\( \\Theta(\\cdot) \\) and its practical application in calculating both external and internal diversity metrics, demonstrating a comprehensive grasp of the topic.",
    "question_token_count": 37,
    "answer_correctness_score": 9,
    "explanation_validity_score": 8,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 67
  },
  {
    "context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n",
    "topic": "Role of cutting-edge research in designing effective benchmarks.",
    "response": "<output_format>\nQuestion: How does the integration of temporal cutoff considerations in benchmark design reflect the broader role of cutting-edge research in maintaining the validity and relevance of AI evaluation tools?\nExplanation: This question requires a deep understanding of how temporal considerations impact benchmark design, the importance of using the most recent data to avoid contamination, and the broader implications of keeping AI evaluation methods aligned with current research trends and practices.\nCorrect Answer: By incorporating temporal cutoffs, benchmarks ensure they remain aligned with the latest research findings and data, thereby validating the AI's ability to handle contemporary and evolving information, which is crucial for its real-world applicability.\n</output_format>",
    "question": "How does the integration of temporal cutoff considerations in benchmark design reflect the broader role of cutting-edge research in maintaining the validity and relevance of AI evaluation tools?",
    "answer": "By incorporating temporal cutoffs, benchmarks ensure they remain aligned with the latest research findings and data, thereby validating the AI's ability to handle contemporary and evolving information, which is crucial for its real-world applicability.",
    "explanation": "This question requires a deep understanding of how temporal considerations impact benchmark design, the importance of using the most recent data to avoid contamination, and the broader implications of keeping AI evaluation methods aligned with current research trends and practices.",
    "question_token_count": 31,
    "answer_correctness_score": 9,
    "explanation_validity_score": 8,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 8,
    "avg_answer_token_count": 42
  },
  {
    "context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n",
    "topic": "Testing a model's intuitive reasoning skills with benchmarks including PIQA, SIQA, and WinoGrande.",
    "response": "<output_format>\nQuestion: Considering the benchmarks designed to test intuitive reasoning skills, how would a model's performance on PIQA, SIQA, and WinoGrande differ in their approach to assessing a model's ability to integrate background knowledge and logical reasoning?\nExplanation: These benchmarks require models to apply common sense and logical reasoning to arrive at plausible answers, which tests their ability to handle real-world scenarios beyond mere fact recall. PIQA and SIQA focus on specific aspects of commonsense reasoning, while WinoGrande evaluates the model's ability to distinguish between similar but distinct commonsense scenarios. The differences in their approaches highlight the nuanced nature of intuitive reasoning tasks.\nCorrect Answer: PIQA and SIQA assess specific aspects of commonsense reasoning, while WinoGrande evaluates the model's ability to distinguish between similar but distinct commonsense scenarios.\n</output_format>",
    "question": "Considering the benchmarks designed to test intuitive reasoning skills, how would a model's performance on PIQA, SIQA, and WinoGrande differ in their approach to assessing a model's ability to integrate background knowledge and logical reasoning?",
    "answer": "PIQA and SIQA assess specific aspects of commonsense reasoning, while WinoGrande evaluates the model's ability to distinguish between similar but distinct commonsense scenarios.",
    "explanation": "These benchmarks require models to apply common sense and logical reasoning to arrive at plausible answers, which tests their ability to handle real-world scenarios beyond mere fact recall. PIQA and SIQA focus on specific aspects of commonsense reasoning, while WinoGrande evaluates the model's ability to distinguish between similar but distinct commonsense scenarios. The differences in their approaches highlight the nuanced nature of intuitive reasoning tasks.",
    "question_token_count": 45,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 33
  },
  {
    "context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n",
    "topic": "Importance of domain-specific annotators in providing accurate ground truth values for correctness assessment.",
    "response": "<output_format>\nQuestion: How does the use of domain-specific annotators impact the evaluation of the correctness criterion in dynamic benchmarking algorithms for LLMs?\nExplanation: The question probes the importance of domain-specific annotators in ensuring accurate ground truth values, which directly influences the reliability of the correctness metric in benchmarking LLMs. It requires a deep understanding of the role of annotators and the implications of their expertise on the evaluation process.\nCorrect Answer: Domain-specific annotators provide precise and reliable ground truth values, enhancing the accuracy of the correctness metric by ensuring that the transformed dataset aligns closely with real-world expectations and standards.\n</output_format>",
    "question": "How does the use of domain-specific annotators impact the evaluation of the correctness criterion in dynamic benchmarking algorithms for LLMs?",
    "answer": "Domain-specific annotators provide precise and reliable ground truth values, enhancing the accuracy of the correctness metric by ensuring that the transformed dataset aligns closely with real-world expectations and standards.",
    "explanation": "The question probes the importance of domain-specific annotators in ensuring accurate ground truth values, which directly influences the reliability of the correctness metric in benchmarking LLMs. It requires a deep understanding of the role of annotators and the implications of their expertise on the evaluation process.",
    "question_token_count": 25,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 9,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 35
  },
  {
    "context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n",
    "topic": "Benefits and practical considerations of collecting real-time data for benchmarking.",
    "response": "<document_analysis>\nThe context discusses various benchmarks that utilize real-time data to mitigate data contamination in evaluating language models. Key points include the use of live datasets such as math competitions, newly emerged knowledge, arXiv papers, coding problems, math problems from forums, and daily forecasting questions. These practices aim to ensure that the benchmarks accurately reflect current knowledge and capabilities of the models. However, the context does not explicitly discuss the benefits and practical considerations of collecting real-time data for benchmarking. Therefore, while there is relevant information, the context itself does not provide enough detail to generate a highly insightful and probing question-answer pair directly addressing the specified topic.\n</document_analysis>\n\n<output_format>\nQuestion: What are the primary benefits and practical challenges associated with utilizing real-time data for benchmarking language models, and how do these considerations influence the design and implementation of such benchmarks?\nExplanation: This question requires a deep understanding of the benefits and challenges of using real-time data in benchmarking, encouraging the respondent to reflect on the trade-offs between timeliness, accuracy, and reliability. It also prompts them to consider how these factors shape the design and ongoing maintenance of benchmarks.\nCorrect Answer: The primary benefits include ensuring that benchmarks reflect the most current knowledge and capabilities of language models, reducing data contamination, and providing more accurate assessments. Practical challenges encompass the need for continuous data collection, handling rapidly changing information, managing data quality, and ensuring that the benchmarks remain relevant and comprehensive.\n</output_format>",
    "question": "What are the primary benefits and practical challenges associated with utilizing real-time data for benchmarking language models, and how do these considerations influence the design and implementation of such benchmarks?",
    "answer": "The primary benefits include ensuring that benchmarks reflect the most current knowledge and capabilities of language models, reducing data contamination, and providing more accurate assessments. Practical challenges encompass the need for continuous data collection, handling rapidly changing information, managing data quality, and ensuring that the benchmarks remain relevant and comprehensive.",
    "explanation": "This question requires a deep understanding of the benefits and challenges of using real-time data in benchmarking, encouraging the respondent to reflect on the trade-offs between timeliness, accuracy, and reliability. It also prompts them to consider how these factors shape the design and ongoing maintenance of benchmarks.",
    "question_token_count": 34,
    "answer_correctness_score": 9,
    "explanation_validity_score": 8,
    "question_clarity_score": 9,
    "question_difficulty_score": 9,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 58
  },
  {
    "context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n",
    "topic": "Computational overhead introduced by encryption methods.",
    "response": "<output_format>\nQuestion: Given that encryption methods for securing evaluation data introduce extra computational overhead, how might a domain expert leverage this fact to design more efficient yet secure systems, and what are the trade-offs involved in such an approach?\nExplanation: This question probes the expert's understanding of both the limitations and potential optimizations of encryption methods. It requires the expert to think about ways to mitigate overhead while still maintaining security, encouraging a reflection on practical implementation strategies.\nCorrect Answer: A domain expert might explore techniques such as homomorphic encryption, lightweight encryption schemes, or optimizing key management processes to reduce computational costs while ensuring data remains secure. The trade-offs involve balancing security with performance, as overly complex encryption may lead to significant delays, while simpler methods may compromise data integrity.\n</output_format>",
    "question": "Given that encryption methods for securing evaluation data introduce extra computational overhead, how might a domain expert leverage this fact to design more efficient yet secure systems, and what are the trade-offs involved in such an approach?",
    "answer": "A domain expert might explore techniques such as homomorphic encryption, lightweight encryption schemes, or optimizing key management processes to reduce computational costs while ensuring data remains secure. The trade-offs involve balancing security with performance, as overly complex encryption may lead to significant delays, while simpler methods may compromise data integrity.",
    "explanation": "This question probes the expert's understanding of both the limitations and potential optimizations of encryption methods. It requires the expert to think about ways to mitigate overhead while still maintaining security, encouraging a reflection on practical implementation strategies.",
    "question_token_count": 42,
    "answer_correctness_score": 9,
    "explanation_validity_score": 8,
    "question_clarity_score": 7,
    "question_difficulty_score": 8,
    "question_groundedness_score": 8,
    "avg_answer_token_count": 59
  },
  {
    "context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n",
    "topic": "The reliability and reproducibility challenges faced by dynamic benchmarking approaches in LLM evaluation.",
    "response": "<output_format>\nQuestion: What critical challenge does the current implementation of dynamic benchmarking for Large Language Models (LLMs) face that could significantly impact the reliability and reproducibility of evaluation results?\nExplanation: The question probes the core issue highlighted in the context regarding the limitations of dynamic benchmarking approaches. It requires a deep understanding of the current challenges and their implications without relying on specific details that might be too technical or context-dependent.\nCorrect Answer: Reliability and reproducibility issues.\n</output_format>",
    "question": "What critical challenge does the current implementation of dynamic benchmarking for Large Language Models (LLMs) face that could significantly impact the reliability and reproducibility of evaluation results?",
    "answer": "Reliability and reproducibility issues.",
    "explanation": "The question probes the core issue highlighted in the context regarding the limitations of dynamic benchmarking approaches. It requires a deep understanding of the current challenges and their implications without relying on specific details that might be too technical or context-dependent.",
    "question_token_count": 32,
    "answer_correctness_score": 10,
    "explanation_validity_score": 8,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 10,
    "avg_answer_token_count": 7
  },
  {
    "context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n",
    "topic": "Directed Acyclic Graphs (DAGs) in Assessing LLM Reasoning Capabilities",
    "response": "<output_format>\nQuestion: How does the transformation of DAGs into natural language descriptions impact the evaluation of LLMs' reasoning capabilities, and what challenges might this introduce?\nExplanation: This question requires a deep understanding of the process described in the context, particularly the step of converting DAGs into natural language. It probes the expert's ability to analyze the implications of such transformations on the evaluation methodology and identify potential challenges.\nCorrect Answer: The transformation introduces challenges related to preserving the structural complexity and logical consistency of the original DAGs in natural language descriptions, which may lead to misinterpretations by LLMs, thereby affecting the accuracy of the evaluation.\n</output_format>",
    "question": "How does the transformation of DAGs into natural language descriptions impact the evaluation of LLMs' reasoning capabilities, and what challenges might this introduce?",
    "answer": "The transformation introduces challenges related to preserving the structural complexity and logical consistency of the original DAGs in natural language descriptions, which may lead to misinterpretations by LLMs, thereby affecting the accuracy of the evaluation.",
    "explanation": "This question requires a deep understanding of the process described in the context, particularly the step of converting DAGs into natural language. It probes the expert's ability to analyze the implications of such transformations on the evaluation methodology and identify potential challenges.",
    "question_token_count": 29,
    "answer_correctness_score": 9,
    "explanation_validity_score": 9,
    "question_clarity_score": 8,
    "question_difficulty_score": 7,
    "question_groundedness_score": 9,
    "avg_answer_token_count": 43
  },
  {
    "context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n",
    "topic": "Practical implications of the correctness score in validating the reliability of dynamic benchmarks for LLMs.",
    "response": "<document_analysis>\nThe provided context focuses on defining the correctness criterion for dynamic benchmarking algorithms used to evaluate LLMs. It introduces an equation to quantify the correctness of a benchmark, involving a transformation process and a scoring function. This analysis identifies that the correctness score is crucial for ensuring reliable evaluations of LLMs. However, the context does not delve deeply into practical implications. Given this, the generated question aims to probe the practical significance of the correctness score in real-world scenarios.\n</document_analysis>\n\n<output_format>\nQuestion: How does the correctness score influence the trustworthiness of dynamic benchmarks in practical LLM evaluation scenarios, especially when considering potential real-world variability and domain-specific complexities?\nExplanation: This question requires a deep understanding of the correctness criterion and its broader implications beyond the mathematical definition. It prompts reflection on how the score translates to real-world reliability and the challenges involved in maintaining accuracy across different domains and contexts.\nCorrect Answer: The correctness score ensures that the benchmark accurately reflects real-world performance, mitigating the risk of misleading evaluations. However, it must account for real-world variability and domain-specific complexities to maintain trustworthiness.\n</output_format>",
    "question": "How does the correctness score influence the trustworthiness of dynamic benchmarks in practical LLM evaluation scenarios, especially when considering potential real-world variability and domain-specific complexities?",
    "answer": "The correctness score ensures that the benchmark accurately reflects real-world performance, mitigating the risk of misleading evaluations. However, it must account for real-world variability and domain-specific complexities to maintain trustworthiness.",
    "explanation": "This question requires a deep understanding of the correctness criterion and its broader implications beyond the mathematical definition. It prompts reflection on how the score translates to real-world reliability and the challenges involved in maintaining accuracy across different domains and contexts.",
    "question_token_count": 31,
    "answer_correctness_score": 8,
    "explanation_validity_score": 8,
    "question_clarity_score": 6,
    "question_difficulty_score": 8,
    "question_groundedness_score": 8,
    "avg_answer_token_count": 39
  }
]