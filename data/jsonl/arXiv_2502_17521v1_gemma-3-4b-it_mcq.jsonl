{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "Open-domain evaluations, such as those provided by AlpacaEval and ArenaHard, offer broader assessments of LLM performance.", "question": "Beyond their individual datasets, what is the primary distinguishing characteristic of open-domain evaluations like AlpacaEval and ArenaHard within the broader context of LLM benchmarking?", "choices": {"A": "They exclusively focus on assessing mathematical reasoning capabilities.", "B": "They utilize a standardized, closed-domain format to ensure consistent evaluation metrics.", "C": "They employ a multi-turn, adversarial interaction paradigm to probe LLM robustness and generalization.", "D": "They are primarily designed to evaluate the factual accuracy of knowledge retrieval systems."}, "answer": "C", "explanation": "The correct answer (C) accurately reflects the described methodology of AlpacaEval and ArenaHard, which involve multi-turn interactions and adversarial prompts to assess a model's broader capabilities.", "question_token_count": 31, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 14}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "Understanding the implications of relying on memorized information versus reasoning during LLM inference is crucial for developing robust evaluation strategies.", "question": "Considering the described challenge of syntactic contamination in LLM benchmarking, what is the primary concern regarding its impact on evaluation accuracy?", "choices": {"A": "It primarily indicates a weakness in the LLM\u2019s syntactic processing abilities.", "B": "It obscures the distinction between an LLM\u2019s ability to recall memorized information and its capacity for reasoning, potentially inflating performance estimates.", "C": "It necessitates a complete overhaul of existing LLM training methodologies.", "D": "It solely impacts the reliability of benchmarks used in academic research."}, "answer": "B", "explanation": "The text explicitly states that understanding and mitigating data contamination is significant because it can \u201coverestimate a model\u2019s true capabilities\u201d and \u201cundermines the validity of benchmarks.\u201d Option B directly reflects this concern.", "question_token_count": 24, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 10, "avg_answer_token_count": 17}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "Describe three specific scenarios that commonly lead to data contamination in LLM training datasets.", "question": "Which of the following scenarios most directly exemplifies \u2018exact contamination\u2019 in the context of LLM training data?", "choices": {"A": "A test prompt containing a paraphrased version of a training example.", "B": "A verbatim instance of a test question appearing within a training dataset.", "C": "A code snippet from a benchmark implementation being included in the training data.", "D": "A documentation excerpt describing the benchmark dataset being used for training."}, "answer": "B", "explanation": "Exact contamination occurs when there is an identical data point present in both the training and test datasets. Option B accurately describes this scenario.", "question_token_count": 22, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 14}
{"context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n", "topic": "Explain the interpretation of the equation presented in the text: \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 / \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225.", "question": "In the context of dynamic benchmarking, what does the equation \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 / \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 primarily represent?", "choices": {"A": "The statistical distribution of the original dataset.", "B": "The ratio of the cost of data transformation to the size of the original dataset.", "C": "The proportional increase in data size after transformation, independent of cost.", "D": "The efficiency of data generation relative to resource expenditure."}, "answer": "D", "explanation": "The equation explicitly states it represents the \"proportion of data that can be generated per unit cost.\"  Option D accurately captures this nuanced interpretation.", "question_token_count": 89, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 1, "question_groundedness_score": 10, "avg_answer_token_count": 12}
{"context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n", "topic": "The decomposition of benchmark creation into specialized agent roles \u2013 planning, generation, verification, and evaluation \u2013 and its impact on overall efficiency.", "question": "Within the context of automated benchmark creation using the BENCHAGENTS framework, what is the primary justification for dividing the process into distinct agent roles (planning, generation, verification, and evaluation)?", "choices": {"A": "To simplify the evaluation process for human annotators.", "B": "To leverage the inherent limitations of single-agent LLMs in complex tasks.", "C": "To enable a more scalable and adaptable benchmark generation process, improving diversity and quality.", "D": "To reduce the computational cost associated with benchmark creation."}, "answer": "C", "explanation": "The text explicitly states that the BENCHAGENTS framework, employing a multi-agent framework, \u201csplits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent. This coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\u201d", "question_token_count": 39, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 13}
{"context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n", "topic": "Current static benchmarking methods suffer from a lack of transparency and rely on assumptions about contaminated models, highlighting a key limitation.", "question": "Considering the observed inverse relationship between training dataset size and data contamination probability, what fundamental characteristic of LLM training processes most directly contributes to this escalating issue?", "choices": {"A": "The inherent biases present within initial training data sets.", "B": "The increasing computational demands of training larger models, leading to reliance on less curated data sources.", "C": "The limitations of post-hoc contamination detection methods, which are unable to effectively identify subtle forms of data corruption.", "D": "The stochastic nature of the training process, introducing unpredictable variations in model behavior."}, "answer": "C", "explanation": "The correct answer is C. The text explicitly states that static benchmarks rely on assumptions about contaminated models and that post-hoc detection methods are insufficient. The other options represent contributing factors but don\u2019t directly address the core problem outlined in the text.", "question_token_count": 31, "answer_correctness_score": 8, "explanation_validity_score": 8, "question_clarity_score": 8, "question_groundedness_score": 6, "avg_answer_token_count": 17}
{"context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n", "topic": "What does the \"Repeat Trials\" metric indicate about the benchmark\u2019s ability to generate novel test cases?", "question": "What does the \u201cRepeat Trials\u201d metric primarily indicate about the benchmark\u2019s capability to generate diverse test cases?", "choices": {"A": "It measures the rate of data overlap between different benchmark versions.", "B": "It estimates the number of transformations needed to completely replicate an existing transformed dataset, indicating the benchmark\u2019s capacity to produce novel variations.", "C": "It assesses the susceptibility of the benchmark to data contamination during training.", "D": "It determines the initial difficulty level of the benchmark."}, "answer": "B", "explanation": "The context explicitly states that \u201cRepeat Trials\u201d quantifies the \u201cexpected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D )\u201d, directly indicating its relevance to generating novel variations.", "question_token_count": 22, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 16}
{"context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n", "topic": "How are the various tasks evaluated within the framework of static benchmarks?", "question": "Within the framework of static benchmarking, what is the primary role of the scoring function (\ud835\udcae(\u22c5))?", "choices": {"A": "To generate new input prompts for the LLM.", "B": "To define the set of expected outputs (\ud835\udcb4) for each task.", "C": "To assess the quality of an LLM\u2019s output by comparing it against the expected outputs.", "D": "To categorize the tasks being evaluated into specific categories."}, "answer": "C", "explanation": "The text explicitly states that the scoring function \u201cevaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4.\u201d", "question_token_count": 24, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 13}
{"context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n", "topic": "What role does the \"oracle\" function play in evaluating the correctness of a dynamic benchmark?", "question": "In the context of evaluating dynamic benchmarks, what is the primary role of the \u201coracle\u201d function?", "choices": {"A": "To generate the initial dataset used for benchmarking.", "B": "To provide the ground truth for assessing the correctness of the benchmark\u2019s output.", "C": "To calculate the scoring function used to measure alignment.", "D": "To identify potential biases within the benchmark\u2019s transformation process."}, "answer": "B", "explanation": "The text explicitly states that the \u201coracle\u201d function \u201creturns the ground truth of its input,\u201d serving as an objective reference for evaluating correctness.", "question_token_count": 21, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 12}
{"context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n", "topic": "Fairness, accountability, and privacy must be central design principles when developing benchmarking frameworks for LLMs.", "question": "Considering the potential for dynamic LLM benchmarks to compromise user privacy, what is the most prudent approach for mitigating this risk within a benchmarking framework\u2019s design?", "choices": {"A": "Prioritize the collection of vast datasets to improve benchmark adaptability.", "B": "Implement differential privacy techniques to obscure individual data contributions.", "C": "Rely exclusively on static benchmarks to guarantee data immutability.", "D": "Minimize data collection and focus solely on model performance metrics."}, "answer": "B", "explanation": "The correct answer is B. The context explicitly states the privacy concerns associated with dynamic benchmarks and suggests differential privacy as a solution. Options A, C, and D are flawed because they either exacerbate the privacy risk or ignore the fundamental issue.", "question_token_count": 30, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 12}
{"context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n", "topic": "ITD\u2019s approach to identifying and rewriting contaminated samples from static benchmarks while preserving difficulty levels.", "question": "Within the ITD framework, how does the contamination detector\u2019s output directly influence the subsequent LLM rewriting process, specifically concerning the preservation of difficulty levels?", "choices": {"A": "The contamination detector simply flags samples as contaminated, and the LLM then generates completely new samples without considering the original difficulty.", "B": "The contamination detector\u2019s output is used to adjust the LLM\u2019s prompt, guiding it to rewrite the sample while maintaining a similar difficulty level, as determined by a separate metric.", "C": "The contamination detector\u2019s output is used to identify the most easily rewritten samples, prioritizing those for modification regardless of difficulty.", "D": "The contamination detector\u2019s output is discarded, and the LLM rewrites all samples based on a pre-defined difficulty scale."}, "answer": "B", "explanation": "The correct answer is B. The context explicitly states that ITD utilizes a contamination detector and then prompts an LLM to rewrite the contaminated samples *while preserving their difficulty levels*. The prompt adjustment is implied by this process.", "question_token_count": 30, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 5, "question_groundedness_score": 9, "avg_answer_token_count": 26}
{"context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n", "topic": "Describe the mathematical notation used to represent a static benchmark and its components.", "question": "What does the symbol \ud835\udcae(\u22c5) in the notation \ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(\u22c5)) represent within the context of a static benchmark?", "choices": {"A": "The set of input prompts.", "B": "The scoring function used to evaluate model outputs.", "C": "The expected output for each input prompt.", "D": "The dataset used to train the model."}, "answer": "B", "explanation": "The text explicitly states that \ud835\udcae(\u22c5) is the \u201cscoring function that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4.\u201d", "question_token_count": 36, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 1, "question_groundedness_score": 10, "avg_answer_token_count": 8}
{"context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n", "topic": "What does the external diversity formula represent in the context of transformed datasets?", "question": "What is the primary mathematical representation of \"external diversity\" as described within the provided context?", "choices": {"A": "It is a measure of the absolute difference between the mean of the transformed dataset and the mean of the seed dataset.", "B": "It is the expected value, across N transformations, of a diversity function (\u0398) that quantifies the difference between each transformed dataset and the original seed dataset.", "C": "It is the variance of the transformed dataset, calculated by subtracting the seed dataset from each transformed dataset.", "D": "It is the correlation coefficient between the transformed dataset and the seed dataset."}, "answer": "B", "explanation": "The correct answer (B) accurately reflects the formula provided in the text: \ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f. The other options misinterpret or misrepresent the mathematical operation involved.", "question_token_count": 19, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 5, "question_groundedness_score": 10, "avg_answer_token_count": 23}
{"context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n", "topic": "How do the proposed metrics, \"Collision Rate\" and \"Repeat Trials,\" specifically address the risk of data contamination in LLM evaluation?", "question": "How do the proposed \"Collision Rate\" and \"Repeat Trials\" metrics fundamentally differ in their approach to evaluating the potential for data contamination in LLM evaluation benchmarks?", "choices": {"A": "Collision Rate measures the diversity of the benchmark dataset, while Repeat Trials assesses the benchmark\u2019s ability to generate entirely new transformations.", "B": "Repeat Trials quantifies the number of transformations needed to fully regenerate a dataset, whereas Collision Rate measures the degree of overlap between transformed datasets.", "C": "Both metrics are directly correlated and provide identical information about benchmark robustness.", "D": "Collision Rate assesses the frequency of specific data points, while Repeat Trials evaluates the overall dataset size."}, "answer": "B", "explanation": "The context explicitly states that Collision Rate measures the percentage of overlap between transformed datasets, and Repeat Trials quantifies the number of transformations needed to regenerate a dataset.", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 21}
{"context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n", "topic": "The proposed design principles for dynamic benchmarking aim to overcome the limitations of existing methods and provide a more robust assessment of LLM data contamination risks.", "question": "What is the primary limitation identified within the context regarding existing dynamic benchmarking methods for mitigating LLM data contamination?", "choices": {"A": "A lack of standardized criteria for evaluating their effectiveness.", "B": "An over-reliance on static benchmarks, leading to inaccurate assessments.", "C": "The inherent difficulty in simulating real-world data contamination scenarios.", "D": "The computational cost associated with dynamic benchmarking procedures."}, "answer": "A", "explanation": "The text explicitly states \u201cwe then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks.\u201d", "question_token_count": 21, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 12}
{"context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n", "topic": "The primary challenge addressed by post-hoc detection methods is ensuring the reliability of performance metrics in machine learning.", "question": "Dekoninck et al. (2024) propose a method for detecting contamination that primarily relies on what mechanism?", "choices": {"A": "Direct overlap detection through n-gram matching.", "B": "Analyzing model behavior under conditions of partial completions.", "C": "Comparing model performance across multiple benchmarks.", "D": "Embedding-based similarity analysis."}, "answer": "C", "explanation": "The text explicitly states that CONSTAT, proposed by Dekoninck et al. (2024), \u201cdetects contamination by comparing model performance across benchmarks.\u201d", "question_token_count": 25, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 8}
{"context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n", "topic": "The increasing probability of data contamination in LLMs necessitates a shift towards dynamic benchmarks due to the limitations of static benchmarks.", "question": "Considering the exponential growth of training datasets and the observed increase in data contamination rates, what is the most critical deficiency currently hindering the effective evaluation of LLMs?", "choices": {"A": "The lack of human annotators for evaluating generated samples.", "B": "The inherent difficulty in scaling dynamic benchmark generation methods to accommodate diverse LLM architectures.", "C": "The absence of standardized criteria for assessing the robustness and reliability of dynamic benchmarks.", "D": "The limited computational resources available for conducting extensive post-hoc contamination detection."}, "answer": "C", "explanation": "The text explicitly states a lack of standardized criteria for evaluating dynamic benchmarks as a key challenge. Options A, B, and D address secondary issues, while option C directly reflects the central argument presented in the passage.", "question_token_count": 33, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 4, "avg_answer_token_count": 14}
{"context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n", "topic": "What are the potential benefits of employing hybrid approaches to dynamic benchmark construction?", "question": "In the context of dynamic benchmark construction, what is the primary rationale for utilizing hybrid approaches, and what specific challenge do they aim to mitigate?", "choices": {"A": "Hybrid approaches are unnecessary for dynamic benchmarks, as rule-based generation is the most efficient method.", "B": "Hybrid approaches leverage LLM-based generation exclusively to maximize data volume.", "C": "Hybrid approaches combine different benchmark construction techniques to address the challenges of interpretability and validation, particularly when using LLM-assisted transformations.", "D": "Hybrid approaches solely rely on temporal cutoff methods for data collection."}, "answer": "C", "explanation": "The text explicitly states that hybrid approaches \"combine the idea of these different approaches\" and are used to address the challenges of interpretability and validation, specifically when LLMs are involved.", "question_token_count": 29, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 6, "avg_answer_token_count": 17}
{"context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n", "topic": "Embedding-based similarity offers a more robust approach to post-hoc detection compared to exact matching.", "question": "Dekoninck et al. (2024) utilize which methodology to detect contamination by comparing model performance across benchmarks?", "choices": {"A": "N-gram matching at various levels, such as tokens.", "B": "Embedding-based similarity, specifically comparing model performance across benchmarks.", "C": "Analyzing model behavior under different conditions, including memorization through masked inputs.", "D": "Direct overlap detection using exact matching."}, "answer": "B", "explanation": "The answer is B because the text explicitly states that Dekoninck et al. (2024) propose CONSTAT, which \u201cdetects contamination by comparing model performance across benchmarks.\u201d", "question_token_count": 25, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 6, "avg_answer_token_count": 12}
{"context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n", "topic": "The conceptual definition of stability in dynamic benchmarking, focusing on variance across trials as an indicator of robustness.", "question": "According to the provided text, what is the primary interpretation of high variance in the complexity measurement function within the context of dynamic benchmarking stability?", "choices": {"A": "It signifies a decrease in task complexity.", "B": "It indicates a lack of data contamination.", "C": "It represents a high degree of robustness and stability.", "D": "It suggests a significant performance improvement."}, "answer": "C", "explanation": "The text states that \u201chigh variance indicates that the dynamic benchmarking method is not stable.\u201d Therefore, high variance represents instability, not stability.", "question_token_count": 28, "answer_correctness_score": 1, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 9}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "How does data contamination impact the reliability of performance metrics for LLMs, and why is this a significant concern?", "question": "Syntactic contamination, as defined in the context, poses a significant challenge to LLM evaluation primarily because it:", "choices": {"A": "Results in a complete loss of information, rendering the benchmark data unusable.", "B": "Introduces a direct and easily detectable duplicate of test data within the training set.", "C": "Creates a scenario where a transformed version of a test example exists in the training data, potentially inflating performance metrics without reflecting genuine model capability.", "D": "Only affects the accuracy of metrics related to factual recall."}, "answer": "C", "explanation": "The correct answer is C. Syntactic contamination involves transforming test data into a form that exists within the training data. This artificially boosts performance metrics, providing a misleading assessment of the LLM\u2019s true capabilities.", "question_token_count": 23, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 18}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "The methodology employed by LiveBench for constructing a reliable benchmark to evaluate LLM performance.", "question": "Considering the challenges presented by LLM knowledge cutoffs, which of the following benchmarks *most directly* addresses the risk of data contamination by utilizing information exclusively from sources after the model\u2019s training cutoff?", "choices": {"A": "AcademicEval, which focuses on evaluating LLM performance through academic writing tasks.", "B": "LiveCodeBench, which leverages human-written coding problems from online competition platforms.", "C": "AntiLeak-Bench, which generates queries about newly emerged knowledge unknown prior to the model's cutoff.", "D": "LiveBench, which employs math competitions from the preceding 12 months."}, "answer": "C", "explanation": "AntiLeak-Bench\u2019s explicit design goal is to assess knowledge *beyond* the model\u2019s cutoff date, directly mitigating data contamination. The other benchmarks focus on evaluating performance within the known knowledge domain.", "question_token_count": 39, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 16}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "The potential for contaminated benchmarks to distort evaluations of LLM generalization and robustness presents a significant challenge to the field.", "question": "Considering the described debate surrounding syntactic contamination, what fundamental limitation does the current inability to definitively distinguish between memorization and reasoning pose to the reliability of LLM benchmark evaluations?", "choices": {"A": "It primarily restricts the types of NLP applications that can benefit from LLM evaluations.", "B": "It introduces a systematic bias favoring models with superior recall capabilities over those with genuine reasoning skills.", "C": "It necessitates the development of entirely new evaluation metrics, independent of existing benchmarks.", "D": "It only affects the evaluation of models trained on smaller datasets."}, "answer": "B", "explanation": "The correct answer is B. The text explicitly states that contaminated benchmarks can lead to an overestimation of a model\u2019s true capabilities by inadvertently testing it on data it has already seen, thus favoring models that simply memorize data.", "question_token_count": 34, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 16}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "The significance of temporal cutoff dates in the context of evaluating Large Language Models.", "question": "What is the primary methodological justification for the development of benchmarks like AntiLeak-Bench and LiveBench in the context of evaluating Large Language Models?", "choices": {"A": "To assess the model\u2019s ability to accurately predict future events based on historical trends.", "B": "To evaluate the model\u2019s performance on tasks requiring knowledge exclusively from its training data, regardless of recency.", "C": "To mitigate the risk of data contamination by utilizing information exclusively from sources after the model's knowledge cutoff date.", "D": "To identify potential biases in the model\u2019s responses related to current social or political events."}, "answer": "C", "explanation": "The correct answer is C. The context explicitly states that AntiLeak-Bench generates queries about \u201cnewly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date\u201d to eliminate data contamination.", "question_token_count": 30, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 19}
{"context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n", "topic": "The creation and maintenance of a GitHub repository for collecting static and dynamic LLM benchmarking methods represent a valuable resource for the research community.", "question": "Considering the documented limitations of existing dynamic LLM benchmarks and the risk of data contamination, what constitutes the most critical operational challenge in maintaining the proposed GitHub repository of benchmarking methods?", "choices": {"A": "Ensuring the repository solely contains static benchmarks to avoid any potential contamination.", "B": "Maintaining a comprehensive and continuously updated catalog of diverse dynamic benchmarking approaches, addressing the identified gap in evaluation criteria.", "C": "Primarily focusing on attracting a large number of contributors to the repository, regardless of the quality of the submitted methods.", "D": "Implementing strict access controls to limit the repository's visibility and prevent unauthorized modifications."}, "answer": "B", "explanation": "The correct answer (B) directly addresses the identified gap \u2013 the lack of standardized criteria for evaluating dynamic benchmarks \u2013 which is the core problem the repository aims to solve. The other options represent misinterpretations or tangential concerns.", "question_token_count": 35, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 3, "avg_answer_token_count": 19}
{"context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n", "topic": "The proprietary nature of many LLM training datasets complicates accurate performance assessment and benchmark development, further compounding the contamination problem.", "question": "Considering the proprietary nature of LLM training datasets, what is the most significant impediment to establishing truly reliable and unbiased benchmarks for evaluating model performance?", "choices": {"A": "The inherent randomness of the web scraping process.", "B": "The difficulty in implementing retrieval-based detection methods.", "C": "The lack of transparency regarding training data, hindering verification of data contamination.", "D": "The limited availability of human-annotated datasets for fine-tuning."}, "answer": "C", "explanation": "The correct answer is C. The text explicitly states that \u201cmany LLMs keep their training data proprietary,\u201d which directly impedes the ability to verify and mitigate data contamination \u2013 the core issue of benchmark reliability.", "question_token_count": 29, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 12}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "What are the key differences between evaluating LLMs on SQL tables (S3Eval) versus randomly generated graphs (DyVal, NPHardEval, Xie et al.)?", "question": "What is a fundamental difference in the data structure utilized by S3Eval for LLM evaluation compared to the graph-based approaches (DyVal, NPHardEval, Xie et al.) described in the context?", "choices": {"A": "S3Eval utilizes naturally occurring SQL databases, while graph-based methods generate synthetic data.", "B": "S3Eval employs randomly generated SQL tables, whereas graph-based methods utilize randomly generated directed acyclic graphs (DAGs) or puzzle structures.", "C": "S3Eval focuses on evaluating LLMs' ability to perform mathematical calculations, while graph-based methods assess logical reasoning.", "D": "S3Eval is designed for evaluating LLMs on simple arithmetic problems, while graph-based methods are used for complex combinatorial problems."}, "answer": "B", "explanation": "The context explicitly states that S3Eval evaluates LLMs on \u201crandomly generated SQL tables\u201d and that graph-based methods utilize \u201crandomly generated directed acyclic graphs (DAGs).\u201d This directly differentiates the core data structures used in each evaluation framework.", "question_token_count": 42, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 24}
{"context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n", "topic": "The evolution of benchmarking strategies, from static to dynamic approaches, reflects the growing difficulty of ensuring the integrity of LLM evaluations as training datasets expand.", "question": "Considering the observed increase in data contamination as LLM training datasets expand, what fundamental characteristic of static benchmarks, as outlined in the text, necessitates the adoption of dynamic evaluation strategies?", "choices": {"A": "Static benchmarks are inherently resistant to contamination due to their reliance on curated, isolated datasets.", "B": "Static benchmarks provide a perfectly transparent and reproducible evaluation process, eliminating the risk of bias.", "C": "Static benchmarks\u2019 effectiveness diminishes proportionally with the size of the training dataset, making them increasingly susceptible to contamination.", "D": "Static benchmarks are universally accepted as the gold standard for LLM evaluation, regardless of training data scale."}, "answer": "C", "explanation": "The text explicitly states that \u201cstatic benchmarks become less effective as training corpora grow\u201d and that \u201cthe probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121\u201d. Option C accurately reflects this causal relationship.", "question_token_count": 36, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 20}
{"context": "###### Safety\nSafety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content. Datasets such as RealToxicityPrompts\u00a0(Gehman et\u00a0al., 2020) and ToxiGen\u00a0(Hartvigsen et\u00a0al., 2022) assess resilience against producing harmful outputs.\n\nBy providing a controlled environment to measure these aspects, safety benchmarks play a critical role in guiding the development of models that are not only powerful outputs but also responsible and trustworthy for real-world applications.\n\n\n###### Language\nLanguage benchmarks assess the LLMs\u2019 proficiency in specific languages.\n\nGLUE\u00a0(Wang, 2018) and SuperGLUE\u00a0(Wang et\u00a0al., 2019) cover tasks from sentiment analysis to language inference, while CLUE\u00a0(Xu et\u00a0al., 2020) targets Chinese language.\n\nTypo-fixing\u00a0(Suzgun et\u00a0al., 2022) is also widely used.\n\n\n###### Reading Comprehension\nReading comprehension tasks test a model\u2019s ability to extract and infer information from text. Benchmarks like SQuAD\u00a0(Rajpurkar et\u00a0al., 2018), QuAC\u00a0(Choi et\u00a0al., 2018), and BoolQ\u00a0(Clark et\u00a0al., 2019) challenge models to understand passages and draw logical conclusions.\n", "topic": "The significance of benchmark datasets like Typo-fixing in assessing language proficiency within LLMs.", "question": "Considering the diverse range of benchmarks presented, what is the primary differentiating factor in the design and purpose of the Typo-fixing dataset compared to benchmarks like SQuAD or GLUE?", "choices": {"A": "Typo-fixing focuses on assessing a model's ability to generate creative narratives.", "B": "Typo-fixing specifically evaluates a model\u2019s proficiency in Chinese language understanding.", "C": "Typo-fixing assesses a model\u2019s ability to correct typographical errors, distinguishing it from tasks involving complex reasoning or language generation.", "D": "Typo-fixing is designed to measure a model\u2019s resistance to generating toxic outputs, similar to RealToxicityPrompts."}, "answer": "C", "explanation": "The correct answer is C. The context states that Typo-fixing is \"widely used,\" implying a specific, focused purpose \u2013 correcting typographical errors \u2013 which contrasts with the broader scope of benchmarks like SQuAD (reading comprehension) or GLUE (general language understanding).", "question_token_count": 38, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 5, "avg_answer_token_count": 21}
{"context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n", "topic": "VarBench\u2019s method for replacing variables in existing benchmarks to generate novel training data.", "question": "According to the text, what is the primary mechanism employed by VarBench to generate new training data?", "choices": {"A": "Rewriting existing benchmark samples using an LLM to create related questions.", "B": "Identifying and replacing variables within existing benchmark samples using an LLM.", "C": "Expanding on examined concepts from the original benchmark using LLMs and knowledge graphs.", "D": "Utilizing a contamination detector to identify and correct contaminated samples in static benchmarks."}, "answer": "B", "explanation": "The text explicitly states, \u201cVarBench (Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\u201d", "question_token_count": 21, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 9, "question_groundedness_score": 10, "avg_answer_token_count": 14}
{"context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n", "topic": "The primary driver for the transition from static to dynamic benchmarking in LLMs is the increasing risk of data contamination due to reliance on large, internet-derived training datasets.", "question": "What fundamental limitation of static benchmarking, as identified within the text, directly motivates the transition to dynamic benchmarking approaches for LLMs?", "choices": {"A": "The reliance on smaller, curated datasets.", "B": "The inherent inability to detect subtle instances of data contamination.", "C": "The lack of standardized evaluation criteria, hindering accurate assessment of effectiveness.", "D": "The computational expense associated with static benchmark construction."}, "answer": "C", "explanation": "The text explicitly states a \u201ccritical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks.\u201d This directly addresses the reason for the shift.", "question_token_count": 26, "answer_correctness_score": 4, "explanation_validity_score": 2, "question_clarity_score": 6, "question_groundedness_score": 8, "avg_answer_token_count": 11}
{"context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n", "topic": "A key gap in current LLM benchmarking research is the absence of standardized criteria for evaluating the effectiveness of dynamic benchmarks.", "question": "What is the primary limitation identified in the research regarding current LLM benchmarking practices?", "choices": {"A": "The reliance on internet-derived training data is a significant concern.", "B": "The transition from static to dynamic benchmarks represents a successful mitigation strategy for data contamination.", "C": "The absence of standardized criteria for evaluating dynamic benchmarks.", "D": "The proposed design principles for dynamic benchmarking are overly complex."}, "answer": "C", "explanation": "The text explicitly states \u201ca critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks.\u201d", "question_token_count": 16, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 2, "avg_answer_token_count": 13}
{"context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n", "topic": "What is the primary concern regarding the correctness of dynamic benchmarking algorithms as presented in the text?", "question": "What is the most significant risk associated with dynamic benchmarking algorithms, as described in the text, concerning the reliability of LLM evaluations?", "choices": {"A": "The reliance on a domain-specific annotator as an oracle inevitably introduces subjective bias into the evaluation process.", "B": "The computational cost of generating large datasets for dynamic benchmarking is prohibitively expensive for many researchers.", "C": "The potential for the generated dataset's outputs to diverge significantly from the ground truth, leading to a false sense of reliability, undermines the benchmark's utility.", "D": "The difficulty in defining a universally applicable scoring function (\ud835\udcae\u2062(\u22c5)) prevents consistent evaluation across different LLMs."}, "answer": "C", "explanation": "The correct answer is C. The text explicitly states that \u201cIf the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability\u2026\u201d This highlights the core concern about the benchmark\u2019s validity.", "question_token_count": 26, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 8, "avg_answer_token_count": 24}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "The distinctions between HumanEval, MBPP, and SWE-Bench benchmarks in evaluating code synthesis and debugging capabilities.", "question": "Considering the evolution of coding benchmarks, what is the primary differentiating factor between SWE-Bench and HumanEval/MBPP regarding the types of errors a model might exhibit?", "choices": {"A": "SWE-Bench focuses exclusively on syntax errors, while HumanEval and MBPP primarily assess semantic correctness.", "B": "HumanEval and MBPP evaluate code functionality, whereas SWE-Bench prioritizes adherence to specific coding style guidelines.", "C": "SWE-Bench assesses the ability to generate complete, functional programs, while HumanEval and MBPP concentrate on debugging existing code.", "D": "HumanEval and MBPP are designed for simpler coding tasks, whereas SWE-Bench presents more complex, multi-step problems."}, "answer": "D", "explanation": "The correct answer (D) highlights that SWE-Bench is intended for more advanced challenges, requiring multi-step problem solving, differentiating it from the more focused tasks of HumanEval and MBPP. The other options misrepresent the core distinctions between the benchmarks.", "question_token_count": 34, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 8, "avg_answer_token_count": 24}
{"context": "###### Safety\nSafety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content. Datasets such as RealToxicityPrompts\u00a0(Gehman et\u00a0al., 2020) and ToxiGen\u00a0(Hartvigsen et\u00a0al., 2022) assess resilience against producing harmful outputs.\n\nBy providing a controlled environment to measure these aspects, safety benchmarks play a critical role in guiding the development of models that are not only powerful outputs but also responsible and trustworthy for real-world applications.\n\n\n###### Language\nLanguage benchmarks assess the LLMs\u2019 proficiency in specific languages.\n\nGLUE\u00a0(Wang, 2018) and SuperGLUE\u00a0(Wang et\u00a0al., 2019) cover tasks from sentiment analysis to language inference, while CLUE\u00a0(Xu et\u00a0al., 2020) targets Chinese language.\n\nTypo-fixing\u00a0(Suzgun et\u00a0al., 2022) is also widely used.\n\n\n###### Reading Comprehension\nReading comprehension tasks test a model\u2019s ability to extract and infer information from text. Benchmarks like SQuAD\u00a0(Rajpurkar et\u00a0al., 2018), QuAC\u00a0(Choi et\u00a0al., 2018), and BoolQ\u00a0(Clark et\u00a0al., 2019) challenge models to understand passages and draw logical conclusions.\n", "topic": "The role of safety benchmarks in evaluating LLM robustness and ethical alignment, specifically referencing datasets like RealToxicityPrompts and ToxiGen.", "question": "Considering the design goals of datasets like RealToxicityPrompts and ToxiGen, what is the primary methodological difference between them in assessing LLM safety?", "choices": {"A": "Both datasets rely on human annotation of toxicity levels for every generated response.", "B": "ToxiGen focuses on detecting subtle biases, while RealToxicityPrompts prioritizes identifying overtly harmful outputs.", "C": "RealToxicityPrompts uses a single, pre-defined toxicity threshold, whereas ToxiGen employs a dynamically adjusted threshold based on model output.", "D": "Both datasets are entirely based on automated analysis of generated text without any human input."}, "answer": "C", "explanation": "The correct answer (C) reflects the nuanced approach of ToxiGen, which adapts its toxicity detection based on the model's output, distinguishing it from RealToxicityPrompts, which uses a static threshold.  Options A and D are incorrect because both datasets incorporate human input. Option B misrepresents the core focus of each dataset.", "question_token_count": 32, "answer_correctness_score": 8, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 4, "avg_answer_token_count": 21}
{"context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n", "topic": "What are the primary distinctions between encryption and label protection as strategies for safeguarding evaluation data?", "question": "What is the fundamental distinction between encryption and label protection as methods for preserving the integrity of evaluation datasets?", "choices": {"A": "Encryption focuses on preventing unauthorized access to the data itself, while label protection prevents models from learning the correct answers.", "B": "Both methods achieve the same goal \u2013 ensuring data confidentiality \u2013 but employ different techniques.", "C": "Encryption primarily addresses the risk of data leakage, whereas label protection mitigates the risk of model memorization.", "D": "Label protection involves concealing the answers, while encryption involves scrambling the data."}, "answer": "A", "explanation": "The correct answer (A) accurately captures the core difference: encryption secures the data itself, preventing access, while label protection shields the answers from model exposure.", "question_token_count": 21, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 19}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "The AIME and CNMO challenges represent recent benchmarks designed to rigorously test a model's mathematical problem-solving skills.", "question": "Considering the described benchmarks (AIME, CNMO, GSM8K, MATH, etc.), which of the following best characterizes the primary distinction between mathematical problem-solving benchmarks and knowledge benchmarks?", "choices": {"A": "Mathematical benchmarks exclusively assess factual recall, while knowledge benchmarks focus on creative problem generation.", "B": "Mathematical benchmarks evaluate a model\u2019s ability to apply learned concepts to novel, multi-step problems, whereas knowledge benchmarks primarily test the breadth of the model\u2019s internal knowledge base.", "C": "Both types of benchmarks are fundamentally equivalent in their evaluation criteria, differing only in the specific datasets used.", "D": "Mathematical benchmarks assess the model's reasoning process, while knowledge benchmarks evaluate the model's ability to synthesize information from multiple sources."}, "answer": "B", "explanation": "The correct answer (B) accurately reflects the core distinction highlighted in the text \u2013 mathematical benchmarks prioritize applying existing knowledge to solve complex, novel problems, whereas knowledge benchmarks focus on demonstrating a broad understanding of factual information. Option A is incorrect as mathematical benchmarks are not just about recall. Option C is incorrect as the benchmarks have distinct goals. Option D is incorrect as the benchmarks evaluate different aspects of an LLM's capabilities.", "question_token_count": 38, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 23}
{"context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n", "topic": "The lack of transparency regarding training data impedes community efforts to verify and mitigate data contamination, highlighting the need for improved evaluation practices.", "question": "Considering the inherent opacity surrounding LLM training data, what is the most significant impediment to establishing truly unbiased and reliable benchmarks for evaluating model performance?", "choices": {"A": "The difficulty in implementing retrieval-based detection methods.", "B": "The potential for overlap between training and evaluation datasets, hindering accurate performance assessment.", "C": "The reliance on human-annotated datasets for fine-tuning.", "D": "The prevalence of synthetic datasets used in training."}, "answer": "B", "explanation": "The text explicitly states that \u201cThis opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\u201d Option B directly reflects this core argument.", "question_token_count": 29, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 12}
{"context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n", "topic": "Provide an example of what the \u201coracle\u201d function could be, as described in the text.", "question": "In the context of dynamic benchmarking, what is the primary function of the \u201coracle\u201d described in the text?", "choices": {"A": "To generate synthetic data for evaluating LLMs.", "B": "To provide a ground truth reference for assessing the correctness of a dynamic benchmark.", "C": "To automatically score the performance of LLMs on a given dataset.", "D": "To modify the input data to improve the accuracy of the benchmark."}, "answer": "B", "explanation": "The text explicitly states that the oracle \u201creturns the ground truth of its input,\u201d serving as an objective reference for correctness evaluation.", "question_token_count": 23, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 13}
{"context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n", "topic": "The subsequent fine-tuning process on human-annotated or synthetic datasets exacerbates the contamination risk, mirroring potential evaluation tasks.", "question": "Considering the challenges of evaluating LLMs, why does fine-tuning on datasets designed to mimic evaluation tasks represent a heightened risk of contamination, beyond simply increasing the volume of potentially overlapping data?", "choices": {"A": "Because synthetic datasets invariably lack the biases present in human-annotated data, reducing the likelihood of contamination.", "B": "Because the process of fine-tuning inherently alters the model\u2019s parameters, effectively removing any overlap with the original training data.", "C": "Because such fine-tuning replicates the evaluation process itself, introducing systematic errors that are difficult to detect.", "D": "Because the similarity between fine-tuning datasets and evaluation tasks creates a feedback loop, reinforcing biases and amplifying contamination."}, "answer": "D", "explanation": "The correct answer (D) highlights the core issue: the feedback loop created by mimicking evaluation tasks during fine-tuning. This process perpetuates and intensifies the contamination, making it a more significant problem than simply adding more potentially contaminated data.", "question_token_count": 37, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 22}
{"context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The mathematical representation of a dynamic benchmark, including the roles of the dataset, transformation function, and timestamped data sets.", "question": "What is the primary rationale for employing a transformation function (T) within the framework of a dynamic benchmark, as described in the provided text?", "choices": {"A": "To ensure the dataset remains static throughout the evaluation process, minimizing the risk of data contamination.", "B": "To introduce a controlled element of randomness into the benchmarking process, allowing for more robust performance analysis.", "C": "To adapt the dataset over time, mitigating potential biases and ensuring a more realistic assessment of the LLM\u2019s capabilities.", "D": "To simplify the dataset, reducing computational complexity and accelerating the benchmarking process."}, "answer": "C", "explanation": "The text explicitly states that the transformation function \"modifies the data set during the benchmarking to avoid possible data contamination.\"", "question_token_count": 29, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 19}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The evolution of LLM evaluation, from static to dynamic benchmarking, and the associated research landscape.", "question": "Considering the inherent risk of data contamination impacting static LLM benchmarks, what fundamental methodological shift does the text primarily advocate for in the evaluation of these models?", "choices": {"A": "Implementing differential privacy techniques during model training to isolate benchmark data.", "B": "Transitioning to continuous, real-time monitoring of LLM training data sources to identify contamination.", "C": "Employing dynamic benchmarking strategies that regenerate or continuously update benchmark datasets, minimizing the risk of data leakage.", "D": "Focusing exclusively on human evaluation of LLM outputs, bypassing automated benchmarking altogether."}, "answer": "C", "explanation": "The text explicitly states the move from static to dynamic benchmarking due to data contamination. Option C accurately reflects this shift, as it describes methods designed to mitigate contamination. Options A, B, and D are incorrect as they propose solutions that are not central to the argument presented in the text.", "question_token_count": 30, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 17}
{"context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n", "topic": "How might a high \"Collision Rate\" impact the reliability of a dynamic benchmark for evaluating LLM capabilities?", "question": "In the context of dynamic benchmarking, what is the primary consequence of a high Collision Rate, and how does this impact the benchmark\u2019s utility?", "choices": {"A": "It indicates a high degree of diversity in the generated test cases.", "B": "It suggests that the benchmark is resistant to data contamination.", "C": "It implies that the benchmark\u2019s ability to generate novel test cases is compromised, potentially leading to inaccurate evaluations of LLM capabilities.", "D": "It signifies that the benchmark requires more computational resources."}, "answer": "C", "explanation": "The text defines Collision Rate as the percentage of overlap between transformed datasets. High overlap suggests limited diversity and a reduced ability to generate truly novel test cases, directly undermining the benchmark\u2019s purpose.", "question_token_count": 29, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 15}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "Explain the difference between \"exact contamination\" and \"syntactic contamination,\" providing illustrative examples of each.", "question": "Which of the following best describes the key distinction between \"exact contamination\" and \"syntactic contamination\" as presented in the text?", "choices": {"A": "Exact contamination refers to instances where test data is paraphrased, while syntactic contamination involves verbatim duplication.", "B": "Exact contamination involves direct duplication of data points, whereas syntactic contamination involves data points that can be transformed syntactically to match training data.", "C": "Both terms describe the same phenomenon, indicating a lack of diversity in the training data.", "D": "Exact contamination only affects text data, while syntactic contamination exclusively impacts code snippets."}, "answer": "B", "explanation": "The correct answer highlights the fundamental difference: exact duplication versus transformation via syntactic operations.", "question_token_count": 28, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 21}
{"context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n", "topic": "Explain how TRUCE\u2019s use of confidential computing and secure multi-party computation contributes to private benchmarking, and why this is crucial for data confidentiality.", "question": "What is the primary strategic advantage of TRUCE\u2019s utilization of confidential computing and secure multi-party computation within the context of private benchmarking, as detailed in the provided text?", "choices": {"A": "It significantly reduces the computational cost of model evaluation.", "B": "It guarantees complete immunity from all forms of data leakage, regardless of system vulnerabilities.", "C": "It ensures that test data and model parameters remain confidential, preventing unauthorized access and potential data contamination during benchmarking.", "D": "It automatically generates diverse test datasets, mitigating the risk of overfitting."}, "answer": "C", "explanation": "The correct answer is C. The text explicitly states that TRUCE\u2019s methods \u201censure that test data and model parameters remain confidential.\u201d The other options are either incorrect or misinterpret the core benefit of the technology.", "question_token_count": 34, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 10, "avg_answer_token_count": 16}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "Recent advancements in LLM evaluation are reflected in benchmarks like MMLU-Redux and MMLU-Pro, which refine existing assessments.", "question": "MMLU-Redux and MMLU-Pro represent an iterative approach to evaluating LLMs. What is the primary distinguishing characteristic of these benchmarks compared to their predecessors like MMLU?", "choices": {"A": "They utilize exclusively mathematical problem-solving tasks.", "B": "They incorporate a significantly reduced dataset size to improve evaluation efficiency.", "C": "They represent refinements of existing assessments, incorporating more challenging question formulations and expanded datasets.", "D": "They are exclusively designed for evaluating models\u2019 ability to generate creative text formats."}, "answer": "C", "explanation": "The text explicitly states that MMLU-Redux and MMLU-Pro \u201crefine these assessments further,\u201d indicating an improvement and adjustment to the original MMLU benchmark.", "question_token_count": 38, "answer_correctness_score": 10, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 14}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "Explain the rationale for utilizing directed acyclic graphs (DAGs) in the DyVal framework and how they are transformed for LLM evaluation.", "question": "What is the primary justification for employing directed acyclic graphs (DAGs) within the DyVal framework for evaluating LLM reasoning capabilities?", "choices": {"A": "DAGs provide a simplified representation of complex SQL queries, enabling faster evaluation.", "B": "DAGs offer a controlled mechanism for adjusting the difficulty of reasoning tasks by manipulating node and edge counts.", "C": "DAGs are inherently more efficient than random graphs for generating SQL tables.", "D": "DAGs guarantee that LLMs will always arrive at the correct solution."}, "answer": "B", "explanation": "The context explicitly states that DAGs are constructed with varying numbers of nodes and edges to control task difficulty.", "question_token_count": 26, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 16}
{"context": "###### Safety\nSafety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content. Datasets such as RealToxicityPrompts\u00a0(Gehman et\u00a0al., 2020) and ToxiGen\u00a0(Hartvigsen et\u00a0al., 2022) assess resilience against producing harmful outputs.\n\nBy providing a controlled environment to measure these aspects, safety benchmarks play a critical role in guiding the development of models that are not only powerful outputs but also responsible and trustworthy for real-world applications.\n\n\n###### Language\nLanguage benchmarks assess the LLMs\u2019 proficiency in specific languages.\n\nGLUE\u00a0(Wang, 2018) and SuperGLUE\u00a0(Wang et\u00a0al., 2019) cover tasks from sentiment analysis to language inference, while CLUE\u00a0(Xu et\u00a0al., 2020) targets Chinese language.\n\nTypo-fixing\u00a0(Suzgun et\u00a0al., 2022) is also widely used.\n\n\n###### Reading Comprehension\nReading comprehension tasks test a model\u2019s ability to extract and infer information from text. Benchmarks like SQuAD\u00a0(Rajpurkar et\u00a0al., 2018), QuAC\u00a0(Choi et\u00a0al., 2018), and BoolQ\u00a0(Clark et\u00a0al., 2019) challenge models to understand passages and draw logical conclusions.\n", "topic": "The purpose and scope of language benchmarks, including datasets such as GLUE, SuperGLUE, and CLUE, and their coverage of tasks like sentiment analysis and language inference.", "question": "Considering the diverse tasks covered by benchmarks such as GLUE and SuperGLUE, what fundamental limitation does the inclusion of datasets like Typo-fixing introduce to the overall assessment of a language model\u2019s capabilities?", "choices": {"A": "Typo-fixing primarily assesses a model\u2019s ability to generate creative and engaging narratives.", "B": "Typo-fixing introduces a bias towards evaluating models\u2019 performance on tasks involving phonetic similarity rather than semantic understanding.", "C": "Typo-fixing focuses exclusively on evaluating a model\u2019s ability to handle grammatical errors in English.", "D": "Typo-fixing provides a standardized measure of a model\u2019s factual recall accuracy."}, "answer": "B", "explanation": "The correct answer is B. GLUE and SuperGLUE evaluate broader language understanding, including tasks like sentiment analysis and inference. Typo-fixing, focused on correcting typos, inherently prioritizes phonetic similarity over semantic meaning, creating a specific and potentially limited evaluation perspective.", "question_token_count": 42, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 8, "avg_answer_token_count": 19}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "What are the primary motivations for developing comprehensive benchmarks for evaluating Large Language Models?", "question": "What is the primary driver behind the development of dynamic benchmarks for Large Language Models, as outlined in the text?", "choices": {"A": "To provide a simpler and more easily interpretable assessment of model capabilities.", "B": "To address the issue of static benchmarks becoming obsolete due to the rapid advancement of LLMs and the risk of data contamination.", "C": "To exclusively focus on evaluating instruction-following tasks within LLMs.", "D": "To eliminate the need for human effort in benchmark creation."}, "answer": "B", "explanation": "The text explicitly states that static benchmarks face challenges as LLMs evolve and introduce data contamination risks. Dynamic benchmarks are presented as a solution to these problems.", "question_token_count": 23, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 16}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "The role of benchmarks such as PIQA, SIQA, and CommonsenseQA in assessing a model\u2019s intuitive reasoning skills and background knowledge integration.", "question": "CommonsenseQA distinguishes itself from other reasoning benchmarks by primarily focusing on evaluating a model's ability to:", "choices": {"A": "Generate executable code snippets.", "B": "Accurately follow complex, multi-step instructions.", "C": "Integrate background knowledge with logical reasoning to answer questions requiring common sense.", "D": "Predict the next word in a sequence."}, "answer": "C", "explanation": "CommonsenseQA explicitly states it \u201crequire[s] the integration of background knowledge with logical reasoning to arrive at plausible answers.\u201d Options A, B, and D represent different benchmark categories described in the text.", "question_token_count": 20, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 10}
{"context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n", "topic": "Analyzing model behavior under conditions like memorization through masked inputs provides an alternative method for detecting contamination.", "question": "Dekoninck et al. (2024) propose a method for detecting contamination that relies on comparing model performance across which benchmarks?", "choices": {"A": "Direct overlap detection using n-gram matching.", "B": "Embedding-based similarity analysis of tokens.", "C": "Performance across multiple benchmarks.", "D": "Mapping metrics to identify exact data overlaps."}, "answer": "C", "explanation": "The text explicitly states that CONSTAT, proposed by Dekoninck et al. (2024), \u201cdetects contamination by comparing model performance across benchmarks.\u201d", "question_token_count": 28, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 10, "avg_answer_token_count": 8}
{"context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n", "topic": "MMLU-CF\u2019s approach to generating novel multiple-choice questions by shuffling answer options and replacing incorrect choices.", "question": "What is the core strategy employed by MMLU-CF to generate novel multiple-choice questions, as described in the context?", "choices": {"A": "Utilizing temporal cutoffs to assess LLM performance.", "B": "Shuffling answer choices and replacing incorrect options with \u201cNone of the other choices.\u201d", "C": "Synthesizing test cases based on predefined rules with minimal collision probability.", "D": "Generating queries adhering to the rules of Mathador games and varying input numbers."}, "answer": "B", "explanation": "The correct answer is B, as the context explicitly states \u201cMMLU-CF (Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \u201cNone of the other choices.\u201d\u201d.", "question_token_count": 26, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 15}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The challenges of data contamination in LLM benchmarking and the shift towards dynamic evaluation strategies.", "question": "Considering the described methods for mitigating data contamination in LLM benchmarks \u2013 data regeneration and continuous dataset updates \u2013 why do these approaches alone represent an insufficient strategy for truly evaluating an LLM\u2019s independent intelligence?", "choices": {"A": "Data regeneration inherently introduces a bias towards the original benchmark distribution, failing to capture the model\u2019s adaptability to unseen data.", "B": "Continuous dataset updates do not address the fundamental problem of models learning from data they were exposed to during training, regardless of the update frequency.", "C": "Both data regeneration and continuous updates are computationally infeasible for large-scale LLMs, limiting their practical application.", "D": "The primary concern with data contamination is the model\u2019s ability to memorize specific examples, not its generalizability to novel situations."}, "answer": "B", "explanation": "The text explicitly states that data regeneration merely reconstructs the original benchmarks, failing to address the core issue of the model already having learned from that data during training. Continuous updates, while helpful, don't fundamentally change the fact that the model's knowledge base is initially influenced by contaminated data.", "question_token_count": 38, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 8, "avg_answer_token_count": 24}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "The approach utilized by LiveCodeBench for gathering new coding problems from online competition platforms.", "question": "What distinguishes LiveCodeBench\u2019s methodology for acquiring coding problems compared to approaches like AntiLeak-Bench and AcademicEval, as highlighted in the provided text?", "choices": {"A": "LiveCodeBench exclusively utilizes data from arXiv papers to assess LLM performance.", "B": "LiveCodeBench continuously gathers new problems from online coding competitions, focusing on recent challenges.", "C": "LiveCodeBench relies solely on data collected from math competitions within the past 12 months.", "D": "LiveCodeBench generates queries about newly emerged knowledge to avoid data contamination."}, "answer": "B", "explanation": "The text explicitly states that LiveCodeBench \u201ccontinuously collects new human-written coding problems from online coding competition platforms like LeetCode.\u201d This distinguishes it from the other approaches which focus on different data sources or strategies.", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 17}
{"context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n", "topic": "Future research should prioritize the development of standardized dynamic evaluation approaches for LLMs.", "question": "Considering the identified limitations regarding the increasing vulnerability of static LLM benchmarks to data contamination, what is the most critical element for establishing a truly standardized dynamic evaluation approach?", "choices": {"A": "Implementing a single, universally applicable metric for assessing LLM performance.", "B": "Developing a system capable of simulating diverse, real-world user interactions to identify subtle biases.", "C": "Establishing rigorous protocols for data provenance and ensuring complete dataset transparency.", "D": "Focusing solely on evaluating LLM performance on established, curated datasets."}, "answer": "C", "explanation": "The context emphasizes the need for standardized dynamic evaluation to combat data contamination. While all options represent relevant considerations, the most critical element is ensuring data provenance and transparency \u2013 a cornerstone of reliable dynamic evaluation.", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 15}
{"context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n", "topic": "What types of costs, beyond monetary expenses, are explicitly mentioned as factors that should be considered when evaluating the scalability of a dynamic benchmark?", "question": "Beyond monetary expenses, what specific types of resources are identified as contributing to the cost associated with a dynamic benchmark\u2019s scalability evaluation?", "choices": {"A": "Computational resources and memory allocation.", "B": "Time spent on data transformation and manual effort in dataset curation.", "C": "Storage capacity and network bandwidth.", "D": "Software licenses and hardware maintenance fees."}, "answer": "B", "explanation": "The text explicitly states that \u201cCost\u201d encompasses \u201cmonetary cost, time spent, or manual effort.\u201d Option B directly reflects this.", "question_token_count": 26, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 9}
{"context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n", "topic": "Existing static benchmarking methods suffer from inherent limitations when addressing data contamination, necessitating the development of dynamic approaches.", "question": "What is the primary impetus for transitioning from static to dynamic benchmarking methods for Large Language Models (LLMs)?", "choices": {"A": "To improve the computational efficiency of LLM evaluations.", "B": "To address the inherent limitations of static benchmarks in mitigating the risks associated with data contamination.", "C": "To simplify the process of creating new LLM training datasets.", "D": "To reduce the reliance on Internet-derived data for LLM training."}, "answer": "B", "explanation": "The text explicitly states that data contamination is a growing concern due to the reliance of LLMs on vast Internet-derived training corpora, necessitating the shift to dynamic benchmarking.", "question_token_count": 23, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 13}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "Canary strings function as unique tokens embedded within datasets to identify instances where an LLM has memorized portions of its training data, indicating potential data leakage.", "question": "In the context of mitigating data contamination in LLMs, what is the primary purpose of embedding \"canary strings\" within a dataset?", "choices": {"A": "To improve the model\u2019s fluency and coherence.", "B": "To serve as unique identifiers for individual training examples.", "C": "To flag instances where the model has memorized portions of its training data.", "D": "To optimize the model\u2019s response time during inference."}, "answer": "C", "explanation": "Canary strings are specifically designed to indicate memorization, not any other characteristic of the model\u2019s output.", "question_token_count": 28, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 12}
{"context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n", "topic": "Further exploration of ethical guidelines is needed regarding data usage, model transparency, and the societal impact of AI benchmarks.", "question": "Considering the potential for bias amplification and the inherent privacy risks associated with dynamic AI benchmarks, what is the most critical initial step in establishing an ethically sound benchmarking framework?", "choices": {"A": "Prioritize the rapid expansion of benchmark datasets to ensure comprehensive coverage.", "B": "Implement robust data anonymization techniques and establish clear data usage policies from the outset.", "C": "Focus solely on developing benchmarks that demonstrably improve model performance metrics.", "D": "Adopt a purely static benchmark approach to eliminate any potential for privacy violations."}, "answer": "B", "explanation": "The correct answer is B, as the context explicitly states privacy and security concerns with dynamic benchmarks. Data anonymization and clear policies are paramount for mitigating these risks.", "question_token_count": 34, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 15}
{"context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n", "topic": "Standardized criteria are currently lacking for evaluating dynamic benchmarks, posing a significant challenge to their effective development and use.", "question": "Considering the documented challenges with both static and dynamic benchmarks, specifically regarding the increasing risk of data contamination and the lack of standardized evaluation criteria for dynamic benchmarks, what is the most critical prerequisite for establishing a reliable and meaningful framework for assessing LLM performance using these dynamic approaches?", "choices": {"A": "Implementing stricter post-hoc contamination detection methods across all benchmark datasets.", "B": "Prioritizing the development of LLMs exclusively trained on curated, isolated datasets to minimize contamination risks.", "C": "Establishing universally accepted metrics and protocols for assessing benchmark fidelity, scalability, and complexity within dynamic benchmark evaluations.", "D": "Focusing solely on expanding the size of dynamic benchmark datasets to compensate for the limitations of static benchmarks."}, "answer": "C", "explanation": "The correct answer (C) directly addresses the core problem identified in the text: the lack of standardized criteria. The other options propose solutions that either don\u2019t address the root issue (A, B, and D) or are impractical given the nature of LLM training and data.", "question_token_count": 53, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 7, "avg_answer_token_count": 20}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "The purpose of AntiLeak-Bench in preventing data contamination when assessing LLM knowledge.", "question": "What is the primary mechanism employed by AntiLeak-Bench to mitigate data contamination in LLM knowledge assessments?", "choices": {"A": "Utilizing historical data from academic publications.", "B": "Generating queries about recently emerged knowledge that the LLM would not have been exposed to during its training.", "C": "Focusing on questions related to established mathematical concepts.", "D": "Employing a temporal cutoff to exclude data collected after the LLM\u2019s knowledge cutoff date."}, "answer": "B", "explanation": "The context explicitly states that AntiLeak-Bench \"generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\"", "question_token_count": 22, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 14}
{"context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n", "topic": "The Auto-Dataset method's strategy for generating new samples that maintain stylistic elements and essential knowledge of original benchmarks.", "question": "According to the text, what is a primary distinction between the sample generation strategy employed by Auto-Dataset compared to other methods like StructEval and ITD?", "choices": {"A": "Auto-Dataset exclusively generates questions at varying cognitive levels.", "B": "Auto-Dataset generates two distinct types of samples: those maintaining original style and knowledge, and those presenting related questions at different cognitive levels.", "C": "Auto-Dataset relies solely on rewriting existing benchmark samples.", "D": "Auto-Dataset utilizes knowledge graphs to extend benchmark concepts."}, "answer": "B", "explanation": "The text explicitly states that Auto-Dataset generates \"two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels.\"", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 16}
{"context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n", "topic": "The potential of multi-agent frameworks for creating scalable and diverse benchmarks, exemplified by Benchmark Self-Evolving and BENCHAGENTS.", "question": "In the context of Benchmark Self-Evolving and BENCHAGENTS, what is the primary strategic advantage of decomposing the benchmark creation process into distinct agent-specific stages (planning, generation, verification, and evaluation)?", "choices": {"A": "Ensuring consistent adherence to the original benchmark's structure, minimizing deviation from the initial task description.", "B": "Facilitating independent agent specialization, leading to increased efficiency in each individual stage and overall benchmark creation speed.", "C": "Introducing redundancy into the process, allowing for multiple agents to independently verify the quality of the benchmark, enhancing robustness.", "D": "Enabling the system to dynamically adapt the benchmark based on real-time performance feedback from human evaluators, optimizing for diversity."}, "answer": "B", "explanation": "The correct answer is (B). The text explicitly states that each stage is handled by a \"specialized LLM agent,\" implying independent expertise and optimized performance within that specific area.  Options A, C, and D misinterpret the purpose of the agent decomposition.", "question_token_count": 43, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 22}
{"context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n", "topic": "What are N-gram metrics and reference-based metrics (e.g., BLEU scores) mentioned as potential examples of functions \u0398(\ud835\udc9fi, \ud835\udc9fj)?", "question": "Within the context of measuring dataset diversity, as exemplified by the function \u0398(\ud835\udc9fi, \ud835\udc9fj), what categories of metrics are explicitly mentioned as potential instances of this function?", "choices": {"A": "Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA).", "B": "N-gram metrics and reference-based metrics, such as BLEU scores.", "C": "Kullback-Leibler divergence and Jensen-Shannon divergence.", "D": "Cosine similarity and Euclidean distance."}, "answer": "B", "explanation": "The text explicitly states that N-gram metrics and reference-based metrics (including BLEU scores) are examples of the function \u0398(\ud835\udc9fi, \ud835\udc9fj).", "question_token_count": 39, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 13}
{"context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n", "topic": "The strategic advantages of employing LLMs as interviewers in evaluating other LLMs, considering the iterative nature of the process.", "question": "In the context of LLM evaluation methodologies described, what is the primary differentiating factor between the approach outlined in \"LLM-as-an-Interviewer\" and \"KIEval\"?", "choices": {"A": "Both methods rely solely on generating questions based on initial static benchmarks.", "B": "\u201cLLM-as-an-Interviewer\u201d employs a single iterative round of questioning, while \u201cKIEval\u201d generates follow-up subtopics and questions based on the examined LLM\u2019s response.", "C": "\u201cLLM-as-an-Interviewer\u201d exclusively utilizes multi-agent frameworks, unlike \u201cKIEval.\u201d", "D": "Both methods are fundamentally identical in their question generation process."}, "answer": "B", "explanation": "The text explicitly states that \u201cLLM-as-an-Interviewer\u201d uses a multi-turn evaluation with follow-up questions, while \u201cKIEval\u201d generates follow-up questions based on the evaluated model\u2019s response. This distinction highlights a core difference in their approaches to iterative questioning.", "question_token_count": 36, "answer_correctness_score": 8, "explanation_validity_score": 9, "question_clarity_score": 5, "question_groundedness_score": 10, "avg_answer_token_count": 21}
{"context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n", "topic": "How does interpretability factor into the design and validation of transformations used in LLM-assisted benchmarks?", "question": "In the context of LLM-assisted benchmarks, what is the primary justification for employing additional validation mechanisms beyond purely interpretable transformations?", "choices": {"A": "Rule-based transformations inherently guarantee data correctness.", "B": "LLM-based transformations are always transparent and easily traceable.", "C": "The scale of transformed data necessitates reduced manual verification efforts.", "D": "LLM-generated transformations lack inherent interpretability, demanding supplementary checks for reliability."}, "answer": "D", "explanation": "The text explicitly states that LLM-based transformations \u201cdepend on the model\u2019s transparency and traceability\u201d and may require \u201cexplainability tools, or human-in-the-loop validation.\u201d", "question_token_count": 25, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 12}
{"context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n", "topic": "Explain the difference between external and internal diversity as defined in the provided text.", "question": "Considering the definitions of external and internal diversity presented, how does the calculation of internal diversity fundamentally differ from the calculation of external diversity regarding the datasets being compared?", "choices": {"A": "Internal diversity measures the similarity between two distinct datasets, while external diversity measures the difference between a transformed dataset and the original dataset.", "B": "Internal diversity calculates the average difference between all possible pairs of transformation trials, whereas external diversity calculates the difference between a single transformed dataset and the seed dataset.", "C": "External diversity relies on N-gram metrics, while internal diversity utilizes BLEU scores, irrespective of the datasets involved.", "D": "Both external and internal diversity calculations are identical, differing only in the specific diversity function (\u0398) employed."}, "answer": "B", "explanation": "The correct answer (B) highlights the key distinction: internal diversity involves comparing *different* transformations of the same dataset, whereas external diversity compares a single transformed dataset to the original. Options A and C are partially correct but miss the core difference. Option D is definitively incorrect.", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 25}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "Describe the challenges associated with scaling the difficulty of graph-based reasoning tasks, as exemplified by the use of the Traveling Salesman Problem (TSP) in NPHardEval.", "question": "How does the exponential growth of the Traveling Salesman Problem (TSP) complicate the scaling of graph-based reasoning tasks within frameworks like NPHardEval?", "choices": {"A": "Increasing the number of nodes and edges in the graph proportionally increases the computational cost of evaluation, providing a straightforward and reliable method for controlling difficulty.", "B": "The inherent complexity of TSP means that simply increasing graph size doesn't reliably correlate with difficulty, as the solution space grows exponentially.", "C": "Graph size is the primary metric used by NPHardEval to determine the difficulty of a problem, ensuring consistent and reproducible results.", "D": "Smaller graphs are inherently easier to solve than larger graphs, regardless of the problem being represented."}, "answer": "B", "explanation": "The correct answer (B) accurately reflects the central challenge described in the context \u2013 the exponential growth of TSP makes simply increasing graph size an insufficient method for controlling difficulty. The other options present misconceptions about the relationship between graph size and problem complexity.", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 25}
{"context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n", "topic": "What are the primary challenges associated with verifying the correctness of transformations generated within dynamic benchmarking frameworks?", "question": "Within the context of dynamic benchmarking, what is the primary impediment to verifying the correctness of transformations generated by LLM-based methods, and what strategy is suggested to mitigate this challenge?", "choices": {"A": "The inherent transparency of LLMs eliminates the need for verification strategies.", "B": "The reliance on LLM-generated data necessitates the use of explainability tools and human-in-the-loop validation.", "C": "Temporal cutoff benchmarks inherently guarantee transformation correctness.", "D": "Rule-based transformations are always more reliable than LLM-based transformations."}, "answer": "B", "explanation": "The context states, \"In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\" Therefore, option B accurately reflects the suggested strategy.", "question_token_count": 36, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 14}
{"context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n", "topic": "CONSTAT is a specific post-hoc detection method that compares model performance across benchmarks to identify contamination.", "question": "CONSTAT\u2019s methodology centers on a comparative analysis of model performance. What fundamental assumption underlies CONSTAT\u2019s effectiveness as a contamination detection method, as suggested by the text?", "choices": {"A": "CONSTAT relies solely on exact n-gram matching to identify contaminated data.", "B": "CONSTAT\u2019s efficacy is predicated on the assumption that model performance consistently degrades across diverse benchmarks when contamination is present.", "C": "CONSTAT\u2019s success hinges on identifying instances where a model exhibits preferential behavior towards original test cases compared to paraphrased ones.", "D": "CONSTAT\u2019s effectiveness is determined by analyzing model responses to masked inputs to detect memorization."}, "answer": "B", "explanation": "The correct answer (B) reflects the core principle of CONSTAT \u2013 comparing performance *across* benchmarks. The text states that CONSTAT \u201cdetects contamination by comparing model performance across benchmarks.\u201d The other options represent alternative contamination detection techniques described in the context, not the fundamental principle of CONSTAT itself.", "question_token_count": 35, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 21}
{"context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n", "topic": "What are the potential vulnerabilities associated with encryption methods, specifically concerning key management and computational costs?", "question": "What is a primary operational vulnerability introduced by the implementation of encryption techniques for data protection, beyond simple key compromise?", "choices": {"A": "The increased reliance on specialized hardware for secure computation.", "B": "The elevated computational costs associated with encryption and decryption processes.", "C": "The potential for introducing subtle biases in the data due to the encryption algorithm itself.", "D": "The limitations of public key cryptography in preventing adversarial attacks."}, "answer": "B", "explanation": "The text explicitly states, \u201cthey introduce extra computational overheads.\u201d Option B directly reflects this statement. Options A, C, and D introduce complexities not directly addressed in the provided context.", "question_token_count": 23, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 8, "avg_answer_token_count": 13}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "The source of data used by LiveAoPSBench for creating benchmarks focused on mathematical problem-solving.", "question": "Why did LiveAoPSBench prioritize collecting math problems from the Art of Problem Solving (AoPS) forum as its data source, rather than, for example, a collection of past math competition problems?", "choices": {"A": "Because AoPS problems tend to be more consistently formatted and easier for LLMs to parse.", "B": "Because the AoPS forum represents a continuous stream of problems reflecting current mathematical challenges and student engagement.", "C": "Because competition problems are inherently more reliable indicators of mathematical understanding than forum discussions.", "D": "Because AoPS problems are exclusively focused on topics covered in standardized exams."}, "answer": "B", "explanation": "The context states \u201cLiveAoPSBench (Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\u201d  Option B accurately reflects the continuous, current nature of the AoPS forum\u2019s problem stream, which is a key differentiator.", "question_token_count": 41, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 7, "avg_answer_token_count": 17}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "Specialized benchmarks, including ControlBench, FRAMES, and GPQA Diamond, target specific technical and long-context challenges within LLM capabilities.", "question": "ControlBench, FRAMES, and GPQA Diamond represent a shift in LLM benchmarking, primarily focusing on what specific type of challenge?", "choices": {"A": "Evaluating factual recall across diverse domains.", "B": "Assessing mathematical problem-solving abilities.", "C": "Targeting technical and long-context capabilities.", "D": "Measuring the model\u2019s ability to generate creative text formats."}, "answer": "C", "explanation": "The text explicitly states that ControlBench, FRAMES, and GPQA Diamond \u201ctarget technical and long-context challenges.\u201d", "question_token_count": 27, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 9}
{"context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n", "topic": "Explain the significance of the dataset (\ud835\udcb3) within the context of a static benchmark.", "question": "Within the framework of static benchmarking, what is the primary consequence of altering the dataset (\ud835\udcb3) used as the seed, and how does this alteration impact the reliability of the benchmark results?", "choices": {"A": "Altering \ud835\udcb3 will have no effect on the benchmark results, as the scoring function (\ud835\udcae) is independent of the input data.", "B": "Modifying \ud835\udcb3 introduces systematic biases into the benchmark, consistently favoring models that perform well on those specific prompts.", "C": "Changing \ud835\udcb3 fundamentally alters the benchmark, rendering previous results invalid and necessitating a complete re-evaluation of model performance.", "D": "Altering \ud835\udcb3 only affects the scoring function (\ud835\udcae), but not the evaluation of the LLM's ability to perform the tasks defined in the benchmark."}, "answer": "C", "explanation": "The text states that \ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae) and that \ud835\udcb3 is the \u201cseed dataset.\u201d Changing the seed dataset directly impacts the benchmark, as it provides the initial set of prompts. Option C most accurately reflects this impact \u2013 a different seed dataset leads to different results, invalidating previous evaluations.", "question_token_count": 39, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 7, "avg_answer_token_count": 27}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "What are some potential strategies for mitigating the risk of data contamination during LLM training?", "question": "Considering the risk of syntactic contamination, what is the most effective approach to mitigate the potential for a test example to be re-identified in the training data through subtle transformations, without resorting to exhaustive manual inspection?", "choices": {"A": "Implement strict data sanitization protocols that remove all punctuation and whitespace from the training data.", "B": "Employ a combination of synonym substitution and morphological variations during training to artificially broaden the training data\u2019s lexical diversity.", "C": "Utilize a novel metric that quantifies the likelihood of a test example being syntactically equivalent to a training example, prioritizing examples with lower scores for inclusion in training.", "D": "Rely on automated tools to detect verbatim copies of test examples within the training data, coupled with a human review process for ambiguous cases."}, "answer": "C", "explanation": "The correct answer is C. Syntactic contamination requires recognizing transformations, not just exact matches. Option C directly addresses this by proposing a metric to quantify the likelihood of syntactic equivalence, which is a more sophisticated approach. Options A and B are irrelevant and would not address the core problem. Option D is a reasonable partial solution but lacks the proactive, analytical element needed to truly mitigate the risk.", "question_token_count": 43, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 26}
{"context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n", "topic": "How is the \"Correctness\" of a dynamic benchmark defined and quantified according to the proposed evaluation criteria?", "question": "According to the text, how is the \"Correctness\" of a dynamic benchmark mathematically defined and what role does the oracle function (\ud835\udca2\u2062(\u22c5)) play in its quantification?", "choices": {"A": "Correctness is determined solely by the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) applied to the transformed dataset\u2019s outputs.", "B": "Correctness is defined as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, determined by the oracle function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) and measured by the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ).", "C": "Correctness is a subjective assessment based on the annotator\u2019s judgment of the transformed dataset\u2019s outputs.", "D": "Correctness is irrelevant to the evaluation of dynamic benchmarks."}, "answer": "B", "explanation": "The correct answer (B) directly reflects the mathematical definition provided in the text, highlighting the role of the oracle function and the scoring function in measuring alignment.", "question_token_count": 38, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 38}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "Define \"exact contamination\" within the context of LLM evaluation data, citing the precise criteria for its occurrence.", "question": "What constitutes \u201cexact contamination\u201d within the framework of LLM evaluation data, as precisely defined in the provided text?", "choices": {"A": "Any instance where the evaluation dataset contains data points that are semantically similar to those in the training dataset.", "B": "The presence of identical data points in both the training and evaluation datasets, irrespective of syntactic variations.", "C": "Instances where the evaluation dataset contains data points that are paraphrased versions of those in the training dataset.", "D": "The inclusion of code snippets from benchmark implementations within the training dataset."}, "answer": "B", "explanation": "The text explicitly states that \u201cexact contamination occurs when there is any exact duplicate in the benchmark dataset.\u201d Option B correctly captures this precise definition. Options A, C, and D describe related but distinct forms of contamination (semantic similarity, paraphrasing, and code inclusion, respectively).", "question_token_count": 23, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 19}
{"context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n", "topic": "StructEval\u2019s utilization of knowledge graphs to expand upon concepts within existing benchmarks using LLMs.", "question": "How does StructEval\u2019s utilization of knowledge graphs specifically mitigate the risk of in-distribution contamination during benchmark expansion, as highlighted in the context?", "choices": {"A": "By generating entirely new benchmark samples unrelated to the original.", "B": "By identifying and removing problematic samples before LLM processing.", "C": "By developing extended questions that build upon existing concepts within the original benchmark.", "D": "By prompting the LLM to prioritize stylistic similarity over factual accuracy."}, "answer": "C", "explanation": "The correct answer is C. The context explicitly states that StructEval \u201cexpands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\u201d The other options misrepresent the method or its purpose.", "question_token_count": 28, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 13}
{"context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n", "topic": "The role of dynamic question generation in enhancing the effectiveness of benchmark evaluations, as demonstrated by TreeEval and KIEval.", "question": "Considering the design principles of TreeEval and KIEval, what fundamental limitation of traditional static benchmark evaluations does dynamic question generation demonstrably address?", "choices": {"A": "Static benchmarks lack the ability to account for nuanced variations in model performance across different contexts.", "B": "Static benchmarks inherently fail to capture the iterative nature of human reasoning and problem-solving.", "C": "Static benchmarks provide an insufficient mechanism for adapting evaluation difficulty based on model proficiency.", "D": "Static benchmarks are incapable of simulating the feedback loop present in human-LLM interactions."}, "answer": "C", "explanation": "The correct answer is C. TreeEval and KIEval both operate on the principle of iteratively refining questions based on the model's previous responses, directly addressing the limitation of static benchmarks which present a fixed set of questions regardless of the model\u2019s understanding.", "question_token_count": 30, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 17}
{"context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n", "topic": "Dynamic LLM benchmarking methods face challenges regarding reliability and reproducibility.", "question": "Considering the identified limitations regarding dynamic LLM benchmarking, what represents the most significant obstacle to achieving reproducible results within this methodology?", "choices": {"A": "The inherent computational cost of evaluating dynamic benchmarks.", "B": "The difficulty in designing benchmarks that accurately reflect real-world usage scenarios.", "C": "The potential for contamination due to model training data, exacerbated by expanding datasets.", "D": "The lack of standardized evaluation metrics for dynamic benchmarking techniques."}, "answer": "C", "explanation": "The correct answer is C. The text explicitly states that dynamic approaches \u201cface challenges in reliability and reproducibility\u201d and that static methods \u201cbecome more vulnerable to contamination as training datasets grow.\u201d Options A, B, and D represent secondary concerns discussed in the text.", "question_token_count": 24, "answer_correctness_score": 8, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 13}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "The significance of benchmarks like ARC and OpenBookQA in challenging language models to integrate background knowledge with logical reasoning.", "question": "In the context of language model evaluation, what fundamental limitation does the ARC and OpenBookQA benchmark specifically target, and how does this differ from traditional reasoning benchmarks?", "choices": {"A": "They primarily assess a model\u2019s ability to generate creative text formats, similar to those evaluated by the HellaSwag dataset.", "B": "They evaluate a model\u2019s capacity for zero-shot learning, testing its performance on tasks it has not been explicitly trained on.", "C": "They challenge models to integrate external, often implicit, background knowledge with logical inference, demanding a more comprehensive understanding than simply applying learned patterns.", "D": "They focus on evaluating a model\u2019s adherence to stylistic constraints within a given prompt, mirroring the objectives of the IFEval benchmark."}, "answer": "C", "explanation": "The correct answer is C. ARC and OpenBookQA are designed to test a model\u2019s ability to use external knowledge (background knowledge) alongside logical reasoning to answer questions. This goes beyond simply recognizing patterns or applying learned associations, as demonstrated by the other answer choices.", "question_token_count": 33, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 26}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "Knowledge benchmarks evaluate an LLM's internal factual knowledge, utilizing datasets like NaturalQuestions and TriviaQA, alongside more comprehensive assessments such as MMLU and BBH.", "question": "Considering the progression of knowledge benchmarks, what fundamental distinction underlies the shift from datasets like NaturalQuestions and TriviaQA to benchmarks such as ControlBench and GPQA Diamond?", "choices": {"A": "The former rely on simple factual recall, while the latter prioritize complex reasoning and multi-step problem-solving.", "B": "NaturalQuestions and TriviaQA assess purely linguistic understanding, whereas ControlBench and GPQA Diamond focus exclusively on mathematical proficiency.", "C": "The earlier datasets utilized a closed-domain approach, whereas the newer benchmarks represent an expansion into open-domain and specialized technical challenges.", "D": "There is no fundamental distinction; all benchmarks ultimately measure the same core capabilities of an LLM."}, "answer": "C", "explanation": "The correct answer (C) highlights the crucial shift from closed-domain knowledge retrieval to open-domain benchmarks incorporating increasingly complex and specialized tasks. The earlier datasets were primarily focused on retrieving known facts, whereas the newer benchmarks demand more sophisticated reasoning and problem-solving skills.", "question_token_count": 35, "answer_correctness_score": 8, "explanation_validity_score": 8, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 22}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "The potential consequences of using data contaminated with information available to the model during training.", "question": "Which of the following best describes the primary motivation behind benchmarks like AntiLeak-Bench and Forecastbench?", "choices": {"A": "To assess the model\u2019s performance on historical data.", "B": "To evaluate the model\u2019s ability to generate creative content.", "C": "To identify questions about knowledge that emerged *after* the model\u2019s training cutoff.", "D": "To measure the model\u2019s accuracy in predicting future events."}, "answer": "C", "explanation": "The context explicitly states that AntiLeak-Bench generates queries about \u201cnewly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date\u201d and Forecastbench \u201cupdates new forecasting questions on a daily basis from different data sources.\u201d", "question_token_count": 22, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 13}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The proposed criteria for evaluating dynamic benchmarks, highlighting the current shortcomings of existing approaches.", "question": "Considering the inherent challenges of data contamination and the reliance on publicly available data for LLM training, what fundamental criteria, beyond mere dataset freshness, would be essential for definitively assessing the integrity and reliability of a dynamic benchmark?", "choices": {"A": "Establishing a rigorous, independently-verified gold standard dataset, completely separate from any LLM training data, to serve as a reference for contamination detection.", "B": "Implementing continuous, automated data provenance tracking mechanisms to definitively trace the origin of all benchmark data points.", "C": "Defining a multi-faceted evaluation framework incorporating statistical analysis, adversarial testing, and human expert review to identify subtle forms of data leakage.", "D": "Prioritizing benchmark datasets that are exclusively generated through synthetic data generation techniques, eliminating any possibility of real-world data contamination."}, "answer": "C", "explanation": "The correct answer (C) acknowledges the complexity of the issue and necessitates a comprehensive approach. Data provenance tracking (B) is difficult to implement effectively, synthetic data (D) may not accurately reflect real-world performance, and a simple gold standard (A) is unrealistic given the scale of LLM training.", "question_token_count": 44, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 26}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "Compare and contrast the methodologies of S3Eval and DyVal in evaluating LLM reasoning capabilities.", "question": "What fundamental difference underlies the data structures employed by S3Eval and DyVal in their evaluation of LLM reasoning?", "choices": {"A": "S3Eval utilizes DAGs, while DyVal employs randomly generated SQL tables.", "B": "S3Eval focuses on SQL queries, whereas DyVal assesses the value of a root node in a graph.", "C": "S3Eval evaluates reasoning on tables, while DyVal evaluates reasoning on directed acyclic graphs.", "D": "S3Eval uses natural language prompts, while DyVal relies on rule-based conversion."}, "answer": "C", "explanation": "The context states that S3Eval assesses LLM accuracy in executing SQL queries on random tables, and DyVal evaluates LLMs using randomly generated DAGs. Therefore, the key difference is the data structure \u2013 tables versus graphs.", "question_token_count": 24, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 19}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "The primary risk addressed by using canary strings in LLM training is the potential for data contamination stemming from the inclusion of static benchmark datasets.", "question": "What is the primary limitation of utilizing canary strings as a method for mitigating data contamination in LLM training, as described in the text?", "choices": {"A": "Canary strings are easily detectable by all LLMs, regardless of training data.", "B": "Their effectiveness relies solely on the inclusion of unique tokens within the dataset.", "C": "They are rendered ineffective if a developer deliberately introduces benchmark data to manipulate model performance.", "D": "Canary strings provide no indication of whether a model is generalizing or memorizing."}, "answer": "C", "explanation": "The text explicitly states, \u201cIf a developer aims to leak benchmarking data to boost scores, this method will not work.\u201d", "question_token_count": 28, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 10, "avg_answer_token_count": 16}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "What is the concept of data contamination in the context of LLM benchmarking, and why is it a concern?", "question": "In the context of LLM benchmarking, how does data contamination fundamentally undermine the reliability of performance metrics, and what is the primary mechanism driving this phenomenon?", "choices": {"A": "Data contamination primarily affects the computational efficiency of LLMs, leading to slower inference times.", "B": "Data contamination arises from the stochastic nature of the training process, resulting in inconsistent model behavior.", "C": "Data contamination occurs when LLMs memorize specific examples from benchmark datasets, leading to inflated performance metrics that do not represent true generalization ability.", "D": "Data contamination is a negligible issue in modern LLMs due to their inherent ability to generalize across diverse data distributions."}, "answer": "C", "explanation": "The correct answer is C. The text explicitly states that \u201cunchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\u201d This refers to the models memorizing examples from the benchmarks, which is the core of the problem.", "question_token_count": 30, "answer_correctness_score": 10, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 8, "avg_answer_token_count": 21}
{"context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n", "topic": "Static benchmarks can inadvertently perpetuate biases due to reliance on outdated or biased data sources.", "question": "In the context of LLM evaluation benchmarks, what is the primary mechanism by which static benchmarks can perpetuate bias?", "choices": {"A": "Dynamic benchmarks inherently introduce bias due to their reliance on continuously updated data.", "B": "Static benchmarks are inherently more accurate than dynamic benchmarks.", "C": "Static benchmarks can inadvertently perpetuate biases through reliance on outdated or biased data sources.", "D": "The use of static benchmarks is entirely immune to bias."}, "answer": "C", "explanation": "The correct answer is C. The passage explicitly states that static benchmarks \"can inadvertently perpetuate biases\u2026 through reliance on outdated or biased data sources.\" Options A, B, and D are factually incorrect based on the provided text.", "question_token_count": 23, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 10, "avg_answer_token_count": 13}
{"context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n", "topic": "Data privacy and commercial concerns contribute to the complexity of addressing data contamination issues in LLM benchmarking.", "question": "Considering the observed increase in contamination probability proportional to the size of the training dataset, what fundamental limitation does this suggest regarding the applicability of traditional static benchmarks for evaluating LLMs trained on contemporary, web-scale corpora?", "choices": {"A": "Dynamic benchmarks inherently provide a more accurate assessment of model performance due to their ability to adapt to evolving data distributions.", "B": "The increasing contamination rate necessitates a shift towards exclusively human-annotated datasets, eliminating the need for automated benchmarking.", "C": "Static benchmarks are fundamentally unreliable for evaluating models trained on large datasets, requiring a complete redesign of the evaluation process.", "D": "The contamination rate is negligible and does not significantly impact the validity of static benchmarks."}, "answer": "C", "explanation": "The correct answer is C. The text explicitly states \u201cOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%", "question_token_count": 43, "answer_correctness_score": 10, "explanation_validity_score": 8, "question_clarity_score": 6, "question_groundedness_score": 8, "avg_answer_token_count": 21}
{"context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n", "topic": "The challenges associated with using temporal cutoffs for LLM evaluation and the risk of data contamination.", "question": "Considering the documented risk of data contamination associated with temporal cutoffs in LLM evaluation, what fundamental methodological flaw underlies the practice of utilizing recent competition results for benchmarking purposes?", "choices": {"A": "Temporal cutoffs ensure the most current and relevant data is used for evaluation.", "B": "Reliance on competition data introduces a systematic bias, as problems and solutions are inevitably recycled across multiple evaluation cycles.", "C": "Temporal cutoffs eliminate the need for human involvement in the evaluation process.", "D": "Recent competition results always accurately reflect an LLM\u2019s true capabilities."}, "answer": "B", "explanation": "The text explicitly states that using recent information from competitions can lead to data contamination because these problems are likely to be reused. Option B directly addresses this concern.", "question_token_count": 34, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 16}
{"context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n", "topic": "What is the significance of a higher score on the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) in the context of evaluating dynamic benchmarks?", "question": "In the context of evaluating dynamic benchmarks, what is the primary significance of achieving a higher score on the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5)?", "choices": {"A": "It indicates a lower level of alignment between the benchmark's output and the ground truth.", "B": "It signifies a greater degree of transformation applied to the original data.", "C": "It reflects a stronger correlation between the transformed dataset and the ground truth, suggesting higher correctness.", "D": "It demonstrates the use of a more complex oracle function."}, "answer": "C", "explanation": "The text explicitly states that \u201cA higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\u201d Therefore, option C is the correct answer.", "question_token_count": 46, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 15}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "The distinction between syntactic transformations considered \u201ccontamination\u201d and a model\u2019s genuine reasoning capability during inference warrants critical examination.", "question": "Considering the argument presented, what is the primary justification for classifying syntactic transformations (like adding prefixes) as \u201ccontamination\u201d during LLM benchmarking, rather than evidence of genuine reasoning?", "choices": {"A": "Syntactic transformations inherently improve a model\u2019s ability to generalize to unseen data.", "B": "Such transformations demonstrate a model\u2019s capacity to effectively utilize semantic relationships within the input.", "C": "They represent a form of memorization, blurring the line between recalling learned patterns and exhibiting independent inference.", "D": "Syntactic transformations always indicate a deeper understanding of the underlying linguistic structure."}, "answer": "C", "explanation": "The context explicitly states that \u201csuch syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference.\u201d", "question_token_count": 36, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 18}
{"context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n", "topic": "Explain the significance of independently transformed versions of a benchmark dataset in relation to quantifying collision.", "question": "In the context of dynamic benchmarking, what is the primary significance of independently transformed versions of a benchmark dataset?", "choices": {"A": "They provide a direct measure of the LLM\u2019s inherent capabilities.", "B": "They quantify the collision rate, indicating potential data contamination and limiting the benchmark\u2019s ability to generate diverse test cases.", "C": "They are used solely to increase the size of the benchmark dataset.", "D": "They serve as a control group for comparing against traditional static benchmarks."}, "answer": "B", "explanation": "The text explicitly states that collision measures the percentage of overlap between transformed datasets, highlighting the risk of contamination.", "question_token_count": 22, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 16}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "Describe the proposed solutions to mitigate data contamination risks, specifically focusing on dynamic benchmarks.", "question": "Considering the potential for data contamination in LLM benchmarking, what is the primary operational challenge inherent in deploying dynamic benchmarks compared to static ones?", "choices": {"A": "Ensuring the dynamic benchmark tasks remain sufficiently diverse to prevent overfitting by the model.", "B": "Maintaining the computational resources required to generate and execute a continuously updating set of benchmark tasks.", "C": "Guaranteeing the dynamic benchmark tasks accurately reflect the real-world distribution of data the model will encounter after deployment.", "D": "Avoiding the introduction of bias into the benchmark through the selection criteria for dynamically generated tasks."}, "answer": "C", "explanation": "The correct answer is (C). The core problem of dynamic benchmarks is ensuring they accurately represent the data the model will encounter in the real world. Static benchmarks are less susceptible to this because they are fixed.", "question_token_count": 27, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 5, "avg_answer_token_count": 19}
{"context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n", "topic": "The potential impact of in-distribution contamination on LLM performance and generalization.", "question": "Considering the methods outlined to address in-distribution contamination, what is the primary limitation of relying on LLM-rewritten benchmarks for evaluating LLM generalization?", "choices": {"A": "LLMs consistently produce lower performance on rewritten benchmarks compared to the original data.", "B": "The use of LLMs introduces a bias towards replicating the stylistic features of the original benchmark samples.", "C": "The inherent risk of in-distribution contamination remains, even when LLMs are employed to generate or modify benchmark data.", "D": "LLM-generated samples always exhibit lower cognitive difficulty than the original benchmark questions."}, "answer": "C", "explanation": "The context explicitly states that \u201cpublicly available rule-generated data may increase the risk of in-distribution contamination.\u201d  The methods described (Auto-Dataset, StructEval, ITD, VarBench) are attempts to *mitigate* this risk, suggesting it\u2019s a persistent concern.", "question_token_count": 28, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 19}
{"context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n", "topic": "How does the definition of scalability \u2013 the ratio of transformed dataset size to transformation cost \u2013 address the potential issues with smaller benchmark datasets?", "question": "How does the defined scalability metric directly mitigate the potential for biased results when evaluating dynamic benchmarking methods using smaller benchmark datasets?", "choices": {"A": "It prioritizes generating datasets with a higher proportion of unique data points, regardless of cost.", "B": "It focuses on minimizing the transformation cost, assuming a larger dataset size inherently reduces statistical error.", "C": "It balances dataset size and transformation cost to maximize the amount of data generated per unit of resource expenditure, addressing statistical uncertainty introduced by smaller datasets.", "D": "It solely emphasizes the computational efficiency of the transformation process, irrespective of the dataset size."}, "answer": "C", "explanation": "The correct answer (C) directly addresses the core issue of smaller datasets causing statistical error. Scalability, as defined, is about generating *more* data *efficiently*, which is essential for reducing the impact of small dataset size on statistical reliability.", "question_token_count": 24, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 2, "question_groundedness_score": 9, "avg_answer_token_count": 21}
{"context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n", "topic": "How might the function \u0398(\ud835\udc9fi, \ud835\udc9fj) be implemented to calculate diversity between two datasets, according to the document?", "question": "Considering the provided definitions, how would the function \u0398(\ud835\udc9fi, \ud835\udc9fj) most accurately be described in terms of its core computational purpose within the context of dataset diversity evaluation?", "choices": {"A": "\u0398(\ud835\udc9fi, \ud835\udc9fj) calculates the average Euclidean distance between individual data points in datasets \ud835\udc9fi and \ud835\udc9fj.", "B": "\u0398(\ud835\udc9fi, \ud835\udc9fj) quantifies the similarity score between datasets \ud835\udc9fi and \ud835\udc9fj based on N-gram overlap.", "C": "\u0398(\ud835\udc9fi, \ud835\udc9fj) measures the difference in the distribution of feature values across datasets \ud835\udc9fi and \ud835\udc9fj.", "D": "\u0398(\ud835\udc9fi, \ud835\udc9fj) computes the cosine similarity between the transformed representations of datasets \ud835\udc9fi and \ud835\udc9fj."}, "answer": "C", "explanation": "The context states that \u0398(\ud835\udc9fi, \ud835\udc9fj) is a function that \u201cmeasures the diversity between two datasets\u201d and provides examples like N-gram metrics and BLEU scores.  Option C most accurately reflects this by describing a measure of distributional difference, aligning with the concept of diversity. Options A, B, and D represent other specific similarity or distance metrics, not the general function \u0398(\ud835\udc9fi, \ud835\udc9fj) as defined in the text.", "question_token_count": 39, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 30}
{"context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n", "topic": "Retrieval-based detection methods for contamination are hampered by the scale and complexity of LLM training corpora, making complete exclusion of evaluation data challenging.", "question": "Considering the scale and diversity of LLM training datasets, what is the primary technical obstacle preventing the complete removal of evaluation data during model development?", "choices": {"A": "The inherent randomness of the web scraping process.", "B": "The limited computational resources available for data curation.", "C": "The difficulty of accurately identifying and removing duplicate data points across diverse sources.", "D": "The proprietary nature of training data, which restricts access and verification."}, "answer": "C", "explanation": "The text explicitly states that the \u201csheer scale and complexity of training corpora\u201d makes complete exclusion of evaluation data challenging. While other factors may contribute, this is the primary technical hurdle identified.", "question_token_count": 28, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 12}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "The consequences of misleading benchmark results, influenced by data contamination, extend to model comparisons, deployment, and policy-making.", "question": "Beyond simple rephrasing, what fundamental methodological challenge does the context identify regarding the classification of \u201ccontamination\u201d in LLM benchmarking, specifically concerning syntactic transformations?", "choices": {"A": "Syntactic transformations are inherently unreliable indicators of a model's reasoning abilities.", "B": "Distinguishing between a model\u2019s ability to recall memorized information and genuine reasoning during inference is exceptionally difficult, regardless of the transformation method.", "C": "Syntactic contamination is easily detectable through statistical analysis of the transformed data.", "D": "All NLP applications prioritize syntactic information over semantic understanding."}, "answer": "B", "explanation": "The text explicitly states, \u201cThere is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference.\u201d", "question_token_count": 33, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 17}
{"context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n", "topic": "How do the cited benchmarks (GLUE, SuperGLUE, HumanEval) exemplify the application of label protection, and what is the intended consequence of withholding test labels?", "question": "How does the deliberate withholding of test labels in benchmarks like GLUE, SuperGLUE, and HumanEval fundamentally alter the evaluation process, and what is the primary rationale for this alteration?", "choices": {"A": "It simplifies the evaluation process by reducing the complexity of the test data.", "B": "It mitigates the risk of model overfitting by preventing the model from memorizing answers during training.", "C": "It allows for faster model development by reducing the time required to generate test datasets.", "D": "It guarantees a more accurate representation of a model\u2019s generalization capabilities."}, "answer": "B", "explanation": "The correct answer is B. The text explicitly states that the key advantage of label protection is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.", "question_token_count": 38, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 16}
