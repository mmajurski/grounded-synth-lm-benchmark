{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "Comparative analysis of coding benchmarks (HumanEval, MBPP, SWE-Bench, Codeforces, Aider) and their respective focuses on code synthesis, debugging, and dynamic problem solving.", "question": "Which pair of coding benchmarks is most distinct in their primary focus due to one emphasizing static code synthesis and debugging, and the other prioritizing dynamic problem solving under competitive conditions?", "choices": {"A": "HumanEval and Codeforces", "B": "MBPP and SWE-Bench", "C": "SWE-Bench and Aider", "D": "HumanEval and MBPP"}, "answer": "A", "explanation": "HumanEval focuses on code synthesis and debugging (static tasks), while Codeforces emphasizes dynamic problem solving in a competitive programming environment, making them the most distinct pair in terms of primary focus.", "question_token_count": 34, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 6}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "The role of domain specificity (e.g., math competitions, coding platforms, academic papers, prediction markets) in enhancing the robustness and reliability of temporal cutoff benchmarks.", "question": "Which characteristic of domain-specific sources, such as recent math competitions or prediction markets, most directly enhances the robustness and reliability of temporal cutoff benchmarks for evaluating large language models?", "choices": {"A": "Their tendency to generate a high volume of data regardless of temporal relevance", "B": "Their capacity to provide continuously updated, unique content not present in pre-cutoff training data", "C": "Their focus on well-established problems that are widely circulated in educational resources", "D": "Their reliance on expert curation to ensure question difficulty and diversity"}, "answer": "B", "explanation": "The robustness and reliability of temporal cutoff benchmarks are most directly enhanced by using domain-specific sources that continuously generate unique, up-to-date content, ensuring that the evaluation data is novel and uncontaminated by the model's training set.", "question_token_count": 34, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 14}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "Formal definition and implications of data contamination in LLM training and evaluation datasets.", "question": "Which scenario most accurately exemplifies syntactic contamination in the context of LLM training and evaluation datasets?", "choices": {"A": "A test example is present in the training data after conversion of all British English spellings to American English.", "B": "A test example appears verbatim in the training data, including all original formatting and punctuation.", "C": "The training data contains a similar topic to the test example, but with completely different wording and meaning.", "D": "The training data includes metadata about the source of the test example, but not the example itself."}, "answer": "A", "explanation": "Syntactic contamination refers to a test data point appearing in the training set after a syntactic transformation that preserves lexical meaning\u2014such as spelling normalization. Option A is correct because converting British to American English is a syntactic change that maintains meaning. Option B is exact contamination, not syntactic. Option C is not contamination because the meaning is different. Option D does not involve the example itself.", "question_token_count": 19, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 20}
{"context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n", "topic": "The implications and limitations of using static benchmarks for evaluating abilities such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, and toxicity detection.", "question": "Which of the following most accurately captures a fundamental limitation of using static benchmarks for evaluating abilities such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, and toxicity detection?", "choices": {"A": "Static benchmarks may not account for the adaptability of models to novel or dynamic real-world scenarios beyond the fixed dataset.", "B": "Static benchmarks inherently prevent any form of quantitative comparison between models on standardized tasks.", "C": "Static benchmarks ensure that all possible aspects of model behavior, including creativity and context adaptation, are thoroughly measured.", "D": "Static benchmarks eliminate the risk of models overfitting to specific evaluation datasets due to their fixed nature."}, "answer": "A", "explanation": "The fundamental limitation is that static benchmarks, by design, use fixed datasets and predetermined outputs, which means they may fail to capture how well a model can generalize or adapt to novel, unforeseen, or dynamic real-world contexts. They do not inherently measure creativity or adaptability, and there is a risk of overfitting to the benchmark itself. Options B, C, and D are incorrect because static benchmarks do enable quantitative comparisons (B), do not guarantee coverage of all behavioral aspects (C), and do not prevent overfitting (D).", "question_token_count": 43, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 2, "avg_answer_token_count": 20}
{"context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n", "topic": "The definition, computation, and interpretation of Collision Rate as a metric for quantifying dataset overlap in dynamic benchmarks.", "question": "Which statement best characterizes Collision Rate as a metric in dynamic benchmarking for dataset overlap?", "choices": {"A": "Collision Rate quantifies the proportion of identical examples between two independently transformed versions of a dataset, reflecting the likelihood of overlap and potential benchmark contamination.", "B": "Collision Rate measures the percentage of training data reused in dynamic benchmarks, indicating the risk of exposing models to previously seen tasks.", "C": "Collision Rate estimates the number of transformation attempts required to regenerate a specific benchmark dataset, providing insight into the novelty of test cases.", "D": "Collision Rate calculates the degree to which benchmark scores are influenced by repeated trials, assessing the statistical reliability of model evaluation."}, "answer": "A", "explanation": "Collision Rate is defined as the percentage of overlap between two independently transformed versions of the benchmark dataset, directly reflecting the potential for overlap and contamination in dynamic benchmarking. Options B, C, and D misinterpret the metric\u2019s definition or conflate it with other concepts like contamination, Repeat Trials, or statistical reliability.", "question_token_count": 17, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 26}
{"context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n", "topic": "Categorization and critical evaluation of dynamic benchmark construction methods, including temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.", "question": "Which dynamic benchmark construction method most inherently supports interpretability and minimizes the need for extensive manual validation, and why might other methods require additional mechanisms to ensure reliability?", "choices": {"A": "Temporal cutoff, because it uses only newly released data which are self-explanatory.", "B": "Rule-based generation, because its predefined transformations are transparent and easily understood.", "C": "LLM-based generation, due to the model's ability to document its generative process.", "D": "Hybrid approaches, since they combine multiple methods and thus eliminate interpretability issues."}, "answer": "B", "explanation": "Rule-based generation is inherently interpretable because its transformations are defined by explicit rules, making the process transparent and reducing the need for manual validation. In contrast, LLM-based or hybrid methods often require additional explainability tools or human oversight to ensure reliability, since their generative processes can be opaque.", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 16}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "Comparison of intuitive, commonsense, and factual reasoning as tested by the various reasoning benchmarks.", "question": "Which set of benchmarks most accurately distinguishes between intuitive, commonsense, and factual reasoning as tested in language model evaluation?", "choices": {"A": "PIQA, SIQA, and HellaSwag for intuitive reasoning; ARC, OpenBookQA, and CommonsenseQA for commonsense reasoning; C-SimpleQA for factual reasoning.", "B": "WinoGrande, ARC, and C-SimpleQA for intuitive reasoning; PIQA, HellaSwag, and CommonsenseQA for commonsense reasoning; OpenBookQA for factual reasoning.", "C": "ARC, PIQA, and OpenBookQA for intuitive reasoning; HellaSwag, SIQA, and C-SimpleQA for commonsense reasoning; WinoGrande for factual reasoning.", "D": "CommonsenseQA, C-SimpleQA, and ARC for intuitive reasoning; HellaSwag, OpenBookQA, and PIQA for factual reasoning; SIQA for commonsense reasoning."}, "answer": "A", "explanation": "Option A correctly matches PIQA, SIQA, and HellaSwag to intuitive reasoning, ARC, OpenBookQA, and CommonsenseQA to commonsense reasoning, and C-SimpleQA to factual reasoning, as outlined in the context. The other options misclassify at least one benchmark.", "question_token_count": 24, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 1, "question_groundedness_score": 1, "avg_answer_token_count": 39}
{"context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n", "topic": "The challenges and limitations associated with using pre-defined rules and publicly available rule-generated data for benchmark dataset generation, particularly regarding sample diversity and in-distribution contamination.", "question": "Which of the following most accurately explains why reliance on pre-defined rules and publicly available rule-generated data can simultaneously decrease sample diversity and increase the risk of in-distribution contamination in benchmark dataset generation?", "choices": {"A": "Pre-defined rules focus on a narrow set of patterns, resulting in repetitive samples, while publicly available rule-generated data may overlap with training sets, causing test set leakage.", "B": "Pre-defined rules always generate high-diversity samples, but publicly available rule-generated data inevitably introduces entirely novel concepts, increasing contamination risk.", "C": "Publicly available rule-generated data inherently guarantees sample diversity, but pre-defined rules ensure out-of-distribution samples, reducing contamination.", "D": "Both pre-defined rules and publicly available rule-generated data maximize the uniqueness of samples and guarantee no overlap with training data."}, "answer": "A", "explanation": "Option A is correct because pre-defined rules tend to produce samples with limited variability, reducing diversity, and publicly available rule-generated data may coincide with data used for training, increasing the risk of in-distribution contamination. The other options incorrectly assert that rule-based or publicly available data inherently increases diversity or prevents contamination.", "question_token_count": 38, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 10, "avg_answer_token_count": 27}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "The approach and reasoning behind NPHardEval's evaluation of LLMs on P and NP problems, with a focus on the Traveling Salesman Problem using randomly synthesized graphs.", "question": "Why does NPHardEval's use of randomly synthesized graphs to evaluate LLMs on the Traveling Salesman Problem provide a robust assessment of reasoning ability across P and NP problems, and what critical advantage does this approach offer over using fixed, hand-crafted instances?", "choices": {"A": "It ensures LLMs are exposed to unbiased, diverse problem structures, preventing overfitting to templates and enabling assessment of generalization and computational complexity handling.", "B": "It simplifies the evaluation process by guaranteeing that all generated instances are of equal difficulty and reduces the computational resources required for assessment.", "C": "It allows evaluators to focus exclusively on NP problems like TSP, excluding P problems, which improves benchmarking accuracy.", "D": "It makes the problem instances easier for LLMs to solve, leading to higher measured performance and more optimistic evaluation results."}, "answer": "A", "explanation": "Randomly synthesizing graphs prevents LLMs from overfitting to specific patterns or templates found in fixed, hand-crafted instances, exposes them to a wide range of structural variations, and rigorously tests their ability to generalize and reason about both P and NP problems\u2014key aspects for robust benchmarking of reasoning ability.", "question_token_count": 53, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 8, "avg_answer_token_count": 25}
{"context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n", "topic": "Formalization of complexity stability using a complexity measurement function and the interpretation of variance in complexity across benchmarking trials.", "question": "Which interpretation most accurately characterizes high variance in the output of a complexity measurement function \u03a8(\u00b7) when applied to multiple dynamically transformed dataset trials within a benchmarking framework?", "choices": {"A": "High variance in \u03a8(\u00b7) across trials indicates consistent and reliable dynamic benchmarking, confirming that the benchmarking method robustly captures true task complexity.", "B": "High variance in \u03a8(\u00b7) across trials suggests instability in the dynamic benchmarking process, making it difficult to attribute performance changes in LLMs to either data contamination or genuine complexity shifts.", "C": "High variance in \u03a8(\u00b7) across trials guarantees that any observed LLM performance drop is solely due to increased complexity and not other confounding factors.", "D": "High variance in \u03a8(\u00b7) across trials is irrelevant to the interpretation of LLM performance drops and does not impact benchmarking conclusions."}, "answer": "B", "explanation": "High variance in the complexity measurement across trials signals that the dynamic benchmarking process is unstable, which complicates the attribution of LLM performance changes to specific causes such as data contamination or altered task complexity.", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 29}
{"context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n", "topic": "The role and importance of static benchmarks as standardized evaluation tools for measuring different aspects of model performance.", "question": "Which characteristic most fundamentally enables static benchmarks to serve as standardized evaluation tools for systematically measuring diverse aspects of model performance?", "choices": {"A": "Their inclusion of varied task categories representing different domains of intelligence.", "B": "Their formal structure combining input prompts, expected outputs, and a scoring function to objectively compare model outputs.", "C": "Their ability to generate new data instances on demand for adaptive testing.", "D": "Their reliance on real-world user interactions to assess model behavior in dynamic contexts."}, "answer": "B", "explanation": "Only a formal structure encompassing input prompts, expected outputs, and a scoring function ensures objective, reproducible, and standardized measurement, distinguishing static benchmarks from approaches based on adaptivity or real-world interactions.", "question_token_count": 23, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 8, "avg_answer_token_count": 16}
{"context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n", "topic": "The function and design considerations of the scoring function \ud835\udcae(.) in evaluating the quality of LLM outputs against expected outputs.", "question": "Which design consideration is most critical for ensuring that the scoring function \ud835\udcae(.) in a static LLM benchmark yields meaningful and robust evaluations across diverse tasks such as math, coding, and language understanding?", "choices": {"A": "Maximizing the strictness of output matching to eliminate all partially correct responses.", "B": "Ensuring that the scoring function is adaptable to the specific characteristics and multiple valid outputs of each task category.", "C": "Standardizing the scoring function to use a single metric, such as BLEU or accuracy, for all tasks to enable fair model comparison.", "D": "Limiting the scoring function to only binary pass/fail judgments to increase evaluation clarity."}, "answer": "B", "explanation": "The core challenge in designing \ud835\udcae(.) is to accommodate the varied nature of tasks (e.g., precision in math vs. open-endedness in language) and to recognize that many tasks admit multiple valid outputs. An adaptable scoring function that reflects these nuances ensures that evaluations are fair, meaningful, and robust across heterogeneous benchmarks, rather than rigid, overly simplistic, or mismatched to the task.", "question_token_count": 40, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 21}
{"context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n", "topic": "Critical analysis of the human effort and ongoing involvement required in benchmark collection and updates for LLM evaluation, including the implications for scalability and reliability.", "question": "Which of the following most accurately explains why increasing automation through rule-based and template-based benchmark generation does not entirely eliminate concerns about scalability and reliability in LLM evaluation?", "choices": {"A": "Automated methods inherently generate only low-quality test cases, making ongoing human involvement essential for meaningful evaluation.", "B": "Even with automated benchmark generation, human oversight remains critical due to persistent challenges such as verification gaps and the risk of data contamination from reused problem structures.", "C": "The extremely low collision probability of automated methods removes the need for any human verification, thus resolving all reliability concerns.", "D": "Automated benchmarks fully prevent data contamination by never reusing elements from prior competitions, thus requiring no further updates or human effort."}, "answer": "B", "explanation": "Option B is correct because, while automation reduces some human burden, issues like verification and contamination still require human judgment and oversight. Options A, C, and D are incorrect\u2014A exaggerates the limitations of automation, C and D falsely claim complete resolution of reliability and contamination issues.", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 6, "avg_answer_token_count": 25}
{"context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n", "topic": "The criteria for selecting or constructing effective static benchmarks to comprehensively evaluate LLM capabilities.", "question": "Which combination of characteristics most fundamentally ensures that a static benchmark provides a comprehensive and reliable evaluation of large language model capabilities?", "choices": {"A": "Covering a diverse set of tasks with clearly defined input-output pairs and an objective, discriminative scoring function.", "B": "Emphasizing only the complexity of tasks to maximally challenge model performance.", "C": "Utilizing advanced scoring metrics regardless of the representativeness of input data.", "D": "Focusing on maximizing the size of the seed dataset to minimize statistical variance."}, "answer": "A", "explanation": "The key to effective static benchmarks lies in integrating task diversity (to assess broad capabilities), precise definition of prompts and outputs (to ensure clarity and replicability), and a robust scoring function (to meaningfully differentiate model performance). Focusing solely on task complexity, advanced metrics, or dataset size neglects other crucial dimensions such as representativeness and objective evaluation.", "question_token_count": 24, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 4, "avg_answer_token_count": 17}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "Examples and sources of exact contamination, such as verbatim test examples and documentation leaks.", "question": "Which scenario most clearly exemplifies exact contamination in the context of LLM training and evaluation?", "choices": {"A": "A test question that appears verbatim in both the training and test datasets.", "B": "A test example that is paraphrased in the training data, using synonyms and altered syntax.", "C": "A test code snippet where variable names are changed but logic remains identical in the training set.", "D": "A test instruction with minor punctuation differences between training and test datasets."}, "answer": "A", "explanation": "Exact contamination is strictly defined as an exact duplicate of a data point in both training and test datasets, such as a verbatim test example. Syntactic changes, paraphrasing, or minor modifications do not constitute exact contamination, even if the core meaning is preserved.", "question_token_count": 18, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 17}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "Strategies for verifying that benchmark data truly postdates the knowledge cutoff and assessing their effectiveness.", "question": "Which of the following approaches presents the most reliable method for verifying that benchmark questions genuinely postdate an LLM's knowledge cutoff, and why might alternative strategies be less effective in ensuring freedom from data contamination?", "choices": {"A": "Relying on continuous collection of newly posted problems from publicly timestamped online competition platforms with verifiable publication dates.", "B": "Generating benchmark questions about topics presumed to be new but without rigorous documentation of their appearance after the cutoff.", "C": "Updating question sets periodically from informal community forums without preserving original post timestamps.", "D": "Designing academic tasks based on recent publications, but without systematic verification of publication or preprint dates."}, "answer": "A", "explanation": "Only option A offers a method where the provenance and timing of the data can be independently and reliably verified through public timestamps, minimizing the risk of inadvertent inclusion of pre-cutoff information. The other approaches lack sufficient safeguards to guarantee temporal validity, either through insufficient documentation or reliance on unverifiable sources.", "question_token_count": 40, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 20}
{"context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The summarized criteria for evaluating the quality of dynamic benchmarks as indicated by the referenced table.", "question": "Which set of criteria most appropriately summarizes the essential qualities for evaluating the quality of dynamic benchmarks in the context of LLM evaluation?", "choices": {"A": "Transparency of evaluation, adaptability to avoid contamination, and the ability to generate benchmarks from scratch.", "B": "High model accuracy, minimal computational cost, and static data reuse.", "C": "Frequent manual labeling, limited dataset size, and strict privacy controls.", "D": "Comprehensive model memorization checks, reliance on fixed test suites, and maximal overlap with training data."}, "answer": "A", "explanation": "Option A synthesizes the key qualities implied in the context: transparency, adaptability (via transformation T(\u00b7) to avoid contamination), and the capability to create benchmarks even from an empty seed dataset. The other options either introduce irrelevant or counterproductive qualities (such as static data reuse, maximal overlap, or frequent manual labeling), or focus on aspects not highlighted as core to dynamic benchmarking quality.", "question_token_count": 25, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 17}
{"context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n", "topic": "The implications of label protection on transparency, independent verification, and reproducibility in machine learning evaluation.", "question": "How does the imposition of label protection most fundamentally compromise the scientific principles of transparency and reproducibility in machine learning evaluation, even when advanced post-hoc contamination detection techniques are employed?", "choices": {"A": "By restricting access to labels, it prevents independent error analysis and external replication of results, making verification and reproducibility reliant on centralized authorities.", "B": "By requiring n-gram matching, it introduces more false negatives, which limits the effectiveness of contamination detection but does not directly affect transparency or reproducibility.", "C": "By mandating embedding-based similarity methods, it obscures model behavior, but independent researchers can still verify results if they have access to embeddings.", "D": "By focusing on behavioral analysis under masked inputs, it compensates for label protection by providing sufficient transparency for reproducibility."}, "answer": "A", "explanation": "The core compromise of label protection is that it restricts direct access to ground-truth labels, which are essential for independent verification and detailed error analysis. This forces reliance on centralized evaluation systems, hampering both transparency and reproducibility, regardless of the sophistication of post-hoc detection methods. Other options either misattribute the source of the limitation or incorrectly assume that advanced detection techniques or certain analytic methods restore transparency.", "question_token_count": 35, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 27}
{"context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n", "topic": "Implications of scalability for the statistical reliability and validity of benchmarking results.", "question": "When evaluating dynamic benchmarking methods, which scenario most critically undermines the statistical reliability and validity of benchmarking results, despite high scalability as measured by the expected ratio of transformed data size to transformation cost?", "choices": {"A": "When transformations generate large datasets, but the diversity of transformations is limited, leading to redundancy in sampled data.", "B": "When monetary cost per transformation is minimized, but the time required for manual validation increases linearly with dataset size.", "C": "When the original dataset is very small, but transformations are highly varied and cost-effective.", "D": "When the transformation cost metric excludes manual effort, considering only computational time."}, "answer": "A", "explanation": "High scalability, defined as generating large datasets per unit cost, can still result in unreliable or invalid benchmarking if the transformations lack diversity, causing redundancy and failing to improve the statistical properties of the benchmark. Merely increasing the dataset size without ensuring diversity does not reduce bias or increase validity.", "question_token_count": 39, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 2, "avg_answer_token_count": 19}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "The methodological challenges in differentiating between model memorization and reasoning capability during syntactic transformations of test data.", "question": "What is the primary methodological challenge in differentiating between model memorization and reasoning capability when test data is syntactically transformed versions of the training data?", "choices": {"A": "Ensuring that the transformed data maintains semantic equivalence with the original.", "B": "Distinguishing whether correct model responses arise from exposure to underlying content or genuine reasoning over new syntax.", "C": "Preventing the model from using statistical patterns learned during training.", "D": "Guaranteeing that syntactic transformations do not alter the task difficulty."}, "answer": "B", "explanation": "The core methodological challenge is that syntactic transformations may not sufficiently obscure the connection to training data, making it difficult to tell if the model is truly reasoning or simply recalling memorized content that has been superficially altered.", "question_token_count": 30, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 15}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "The implicit assumptions and broader implications of benchmark selection and development for advancing LLM assessment methodologies.", "question": "Which of the following most accurately describes a significant but often overlooked consequence of the implicit assumptions in benchmark selection and development for LLM assessment methodologies?", "choices": {"A": "Benchmarks inherently favor LLMs with larger parameter counts, leading to an overestimation of scaling effects and underrepresentation of algorithmic innovations.", "B": "The scope and evolution of benchmarks shape research priorities, potentially narrowing the perceived frontier of LLM capabilities and inadvertently constraining innovation in untested domains.", "C": "The continual refinement of benchmarks ensures unbiased and comprehensive measurement of all LLM capabilities, mitigating the risk of progress illusion.", "D": "The inclusion of both technical and open-domain tasks in benchmarks guarantees that LLMs are equally evaluated on reasoning and factual recall across all subject areas."}, "answer": "B", "explanation": "Option B highlights how the implicit assumptions underlying benchmark selection can influence the direction of research and the very definition of progress, possibly restricting attention to domains that are well-benchmarked and sidelining others, thus affecting the trajectory of LLM development. The other options either misstate the nature of benchmark influence (A, D) or erroneously assume that refinement eliminates bias and incompleteness (C).", "question_token_count": 28, "answer_correctness_score": 8, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 2, "avg_answer_token_count": 27}
{"context": "###### Safety\nSafety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content. Datasets such as RealToxicityPrompts\u00a0(Gehman et\u00a0al., 2020) and ToxiGen\u00a0(Hartvigsen et\u00a0al., 2022) assess resilience against producing harmful outputs.\n\nBy providing a controlled environment to measure these aspects, safety benchmarks play a critical role in guiding the development of models that are not only powerful outputs but also responsible and trustworthy for real-world applications.\n\n\n###### Language\nLanguage benchmarks assess the LLMs\u2019 proficiency in specific languages.\n\nGLUE\u00a0(Wang, 2018) and SuperGLUE\u00a0(Wang et\u00a0al., 2019) cover tasks from sentiment analysis to language inference, while CLUE\u00a0(Xu et\u00a0al., 2020) targets Chinese language.\n\nTypo-fixing\u00a0(Suzgun et\u00a0al., 2022) is also widely used.\n\n\n###### Reading Comprehension\nReading comprehension tasks test a model\u2019s ability to extract and infer information from text. Benchmarks like SQuAD\u00a0(Rajpurkar et\u00a0al., 2018), QuAC\u00a0(Choi et\u00a0al., 2018), and BoolQ\u00a0(Clark et\u00a0al., 2019) challenge models to understand passages and draw logical conclusions.\n", "topic": "The rationale and methodologies behind typo-fixing benchmarks and their importance in assessing LLM resilience to language errors.", "question": "Which rationale best explains the importance of typo-fixing benchmarks in evaluating large language models, and which methodology most effectively assesses their resilience to language errors?", "choices": {"A": "They ensure models can accurately translate between languages with typographical errors, using machine translation tasks with intentional spelling mistakes.", "B": "They measure a model's ability to generate creative content from noisy prompts, employing generative benchmarks seeded with typographical noise.", "C": "They test the model's robustness to real-world user input errors by introducing systematic or random typos into standard datasets and assessing correction or comprehension.", "D": "They evaluate the ethical alignment of model outputs in response to misspelled toxic prompts, using toxicity benchmarks with typographical perturbations."}, "answer": "C", "explanation": "Option C is correct because typo-fixing benchmarks are designed to simulate real-world language errors by adding typographical mistakes to input data, thereby testing the model\u2019s resilience in correction or comprehension tasks\u2014a key measure of robustness for practical deployment.", "question_token_count": 30, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 3, "avg_answer_token_count": 25}
{"context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n", "topic": "The use of embedding-based similarity and improved mapping metrics for more robust contamination detection.", "question": "In the context of post-hoc contamination detection, what is the principal reason embedding-based similarity and improved mapping metrics provide greater robustness over traditional n-gram matching techniques?", "choices": {"A": "They enable exact token sequence alignment, eliminating the need for further behavioral analysis.", "B": "They capture semantic and paraphrastic overlaps that n-gram matching would miss, reducing false negatives due to linguistic variability.", "C": "They ensure that all performance metrics remain transparent and independently verifiable, regardless of label protection.", "D": "They focus exclusively on model outputs rather than underlying dataset similarities, thus bypassing overlap detection."}, "answer": "B", "explanation": "Embedding-based similarity and improved mapping metrics enhance robustness by capturing semantic relationships and paraphrastic overlaps that traditional n-gram matching misses. This addresses the problem of false negatives caused by linguistic variability, such as rewording or paraphrasing, which exact token or word matching would fail to detect. Options A, C, and D are incorrect because they either misrepresent the function of these methods or attribute unrelated benefits.", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 19}
{"context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n", "topic": "Methodological approaches employed by Auto-Dataset for generating new samples, including the retention of original stylistics and knowledge, and the creation of questions targeting different cognitive levels as defined by Bloom et al. (1956).", "question": "Which methodological rationale most accurately underpins Auto-Dataset's dual approach of generating new benchmark samples that both retain original stylistic and knowledge characteristics and target varied cognitive levels, as opposed to approaches focusing solely on variable substitution or contamination detection?", "choices": {"A": "To ensure both the authenticity of language understanding and comprehensive skill assessment aligned with cognitive hierarchies.", "B": "To minimize computational overhead by restricting sample types to only stylistic variations.", "C": "To eliminate the need for manual evaluation by automating contamination detection and variable replacement.", "D": "To prioritize novelty over content fidelity, focusing on maximally divergent samples regardless of cognitive complexity."}, "answer": "A", "explanation": "Auto-Dataset\u2019s methodology deliberately balances fidelity to original stylistics and knowledge (authenticity) with the creation of questions spanning Bloom\u2019s cognitive levels (comprehensive assessment), ensuring both realistic evaluation and broad cognitive coverage\u2014unlike approaches limited to variable substitution or contamination detection.", "question_token_count": 47, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 17}
{"context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n", "topic": "Investigation of MMLU-CF's template-driven approach to multiple-choice question generation, including the effects of answer choice shuffling and introducing \"None of the other choices.\"", "question": "What is a potential psychometric consequence of introducing both answer choice shuffling and the random replacement of incorrect options with \"None of the other choices\" in a template-driven multiple-choice question generation system like MMLU-CF?", "choices": {"A": "It can reduce answer position bias but may unintentionally increase the difficulty of distinguishing correct answers for both humans and models.", "B": "It guarantees that every question will have a unique correct answer, eliminating ambiguity entirely.", "C": "It ensures that all distractors are equally plausible, thereby standardizing question difficulty across the dataset.", "D": "It decreases the diversity of question types, potentially leading to overfitting on specific answer formats."}, "answer": "A", "explanation": "Shuffling answer choices helps reduce answer position bias, while introducing \"None of the other choices\" as a distractor can make it more challenging to identify the correct answer, as it introduces an atypical distractor that requires more careful reasoning, potentially increasing item difficulty or ambiguity.", "question_token_count": 46, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 20}
{"context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n", "topic": "The reliance on centralized evaluation systems due to label protection and its impact on detailed error analysis.", "question": "How does reliance on centralized evaluation systems, enforced by label protection, fundamentally constrain detailed error analysis and affect the reproducibility of model evaluation?", "choices": {"A": "By limiting access to raw data and labels, centralized evaluation systems prevent researchers from conducting granular error analyses and independently verifying results, thereby undermining reproducibility.", "B": "Centralized evaluation systems enhance reproducibility by standardizing metrics, allowing researchers to perform more detailed error analysis without data access.", "C": "Reliance on centralized evaluation systems enables researchers to bypass label protection and freely access detailed error logs for independent verification.", "D": "By providing only aggregate performance metrics, centralized evaluation systems ensure both transparency and granularity in error analysis."}, "answer": "A", "explanation": "The correct answer is A. Label protection restricts access to ground-truth labels and data, forcing reliance on centralized evaluation. This setup gives only aggregate performance metrics, impeding detailed error analysis and independent verification, and thus undermining reproducibility. The other options incorrectly suggest increased granularity or access, which is explicitly contradicted in the context.", "question_token_count": 27, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 24}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "The implications of adversarial or unethical behavior (e.g., intentional data leakage) on the effectiveness of canary-based mitigation strategies.", "question": "Which of the following most accurately explains why canary-based data contamination detection fails to prevent score inflation in the presence of intentional benchmark data leakage by model developers?", "choices": {"A": "Canary strings can be trivially removed from model training data using automated preprocessing tools.", "B": "The detection of canary strings depends on developer cooperation, allowing adversarial actors to selectively exclude or obfuscate evidence of contamination.", "C": "Canary-based methods inherently reduce model generalization, making them ineffective against sophisticated data leakage techniques.", "D": "Canary strings are detectable by models, leading to their suppression during inference regardless of training data contamination."}, "answer": "B", "explanation": "The effectiveness of canary-based mitigation relies on honest developer behavior; adversarial developers can intentionally leak data or ignore contamination markers, rendering canary-based detection and enforcement ineffective.", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 21}
{"context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n", "topic": "Application of the scalability metric to compare different dynamic benchmarking approaches.", "question": "When comparing dynamic benchmarking methods using the scalability metric described, which approach would achieve the highest scalability score if one method produces very large transformed datasets but at exponentially increasing cost per transformation, while another yields moderately sized datasets at consistently low cost per transformation?", "choices": {"A": "The method producing very large datasets at exponentially increasing cost per transformation.", "B": "The method yielding moderately sized datasets at consistently low cost per transformation.", "C": "Both methods would achieve equal scalability scores, as dataset size is the primary factor.", "D": "Scalability cannot be determined without knowing the specific transformation details."}, "answer": "B", "explanation": "Scalability is defined as the expected ratio of transformed dataset size to the cost of transformation. If the cost increases exponentially, the gain in dataset size is offset by the rapidly rising costs, resulting in a lower ratio compared to a method that consistently yields moderate datasets at low, stable cost.", "question_token_count": 49, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 14}
{"context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n", "topic": "The strategy and implications of variable identification and replacement by LLMs in VarBench to create novel benchmark samples.", "question": "Which of the following best captures a critical challenge posed by relying on LLMs for variable identification and replacement in VarBench when generating novel benchmark samples?", "choices": {"A": "Ensuring that variable replacement does not inadvertently simplify the underlying reasoning required by the sample.", "B": "Guaranteeing that all replaced variables are unique across the entire benchmark, avoiding duplication.", "C": "Preventing LLMs from introducing out-of-domain knowledge during variable substitution.", "D": "Maintaining the original syntactic structure while permitting free-form semantic alterations."}, "answer": "A", "explanation": "The key challenge is to ensure that when LLMs identify and replace variables, the essential reasoning and difficulty of the original sample are preserved; otherwise, replacements might unintentionally make the sample easier or alter its intended cognitive demand. While unique variable naming, avoiding out-of-domain content, and syntactic fidelity are relevant, the core issue is the potential for simplification or distortion of the sample's reasoning requirements.", "question_token_count": 30, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 5, "avg_answer_token_count": 16}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The potential consequences of inadequate benchmarking for the evaluation and perceived progress of LLMs.", "question": "What is the most significant risk associated with relying on static benchmarks that are insufficiently updated to match evolving LLM capabilities?", "choices": {"A": "They may encourage overfitting to benchmark tasks without improving general abilities.", "B": "They primarily slow down the computational speed of model evaluation.", "C": "They ensure that contamination detectors are no longer necessary.", "D": "They provide a more accurate measure of model performance over time."}, "answer": "A", "explanation": "Inadequate or outdated static benchmarks can lead to overfitting, where models are optimized to perform well on these benchmarks without genuine improvements in general capability. This undermines the evaluation's validity and can distort perceived progress.", "question_token_count": 24, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 12}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "Comparative analysis of methodologies employed by LiveBench, AntiLeak-Bench, AcademicEval, LiveCodeBench, LiveAoPSBench, and Forecastbench for sourcing up-to-date evaluation data.", "question": "Which of the following statements most accurately differentiates AntiLeak-Bench from the other benchmarks mentioned in terms of its methodology for sourcing up-to-date evaluation data?", "choices": {"A": "AntiLeak-Bench continuously scrapes new problems from online coding competition platforms to ensure dataset freshness.", "B": "AntiLeak-Bench generates queries specifically about knowledge that was unknown prior to the model's knowledge cutoff, uniquely aiming to eliminate data contamination from emergent information.", "C": "AntiLeak-Bench sources new forecasting questions on a daily basis from prediction markets and other data sources.", "D": "AntiLeak-Bench collects live math problems from online math forums updated every few months."}, "answer": "B", "explanation": "Option B correctly captures AntiLeak-Bench's distinctive approach of focusing on generating queries about newly emerged knowledge that was not available before the LLM's knowledge cutoff, directly targeting potential data contamination. The other options describe the methodologies of LiveCodeBench, Forecastbench, and LiveAoPSBench, respectively.", "question_token_count": 33, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 23}
{"context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n", "topic": "Analysis of Mathador-LM's strategy for generating evaluation queries based on Mathador game rules and the significance of input number variation.", "question": "When generating evaluation queries for LLMs using Mathador-LM, why is it particularly significant to vary input numbers in addition to adhering to Mathador game rules, as opposed to relying solely on template-based or answer-shuffling approaches?", "choices": {"A": "It ensures each problem instance is unique, reducing data contamination and overlap with existing or reused problems.", "B": "It primarily increases the computational difficulty of solving each query, making evaluation more challenging for LLMs.", "C": "It minimizes the need for human involvement in verification by automating the answer-checking process.", "D": "It standardizes the format of evaluation queries, allowing for easier comparison across different LLMs."}, "answer": "A", "explanation": "Varying input numbers while adhering to Mathador rules produces unique and novel evaluation instances, which addresses data contamination and problem reuse\u2014issues that template-based or answer-shuffling methods may not fully resolve.", "question_token_count": 48, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 9, "question_groundedness_score": 9, "avg_answer_token_count": 19}
{"context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n", "topic": "Synthesis and evaluation of recent advancements in data contamination research and their significance for future research directions in LLM evaluation.", "question": "Which of the following best captures a key implication of the current lack of standardized evaluation criteria for dynamic benchmarks in mitigating data contamination risks in large language models?", "choices": {"A": "It enables researchers to rapidly deploy novel benchmarks without concern for comparability, fostering innovation but potentially undermining cross-model evaluation consistency.", "B": "It ensures that all dynamic benchmarks are inherently more robust than static benchmarks, guaranteeing superior contamination mitigation.", "C": "It allows static benchmarks to remain the gold standard for contamination mitigation in LLM research, rendering dynamic benchmarks largely redundant.", "D": "It eliminates the need for continual updates to benchmarking methodologies, thereby simplifying the evaluation process for LLMs."}, "answer": "A", "explanation": "The absence of standardized criteria for dynamic benchmarks allows for methodological freedom and innovation but simultaneously hampers consistent cross-model evaluation, making it harder to compare results and reliably assess contamination mitigation effectiveness.", "question_token_count": 31, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 22}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "The concept and mechanism of canary strings as a method for detecting data contamination in LLM benchmark datasets.", "question": "Which of the following best characterizes the operational mechanism and primary limitation of canary strings as a method for detecting data contamination in LLM benchmark datasets?", "choices": {"A": "Canary strings are unique tokens embedded in training data that, when generated by a model, signal generalization rather than memorization; their main limitation is vulnerability to adversarial removal during model evaluation.", "B": "Canary strings are non-unique tokens added to test data to trigger model errors, indicating overfitting; their main limitation is that they can be detected only by automated scripts, not human reviewers.", "C": "Canary strings are deliberately unique markers inserted into benchmark datasets that, if produced by the model, reveal memorization of training data; their main limitation is ineffectiveness if trainers are not vigilant or act maliciously.", "D": "Canary strings are commonly used tokens scattered throughout datasets to monitor model perplexity; their main limitation is that they can inflate evaluation scores if not properly balanced."}, "answer": "C", "explanation": "Option C accurately describes that canary strings are deliberately unique markers embedded in benchmark datasets, and their presence in model output reveals memorization rather than generalization. The principal limitation is that this method depends on the vigilance and integrity of those evaluating the model; if trainers ignore or intentionally misuse the mechanism, it fails to prevent or detect contamination. Other options either mischaracterize the mechanism, the uniqueness, or the nature of the limitation.", "question_token_count": 29, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 10, "avg_answer_token_count": 38}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The proposed methods to enhance static benchmarking, such as data encryption and post-hoc contamination detection, and their respective strengths and weaknesses.", "question": "In the context of improving static benchmarks to mitigate data contamination in large language model evaluation, which statement best distinguishes the primary limitation of data encryption from post-hoc contamination detection?", "choices": {"A": "Data encryption can prevent benchmark inclusion in training data but may reduce benchmark accessibility, while post-hoc detection can identify contamination only after training but cannot prevent it.", "B": "Data encryption is always more effective than post-hoc detection because it guarantees benchmarks are never leaked, while post-hoc detection is unreliable due to model opacity.", "C": "Post-hoc contamination detection ensures benchmarks remain private, while data encryption can only detect contamination after training has occurred.", "D": "Both methods are equally limited in that neither can reduce contamination risk before training occurs."}, "answer": "A", "explanation": "Option A accurately captures that data encryption acts preemptively but can reduce accessibility, while post-hoc detection is reactive and cannot prevent contamination. Option B is subtly incorrect: encryption is not \"always\" effective, nor does it \"guarantee\" no leakage, and post-hoc detection's unreliability is not solely due to model opacity. Option C reverses the roles of the methods. Option D incorrectly asserts both methods act only after training.", "question_token_count": 34, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 4, "avg_answer_token_count": 25}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The conceptualization and implementation of dynamic benchmarking strategies, including continuously updated datasets and benchmark regeneration, to address contamination risks.", "question": "Which of the following most accurately describes a fundamental limitation of current dynamic benchmarking strategies\u2014such as continuously updating datasets or regenerating benchmarks\u2014in mitigating data contamination risks for LLM evaluation?", "choices": {"A": "They fully eliminate contamination by ensuring all benchmark data is always excluded from training sets.", "B": "They reduce, but do not entirely eliminate, contamination due to challenges in tracing LLM training data sources and aligning updates with training timelines.", "C": "They introduce significant bias by only using synthetic data, which fails to test real-world LLM capabilities.", "D": "They require all LLM developers to share proprietary training data, which is not feasible due to privacy concerns."}, "answer": "B", "explanation": "Option B reflects the core issue: dynamic benchmarking strategies, while mitigating contamination, cannot guarantee complete elimination because it is often impossible to know exactly what data an LLM has been trained on or to perfectly synchronize benchmark updates with all training runs. Option A is incorrect as complete elimination is not achieved. Option C is a misrepresentation; dynamic benchmarks are not necessarily limited to synthetic data. Option D addresses a practical challenge but is not the fundamental limitation inherent to the benchmarking strategies themselves.", "question_token_count": 35, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 8, "avg_answer_token_count": 21}
{"context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n", "topic": "The motivations for and implications of transitioning from static to dynamic benchmarks for LLM evaluation, with a focus on data contamination and fairness.", "question": "Which primary advantage does dynamic benchmarking provide over static benchmarking in large language model evaluation, particularly regarding data contamination and fairness, and what critical trade-off does this introduce in the context of interpretability?", "choices": {"A": "It eliminates contamination by using only human-curated data, but increases fairness at the cost of reduced scalability.", "B": "It minimizes contamination by generating novel data, thereby enhancing fairness, but introduces challenges in interpretability due to increased reliance on automated transformations.", "C": "It improves scalability by automating data generation, but increases contamination risk and undermines fairness.", "D": "It standardizes evaluation through repeated data reuse, enhancing interpretability but reducing fairness due to greater contamination."}, "answer": "B", "explanation": "Dynamic benchmarking's key advantage is the minimization of data contamination through the creation of novel data, which in turn promotes fairness in LLM evaluation. However, this benefit comes with the critical trade-off of interpretability challenges, as the transformation processes\u2014especially those involving LLMs\u2014can be less transparent and harder to manually verify, necessitating additional mechanisms for reliability.", "question_token_count": 39, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 8, "avg_answer_token_count": 22}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "The influence of benchmark contamination on research progress, model comparison, deployment decisions, and policy-making in LLM development.", "question": "Which of the following best captures a subtle but critical risk posed by benchmark contamination in the context of LLM development, beyond immediate overestimation of model performance?", "choices": {"A": "It may cause researchers to prioritize syntactic over semantic capabilities, resulting in models that excel at superficial tasks but lack true language understanding.", "B": "It can create a persistent illusion of research progress, distorting comparative evaluations, deployment strategies, and policy decisions based on unreliable indicators of generalization.", "C": "It primarily leads to increased computational costs during model retraining cycles due to the need for more extensive deduplication of training data.", "D": "It ensures that LLMs become overly reliant on benchmark-specific patterns, thereby reducing their ability to perform in open-ended real-world tasks."}, "answer": "B", "explanation": "Option B most comprehensively and accurately articulates the subtle, systemic risk whereby contaminated benchmarks propagate a false sense of advancement, undermining research, comparative analysis, deployment, and policy-making. Options A and D are plausible but focus more narrowly on capability drift or overfitting, while C misattributes the principal consequence.", "question_token_count": 31, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 1, "question_groundedness_score": 8, "avg_answer_token_count": 27}
{"context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n", "topic": "The future directions for LLM benchmarking, with an emphasis on the need to develop and standardize evaluation criteria for dynamic benchmarks.", "question": "Which of the following most accurately captures a critical challenge in standardizing evaluation criteria for dynamic LLM benchmarks compared to static benchmarks?", "choices": {"A": "Dynamic benchmarks inherently lack scalability, making it impossible to evaluate models at scale.", "B": "The evolving nature of dynamic benchmarks complicates the establishment of consistent, fair, and contamination-resistant evaluation standards.", "C": "Data contamination is only a problem for static benchmarks and is automatically resolved in dynamic benchmarks.", "D": "Standardizing criteria for dynamic benchmarks is unnecessary because dynamic benchmarks guarantee perfect correctness."}, "answer": "B", "explanation": "Option B correctly identifies that the dynamic and evolving characteristics of such benchmarks present unique challenges to standardization, especially regarding fairness and contamination resistance. Options A, C, and D are incorrect or overly simplistic; A misattributes the issue solely to scalability, C dismisses contamination in dynamic benchmarks, and D falsely assumes inherent correctness.", "question_token_count": 25, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 18}
{"context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n", "topic": "The practical implications of external and internal diversity measurements for evaluating the effectiveness of data transformation or augmentation methods.", "question": "In the context of evaluating data transformation or augmentation methods, which scenario most accurately illustrates a pitfall that arises when maximizing internal diversity without sufficient attention to external diversity, and what is the likely consequence for downstream model performance?", "choices": {"A": "Transformation outputs are highly varied from one another but remain closely similar to the original dataset, resulting in limited model generalization gains.", "B": "Transformation outputs are highly similar to each other and also highly distinct from the original dataset, causing the model to overfit to novel patterns.", "C": "Both internal and external diversity are maximized, leading to an unstable training process and reduced convergence.", "D": "Both internal and external diversity are minimized, ensuring consistent but unhelpful augmentation for the model."}, "answer": "A", "explanation": "Maximizing internal diversity ensures that transformation outputs differ from each other, but if external diversity is low, the outputs still resemble the seed dataset. This fails to introduce meaningful new information, limiting the augmentation\u2019s potential to improve generalization in downstream models.", "question_token_count": 44, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 23}
{"context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n", "topic": "Examining the potential for misuse of benchmarking results, including artificially inflating model performance or selecting biased evaluation criteria.", "question": "Which scenario most effectively illustrates a subtle yet impactful way in which benchmarking results can be misused to artificially enhance the perceived fairness and capability of an AI model, despite underlying ethical concerns?", "choices": {"A": "Publishing all raw benchmarking data to maximize transparency for independent review.", "B": "Selectively designing evaluation criteria that favor strengths of a specific model, while omitting tasks where the model underperforms.", "C": "Utilizing dynamic benchmarks that continuously update with real-world data to ensure ongoing relevance.", "D": "Including outdated test cases from static benchmarks to maintain consistency in longitudinal evaluations."}, "answer": "B", "explanation": "Option B describes the practice of selectively constructing evaluation criteria or benchmarks to align with a model's strengths, thereby artificially inflating its apparent performance and fairness. This approach can mislead stakeholders about the model's true capabilities and ethical standing, representing a subtle but significant misuse of benchmarking.", "question_token_count": 37, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 5, "avg_answer_token_count": 17}
{"context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n", "topic": "Designing benchmarking frameworks that balance fairness, accountability, and privacy to prevent harm or disadvantage to specific user groups or research domains.", "question": "Which design pitfall in benchmarking frameworks is most likely to undermine both fairness and accountability, while also increasing the risk of harm or disadvantage to specific user groups, even if privacy is rigorously protected?", "choices": {"A": "Using static benchmarks that rely on outdated or biased data sources", "B": "Overemphasizing transparency at the expense of model privacy", "C": "Focusing solely on dynamic benchmarks without regular data validation", "D": "Allowing selective reporting of benchmarking results by model developers"}, "answer": "A", "explanation": "While privacy protection is crucial, relying on static benchmarks constructed from outdated or biased data can embed systemic biases and perpetuate unfairness, directly undermining both fairness and accountability. This can cause lasting harm to marginalized user groups or research domains, even when privacy is not compromised.", "question_token_count": 39, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 11}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The absence of systematic surveys and evaluation criteria for dynamic benchmarking methods in existing literature.", "question": "What is the most significant consequence of the current lack of systematic surveys and evaluation criteria for dynamic benchmarking methods in the evaluation of large language models?", "choices": {"A": "It hinders the transparent comparison and standardization of dynamic benchmarking approaches.", "B": "It leads to an overemphasis on static benchmarks in model evaluation.", "C": "It causes researchers to avoid using dynamic benchmarks altogether.", "D": "It makes data contamination detection entirely impossible."}, "answer": "A", "explanation": "Without systematic surveys and clear evaluation criteria, the field lacks a unified framework to compare, standardize, and assess the strengths and limitations of dynamic benchmarking methods. This absence impedes transparency and slows the development of robust, reliable benchmarks for LLMs. The other options either overstate the consequence (D), misattribute causality (B), or describe an outcome not supported by the context (C).", "question_token_count": 29, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 9, "question_groundedness_score": 10, "avg_answer_token_count": 12}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "Critical evaluation of the adequacy and limitations of current benchmarks in capturing the full spectrum of language model reasoning, coding, and instruction following abilities.", "question": "Which of the following most accurately encapsulates a critical limitation shared by the current landscape of coding, instruction following, and reasoning benchmarks in evaluating the comprehensive abilities of advanced language models?", "choices": {"A": "They primarily assess isolated skills within narrowly defined domains, often neglecting tasks that require the seamless integration of reasoning, instruction following, and coding across diverse, real-world scenarios.", "B": "They uniformly lack language diversity, restricting evaluation exclusively to English-language tasks and thereby underestimating multilingual model capabilities.", "C": "They focus excessively on open-ended tasks, failing to provide objective metrics for model performance across standardized problem sets.", "D": "They emphasize dynamic, real-time human interaction, which leads to inconsistent and non-reproducible evaluation outcomes."}, "answer": "A", "explanation": "The main shared limitation is that most benchmarks focus on discrete, domain-specific competencies\u2014such as coding, instruction following, or reasoning\u2014rather than evaluating language models on complex tasks that demand the integration of these skills in realistic contexts. While some benchmarks address language diversity or dynamic problem solving, the broader challenge remains the lack of holistic, cross-domain evaluation. The other options either exaggerate specific limitations (e.g., lack of language diversity, focus on open-endedness, or dynamic interaction) or do not accurately reflect the prevailing state of benchmark design.", "question_token_count": 36, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 8, "avg_answer_token_count": 25}
{"context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n", "topic": "Analysis of the effectiveness and limitations of encryption methods in preventing data leakage and their dependence on robust key management.", "question": "In the context of securing evaluation data against leakage, which scenario most critically undermines the effectiveness of encryption methods, even if state-of-the-art algorithms and protocols are used?", "choices": {"A": "Minor text variations in test data bypassing decontamination filters", "B": "Public release of encrypted test data alongside a confidential license", "C": "Exposure or compromise of private encryption keys due to inadequate key management", "D": "Increased computational overhead from implementing secure multi-party computation"}, "answer": "C", "explanation": "While computational overhead and decontamination bypasses are notable concerns, the exposure or compromise of private encryption keys fundamentally breaks the security guarantees of encryption, rendering even the most advanced algorithms ineffective and allowing unauthorized access to protected data.", "question_token_count": 35, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 11}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "The significance of querying LLMs for the value of the root node in DAG-based reasoning tasks.", "question": "In the context of evaluating reasoning abilities of large language models using directed acyclic graphs (DAGs), what is the primary significance of querying for the value of the root node after transforming the DAG into a natural language description?", "choices": {"A": "It measures the model\u2019s ability to aggregate and propagate information through all dependencies in the graph, reflecting multi-step compositional reasoning.", "B": "It isolates the assessment to the simplest node, minimizing the complexity of the reasoning required for accurate prediction.", "C": "It ensures that only local relationships between adjacent nodes are tested, avoiding global dependency analysis.", "D": "It verifies the model\u2019s proficiency in parsing natural language without requiring any reasoning about graph structure."}, "answer": "A", "explanation": "Querying the value of the root node in a DAG-based reasoning task compels the model to combine information from all subordinate nodes and dependencies, thereby evaluating its capacity for complex, multi-step, and dependency-aware reasoning\u2014far beyond simple or local inference.", "question_token_count": 44, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 21}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "The role and comparative focus of NaturalQuestions and TriviaQA in assessing real-world information retrieval by LLMs.", "question": "Which best characterizes the comparative focus of NaturalQuestions and TriviaQA in the context of evaluating large language models, distinguishing them from benchmarks like MMLU or BBH?", "choices": {"A": "They primarily assess models' capacity for technical long-context reasoning.", "B": "They are designed to test retrieval of real-world information rather than multi-domain or internal knowledge.", "C": "They focus on the evaluation of multi-step mathematical reasoning in diverse problem sets.", "D": "They measure the ability to generate new knowledge not present in training data."}, "answer": "B", "explanation": "NaturalQuestions and TriviaQA are specifically aimed at evaluating how well models can retrieve real-world information, distinguishing them from benchmarks like MMLU or BBH that focus on multi-domain knowledge or internal knowledge representation.", "question_token_count": 35, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 15}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "The role of multilingual instruction following benchmarks (especially C-Eval and C-SimpleQA) in evaluating cross-lingual model performance.", "question": "Which of the following best captures the primary contribution of benchmarks like C-Eval and C-SimpleQA in the context of evaluating cross-lingual language models?", "choices": {"A": "They provide comprehensive coverage of multilingual coding abilities, ensuring robust programming evaluation across languages.", "B": "They focus on language-specific tasks, enabling targeted assessment of instruction following and factuality in Chinese, thus exposing cross-lingual generalization strengths and weaknesses in models.", "C": "They exclusively test reasoning skills in English, minimizing the influence of linguistic diversity on model evaluation.", "D": "They benchmark a model\u2019s ability to translate between multiple languages, directly measuring translation proficiency."}, "answer": "B", "explanation": "Option B is correct because C-Eval and C-SimpleQA are designed for Chinese instruction following and factual question answering, respectively, enabling targeted evaluation of a model's performance in non-English contexts and helping to identify cross-lingual strengths and weaknesses. The other options either mischaracterize the scope, language, or evaluation focus of these benchmarks.", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 21}
{"context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n", "topic": "Critical evaluation of the comparative strengths and weaknesses of static versus dynamic approaches to LLM benchmarking, with a focus on data contamination risks.", "question": "When evaluating large language models, which nuanced trade-off best distinguishes static benchmarking from dynamic benchmarking in the context of data contamination and methodological reliability as training datasets expand?", "choices": {"A": "Static benchmarks become increasingly vulnerable to contamination, but maintain high reliability and reproducibility; dynamic benchmarks reduce contamination risk but introduce challenges in consistent and reproducible evaluation.", "B": "Dynamic benchmarks are more vulnerable to contamination due to fixed datasets, while static benchmarks maintain both low contamination and high reproducibility as datasets grow.", "C": "Both static and dynamic benchmarks offer equal protection against contamination, but static benchmarks are less reliable in terms of result reproducibility.", "D": "Static benchmarks improve in contamination resistance with dataset growth, whereas dynamic benchmarks face worsening contamination and reproducibility issues."}, "answer": "A", "explanation": "The key trade-off is that static approaches, while consistent and reproducible, become more susceptible to data contamination as training datasets expand. In contrast, dynamic approaches can mitigate contamination by varying test data but raise concerns about reliability and reproducibility of results.", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 25}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The role of human effort in designing, maintaining, and updating LLM benchmarks to ensure continued relevance and challenge.", "question": "Which factor most fundamentally explains why ongoing human effort is indispensable in the design, maintenance, and updating of LLM benchmarks to ensure their continued relevance and challenge?", "choices": {"A": "The persistent need to manually label model outputs for statistical evaluation.", "B": "The necessity to anticipate and respond to evolving model capabilities and new forms of data contamination that static or automated systems alone cannot reliably address.", "C": "The requirement to maintain fixed datasets for reproducibility across evaluation cycles.", "D": "The benefit of reducing computational costs associated with benchmark testing."}, "answer": "B", "explanation": "While automation and dynamic benchmarks can assist, only sustained human oversight can adapt benchmarks to new capabilities, unanticipated data contamination, and changing evaluation needs\u2014ensuring benchmarks remain relevant, challenging, and robust. The other options either conflate human effort with routine tasks (A, C) or focus on cost rather than substantive benchmark quality (D).", "question_token_count": 31, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 8, "avg_answer_token_count": 16}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "The purpose and design considerations of math benchmarks like GSM8K and MATH in evaluating multi-step reasoning in LLMs.", "question": "Which feature most fundamentally distinguishes benchmarks like GSM8K and MATH from simpler math evaluation datasets when assessing large language models?", "choices": {"A": "Emphasis on multi-step reasoning processes within complex problem scenarios", "B": "Reliance on real-world factual retrieval from external knowledge bases", "C": "Exclusive focus on open-domain question answering across multiple subjects", "D": "Limitation to single-step arithmetic calculations without context"}, "answer": "A", "explanation": "GSM8K and MATH are specifically designed to require multi-step reasoning through complex mathematical problems, unlike simpler datasets that focus on single-step calculations or factual recall.", "question_token_count": 24, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 6, "avg_answer_token_count": 11}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "The role of randomization and task difficulty variation in preventing overfitting and ensuring generalizability in LLM evaluation frameworks.", "question": "In the context of evaluating large language models (LLMs) on tasks involving randomly generated SQL tables and graphs of varying complexity, which combination of methodological choices is most effective in preventing overfitting and ensuring that observed performance reflects true generalizability, rather than exploitation of dataset-specific patterns?", "choices": {"A": "Fixed datasets with systematic variation in task difficulty only", "B": "Randomly generated datasets with static, uniform task difficulty", "C": "Both randomization of datasets and systematic variation in task difficulty", "D": "Fixed datasets with both low and high task difficulty levels"}, "answer": "C", "explanation": "Only the combination of randomizing datasets and systematically varying task difficulty robustly prevents overfitting and encourages generalizability by minimizing the chance of memorization and exposing models to a diverse range of challenges.", "question_token_count": 56, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 4, "avg_answer_token_count": 10}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "Challenges and implications of continuously updating benchmark datasets for the ongoing evaluation of LLMs.", "question": "Which of the following represents a subtle but critical challenge introduced by continuously updating benchmark datasets to align with LLM knowledge cutoffs, beyond simply preventing data contamination?", "choices": {"A": "Ensuring that evaluation results remain reproducible and comparable over time as benchmark content changes.", "B": "Guaranteeing that all new benchmark items are of uniform difficulty regardless of source or timing.", "C": "Preventing the inclusion of any questions that require subjective interpretation or open-ended reasoning.", "D": "Achieving perfect domain coverage by sourcing questions from every possible area of recent knowledge."}, "answer": "A", "explanation": "While updating benchmarks helps prevent data contamination, it introduces the complex challenge of maintaining reproducibility and comparability, as results may become difficult to interpret longitudinally when benchmark content evolves. Uniform difficulty, avoidance of open-ended questions, and exhaustive domain coverage are important but not inherent or unique complications of temporal updating.", "question_token_count": 31, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 17}
{"context": "###### Safety\nSafety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content. Datasets such as RealToxicityPrompts\u00a0(Gehman et\u00a0al., 2020) and ToxiGen\u00a0(Hartvigsen et\u00a0al., 2022) assess resilience against producing harmful outputs.\n\nBy providing a controlled environment to measure these aspects, safety benchmarks play a critical role in guiding the development of models that are not only powerful outputs but also responsible and trustworthy for real-world applications.\n\n\n###### Language\nLanguage benchmarks assess the LLMs\u2019 proficiency in specific languages.\n\nGLUE\u00a0(Wang, 2018) and SuperGLUE\u00a0(Wang et\u00a0al., 2019) cover tasks from sentiment analysis to language inference, while CLUE\u00a0(Xu et\u00a0al., 2020) targets Chinese language.\n\nTypo-fixing\u00a0(Suzgun et\u00a0al., 2022) is also widely used.\n\n\n###### Reading Comprehension\nReading comprehension tasks test a model\u2019s ability to extract and infer information from text. Benchmarks like SQuAD\u00a0(Rajpurkar et\u00a0al., 2018), QuAC\u00a0(Choi et\u00a0al., 2018), and BoolQ\u00a0(Clark et\u00a0al., 2019) challenge models to understand passages and draw logical conclusions.\n", "topic": "Comparative analysis of language proficiency benchmarks, including GLUE, SuperGLUE, and CLUE, and their coverage of diverse linguistic tasks and languages in LLM evaluation.", "question": "Which statement most accurately characterizes the comparative coverage and limitations of GLUE, SuperGLUE, and CLUE benchmarks in evaluating large language models across diverse linguistic tasks and languages?", "choices": {"A": "GLUE, SuperGLUE, and CLUE all equally represent a broad spectrum of linguistic tasks and multiple languages, making them interchangeable for multilingual LLM evaluation.", "B": "GLUE and SuperGLUE focus on a range of English-language tasks such as sentiment analysis and language inference, while CLUE is designed specifically for Chinese, leading to limited cross-linguistic task comparability.", "C": "CLUE offers broader task diversity than GLUE and SuperGLUE by including both English and Chinese tasks, making it the preferred benchmark for multilingual assessment.", "D": "GLUE and CLUE both primarily assess typo-fixing tasks across various languages, whereas SuperGLUE exclusively targets reading comprehension in English."}, "answer": "B", "explanation": "Only GLUE and SuperGLUE are described as covering a range of tasks (sentiment analysis, inference) in English, whereas CLUE is explicitly stated as targeting the Chinese language. This creates a limitation for cross-linguistic comparison, as task and language coverage are not aligned across these benchmarks. The other options either misstate task diversity or language coverage.", "question_token_count": 35, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 34}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "The methods and significance of open-domain evaluation as implemented in AlpacaEval and ArenaHard.", "question": "In the context of large language model evaluation, how do open-domain benchmarks like AlpacaEval and ArenaHard fundamentally differ from traditional domain-specific benchmarks, and what is their principal significance in assessing advanced LLM capabilities?", "choices": {"A": "They allow models to demonstrate complex, unconstrained reasoning and adaptability by evaluating performance across diverse, unstructured tasks, revealing generalization and robustness not captured by narrowly focused benchmarks.", "B": "They focus exclusively on technical or mathematical problem solving, thereby providing a direct measure of computational accuracy and algorithmic efficiency in LLMs.", "C": "They restrict evaluations to predefined factual queries, ensuring high reliability but limiting assessment to rote memorization and recall.", "D": "They utilize only long-context inputs to test memory retention, isolating the impact of context length on language understanding without assessing reasoning or adaptability."}, "answer": "A", "explanation": "The core distinction of open-domain evaluation as implemented in AlpacaEval and ArenaHard lies in their broad, unconstrained task coverage, enabling assessment of reasoning, adaptability, and generalization beyond what domain-specific or narrowly scoped benchmarks offer. This approach is significant because it reveals strengths and weaknesses in LLM performance that are not apparent in traditional, constrained benchmarks.", "question_token_count": 42, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 8, "avg_answer_token_count": 27}
{"context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n", "topic": "The broader impact of data contamination and lack of transparency on the research community\u2019s ability to assess LLM performance.", "question": "Which of the following most accurately captures a fundamental consequence of both widespread data contamination and lack of transparency in LLM training data on the research community's efforts to evaluate language model performance?", "choices": {"A": "They primarily lead to inflated benchmark scores that are easily detectable by retrieval-based methods.", "B": "They undermine the reliability of benchmarks by obstructing the identification and mitigation of evaluation data overlap, thereby eroding trust in comparative model assessments.", "C": "They mostly impact the speed of LLM development, causing delays in model deployment due to repeated data audits.", "D": "They result solely in models overfitting to their training data, without affecting broader community evaluation practices."}, "answer": "B", "explanation": "The combined effect of data contamination and opaque training data sources is to undermine the reliability of benchmarks, as the community cannot reliably detect or mitigate overlap between training and evaluation data. This erodes trust in comparative assessments of LLMs, since genuine generalization cannot be verified. The other options are either incomplete, incorrect, or miss the broader systemic implications.", "question_token_count": 36, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 21}
{"context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n", "topic": "Analysis of the risks and mechanisms of data contamination in large language model (LLM) training and evaluation.", "question": "Which of the following best explains why the lack of standardized evaluation criteria for dynamic benchmarks poses a critical challenge to effectively mitigating data contamination in LLM evaluation?", "choices": {"A": "It undermines the reproducibility of benchmarking results, making it difficult to compare model performance across different studies.", "B": "It allows for the unintentional inclusion of training data in benchmarks, directly increasing the likelihood of contamination.", "C": "It prevents the identification and exclusion of overlapping data between training and test sets, rendering dynamic benchmarks ineffective.", "D": "It leads to inconsistent benchmark updates, which paradoxically can reintroduce contaminated data despite dynamic design."}, "answer": "A", "explanation": "While dynamic benchmarks aim to reduce contamination, without standardized evaluation criteria, the results from dynamic benchmarks become difficult to interpret or compare across different models and studies, undermining their effectiveness and reproducibility as a tool for trustworthy evaluation.", "question_token_count": 30, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 21}
{"context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n", "topic": "Behavioral analysis approaches in post-hoc detection, including memorization tests with masked inputs and partial completions.", "question": "Which of the following best explains the primary advantage of using behavioral analysis methods\u2014such as memorization tests with masked inputs and partial completions\u2014over n-gram or embedding-based similarity approaches for post-hoc contamination detection in language models?", "choices": {"A": "They directly assess whether a model can recall specific content from the training set, even when surface forms are altered or partially hidden.", "B": "They eliminate the need for performance benchmarking across different datasets.", "C": "They guarantee the identification of all contamination cases by covering every possible overlap.", "D": "They primarily enhance computational efficiency over traditional overlap detection methods."}, "answer": "A", "explanation": "Behavioral analysis methods like masked input memorization and partial completions are designed to probe the model\u2019s ability to recall or reproduce specific information despite modifications to the input. This allows detection of memorization and contamination that pure surface-level or embedding-based similarity methods may miss, especially in cases where overlaps are subtle or paraphrased.", "question_token_count": 46, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 8, "avg_answer_token_count": 16}
{"context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n", "topic": "The methodology of KIEval in leveraging initial static benchmark questions to drive dynamic, response-dependent follow-up questioning.", "question": "In the context of interactive LLM evaluation, what is the distinctive methodological feature of KIEval that differentiates it from other multi-turn evaluation frameworks like TreeEval and LLM-as-an-Interviewer?", "choices": {"A": "It generates all evaluation questions in advance based solely on a static benchmark without considering model responses.", "B": "It uses initial static benchmark questions as anchors, then adaptively generates follow-up questions contingent on the evaluated model's responses.", "C": "It employs a multi-agent system to collaboratively create and verify benchmark questions in real time.", "D": "It paraphrases static benchmark questions and offers feedback without generating new, model-dependent follow-ups."}, "answer": "B", "explanation": "KIEval's distinguishing feature is its two-stage process: beginning with a static benchmark question, it then generates follow-up questions that are directly contingent on the LLM's specific response, making the evaluation adaptive and response-dependent. Other frameworks either generate initial questions dynamically (TreeEval), paraphrase and provide feedback (LLM-as-an-Interviewer), or use multi-agent collaboration (multi-agent methods), but do not anchor the process in a static benchmark with subsequent adaptive questioning.", "question_token_count": 40, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 20}
{"context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n", "topic": "Implications of high variance in complexity for the stability and reliability of dynamic benchmarking methods.", "question": "What is a primary implication of high variance in complexity across trials when using dynamic benchmarking methods for evaluating large language models?", "choices": {"A": "It undermines the ability to attribute performance changes specifically to model or data factors, reducing the reliability of the benchmark.", "B": "It consistently leads to underestimation of model performance by artificially lowering accuracy scores.", "C": "It ensures that complexity metrics are generalizable across different domains and applications.", "D": "It indicates that data contamination is the primary cause of observed performance drops."}, "answer": "A", "explanation": "High variance in complexity means the benchmarking method is unstable; performance changes could result from uncontrolled changes in task difficulty rather than model or data issues, making attribution unreliable. The other options are incorrect: B is too specific and not always true; C is the opposite of what high variance implies; D confuses the effect (complexity variance) with a single cause (contamination).", "question_token_count": 24, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 17}
{"context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n", "topic": "The concept and definition of collision in dynamic benchmarking, including its significance for benchmark robustness.", "question": "In the context of dynamic benchmarking for large language models, which statement best encapsulates the significance of collision, and its impact on benchmark robustness?", "choices": {"A": "Collision describes the proportion of identical items between two benchmark datasets, where a high collision rate indicates increased robustness through consistent evaluation.", "B": "Collision refers to the overlap in transformed benchmark datasets, with a high collision rate undermining robustness by limiting novelty and increasing the risk of data contamination.", "C": "Collision quantifies the number of failed model predictions on a benchmark, with higher collision reflecting stronger model generalization and benchmark robustness.", "D": "Collision measures the computational cost of generating new benchmarks, with lower collision indicating less reliable robustness due to resource constraints."}, "answer": "B", "explanation": "Option B is correct because collision is defined as the extent of overlap in transformed datasets, and a high collision rate reduces benchmark robustness by limiting novelty and increasing data contamination risk. The other options either misdefine collision or invert its relationship to robustness.", "question_token_count": 29, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 25}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "The rationale and mechanisms behind using post-knowledge cutoff data to prevent data contamination in LLM evaluation benchmarks.", "question": "Which of the following most accurately explains why using data collected after the LLM's knowledge cutoff date is considered an effective strategy to prevent data contamination in evaluation benchmarks?", "choices": {"A": "It ensures that evaluation data contains only novel types of tasks that the model architecture has never seen before.", "B": "It guarantees that the evaluation data has not been included in the model's training data, thus minimizing the risk of performance inflation due to prior exposure.", "C": "It allows benchmarks to focus exclusively on domains where the model is known to perform poorly, thereby providing a more rigorous assessment.", "D": "It allows for the continuous updating of benchmarks, which completely eliminates the possibility of any overlap between training and evaluation data, regardless of data source."}, "answer": "B", "explanation": "Using post-cutoff data minimizes the risk of data contamination because such data, by definition, was unavailable during the model\u2019s training process, making it unlikely that the model has been exposed to this information. This directly addresses concerns about inflated evaluation results due to test data leakage into training sets. However, while continuous updating can reduce risk, it does not absolutely guarantee elimination of overlap from all possible sources, and novelty of task type or domain focus is not the primary mechanism.", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 26}
{"context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n", "topic": "The implications of collision metrics for the continued effectiveness and integrity of dynamic benchmarks in evaluating LLM capabilities, especially in the presence of potential training data contamination.", "question": "In the context of dynamic benchmarking for LLMs, which scenario most accurately signals that a benchmark\u2019s ability to reflect true model capabilities is compromised due to training data contamination, as assessed by collision metrics?", "choices": {"A": "The collision rate between independently transformed benchmark datasets is low, and the expected number of repeat trials to regenerate a transformed dataset is high.", "B": "The collision rate between independently transformed benchmark datasets is high, and the expected number of repeat trials to regenerate a transformed dataset is low.", "C": "The collision rate is low, but the expected number of repeat trials to regenerate a transformed dataset is also low.", "D": "The collision rate is high, but the expected number of repeat trials to regenerate a transformed dataset is high."}, "answer": "B", "explanation": "High collision rate means there is significant overlap between different transformations, so new tests are likely to repeat contaminated data. A low expected number of repeat trials means it does not take many attempts to generate previously seen data, indicating poor novelty and compromised benchmark integrity in the face of contamination.", "question_token_count": 40, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 5, "question_groundedness_score": 6, "avg_answer_token_count": 24}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "Challenges associated with detecting syntactic contamination compared to exact contamination.", "question": "Which of the following best explains why detecting syntactic contamination in LLM benchmarks poses greater challenges than detecting exact contamination?", "choices": {"A": "Syntactic contamination requires computationally expensive semantic equivalence checks beyond simple string matching.", "B": "Syntactic contamination can only be detected by comparing dataset metadata, which is often unavailable.", "C": "Syntactic contamination is limited to code snippets, making it easier to overlook in natural language benchmarks.", "D": "Syntactic contamination is caused by random data errors, which are inherently unpredictable."}, "answer": "A", "explanation": "Detecting exact contamination involves identifying verbatim duplicates, typically achievable via string matching algorithms. In contrast, syntactic contamination involves transformed variants\u2014such as paraphrased or morphologically altered text\u2014that retain meaning but differ in form. Identifying these requires semantic equivalence assessments, which are significantly more complex and computationally demanding than simple duplication checks.", "question_token_count": 22, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 18}
{"context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n", "topic": "The importance of accurately measuring and controlling dataset complexity when assessing LLM performance on dynamically transformed benchmarks.", "question": "Why is ensuring low variance in dataset complexity across dynamically transformed benchmarks crucial for accurately interpreting LLM performance drops, and what is a primary limitation of current complexity measurement approaches that complicates this objective?", "choices": {"A": "Low variance ensures that observed performance drops are attributable to genuine model weaknesses rather than uncontrolled complexity shifts, but most complexity metrics are domain-specific and do not generalize across tasks.", "B": "Low variance guarantees that all LLMs perform equally across benchmarks, though current metrics are generally too sensitive to minor data changes.", "C": "Low variance eliminates the need for any contamination checks, but prevailing complexity metrics tend to underestimate actual complexity increases.", "D": "Low variance directly improves LLM training outcomes, though most metrics overestimate the impact of data augmentation."}, "answer": "A", "explanation": "The primary reason for ensuring low variance in dataset complexity is to attribute LLM performance drops to actual model limitations rather than confounding complexity changes; however, most available complexity metrics are limited by their domain specificity and lack of cross-task generalizability, complicating consistent measurement.", "question_token_count": 38, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 25}
{"context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n", "topic": "Evaluation of the implications of focusing on high-level concepts versus technical implementation details in surveys of LLM benchmarking, particularly for practitioner applicability.", "question": "In the context of surveys on LLM benchmarking, what is the most significant potential consequence for practitioners when surveys emphasize high-level concepts rather than detailed technical implementations?", "choices": {"A": "Practitioners may struggle to reproduce or implement methods effectively in real-world settings.", "B": "Practitioners are more likely to overlook the theoretical motivations behind benchmarking approaches.", "C": "Practitioners will be unable to understand the broader evolution of benchmarking strategies.", "D": "Practitioners may overemphasize minor technical differences at the expense of core concepts."}, "answer": "A", "explanation": "The absence of technical implementation details in surveys primarily limits practitioners' ability to translate high-level concepts into actionable procedures, making practical reproduction and effective application challenging.", "question_token_count": 31, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 17}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "The evolution and interplay of benchmark datasets in addressing the increasing complexity and diversity of LLM capabilities.", "question": "How does the evolution and specialization of benchmark datasets, as seen in the progression from foundational datasets like GSM8K and MMLU to recent challenges such as AIME 2024, CNMO 2024, MMLU-Redux, and MMLU-Pro, reflect both the increasing complexity of LLM capabilities and the need for nuanced assessment, and what strategic tension does this create in the design of future benchmarks?", "choices": {"A": "It leads to ever-broader, less specialized benchmarks that minimize domain-specific challenges and focus only on general capabilities, reducing the need for continuous dataset refinement.", "B": "It results in a balance between developing narrowly focused, domain-specific benchmarks and broad, multi-domain evaluations, highlighting a tension between measuring specialized skills and generalization as LLMs advance.", "C": "It causes benchmark development to stagnate, as newer LLMs have already surpassed the difficulty of existing datasets, making further assessment unnecessary.", "D": "It eliminates the need for technical or long-context benchmarks, since open-domain evaluations alone are sufficient to capture all aspects of LLM performance."}, "answer": "B", "explanation": "The evolution from foundational to advanced and specialized benchmarks demonstrates both the increasing complexity of tasks LLMs can handle and the need to assess nuanced capabilities. This progression creates a strategic tension between designing benchmarks that evaluate specialized, domain-specific abilities and those that assess broad, generalized competence, necessitating careful calibration to ensure comprehensive LLM evaluation.", "question_token_count": 85, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 8, "avg_answer_token_count": 30}
{"context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n", "topic": "The capabilities and limitations of retrieval-based detection methods for identifying training-evaluation data overlap in LLMs.", "question": "Which factor most fundamentally limits the effectiveness of retrieval-based detection methods in identifying training-evaluation data overlap in large language models?", "choices": {"A": "The high similarity between synthetic fine-tuning data and evaluation tasks", "B": "The proprietary nature and inaccessibility of much of the training data", "C": "The inability to process massive web-scraped corpora with current retrieval algorithms", "D": "The ambiguity of annotation guidelines for human-labeled datasets"}, "answer": "B", "explanation": "While similarity between fine-tuning and evaluation data (A) and processing scale (C) present challenges, the most fundamental limitation is that if large portions of training data are proprietary and inaccessible (B), no retrieval method can comprehensively verify overlap. Without transparency, even perfect retrieval algorithms cannot operate on unknown data, making independent contamination assessment and mitigation impossible.", "question_token_count": 25, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 13}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "The motivation and impact of benchmark refinements and extensions exemplified by MMLU-Redux and MMLU-Pro.", "question": "Which of the following most accurately characterizes the motivation and impact behind benchmark refinements and extensions, as exemplified by MMLU-Redux and MMLU-Pro, in the context of evaluating large language models?", "choices": {"A": "They primarily serve to expand the dataset size, allowing models to be tested on a greater volume of similar knowledge tasks.", "B": "They aim to address existing benchmark limitations by providing more rigorous, diverse, and diagnostically powerful assessments, thereby enabling finer-grained discrimination of model capabilities.", "C": "Their main contribution is introducing new subject domains not previously covered, ensuring coverage of all possible knowledge areas.", "D": "They focus on replacing outdated benchmarks altogether, rather than refining or extending existing evaluation methodologies."}, "answer": "B", "explanation": "Option B is correct because MMLU-Redux and MMLU-Pro are designed to refine existing assessments, improving rigor and diagnostic value, which allows for more precise evaluation of language model knowledge and capabilities. Options A and C mischaracterize the nature of these refinements as mere expansion or domain addition, while D incorrectly suggests a replacement rather than enhancement approach.", "question_token_count": 43, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 23}
{"context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The conceptual motivation for dynamic benchmarking as a response to static benchmarking limitations in LLM evaluation.", "question": "Which of the following most accurately captures the conceptual motivation for dynamic benchmarking in LLM evaluation, as contrasted with static benchmarking?", "choices": {"A": "To enable deterministic reproducibility of evaluation results by fixing the dataset for all models.", "B": "To mitigate data contamination and enhance evaluation fidelity by adaptively transforming the test set over time, thereby overcoming the limitations of static datasets.", "C": "To prioritize computational efficiency by minimizing data transformations during evaluation.", "D": "To ensure that evaluation always uses publicly available benchmark datasets, regardless of training data overlap."}, "answer": "B", "explanation": "Option B correctly reflects the motivation for dynamic benchmarking: it adaptively transforms the evaluation dataset to prevent contamination and provide a more faithful, transparent evaluation, directly addressing the challenges static benchmarks face due to data leakage and lack of adaptability. The other options either misstate the goal (A, C) or conflate accessibility with fidelity (D).", "question_token_count": 25, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 18}
{"context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n", "topic": "The function and effectiveness of contamination detectors in ITD for identifying and addressing contaminated samples in static benchmarks, and the process for LLM-driven benchmark rewriting while maintaining sample difficulty.", "question": "In the ITD system for benchmark rewriting, what is the primary rationale for employing a contamination detector prior to LLM-driven rewriting, and how does maintaining sample difficulty during the rewriting process fundamentally impact the integrity of the resulting benchmarks?", "choices": {"A": "The contamination detector ensures only contaminated samples are rewritten, preserving the statistical distribution of the dataset, while maintaining sample difficulty prevents skewing benchmark evaluations by avoiding unintended simplification or complication of tasks.", "B": "The contamination detector identifies all samples for rewriting to maximize diversity, while maintaining sample difficulty ensures the rewritten samples are stylistically similar to the originals.", "C": "The contamination detector filters out redundant samples, and maintaining difficulty increases the cognitive load for model evaluation.", "D": "The contamination detector selects samples based on variable content, while maintaining difficulty allows for expansion to different cognitive levels as defined by Bloom's taxonomy."}, "answer": "A", "explanation": "The ITD system uses a contamination detector to specifically identify contaminated samples so that only those are rewritten, thereby preserving the intended statistical properties of the benchmark. Maintaining sample difficulty is critical, as it ensures that the rewritten samples remain equally challenging, thus preserving the validity and fairness of subsequent evaluations and avoiding bias introduced by easier or harder samples.", "question_token_count": 45, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 8, "avg_answer_token_count": 28}
{"context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n", "topic": "Comparison between traditional model and LLM training-evaluation data separation and its impact on contamination risks.", "question": "Which factor most fundamentally distinguishes the risk of evaluation data contamination in LLMs from traditional models, and why does this distinction undermine the reliability of performance benchmarks for LLMs?", "choices": {"A": "The use of web-scraped datasets in LLMs, leading to unavoidable overlap with evaluation data and limiting transparency for external verification.", "B": "The exclusive use of synthetic data during LLM pre-training, which always matches evaluation tasks directly.", "C": "The reliance on retrieval-based detection methods in LLMs, which are always superior to manual dataset curation.", "D": "The open-sourcing of LLM training data, making contamination detection trivial and benchmarks more reliable."}, "answer": "A", "explanation": "The core distinction lies in LLMs' pre-training on vast, web-scraped datasets, which increases the likelihood of unintentional overlap with evaluation data. Unlike traditional models with explicit data separation, LLMs' opaque and proprietary training corpora impede thorough external scrutiny, making it difficult to guarantee benchmark reliability.", "question_token_count": 34, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 21}
{"context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n", "topic": "Comparative evaluation of encryption techniques (including public key encryption, confidential computing, and secure multi-party computation) for securing evaluation data in machine learning benchmarks.", "question": "When securing evaluation data for machine learning benchmarks, which scenario most clearly demonstrates an inherent limitation of public key encryption and confidential computing approaches compared to label protection?", "choices": {"A": "A scenario where the private key used for encryption is inadvertently exposed, allowing an adversary to access the test data.", "B": "A scenario where minor textual variations in test data bypass decontamination filters, leading to data contamination during training.", "C": "A scenario where an evaluator, with exclusive access to test labels, unintentionally leaks them during public reporting.", "D": "A scenario where computational overhead makes secure multi-party computation impractical for real-time evaluation."}, "answer": "A", "explanation": "Public key encryption and confidential computing depend critically on the secrecy of cryptographic keys; if a private key is exposed, the confidentiality of the encrypted evaluation data is lost. This risk is unique to these methods and is not present in label protection, which does not rely on cryptographic keys for its core security guarantee.", "question_token_count": 31, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 21}
{"context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n", "topic": "Assessment of label protection strategies in major benchmarks (e.g., GLUE, SuperGLUE, HumanEval) and their role in maintaining evaluation integrity.", "question": "Which aspect best explains why label protection is considered crucial for maintaining the integrity of evaluations in major machine learning benchmarks such as GLUE, SuperGLUE, and HumanEval?", "choices": {"A": "It prevents test data from being accessed by unauthorized parties through cryptographic means, ensuring absolute confidentiality.", "B": "It ensures that only authorized evaluators can access true test set labels, thereby preventing models from being trained or evaluated on answers and reducing data contamination risks.", "C": "It allows public access to both test inputs and labels, relying on advanced decontamination to mitigate contamination.", "D": "It encrypts both model parameters and evaluation data, eliminating the need for separate key management."}, "answer": "B", "explanation": "The primary role of label protection in benchmarks like GLUE, SuperGLUE, and HumanEval is to keep true test set labels hidden from public access, which prevents models from learning or memorizing answers during training and thus preserves evaluation integrity. Unlike encryption, which focuses on restricting access to the data itself, label protection specifically targets the risk of answer leakage and data contamination by controlling label exposure. Options A and D conflate label protection with encryption, while C misrepresents the practice by suggesting public access and reliance on decontamination.", "question_token_count": 35, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 22}
{"context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n", "topic": "Evaluation of the methodology and effectiveness of rule-based test case generation, with particular attention to collision probability and its ramifications for benchmark uniqueness.", "question": "In the context of benchmark creation for LLM evaluation, which subtle limitation most directly challenges the presumed effectiveness of rule-based test case generation in ensuring benchmark uniqueness, despite its extremely low collision probability?", "choices": {"A": "The potential for insufficient coverage of conceptual diversity due to rigid or simplistic underlying rules.", "B": "The persistent risk of data contamination from previously published competition problems.", "C": "The randomization of answer choices leading to statistically insignificant variations in test cases.", "D": "The high human effort required for verification and continual updating of benchmarks."}, "answer": "A", "explanation": "While rule-based generation minimizes the chance of direct duplication (collision), its effectiveness in guaranteeing true uniqueness is undermined if the rules themselves are too rigid or simplistic, resulting in test cases that, while not identical, lack sufficient diversity or novelty. This limitation is subtler than risks of data contamination, randomization artifacts, or human labor, as it directly impacts the qualitative uniqueness of the benchmark, not just quantitative duplication.", "question_token_count": 38, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 6, "avg_answer_token_count": 14}
{"context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n", "topic": "In-depth assessment of the GSM-Symbolic system's use of query templates and placeholder variables for generating diverse, dynamic math benchmarks.", "question": "What is a primary advantage of GSM-Symbolic's use of query templates with placeholder variables for dynamic math benchmark generation compared to methods relying on reusing competition problems or strictly rule-based synthesis?", "choices": {"A": "It enables automatic creation of diverse problem instances while minimizing the risk of data contamination.", "B": "It guarantees that all generated problems are of equivalent difficulty to original competition problems.", "C": "It completely eliminates the need for human oversight in benchmark creation and verification.", "D": "It ensures that every generated instance is entirely novel and cannot be replicated by any other system."}, "answer": "A", "explanation": "GSM-Symbolic's template-based generation with placeholder variables allows for dynamic, automated creation of diverse math problems, reducing dependence on real competition problems and thereby minimizing data contamination. It also surpasses strict rule-based synthesis in flexibility, but does not guarantee equivalent difficulty, complete elimination of human oversight, or absolute uniqueness of instances.", "question_token_count": 39, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 16}
{"context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n", "topic": "Developing and applying ethical guidelines for data usage, model transparency, and societal impact in the context of AI benchmark development and evaluation.", "question": "Which of the following strategies most effectively addresses both the risk of bias perpetuation in static benchmarks and privacy concerns in dynamic benchmarks when developing ethical guidelines for AI benchmarking frameworks?", "choices": {"A": "Mandating open access to all benchmark data and evaluation results to promote transparency.", "B": "Implementing ongoing audits of benchmark construction processes combined with robust data anonymization protocols.", "C": "Restricting benchmark updates to annually to limit data exposure and reduce bias accumulation.", "D": "Requiring models to be evaluated only on proprietary, unpublished datasets to prevent contamination."}, "answer": "B", "explanation": "Option B combines proactive oversight (ongoing audits) to detect and mitigate bias in static benchmarks, with robust anonymization to protect privacy in dynamic benchmarks, thus comprehensively addressing both core ethical risks described.", "question_token_count": 34, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 16}
{"context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n", "topic": "The impact of post-training fine-tuning with human-annotated and synthetic datasets on contamination risks in LLMs.", "question": "Which of the following most accurately explains how post-training fine-tuning with both human-annotated and synthetic datasets amplifies contamination risks in large language models, particularly in the context of proprietary training data and benchmarking?", "choices": {"A": "Fine-tuning on human-annotated datasets alone introduces contamination by replicating evaluation data, while synthetic datasets are immune due to their artificial nature, and proprietary training data only impacts transparency.", "B": "Both human-annotated and synthetic datasets used in post-training fine-tuning can closely mirror evaluation tasks, increasing the likelihood of contamination; when training data is proprietary, it becomes difficult to detect or mitigate this overlap, undermining the reliability of benchmarks.", "C": "Synthetic datasets primarily cause contamination risks, while human-annotated datasets are less likely to overlap with evaluation data, and proprietary data enhances rather than impedes benchmark reliability.", "D": "The use of retrieval-based detection methods is sufficient to eliminate contamination risks from both human-annotated and synthetic datasets, regardless of whether the training data is proprietary or public."}, "answer": "B", "explanation": "Option B synthesizes the key points: both types of fine-tuning data can resemble evaluation tasks, thereby increasing contamination risk, and proprietary data practices obscure the ability to detect and mitigate such overlaps, compromising benchmark validity. The other options either misrepresent the relative risks, the nature of synthetic datasets, or overstate the effectiveness of detection methods.", "question_token_count": 42, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 9, "avg_answer_token_count": 39}
{"context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n", "topic": "Challenges and limitations associated with generalizing existing complexity metrics across different benchmark domains.", "question": "Which of the following best explains a fundamental challenge in generalizing complexity metrics across different benchmark domains for evaluating large language models?", "choices": {"A": "Domain-specific complexity metrics often capture features unique to a particular task, making them unreliable indicators of complexity when applied to unrelated domains.", "B": "Most complexity metrics are inherently stable across domains, but they are computationally expensive and impractical to implement.", "C": "The variance in performance of large language models is entirely due to data contamination, not to differences in complexity metrics.", "D": "Universal complexity metrics already exist and are widely adopted, but researchers prefer domain-specific alternatives for convenience."}, "answer": "A", "explanation": "Option A is correct because the core challenge is that complexity metrics are typically designed to measure characteristics relevant only within a specific domain, such as graph complexity for reasoning problems, and fail to capture relevant complexity in other domains, leading to unreliable or misleading evaluations. Options B, C, and D each misrepresent the problem: B incorrectly asserts stability and misattributes the limitation, C ignores the role of complexity, and D falsely claims universal metrics exist and are avoided for convenience.", "question_token_count": 25, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 22}
{"context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n", "topic": "The limitations of static benchmarking methods, focusing on transparency, label protection, and the challenges of post-hoc contamination detection.", "question": "Which of the following most fundamentally limits the effectiveness of static benchmarking methods for LLM evaluation as models and corpora scale, considering issues of transparency, label protection, and post-hoc contamination detection?", "choices": {"A": "The difficulty in publicly releasing benchmark datasets due to proprietary restrictions, which prevents any form of evaluation.", "B": "The inability to guarantee that benchmark labels remain concealed from model developers, undermining transparency and facilitating data leakage.", "C": "The lack of standardization in benchmark scoring, making results incomparable across different static benchmarks.", "D": "The impossibility of reliably detecting training data contamination after model deployment, due to the opacity and size of modern training corpora."}, "answer": "D", "explanation": "While label protection and transparency are important, the most fundamental limitation arises from the technical impossibility of reliably detecting contamination post-hoc as training data and model scale increase, making static benchmarks increasingly outdated and unreliable.", "question_token_count": 38, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 8, "avg_answer_token_count": 21}
{"context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n", "topic": "The importance and challenges of ensuring interpretability in transformation processes for dynamic benchmarking of LLMs.", "question": "Which of the following best captures a fundamental challenge in ensuring interpretability during transformation processes for dynamic benchmarking of large language models, particularly when using LLM-assisted generation?", "choices": {"A": "Rule-based transformations inherently lack transparency, making manual validation indispensable.", "B": "LLM-assisted transformations, unlike rule-based ones, often require additional explainability or human oversight due to limited traceability of their decision-making processes.", "C": "Temporal cutoff approaches inherently guarantee interpretability, removing the need for any validation.", "D": "Hybrid approaches eliminate the need for interpretability by combining rule-based and LLM-based methods."}, "answer": "B", "explanation": "The key challenge is that LLM-assisted transformations are not inherently interpretable; their inner workings are often opaque, which necessitates supplementary mechanisms such as explainability tools or human-in-the-loop validation to ensure correctness and reliability. In contrast, rule-based transformations are by nature interpretable and do not present this challenge.", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 19}
{"context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n", "topic": "Mechanisms and evaluation strategies employed in LLM-as-an-Interviewer for multi-turn, interactive assessment of language models.", "question": "Which feature most fundamentally distinguishes the LLM-as-an-Interviewer evaluation strategy from other interactive assessment frameworks for language models?", "choices": {"A": "Conducting multi-turn evaluations using paraphrased queries and adaptive follow-up questions with feedback based on the model's responses.", "B": "Employing multiple specialized LLM agents for planning, generation, verification, and evaluation of benchmarks.", "C": "Generating follow-up questions solely from a static benchmark without adapting to the model\u2019s responses.", "D": "Relying exclusively on single-turn question-answering using unaltered queries from static benchmarks."}, "answer": "A", "explanation": "LLM-as-an-Interviewer is distinguished by its use of an interviewer LLM that not only paraphrases benchmark queries but also conducts multi-turn interactions, posing adaptive follow-up questions and providing feedback contingent on the examined model\u2019s previous responses. This dynamic engagement sets it apart from methods relying solely on static benchmarks, single-turn evaluation, or multi-agent planning.", "question_token_count": 24, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 20}
{"context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n", "topic": "Critical examination of the transformation from static to dynamic benchmarking in response to data contamination concerns.", "question": "Which of the following most accurately captures a fundamental limitation introduced by the shift from static to dynamic benchmarking in LLM evaluation, as motivated by data contamination concerns?", "choices": {"A": "Dynamic benchmarks eliminate the need for any standardization, as their content changes frequently.", "B": "Dynamic benchmarking, while reducing contamination risk, introduces challenges due to the absence of universally accepted evaluation criteria.", "C": "The transformation to dynamic benchmarking ensures that all evaluation data is inherently free from contamination.", "D": "Dynamic benchmarks inherently provide better model performance comparability across time and systems."}, "answer": "B", "explanation": "The shift to dynamic benchmarking aims to reduce data contamination but introduces a critical limitation: the lack of standardized criteria for evaluating dynamic benchmarks, making it difficult to ensure fairness, consistency, and comparability in LLM evaluation.", "question_token_count": 31, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 17}
{"context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n", "topic": "The interpretation and application of \u0398(\u00b7) as a metric for quantifying diversity between datasets, including possible metric choices like N-gram or BLEU scores.", "question": "When using \u0398(\u00b7) to quantify both external and internal diversity between datasets, what is a subtle but critical limitation introduced by selecting BLEU score as \u0398(\u00b7) compared to N-gram-based metrics, specifically in the context of capturing true semantic diversity between datasets?", "choices": {"A": "BLEU may overestimate diversity by rewarding lexical variation regardless of semantic equivalence.", "B": "BLEU may underestimate diversity due to insensitivity to word order variations.", "C": "BLEU may fail to distinguish semantic diversity because it is primarily a surface-level, reference-based metric focused on n-gram overlap.", "D": "BLEU is more sensitive to syntactic diversity than N-gram-based metrics, leading to inconsistent diversity estimates."}, "answer": "C", "explanation": "BLEU is a reference-based metric that primarily measures n-gram overlap between datasets, making it sensitive to surface-level changes but relatively insensitive to deeper semantic differences. This limitation means BLEU may not capture true semantic diversity, whereas N-gram-based metrics can be tuned to account for different aspects, but both are fundamentally limited in semantic sensitivity.", "question_token_count": 52, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 4, "avg_answer_token_count": 19}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "The structure, scope, and intended improvements offered by multi-domain knowledge benchmarks such as MMLU, BBH, and AGI Eval.", "question": "Which of the following best characterizes the primary intended improvement offered by multi-domain knowledge benchmarks such as MMLU, BBH, and AGI Eval compared to single-domain evaluations?", "choices": {"A": "They enable the measurement of factual recall limited to a single discipline.", "B": "They offer comprehensive assessment by evaluating diverse subject areas, revealing model generalization and cross-domain competence.", "C": "They focus exclusively on long-context technical reasoning within a narrowly defined topic.", "D": "They are designed to test only the retrieval of recent real-world information."}, "answer": "B", "explanation": "The core advancement of multi-domain benchmarks like MMLU, BBH, and AGI Eval is their ability to assess LLMs across a wide range of subjects, enabling evaluation of both generalization and cross-domain knowledge\u2014something single-domain benchmarks cannot provide.", "question_token_count": 35, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 9, "question_groundedness_score": 9, "avg_answer_token_count": 16}
{"context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The legal and privacy constraints affecting access to training datasets and their impact on contamination detection.", "question": "In scenarios where legal and privacy constraints restrict access to training datasets, which of the following best explains how dynamic benchmarking modifies the approach to contamination detection, and what is a key limitation that remains despite its adoption?", "choices": {"A": "Dynamic benchmarking continuously transforms evaluation data to reduce overlap with unknown training data, but cannot guarantee absence of contamination without knowledge of all prior transformations.", "B": "Dynamic benchmarking replaces the need for contamination detection by ensuring all evaluation data is always novel and unrelated to any possible training data.", "C": "Dynamic benchmarking enables direct comparison between static and dynamic datasets, thereby automatically identifying contaminated instances through statistical methods.", "D": "Dynamic benchmarking provides complete transparency into both training and evaluation data, fully eliminating contamination concerns."}, "answer": "A", "explanation": "A is correct because dynamic benchmarking attempts to minimize the risk of contamination by continually updating evaluation data, making it less likely for overlap to occur with inaccessible training datasets. However, the absence of full access to the training data and the accumulation of transformations over time mean contamination can never be completely ruled out; without comprehensive records, there is residual uncertainty. The other options either overstate the guarantees or misunderstand the mechanism.", "question_token_count": 42, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 23}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "The limitations of the canary string approach, including dependency on model trainers' awareness and intent.", "question": "Which of the following most fundamentally limits the effectiveness of the canary string approach in detecting benchmark data contamination in language model training?", "choices": {"A": "The inability of canary strings to detect paraphrased or obfuscated benchmark content.", "B": "The risk that model trainers may lack awareness of or deliberately ignore canary string markers.", "C": "The possibility that canary strings themselves become part of the general training distribution.", "D": "The technical challenge of embedding canary strings without affecting dataset quality."}, "answer": "B", "explanation": "The core limitation of the canary string approach is its dependence on model trainers\u2019 awareness and willingness to act on canary detections; if trainers are unaware of or intentionally disregard these markers (for example, to artificially boost benchmark scores), the method fails to function as an effective safeguard.", "question_token_count": 26, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 16}
{"context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n", "topic": "How opacity in LLM training data exacerbates contamination risks and impedes performance verification.", "question": "Which of the following most accurately explains how the opacity of LLM training data amplifies contamination risks and obstructs reliable performance verification?", "choices": {"A": "It prevents external parties from determining whether evaluation data overlaps with training data, making it impossible to fully validate benchmarks and assess true generalization.", "B": "It leads to larger model sizes, which inherently increase the chance of memorizing evaluation data regardless of dataset transparency.", "C": "It increases the computational cost of training, resulting in less frequent model evaluation and consequently more contamination.", "D": "It restricts the diversity of training data, causing models to overfit to evaluation tasks and obscuring performance metrics."}, "answer": "A", "explanation": "Only option A correctly identifies that opacity\u2014by hiding the contents of training data\u2014prevents external verification of data overlap, thus amplifying contamination risk and undermining the reliability of performance assessments. The other options either misattribute the problem to model size, computational cost, or data diversity, none of which directly relate to the specific issue of opaque training data complicating contamination detection and benchmark validation.", "question_token_count": 26, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 23}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The limitations of traditional static benchmarks such as HumanEval in ensuring fair and accurate assessment of LLM capabilities.", "question": "Which of the following most fundamentally undermines the reliability of traditional static benchmarks like HumanEval for assessing LLM capabilities, even when techniques such as dataset encryption or post-hoc contamination detection are applied?", "choices": {"A": "The inability to measure model generalization across diverse tasks.", "B": "The untraceable inclusion of benchmark data in LLM training sets due to the open availability of benchmarks and opacity of training data.", "C": "The limited scale and representativeness of static benchmark datasets relative to real-world tasks.", "D": "The lack of appropriate evaluation metrics for measuring LLM performance on static benchmarks."}, "answer": "B", "explanation": "While issues like dataset scale and evaluation metrics are important, the core problem with static benchmarks such as HumanEval is that their public availability means benchmark data can be unintentionally included in LLM training corpora, especially since the exact contents of these corpora are often unknown or untraceable. This leads to contamination that cannot be reliably mitigated by encryption or post-hoc detection, fundamentally compromising the fairness and accuracy of LLM evaluation.", "question_token_count": 38, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 8, "avg_answer_token_count": 17}
{"context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n", "topic": "Evaluation of methods designed to enhance static benchmarks and their inherent limitations in mitigating data contamination.", "question": "Which of the following best explains why methods designed to enhance static benchmarks face inherent limitations in mitigating data contamination for large language model evaluation?", "choices": {"A": "Static benchmarks, even when augmented or filtered, cannot fully prevent overlap with training data due to their finite and unchanging nature.", "B": "Enhanced static benchmarks always introduce bias that skews model performance metrics, regardless of contamination.", "C": "Static benchmarks inherently lack the ability to measure model generalization, making contamination irrelevant.", "D": "The process of enhancing static benchmarks is too computationally expensive to implement at scale."}, "answer": "A", "explanation": "The core limitation is that static benchmarks, no matter how they are improved or filtered, are fixed datasets; as long as they remain static, there is always a risk that portions are present in the model\u2019s vast training data. This makes it impossible to guarantee full mitigation of data contamination, unlike dynamic benchmarks, which can adapt and generate novel evaluation items.", "question_token_count": 27, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 19}
{"context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n", "topic": "The principles and effectiveness of hybrid generation methods in benchmark construction, including temporal cutoff, LLM-based, and graph-based approaches as used in LatestEval, DARG, and C2LEVA.", "question": "In the context of benchmark construction for LLM evaluation, which of the following most accurately explains how the hybrid approaches used by LatestEval, DARG, and C2LEVA address data contamination and evaluation reliability, and what key tradeoff remains unresolved despite their use?", "choices": {"A": "LatestEval and DARG both use graph-based perturbation exclusively to prevent contamination, whereas C2LEVA relies on temporal cutoff alone, but none address scalability in dynamic benchmarking.", "B": "LatestEval combines temporal cutoff with LLM-based generation to utilize real-time content, DARG uses LLM-based and graph-based methods to perturb reasoning graphs, and C2LEVA integrates temporal cutoff, LLM-based, and graph-based methods for contamination-free bilingual benchmarks; however, balancing correctness with scalability in dynamic benchmarks remains a key unresolved challenge.", "C": "All three benchmarks use only LLM-based generation to generate novel data, fully eliminating contamination but introducing label protection issues in static benchmarks.", "D": "LatestEval, DARG, and C2LEVA each focus solely on dynamic benchmarking, which inherently resolves both contamination and complexity control without the need for hybrid approaches."}, "answer": "B", "explanation": "Option B is correct because it accurately captures the distinct hybrid strategies: LatestEval's use of temporal cutoff and LLM-based methods, DARG's graph-based and LLM-based perturbation, and C2LEVA's integration of all three methods. It also identifies the unresolved challenge of balancing correctness with scalability in dynamic benchmarks. The other options are incorrect due to oversimplification or factual inaccuracies regarding the methods and their impact on contamination and benchmark challenges.", "question_token_count": 52, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 40}
{"context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The inherent limitations of post-hot detection methods for identifying data contamination in LLM training.", "question": "Which of the following best captures a fundamental reason why post-hot detection methods are inherently limited in identifying data contamination in LLM training, even beyond technical improvements in detection algorithms?", "choices": {"A": "They rely on model behaviors that may not consistently indicate contamination across varying architectures and tasks.", "B": "They require training datasets that are always publicly available and unrestricted by privacy or legal considerations.", "C": "They can only detect contamination if the model achieves perfect memorization of training data.", "D": "They are limited because static benchmarking schemes are always more transparent than dynamic ones."}, "answer": "A", "explanation": "The core limitation is that post-hot detection methods depend on assumptions about model behavior (such as memorization or perplexity differences) that do not reliably generalize across different models and tasks, making detection fundamentally uncertain even with technical advances.", "question_token_count": 34, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 8, "avg_answer_token_count": 17}
{"context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n", "topic": "The role of massive, web-scraped pre-training datasets in increasing the likelihood of evaluation data overlap in LLMs.", "question": "Which factor most fundamentally exacerbates the risk of evaluation data contamination in large language models compared to traditional models, and why do current mitigation strategies remain insufficient despite recent advances?", "choices": {"A": "The use of human-annotated fine-tuning data, because it closely matches evaluation tasks and cannot be reliably distinguished.", "B": "The proprietary nature of LLM training datasets, because it prevents open scrutiny and verification of overlaps.", "C": "The scale and indiscriminate nature of web-scraped pre-training corpora, because they make comprehensive exclusion of evaluation data infeasible even with advanced detection methods.", "D": "The reliance on synthetic datasets in post-training, because they are often generated with evaluation prompts and thus inherently leak test data."}, "answer": "C", "explanation": "While all factors contribute to contamination risk, the primary exacerbating element is the massive and indiscriminate web-scraped pre-training data, which by sheer volume and diversity makes it nearly impossible to guarantee that evaluation data (or close variants) have not been included. This challenge outpaces the capabilities of current retrieval-based detection methods, especially given the complexity and opacity of modern LLM datasets.", "question_token_count": 34, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 25}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The concept and mechanisms of contamination detection in LLM benchmarking, including the development and use of contamination detectors.", "question": "Which of the following best explains why contamination detectors alone are insufficient for maintaining the integrity of LLM benchmarking as models continue to evolve?", "choices": {"A": "Contamination detectors can only identify contamination after it has occurred, while dynamic benchmarks proactively prevent exposure.", "B": "Contamination detectors are unable to detect contamination in instruction-following tasks, necessitating manual review.", "C": "Contamination detectors are computationally expensive and thus impractical for large-scale benchmarks.", "D": "Contamination detectors are effective only for coding tasks and not for general-purpose benchmarks."}, "answer": "A", "explanation": "The main limitation of contamination detectors is that they are reactive tools\u2014they can quantify contamination risk only after overlaps between training and benchmark data have occurred. As LLMs continue to train on expanding datasets, static benchmarks become increasingly vulnerable to such contamination. Dynamic benchmarks, on the other hand, proactively mitigate the risk by continuously updating or changing evaluation data to stay ahead of the training corpus, thereby maintaining benchmarking integrity.", "question_token_count": 26, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 8, "avg_answer_token_count": 18}
{"context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n", "topic": "Trade-offs and optimization considerations between dataset size and transformation cost in dynamic benchmarks.", "question": "Which scenario best illustrates a suboptimal outcome when optimizing dynamic benchmark scalability, despite generating a large transformed dataset?", "choices": {"A": "Generating a large dataset with high transformation costs that outweigh statistical error reduction benefits", "B": "Producing a small dataset with minimal cost, resulting in high statistical error during benchmarking", "C": "Generating a dataset whose size and cost are balanced, maximizing the proportion of data per unit cost", "D": "Increasing both dataset size and transformation cost proportionally, maintaining constant scalability"}, "answer": "A", "explanation": "While generating a large dataset can reduce statistical errors, if the associated transformation cost increases disproportionately, the overall scalability (data per unit cost) can decrease, leading to a suboptimal outcome. The ideal is to maximize data per unit cost, not just dataset size. Options B and D do not fit the scenario of having a large dataset; C describes the optimal balance.", "question_token_count": 22, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 1, "question_groundedness_score": 4, "avg_answer_token_count": 16}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The necessity for, and formulation of, robust criteria to evaluate the effectiveness and reliability of dynamic LLM benchmarks.", "question": "Which property is most critical for criteria evaluating the effectiveness and reliability of dynamic LLM benchmarks in mitigating data contamination, given the limitations described in static benchmarking approaches?", "choices": {"A": "The ability to regenerate benchmarks independently of previous datasets", "B": "Transparency in benchmark construction and widespread public availability", "C": "Continuous adaptation of benchmarks based on LLM training data timestamps", "D": "Demonstrable resistance to inadvertent inclusion in future LLM training data"}, "answer": "D", "explanation": "While regenerating benchmarks and adapting them based on training data timelines are important, and transparency is valued, the central challenge emphasized is mitigating contamination by ensuring benchmarks are resistant to inclusion in future model training datasets\u2014since contamination undermines reliability and effectiveness. Thus, demonstrable resistance to such contamination is the most critical property.", "question_token_count": 31, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 11}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "Underlying assumptions and risks regarding benchmark integrity in LLM research and evaluation.", "question": "Which scenario most fundamentally threatens the validity of LLM evaluation benchmarks, and why is it particularly insidious compared to more overt forms of contamination?", "choices": {"A": "The presence of exact duplicates of benchmark items in the training set, because it allows models to memorize and regurgitate answers.", "B": "Syntactic contamination, where benchmark items appear in the training set after subtle transformations, because it can evade detection and create an illusion of generalization.", "C": "Random sampling errors in benchmark construction, because they reduce statistical power.", "D": "Benchmark items drawn from the same domain as the training data, because it increases topical overlap."}, "answer": "B", "explanation": "Syntactic contamination is particularly insidious because it may not be detected by simple duplication checks; models may encounter paraphrased or transformed versions of test items during training, leading to inflated performance estimates and undermining claims of generalization, all while maintaining the appearance of benchmark integrity.", "question_token_count": 29, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 8, "avg_answer_token_count": 22}
{"context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n", "topic": "The mathematical expressions for external and internal diversity, including the role of the expectation operator and the diversity function \u0398(\u00b7).", "question": "Which of the following best describes the mathematical distinction between external and internal diversity as formalized using the expectation operator and the diversity function \u0398(\u00b7)?", "choices": {"A": "External diversity computes the expectation of \u0398 between each transformed dataset and the seed dataset, while internal diversity computes the expectation of \u0398 over all pairs of transformed datasets with distinct indices.", "B": "External diversity computes the expectation of \u0398 between all pairs of transformed datasets, while internal diversity computes the expectation of \u0398 between each transformed dataset and the seed dataset.", "C": "External diversity computes the mean of \u0398 over all possible permutations of the transformed datasets, while internal diversity computes the expectation of \u0398 between each transformed dataset and itself.", "D": "Both external and internal diversity compute the expectation of \u0398 between the transformed dataset and the seed dataset, but internal diversity applies an additional normalization factor."}, "answer": "A", "explanation": "The key distinction is that external diversity involves averaging \u0398 between each transformed dataset and the seed dataset (fixed reference), whereas internal diversity averages \u0398 over all pairs of transformed datasets with different indices (i \u2260 j), capturing variability among transformations themselves.", "question_token_count": 29, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 1, "question_groundedness_score": 10, "avg_answer_token_count": 32}
{"context": "###### Safety\nSafety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content. Datasets such as RealToxicityPrompts\u00a0(Gehman et\u00a0al., 2020) and ToxiGen\u00a0(Hartvigsen et\u00a0al., 2022) assess resilience against producing harmful outputs.\n\nBy providing a controlled environment to measure these aspects, safety benchmarks play a critical role in guiding the development of models that are not only powerful outputs but also responsible and trustworthy for real-world applications.\n\n\n###### Language\nLanguage benchmarks assess the LLMs\u2019 proficiency in specific languages.\n\nGLUE\u00a0(Wang, 2018) and SuperGLUE\u00a0(Wang et\u00a0al., 2019) cover tasks from sentiment analysis to language inference, while CLUE\u00a0(Xu et\u00a0al., 2020) targets Chinese language.\n\nTypo-fixing\u00a0(Suzgun et\u00a0al., 2022) is also widely used.\n\n\n###### Reading Comprehension\nReading comprehension tasks test a model\u2019s ability to extract and infer information from text. Benchmarks like SQuAD\u00a0(Rajpurkar et\u00a0al., 2018), QuAC\u00a0(Choi et\u00a0al., 2018), and BoolQ\u00a0(Clark et\u00a0al., 2019) challenge models to understand passages and draw logical conclusions.\n", "topic": "The specific challenges and capabilities assessed by reading comprehension benchmarks such as SQuAD, QuAC, and BoolQ in testing LLMs' ability to extract, infer, and reason from textual passages.", "question": "Which of the following best captures the distinct challenge that BoolQ presents to language models compared to SQuAD and QuAC in evaluating reading comprehension abilities?", "choices": {"A": "Requiring extraction of specific answer spans from a passage", "B": "Demanding open-ended, multi-turn dialog understanding for question answering", "C": "Necessitating binary inference and logical reasoning to determine yes/no answers from ambiguous passages", "D": "Emphasizing correction of typographical errors before answering questions"}, "answer": "C", "explanation": "BoolQ is unique among these benchmarks in that it focuses on yes/no questions, requiring models to make binary decisions often based on subtle inference and logical reasoning, rather than straightforward extraction (SQuAD) or dialog-based QA (QuAC). Option C captures this distinction, while the other options correspond to the core challenges of SQuAD, QuAC, or unrelated tasks.", "question_token_count": 31, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 13}
{"context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The handling and construction of dynamic benchmarking datasets when the seed dataset is empty.", "question": "When constructing a dynamic benchmarking dataset for LLMs with an empty seed dataset, which methodological principle is most critical to ensure the resulting benchmark remains both representative and free from data contamination?", "choices": {"A": "Generating evaluation data using random sampling from the model's output distribution without any external constraints", "B": "Designing data generation processes that incorporate principled constraints to ensure diversity, relevance, and minimal overlap with potential training data", "C": "Reusing evaluation examples from previous benchmarking tasks with minor modifications to avoid direct duplication", "D": "Allowing human annotators to freely create evaluation items based solely on their intuition and experience"}, "answer": "B", "explanation": "While generating data without a seed requires creativity, the most critical methodological principle is to use principled constraints during data generation to ensure the resulting benchmark is diverse, relevant, and minimizes overlap with any possible training data, thereby avoiding contamination and preserving evaluation integrity. Other options risk bias, lack of representativeness, or inadvertent data leakage.", "question_token_count": 36, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 4, "avg_answer_token_count": 18}
