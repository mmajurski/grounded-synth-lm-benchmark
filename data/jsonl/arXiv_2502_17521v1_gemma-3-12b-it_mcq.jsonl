{"context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n", "topic": "The technical and logistical challenges associated with detecting and mitigating data contamination within the massive scale of LLM training corpora.", "question": "How does the prevailing practice of maintaining LLM training data as proprietary information most fundamentally impede the development and deployment of robust data contamination mitigation strategies?", "choices": {"A": "It primarily limits the ability to fine-tune models on synthetic datasets designed to counteract contamination.", "B": "It restricts the scope of retrieval-based detection methods, as access to the training corpus is necessary for comprehensive searches.", "C": "It prevents external researchers from independently verifying the extent of contamination and proposing targeted remediation techniques.", "D": "It renders the existing human-annotated datasets unsuitable for evaluating model performance, necessitating the creation of entirely new benchmarks."}, "answer": "C", "explanation": "The text explicitly states that the proprietary nature of training data \"impedes the community\u2019s ability to verify and mitigate potential overlaps.\" This directly relates to external researchers being unable to assess and address contamination.", "question_token_count": 28, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 10, "avg_answer_token_count": 21}
{"context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n", "topic": "Explain the mathematical formulas provided for calculating external and internal diversity, including the meaning of each symbol and variable.", "question": "What is the primary distinction in calculation and interpretation between the formulas presented for external and internal diversity, and what does the symbol \u0398(\u22c5) represent within both equations?", "choices": {"A": "External diversity measures variation between individual transformed datasets and the seed dataset, while internal diversity measures variation between different transformation trials; \u0398(\u22c5) represents a function quantifying the semantic similarity between datasets.", "B": "External diversity is calculated across all transformed datasets simultaneously, while internal diversity is calculated pairwise; \u0398(\u22c5) represents a statistical significance test used to determine if the datasets are significantly different.", "C": "External diversity focuses on the relationship between a single transformed dataset and the original, while internal diversity examines relationships between multiple transformed datasets; \u0398(\u22c5) is a diversity metric, such as BLEU score, used to quantify dataset dissimilarity.", "D": "Internal diversity is calculated using a moving average, while external diversity is calculated using a weighted average; \u0398(\u22c5) represents a normalization factor used to scale the diversity scores."}, "answer": "C", "explanation": "The correct answer highlights the core difference: external diversity compares transformed datasets to the seed dataset, while internal diversity compares different transformation trials. \u0398(\u22c5) is explicitly defined as a function that measures the diversity (or dissimilarity) between two datasets.", "question_token_count": 34, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 39}
{"context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n", "topic": "Detail the approach of CONSTAT and its role in detecting contamination by comparing model performance across benchmarks.", "question": "Which principle underpins the contamination detection approach of CONSTAT?", "choices": {"A": "It identifies overlaps between training and testing data using n-gram matching.", "B": "It analyzes model behavior through masked inputs to assess memorization.", "C": "It compares model performance across different benchmarks to detect contamination.", "D": "It leverages embedding-based similarity to identify semantic overlap between datasets."}, "answer": "C", "explanation": "CONSTAT, as described in the text, distinguishes itself by analyzing model performance across benchmarks to detect contamination. This comparative approach is its defining characteristic.", "question_token_count": 13, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 2, "question_groundedness_score": 1, "avg_answer_token_count": 13}
{"context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n", "topic": "The critical role of label protection in maintaining evaluation integrity and mitigating data contamination risks in machine learning benchmarks.", "question": "Considering the vulnerabilities inherent in both encryption and label protection strategies for maintaining evaluation integrity in machine learning benchmarks, which of the following most accurately describes the primary advantage of label protection in mitigating data contamination risks?", "choices": {"A": "Label protection offers superior computational efficiency compared to encryption, reducing the overhead associated with secure benchmarking.", "B": "Label protection directly prevents unauthorized access to the raw evaluation data, safeguarding it from inclusion in training sets, unlike encryption which only obscures the data.", "C": "Label protection's effectiveness is independent of key management protocols, eliminating the risk of compromise that plagues encryption-based methods.", "D": "Label protection specifically addresses the threat of minor text variations bypassing decontamination methods, a vulnerability that encryption struggles to overcome."}, "answer": "B", "explanation": "Label protection's key advantage is preventing models from learning or memorizing test labels, directly mitigating data contamination risks. This is distinct from encryption, which focuses on preventing access to the raw data.", "question_token_count": 40, "answer_correctness_score": 6, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 24}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "Compare and contrast the different approaches used by LiveBench, AntiLeak-Bench, AcademicEval, LiveCodeBench, LiveAoPSBench, and Forecastbench in constructing their benchmarks.", "question": "Which of the following best describes a primary distinction in the methodologies employed by LiveBench and AntiLeak-Bench for constructing benchmarks that mitigate data contamination in LLM evaluations?", "choices": {"A": "LiveBench relies on continuously updating questions related to recent events, while AntiLeak-Bench focuses on generating queries about knowledge that emerged *after* the model's training cutoff.", "B": "LiveBench utilizes data from prediction markets, whereas AntiLeak-Bench leverages arXiv papers for its benchmark construction.", "C": "LiveBench primarily collects coding problems from online platforms, while AntiLeak-Bench gathers math problems from forums.", "D": "LiveBench updates questions monthly, while AntiLeak-Bench updates questions daily."}, "answer": "A", "explanation": "AntiLeak-Bench\u2019s key distinction lies in its deliberate generation of queries about information that was entirely unknown before the model's cutoff date, directly addressing potential contamination. LiveBench updates questions on recent events.", "question_token_count": 34, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 24}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "Describe syntactic contamination, including the role of syntactic transformations and the preservation of lexical meaning.", "question": "Which of the following best describes the critical characteristic distinguishing syntactic contamination from other forms of data contamination in the context of Large Language Models?", "choices": {"A": "The presence of identical data points in both training and evaluation datasets.", "B": "The ability to identify a test data point within the training data after applying transformations that alter sentence structure but maintain the original word choices.", "C": "The utilization of complex algorithms to obfuscate data during the training process, hindering direct memorization.", "D": "The incorporation of diverse data sources, increasing the likelihood of encountering similar phrases across datasets."}, "answer": "B", "explanation": "Syntactic contamination specifically involves transformations that preserve lexical meaning while altering sentence structure. This distinguishes it from exact contamination (identical data) and other potential issues.", "question_token_count": 27, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 20}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "Explain how Forecastbench updates its benchmark questions and the types of data sources it uses for forecasting.", "question": "What is the primary mechanism Forecastbench employs to maintain benchmark question relevance and minimize data contamination relative to other mentioned benchmarks?", "choices": {"A": "Forecastbench generates new queries about newly emerged knowledge unknown before the model\u2019s knowledge cutoff date.", "B": "Forecastbench updates its questions on a daily basis using prediction markets and other diverse data sources.", "C": "Forecastbench collects questions based on the latest information sources, such as math competitions from the past 12 months.", "D": "Forecastbench designs academic writing tasks on the latest arXiv papers."}, "answer": "B", "explanation": "Forecastbench's daily updates from diverse data sources are its key strategy for maintaining relevance and minimizing contamination, contrasting with the periodic updates or specific data sources used by other benchmarks.", "question_token_count": 24, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 18}
{"context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n", "topic": "Describe the evolution of post-hoc detection techniques, from initial n-gram matching to more advanced embedding-based similarity and mapping metrics.", "question": "What is the primary impetus for the transition from n-gram matching to embedding-based similarity and mapping metrics in post-hoc detection of data contamination?", "choices": {"A": "N-gram matching is computationally less expensive, making it preferable for large datasets.", "B": "Embedding-based methods offer a more nuanced approach to identifying overlaps, addressing the limitations of exact n-gram matches and reducing false negatives.", "C": "Embedding-based methods are required to detect contamination introduced by paraphrased or slightly modified test cases, which n-gram matching cannot.", "D": "N-gram matching is inherently more accurate for detecting subtle semantic similarities indicative of data contamination."}, "answer": "B", "explanation": "The text explicitly states that exact matching with n-grams often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity and mapping metrics. This highlights the primary impetus for the transition.", "question_token_count": 29, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 21}
{"context": "###### Safety\nSafety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content. Datasets such as RealToxicityPrompts\u00a0(Gehman et\u00a0al., 2020) and ToxiGen\u00a0(Hartvigsen et\u00a0al., 2022) assess resilience against producing harmful outputs.\n\nBy providing a controlled environment to measure these aspects, safety benchmarks play a critical role in guiding the development of models that are not only powerful outputs but also responsible and trustworthy for real-world applications.\n\n\n###### Language\nLanguage benchmarks assess the LLMs\u2019 proficiency in specific languages.\n\nGLUE\u00a0(Wang, 2018) and SuperGLUE\u00a0(Wang et\u00a0al., 2019) cover tasks from sentiment analysis to language inference, while CLUE\u00a0(Xu et\u00a0al., 2020) targets Chinese language.\n\nTypo-fixing\u00a0(Suzgun et\u00a0al., 2022) is also widely used.\n\n\n###### Reading Comprehension\nReading comprehension tasks test a model\u2019s ability to extract and infer information from text. Benchmarks like SQuAD\u00a0(Rajpurkar et\u00a0al., 2018), QuAC\u00a0(Choi et\u00a0al., 2018), and BoolQ\u00a0(Clark et\u00a0al., 2019) challenge models to understand passages and draw logical conclusions.\n", "topic": "Explain the critical role of safety benchmarks, such as RealToxicityPrompts and ToxiGen, in guiding the development of responsible and trustworthy LLMs for real-world applications.", "question": "Beyond merely identifying toxic outputs, how do safety benchmarks like RealToxicityPrompts and ToxiGen fundamentally shape the iterative development process of Large Language Models (LLMs) toward achieving trustworthy real-world utility?", "choices": {"A": "They primarily serve as a post-deployment validation tool, confirming the absence of toxicity before release.", "B": "They enable targeted fine-tuning strategies by providing granular feedback on specific failure modes, allowing developers to proactively mitigate risks.", "C": "Their main function is to establish a competitive landscape, driving model developers to achieve the lowest toxicity scores regardless of broader ethical considerations.", "D": "They are largely symbolic, offering limited practical guidance due to the inherent difficulty in comprehensively capturing all forms of potential harm."}, "answer": "B", "explanation": "Safety benchmarks are crucial for guiding the development process. They provide specific feedback on failure modes, enabling targeted fine-tuning and proactive risk mitigation.", "question_token_count": 43, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 23}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "Describe the different types of tasks currently employed in LLM benchmarking, providing examples such as instruction-following and coding tasks, and explain what specific capabilities each task aims to evaluate.", "question": "Which of the following best describes the primary rationale behind the use of coding tasks in the benchmarking of Large Language Models (LLMs)?", "choices": {"A": "To measure the LLM's proficiency in generating diverse natural language text formats.", "B": "To assess the LLM\u2019s ability to synthesize information from various sources and produce coherent summaries.", "C": "To evaluate the LLM's capacity to produce, interpret, and manipulate programming code, reflecting its logical reasoning capabilities.", "D": "To quantify the LLM\u2019s adherence to grammatical rules and stylistic conventions in written communication."}, "answer": "C", "explanation": "Coding tasks are specifically designed to evaluate a model's ability to handle programming code, which reflects its logical reasoning and computational understanding.", "question_token_count": 27, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 18}
{"context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n", "topic": "Describe the core concept of Interactive Evaluation and how it draws inspiration from human interview processes.", "question": "Which of the following best encapsulates the fundamental design principle underpinning Interactive Evaluation methodologies for LLMs?", "choices": {"A": "Utilizing multiple LLMs in a collaborative setting to dynamically generate and refine evaluation benchmarks.", "B": "Replicating the iterative questioning and feedback loop characteristic of human interviews to assess LLM understanding.", "C": "Employing static benchmarks and automated scoring systems to measure LLM performance on predefined tasks.", "D": "Leveraging large datasets of human-generated text to train LLMs to mimic conversational styles."}, "answer": "B", "explanation": "Interactive Evaluation is explicitly stated as being \"inspired by the human interview process,\" emphasizing the iterative questioning and feedback loop. The other options describe different evaluation approaches or training methodologies.", "question_token_count": 22, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 9, "avg_answer_token_count": 18}
{"context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n", "topic": "Discuss the relationship between data contamination, the availability of benchmark algorithms, and the need for dynamic benchmarking techniques.", "question": "How does the public availability of benchmark algorithms fundamentally compromise the validity of static benchmarks and necessitate the development of dynamic benchmarking strategies to mitigate data contamination?", "choices": {"A": "Public algorithms enable adversaries to precisely replicate benchmark data, rendering subsequent evaluations meaningless.", "B": "Public algorithms allow for iterative refinement of LLMs specifically tailored to excel on the benchmark, but do not affect overall validity.", "C": "Public algorithms introduce a risk of data leakage through unintentional replication during LLM training, leading to artificially inflated performance scores.", "D": "Public algorithms are irrelevant to data contamination, as the core issue is the inherent bias within the dataset itself."}, "answer": "C", "explanation": "The core issue is that a publicly available algorithm allows for the creation of training data that mimics the benchmark, leading to inflated performance. This directly compromises the validity of the benchmark.", "question_token_count": 30, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 6, "avg_answer_token_count": 22}
{"context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n", "topic": "Describe the role of explainability tools and human-in-the-loop validation in ensuring the reliability and correctness of LLM-assisted transformations within dynamic benchmarks.", "question": "Within the context of dynamic benchmarks assessing LLMs, what primary deficiency of LLM-assisted transformations necessitates the implementation of explainability tools or human-in-the-loop validation?", "choices": {"A": "LLM-assisted transformations are inherently biased, requiring validation to ensure fairness across diverse datasets.", "B": "The large volume of transformed data generated by dynamic benchmarks makes manual verification impractical without automated assistance.", "C": "LLM transformations lack the transparency and traceability of rule-based methods, making their correctness difficult to ascertain.", "D": "Dynamic benchmarks are susceptible to data contamination, and explainability tools are needed to identify and mitigate this risk."}, "answer": "C", "explanation": "The text explicitly states that LLM-assisted transformations depend on the model\u2019s transparency and traceability, implying a deficiency in these areas compared to rule-based methods. This lack of transparency necessitates additional mechanisms for validation.", "question_token_count": 33, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 5, "question_groundedness_score": 10, "avg_answer_token_count": 20}
{"context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n", "topic": "How do pre-defined rules in LLM-generated datasets potentially limit sample diversity and increase the risk of in-distribution contamination during training?", "question": "Which of the following best describes the most significant challenge posed by the reliance on pre-defined rules in LLM-generated datasets concerning model training?", "choices": {"A": "Reduced computational cost associated with dataset creation.", "B": "Increased ability to precisely control the difficulty level of generated samples.", "C": "A heightened susceptibility to overfitting due to limited sample diversity and potential in-distribution contamination.", "D": "Improved generalization capabilities stemming from the systematic exploration of specific knowledge domains."}, "answer": "C", "explanation": "The context explicitly states that pre-defined rules can limit sample diversity and increase the risk of in-distribution contamination, directly leading to overfitting.", "question_token_count": 27, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 13}
{"context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n", "topic": "Describe the different forms that the \"Cost\" function can take in the context of dynamic benchmarking, providing examples of monetary cost, time spent, and manual effort.", "question": "Within the context of dynamic benchmarking, how does the specific formulation of the \"Cost\" function most directly influence the evaluation of scalability?", "choices": {"A": "The Cost function primarily impacts the statistical error margin, with lower costs leading to more reliable benchmarking.", "B": "The Cost function dictates the size of the original dataset, directly limiting the potential for scalability.", "C": "The Cost function\u2019s value, encompassing monetary cost, time, or manual effort, determines the proportion of data generated per unit cost, thereby affecting scalability assessment.", "D": "The Cost function only influences the transformed dataset size, with no bearing on the overall scalability evaluation."}, "answer": "C", "explanation": "The text explicitly states that the Cost function measures \"monetary cost, time spent, or manual effort\" and that this value determines the proportion of data generated per unit cost, directly influencing the scalability assessment.", "question_token_count": 27, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 22}
{"context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n", "topic": "Explain the role of the contamination detector in the ITD methodology, and how it contributes to the rewriting of samples while preserving their difficulty levels.", "question": "Within the ITD methodology, what is the primary function of the contamination detector, and how does this function enable the preservation of sample difficulty during the rewriting process?", "choices": {"A": "To generate new, stylistically similar samples while varying the cognitive level of the question.", "B": "To identify and remove samples from static benchmarks that pose a risk of in-distribution contamination during training.", "C": "To prompt LLMs to replace variables in existing samples, thereby creating new, diverse instances.", "D": "To expand on existing concepts by leveraging LLMs and knowledge graphs to create extended question series."}, "answer": "B", "explanation": "The contamination detector in ITD specifically identifies contaminated samples, allowing for targeted rewriting that aims to maintain the original difficulty level. This targeted approach is crucial for preserving the integrity of the benchmark.", "question_token_count": 33, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 19}
{"context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n", "topic": "Discuss the current challenges faced by both static and dynamic benchmarking methods, as identified in the text, and provide specific examples of these challenges.", "question": "Which of the following best encapsulates the core limitations of dynamic benchmarking methods, as contrasted with the issues inherent in static approaches, according to the provided analysis?", "choices": {"A": "Dynamic benchmarks primarily struggle with label protection and the assumption of model contamination, mirroring the concerns of static benchmarking.", "B": "While addressing static benchmarking's vulnerability to data contamination, dynamic methods face challenges in achieving scalability alongside maintaining evaluation correctness and a lack of standardized criteria.", "C": "Dynamic benchmarks excel in transparency but are hampered by the complexities of web-scale data and commercial concerns, issues largely absent in static benchmarking.", "D": "The primary limitation of dynamic benchmarks lies in their inability to control the complexity of evaluation, a feature readily available in static methods."}, "answer": "B", "explanation": "The text explicitly states that dynamic benchmarks face challenges in balancing correctness with scalability and a lack of standardized criteria. This distinguishes them from static benchmarks which have issues with transparency and assumptions about contaminated models.", "question_token_count": 33, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 26}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "Evaluate the utility of contamination detectors and dynamic benchmarks as strategies for mitigating the challenges posed by static benchmarks and data contamination in LLM evaluation.", "question": "How do contamination detectors and dynamic benchmarks fundamentally differ in their approach to addressing the shortcomings of static benchmarks in LLM evaluation?", "choices": {"A": "Contamination detectors proactively prevent model training on benchmark data, while dynamic benchmarks adjust difficulty in response to model performance.", "B": "Contamination detectors measure the extent of data overlap between training data and benchmarks, whereas dynamic benchmarks continuously evolve to remain challenging.", "C": "Contamination detectors focus on improving the quality of benchmark datasets, while dynamic benchmarks aim to assess a wider range of model capabilities.", "D": "Contamination detectors are primarily used to fine-tune LLMs, while dynamic benchmarks are used to evaluate pre-trained models."}, "answer": "B", "explanation": "Contamination detectors assess the presence of benchmark data in the training set, while dynamic benchmarks adapt the benchmark itself to maintain difficulty as models improve.", "question_token_count": 24, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 4, "avg_answer_token_count": 25}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "Explain the principle behind canary strings and how their presence in an LLM's output indicates data contamination.", "question": "Why are canary strings valuable in assessing LLM performance, beyond simply confirming the presence of specific training data?", "choices": {"A": "They guarantee that LLMs are generalizing beyond their training data, ensuring robust performance across diverse inputs.", "B": "They provide a mechanism to identify instances where an LLM has memorized portions of its training data, indicating a lack of true generalization.", "C": "They allow model developers to directly compare the performance of different LLMs on identical datasets, eliminating variability.", "D": "They automatically filter out contaminated instances, ensuring that benchmark results accurately reflect an LLM\u2019s inherent capabilities."}, "answer": "B", "explanation": "Canary strings don't guarantee generalization or eliminate variability. Their primary value lies in identifying memorization, not in guaranteeing performance or simplifying comparisons.", "question_token_count": 22, "answer_correctness_score": 10, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 22}
{"context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n", "topic": "Describe a hybrid approach to dynamic benchmark construction, and explain how it combines elements of the other approaches.", "question": "What is the defining characteristic of a hybrid dynamic benchmark construction approach, according to the provided text?", "choices": {"A": "It exclusively utilizes newly released information, similar to temporal cutoff benchmarks.", "B": "It relies solely on predefined rules to generate novel data points, mirroring rule-based generation.", "C": "It integrates aspects of temporal cutoff, rule-based generation, and LLM-based generation methods.", "D": "It prioritizes data collection processes identical to those used in static benchmarks."}, "answer": "C", "explanation": "Hybrid approaches are defined by their combination of different techniques, specifically incorporating elements from temporal cutoff, rule-based generation, and LLM-based generation.", "question_token_count": 20, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 16}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "Explain the nature of the academic writing tasks designed by AcademicEval and the source material used.", "question": "What is the defining characteristic of the academic writing tasks evaluated by AcademicEval?", "choices": {"A": "They assess the ability to summarize historical scientific literature.", "B": "They focus on generating text based on recently published arXiv papers.", "C": "They evaluate performance on established coding competition platforms.", "D": "They test the model's ability to predict future events using prediction markets."}, "answer": "B", "explanation": "AcademicEval is specifically designed to evaluate academic writing tasks using the latest arXiv papers, ensuring the assessment is based on current research.", "question_token_count": 16, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 8, "avg_answer_token_count": 12}
{"context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n", "topic": "The significance of accounting for complexity in dynamic benchmarks for LLMs to distinguish between performance degradation caused by data contamination versus increased task difficulty.", "question": "Within the context of dynamic benchmarks for LLMs, what does a high variance in the complexity measurement function \u03a8(\u22c5) indicate according to the provided stability formulation?", "choices": {"A": "A consistently increasing task difficulty across different trials, implying robust benchmark performance.", "B": "A reliable and stable assessment of the seed dataset's complexity, irrespective of dynamic transformations.", "C": "An unstable dynamic benchmarking method, potentially masking the true impact of data contamination.", "D": "A negligible impact of dynamic transformations on the overall complexity of the benchmark dataset."}, "answer": "C", "explanation": "The equation presented explicitly states that high variance in complexity indicates an unstable dynamic benchmarking method. This instability can obscure the true cause of performance degradation.", "question_token_count": 33, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 16}
{"context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n", "topic": "The importance of incorporating fairness, accountability, and privacy considerations into the design of LLM benchmarking frameworks.", "question": "Considering the inherent trade-offs between static and dynamic LLM benchmarks, what is the most critical architectural modification to a benchmarking framework to proactively mitigate the long-term societal impact of potentially biased or unfairly selective evaluations?", "choices": {"A": "Implementing differential privacy techniques solely on dynamic benchmark data collection to obscure individual user contributions.", "B": "Establishing a diverse, rotating panel of independent auditors to periodically review and validate benchmark datasets and evaluation metrics.", "C": "Prioritizing the use of static benchmarks composed of synthetic data generated from known unbiased sources.", "D": "Employing adversarial training techniques to specifically target and eliminate biases within the LLMs being benchmarked."}, "answer": "B", "explanation": "The most critical architectural modification involves ongoing, independent review and validation. This ensures continuous assessment and adaptation to evolving biases and societal impacts, addressing the limitations of both static and dynamic approaches. While other options offer partial mitigation, they don't provide the continuous oversight needed for long-term societal impact.", "question_token_count": 42, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 6, "avg_answer_token_count": 20}
{"context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n", "topic": "Describe the limitations of attempting to enhance static benchmarks to effectively reduce the risk of data contamination in LLMs.", "question": "Why are attempts to enhance static benchmarks ultimately insufficient for effectively mitigating data contamination risks in large language models?", "choices": {"A": "Static benchmarks fail because they rely on fixed datasets, which can become contaminated over time as LLMs are continuously retrained on increasingly large and diverse corpora.", "B": "Static benchmarks are inadequate due to their inability to account for the evolving nature of LLM training data, which introduces new contamination risks beyond those initially assessed.", "C": "Static benchmarks struggle because their evaluation methodologies are not adaptable to the dynamic and opaque nature of LLM training processes, hindering effective contamination detection.", "D": "Static benchmarks are limited by the fact that they do not incorporate methods for actively identifying and removing contaminated data from the evaluation sets."}, "answer": "B", "explanation": "The abstract states that enhancing static benchmarks has \"inherent limitations,\" implying a fundamental issue with the approach itself. While all options touch on aspects of the problem, option B best captures the core issue - the inability to account for the *evolving* nature of training data and the consequent risks.", "question_token_count": 21, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 29}
{"context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n", "topic": "The purpose and function of post-hoc detection methods in mitigating data contamination within large language model training and evaluation.", "question": "Which of the following best explains the rationale for progressing from n-gram matching to embedding-based similarity in post-hoc data contamination detection within large language models?", "choices": {"A": "N-gram matching is computationally less expensive, making it preferable for evaluating very large models.", "B": "Embedding-based similarity addresses the issue of false negatives inherent in exact n-gram matching due to paraphrasing or slight variations in wording.", "C": "Embedding-based similarity is required to detect contamination when the training and test data are in different languages.", "D": "N-gram matching is more effective at identifying deliberate attempts to inject malicious data into the training set."}, "answer": "B", "explanation": "The text explicitly states that exact matching (including n-gram matching) often leads to false negatives, motivating the adoption of more robust techniques like embedding-based similarity.", "question_token_count": 30, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 21}
{"context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n", "topic": "Explain the concept of a temporal cutoff in dynamic benchmarks and how it relates to the data collection process used in static benchmarks.", "question": "What fundamental distinction in data acquisition characterizes a temporal cutoff approach in dynamic benchmarks compared to the methodology employed in static benchmarks?", "choices": {"A": "Static benchmarks rely on retrospective data analysis, while temporal cutoff benchmarks utilize data from prior evaluation periods.", "B": "Temporal cutoff benchmarks generate synthetic data, whereas static benchmarks utilize curated datasets.", "C": "Static benchmarks gather data from newly released information, while temporal cutoff benchmarks use historical data.", "D": "Temporal cutoff benchmarks involve manual data labeling, unlike static benchmarks which employ automated processes."}, "answer": "C", "explanation": "Temporal cutoff benchmarks mimic static benchmarks' data collection process but specifically use data from newly released information, differentiating it from the use of existing datasets in static benchmarks.", "question_token_count": 24, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 17}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The role of timestamp-based updates and data regeneration in minimizing contamination within dynamic benchmarking strategies.", "question": "Within the context of dynamic LLM benchmarking, what is the primary distinction between utilizing timestamp-based updates and regenerating benchmark data to mitigate data contamination?", "choices": {"A": "Timestamp-based updates focus on excluding data points based on their creation date relative to model training, while data regeneration involves constructing entirely new benchmark instances.", "B": "Data regeneration primarily relies on obfuscating existing data points, whereas timestamp-based updates involve adding noise to the training data.", "C": "Timestamp-based updates are suitable for continuous evaluation, while data regeneration is a one-time process for establishing a clean benchmark.", "D": "Data regeneration ensures complete elimination of contamination, whereas timestamp-based updates only reduce the likelihood."}, "answer": "A", "explanation": "Timestamp-based updates leverage temporal information to exclude potentially contaminated data, while data regeneration creates new benchmarks. Option A accurately captures this distinction. Option B is incorrect as it describes obfuscation, not regeneration. Option C is misleading as both methods can be applied in continuous evaluation. Option D is too absolute; neither method guarantees complete elimination.", "question_token_count": 29, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 24}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The importance of post-hoc contamination detection techniques and how they relate to the broader goal of reliable LLM evaluation.", "question": "Given the inherent challenges of tracing training data in Large Language Models (LLMs), what is the primary limitation of relying solely on post-hoc contamination detection techniques to ensure reliable evaluation, and why does this necessitate a broader strategic approach to benchmarking?", "choices": {"A": "Post-hoc detection only identifies contamination after training, failing to prevent its influence on model behavior and limiting the ability to assess true generalization.", "B": "Post-hoc detection is computationally expensive and impractical for large-scale LLMs, making it unsuitable for continuous evaluation.", "C": "Post-hoc detection can only identify contamination in specific datasets, not across the entire training corpus, leading to an incomplete assessment.", "D": "Post-hoc detection is inherently biased towards identifying contamination in commonly used benchmarks, potentially overlooking issues in less-explored areas."}, "answer": "A", "explanation": "Post-hoc methods are reactive, addressing contamination *after* it's occurred. This means the model has already been influenced, and the evaluation reflects a potentially contaminated state, rather than true generalization capability. A broader strategic approach, like dynamic benchmarking, is needed to proactively mitigate contamination.", "question_token_count": 49, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 24}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "Discuss the broader implications of using temporal cutoffs in LLM evaluation and why it is considered a crucial practice.", "question": "Why is employing temporal cutoffs in LLM evaluation considered a crucial practice, extending beyond simply mitigating data contamination?", "choices": {"A": "It ensures the model's responses reflect current events, making them more relevant to real-world applications.", "B": "It isolates the model's inherent reasoning capabilities, independent of its ability to memorize recent information.", "C": "It prevents the model from exploiting knowledge gained after its training cutoff, providing a more accurate assessment of its general knowledge and reasoning.", "D": "It allows for continuous model improvement by directly comparing performance against a constantly evolving dataset."}, "answer": "C", "explanation": "Temporal cutoffs are crucial because they isolate the model's inherent reasoning abilities from its ability to memorize recent information, providing a more accurate assessment of its general knowledge. While other options touch on related aspects, this is the core reason.", "question_token_count": 22, "answer_correctness_score": 10, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 20}
{"context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n", "topic": "The fundamental reasons why LLMs are inherently more vulnerable to data contamination than traditional machine learning models.", "question": "Given the distinct lifecycle of Large Language Models (LLMs), which of the following best explains why they exhibit a heightened susceptibility to data contamination compared to traditional machine learning models?", "choices": {"A": "Traditional models typically utilize smaller, curated datasets, reducing the probability of overlap with evaluation data, whereas LLMs are trained on vast, uncontrolled web scrapes.", "B": "The iterative fine-tuning process in LLMs, specifically the reliance on synthetic datasets, introduces systematic biases that mimic evaluation tasks, a phenomenon absent in traditional models.", "C": "LLMs' architectural design, particularly the use of transformer networks, inherently lacks the regularization techniques necessary to prevent memorization of evaluation data.", "D": "The proprietary nature of LLM training data prevents rigorous auditing and validation, a constraint not typically present in the development of traditional machine learning models."}, "answer": "A", "explanation": "The correct answer highlights the combination of massive, uncontrolled pre-training data and subsequent fine-tuning on datasets that may resemble evaluation tasks. This lifecycle inherently increases the risk of contamination.", "question_token_count": 35, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 30}
{"context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n", "topic": "Detail the process of TreeEval, including how it generates initial questions, follow-up subtopics, and corresponding questions.", "question": "How does TreeEval's question generation process differ from a static benchmark evaluation approach?", "choices": {"A": "TreeEval generates subsequent questions solely based on the initial topic, regardless of the LLM\u2019s responses.", "B": "TreeEval generates follow-up subtopics and questions based on the previous topic and the LLM\u2019s response to the prior question.", "C": "TreeEval utilizes a multi-agent framework to dynamically create follow-up questions based on the LLM\u2019s response.", "D": "TreeEval relies on paraphrasing existing queries from static benchmarks and then posing them as follow-up questions."}, "answer": "B", "explanation": "TreeEval distinguishes itself by generating subsequent questions based on both the initial topic and the LLM's previous response, enabling a more nuanced and iterative evaluation.", "question_token_count": 17, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 9, "avg_answer_token_count": 22}
{"context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n", "topic": "Differentiate between static and dynamic benchmarking methods for LLMs, specifically addressing their respective advantages and disadvantages in mitigating data contamination risks.", "question": "How does the dynamic benchmarking approach, compared to static benchmarking, most effectively address the risk of data contamination in LLMs, considering the inherent trade-offs of each method?", "choices": {"A": "Dynamic benchmarking offers a superior advantage by entirely eliminating data contamination risks through real-time data filtering, while static benchmarking remains vulnerable due to its reliance on fixed datasets.", "B": "Dynamic benchmarking primarily mitigates data contamination by introducing adaptive evaluation strategies, but can be computationally expensive and complex to implement, unlike the simpler static approaches.", "C": "Static benchmarking provides a more robust defense against data contamination as it employs rigorous pre-filtering of training data, making it inherently superior to dynamic methods which rely on reactive adjustments.", "D": "Both static and dynamic benchmarking fundamentally rely on curated datasets, and their effectiveness is primarily determined by the quality of the data curation process rather than the benchmarking methodology itself."}, "answer": "B", "explanation": "Dynamic benchmarking addresses data contamination through adaptive evaluation, a key advantage over static methods. However, it introduces complexities and computational costs.", "question_token_count": 34, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 32}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "Discuss the broader significance of data contamination in the context of LLM training and evaluation, and why it poses a challenge for ensuring model generalization.", "question": "Assuming widespread adoption of canary strings in LLM training data, what is the most likely long-term consequence for the reliability of static benchmark evaluations, and why?", "choices": {"A": "Static benchmarks will become completely reliable as models are incentivized to avoid generating canary strings, demonstrating true generalization.", "B": "The reliance on canary strings will shift the focus of model training towards memorizing the specific canary strings used, creating a new form of overfitting.", "C": "Static benchmarks will maintain their reliability as the inherent limitations of the canary string approach are well understood and accounted for in model evaluation.", "D": "The effectiveness of canary strings will gradually diminish as malicious actors develop sophisticated techniques to obfuscate or mimic the appearance of canary strings within model outputs."}, "answer": "D", "explanation": "The limitations section highlights that developers can intentionally leak benchmarking data. This suggests a potential arms race where developers attempt to circumvent canary string detection, leading to increasingly sophisticated obfuscation techniques. This undermines the reliability of static benchmarks.", "question_token_count": 32, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 28}
{"context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n", "topic": "Describe the core principle behind rule-based test case generation and why it is valued in LLM evaluation.", "question": "Why is the minimal collision probability characteristic of rule-based test case generation particularly valuable in the context of Large Language Model (LLM) evaluation?", "choices": {"A": "It ensures that LLMs are evaluated on a diverse range of problems, maximizing the potential for identifying weaknesses.", "B": "It reduces the computational cost of evaluating LLMs by generating simpler, less complex test cases.", "C": "It minimizes the risk of LLMs memorizing or overfitting to specific test cases, leading to more reliable performance assessments.", "D": "It allows for the automated generation of test cases that perfectly mimic real-world scenarios, improving the ecological validity of evaluations."}, "answer": "C", "explanation": "A low collision probability means that the generated test cases are unique and not repetitive. This is valuable because it prevents LLMs from simply memorizing test cases, ensuring the evaluation accurately reflects their generalization ability and identifies genuine weaknesses.", "question_token_count": 29, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 10, "avg_answer_token_count": 22}
{"context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n", "topic": "Define \"collision\" in the context of dynamic benchmarking and explain why it is a significant concern when evaluating Large Language Models (LLMs).", "question": "Why is the concept of \"collision\" a critical consideration in the dynamic benchmarking of Large Language Models (LLMs)?", "choices": {"A": "It indicates the benchmark\u2019s ability to generate diverse test cases without any overlap, ensuring a comprehensive evaluation of LLM capabilities.", "B": "It reflects the degree to which different transformations of a benchmark dataset result in overlapping data, potentially compromising the benchmark's ability to detect data contamination.", "C": "It measures the computational resources required to generate a single transformed dataset, impacting the practicality of dynamic benchmarking.", "D": "It represents the statistical significance of the results obtained from a benchmark, influencing the reliability of LLM performance assessments."}, "answer": "B", "explanation": "Collision, as defined in the context, directly relates to the overlap between different transformations of a benchmark dataset. This overlap poses a risk of data contamination if the benchmark is used to train LLMs, thereby undermining the benchmark's ability to accurately assess LLM capabilities.", "question_token_count": 24, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 5, "question_groundedness_score": 9, "avg_answer_token_count": 24}
{"context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n", "topic": "The formal definition of stability in dynamic benchmarking, as expressed through the variance of complexity measurements across different trials, and its interpretation.", "question": "Assuming \u03a8(\u22c5) represents a validated complexity measurement function, what is the most accurate implication of a high variance in the stability metric derived from multiple trials of a dynamic benchmark?", "choices": {"A": "The benchmark is likely accurately reflecting an increase in the underlying task complexity.", "B": "Data contamination is the most probable cause of the observed performance fluctuations.", "C": "The benchmark's consistency across trials is questionable, potentially misrepresenting LLM performance.", "D": "The complexity measurement function, \u03a8(\u22c5), requires further refinement to accurately capture task difficulty."}, "answer": "C", "explanation": "A high variance in the stability metric directly indicates inconsistency in complexity measurements across trials, casting doubt on the benchmark's reliability and ability to accurately reflect the LLM's true capabilities.", "question_token_count": 36, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 16}
{"context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "Define a dynamic benchmark according to the provided mathematical formulation, clearly explaining the roles of \ud835\udc9f, T(\u22c5), and \ud835\udc9ft.", "question": "In the context of dynamic benchmarking for LLMs, what is the primary function of the transformation function, T(\u22c5), and how does it relate to the static benchmark dataset, \ud835\udc9f, and the dynamic dataset, \ud835\udc9ft?", "choices": {"A": "T(\u22c5) generates the initial static benchmark dataset \ud835\udc9f from scratch when \ud835\udc9f is empty, ensuring a diverse evaluation set.", "B": "T(\u22c5) modifies the static benchmark dataset \ud835\udc9f iteratively to create the dynamic dataset \ud835\udc9ft, mitigating data contamination during evaluation.", "C": "T(\u22c5) validates the accuracy of the static benchmark dataset \ud835\udc9f before it is used to construct the dynamic dataset \ud835\udc9ft.", "D": "T(\u22c5) represents the evaluation metric used to assess the performance of the LLM on the dynamic dataset \ud835\udc9ft."}, "answer": "B", "explanation": "The text explicitly states that T(\u22c5) \"modifies the data set during the benchmarking to avoid possible data contamination.\" This directly relates to its function of transforming the static dataset \ud835\udc9f into the dynamic dataset \ud835\udc9ft.", "question_token_count": 48, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 27}
{"context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n", "topic": "Discuss the implications of relying on LLMs for generating dynamic benchmarks, specifically considering the potential for introducing biases or inaccuracies into the evaluation process.", "question": "Within the context of dynamic benchmarking, what is the most critical challenge introduced when leveraging Large Language Models (LLMs) for generating transformed evaluation data, despite their generative capabilities?", "choices": {"A": "The sheer volume of data generated by LLMs necessitates extensive manual validation, significantly increasing evaluation costs.", "B": "The lack of inherent interpretability and traceability in LLM transformations requires supplementary validation mechanisms to ensure data reliability and mitigate potential biases.", "C": "Rule-based transformation methods are inherently less scalable than LLM-based approaches, limiting the diversity of dynamic benchmarks.", "D": "Temporal cutoff approaches, while minimizing data contamination, are inherently limited by the availability of newly released information."}, "answer": "B", "explanation": "The text explicitly states that LLM-assisted transformations depend on the model\u2019s transparency and traceability, and that additional mechanisms are needed to ensure reliability. This highlights the challenge of interpretability.", "question_token_count": 35, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 6, "avg_answer_token_count": 22}
{"context": "###### Safety\nSafety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content. Datasets such as RealToxicityPrompts\u00a0(Gehman et\u00a0al., 2020) and ToxiGen\u00a0(Hartvigsen et\u00a0al., 2022) assess resilience against producing harmful outputs.\n\nBy providing a controlled environment to measure these aspects, safety benchmarks play a critical role in guiding the development of models that are not only powerful outputs but also responsible and trustworthy for real-world applications.\n\n\n###### Language\nLanguage benchmarks assess the LLMs\u2019 proficiency in specific languages.\n\nGLUE\u00a0(Wang, 2018) and SuperGLUE\u00a0(Wang et\u00a0al., 2019) cover tasks from sentiment analysis to language inference, while CLUE\u00a0(Xu et\u00a0al., 2020) targets Chinese language.\n\nTypo-fixing\u00a0(Suzgun et\u00a0al., 2022) is also widely used.\n\n\n###### Reading Comprehension\nReading comprehension tasks test a model\u2019s ability to extract and infer information from text. Benchmarks like SQuAD\u00a0(Rajpurkar et\u00a0al., 2018), QuAC\u00a0(Choi et\u00a0al., 2018), and BoolQ\u00a0(Clark et\u00a0al., 2019) challenge models to understand passages and draw logical conclusions.\n", "topic": "Analyze the purpose of reading comprehension benchmarks like SQuAD, QuAC, and BoolQ and explain how they challenge LLMs to demonstrate their understanding and inferential abilities.", "question": "Which of the following best encapsulates the primary challenge presented by reading comprehension benchmarks like SQuAD, QuAC, and BoolQ for Large Language Models?", "choices": {"A": "They primarily evaluate an LLM's ability to memorize factual information from a wide range of sources.", "B": "They necessitate models to move beyond statistical pattern recognition and demonstrate a capacity for logical inference and contextual understanding.", "C": "They focus on assessing the speed and efficiency with which an LLM can process and extract keywords from a given text.", "D": "They aim to determine whether an LLM can accurately replicate the writing style and tone of the original text passage."}, "answer": "B", "explanation": "These benchmarks are designed to test if LLMs can truly understand the text, not just recognize patterns. They require logical inference and understanding context, going beyond simple memorization or keyword extraction.", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 21}
{"context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n", "topic": "Outline the four distinct stages involved in the BENCHAGENTS framework for automated benchmark creation, including the role of each specialized LLM agent.", "question": "Within the BENCHAGENTS framework, what is the primary function of the agent responsible for the 'verification' stage?", "choices": {"A": "To initially formulate the benchmark questions based on a given task description.", "B": "To assess the quality and accuracy of the benchmarks generated by the 'generation' agent.", "C": "To dynamically extend existing benchmarks by iteratively refining the evaluation process.", "D": "To conduct multi-round evaluations of the examined LLM, posing follow-up questions and providing feedback."}, "answer": "B", "explanation": "The BENCHAGENTS framework utilizes a multi-agent approach where each stage is handled by a specialized LLM agent. Verification is the assessment of the benchmarks.", "question_token_count": 25, "answer_correctness_score": 10, "explanation_validity_score": 8, "question_clarity_score": 7, "question_groundedness_score": 5, "avg_answer_token_count": 16}
{"context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n", "topic": "Explain how the Collision Rate metric quantifies the potential for data contamination in dynamic benchmarking and what a high Collision Rate indicates.", "question": "A dynamic benchmark exhibits a high Collision Rate. What is the most probable consequence of this observation regarding the benchmark's utility in evaluating Large Language Models (LLMs)?", "choices": {"A": "The benchmark will effectively distinguish between LLMs trained on diverse datasets, accurately reflecting their relative capabilities.", "B": "The benchmark will provide an inflated assessment of LLM performance due to the potential for overlapping training data.", "C": "The benchmark's Repeat Trials metric will decrease, indicating a greater ability to generate novel test cases.", "D": "The benchmark will be less susceptible to adversarial attacks, resulting in a more robust evaluation of LLM security."}, "answer": "B", "explanation": "A high Collision Rate signifies significant overlap between different transformed versions of the benchmark. This overlap implies a high probability that an LLM trained on one version of the benchmark may have already encountered similar data in another version, leading to an inflated performance assessment.", "question_token_count": 33, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 10, "avg_answer_token_count": 20}
{"context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n", "topic": "The rationale behind using encryption methods to secure evaluation data and prevent its inclusion in training sets, as proposed by Jacovi et al. (2023).", "question": "Why is the combination of public key encryption and a \"No Derivatives\" license particularly effective in securing evaluation data, as proposed by Jacovi et al. (2023), within the context of mitigating data contamination risks in machine learning?", "choices": {"A": "It primarily addresses the computational overhead associated with encryption, making it a practical solution for large datasets.", "B": "It effectively prevents automated crawling and reuse of the encrypted data, thereby hindering its incorporation into training sets, while also offering a robust defense against minor text variations.", "C": "It ensures complete data confidentiality by preventing any access to the data even by authorized evaluators, improving evaluation integrity.", "D": "It leverages confidential computing and secure multi-party computation to enable private benchmarking, guaranteeing that test data and model parameters remain confidential."}, "answer": "B", "explanation": "Jacovi et al. (2023) specifically chose this combination to block automated crawling and reuse, which is a common pathway for data contamination. The \"No Derivatives\" license reinforces this by legally restricting derivative works.", "question_token_count": 47, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 25}
{"context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n", "topic": "How the proprietary nature of LLM training data complicates the assessment of model performance and hinders community efforts to ensure fair evaluation.", "question": "How does the prevalent practice of keeping LLM training data proprietary most significantly compromise the development of universally accepted and demonstrably fair performance benchmarks?", "choices": {"A": "It primarily reduces the computational resources available for evaluating model performance, forcing reliance on smaller, less representative datasets.", "B": "It fundamentally limits the ability to verify and mitigate data overlaps between training and evaluation sets, potentially inflating reported performance metrics.", "C": "It encourages a shift towards synthetic benchmarks, which are inherently less reflective of real-world performance and introduce new biases.", "D": "It incentivizes the creation of highly specialized benchmarks tailored to specific models, thereby hindering comparisons across different architectures."}, "answer": "B", "explanation": "The text explicitly states that the proprietary nature of training data \"impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\" This makes accurate assessment and fair benchmarking difficult.", "question_token_count": 28, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 23}
{"context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "Explain how timestamps are utilized in dynamic benchmarking and what the significance of N, the total timestamp number, represents.", "question": "In the context of dynamic benchmarking for LLMs, what does the total timestamp number, 'N', fundamentally signify in relation to the benchmark's evaluation process?", "choices": {"A": "It represents the maximum number of unique LLMs that can be evaluated using the benchmark.", "B": "It indicates the finite duration over which the benchmark data is transformed to prevent data contamination, ensuring a temporally diverse evaluation.", "C": "It defines the total size of the initial static dataset used to generate the dynamic benchmark.", "D": "It signifies the number of distinct transformation functions applied to the initial dataset."}, "answer": "B", "explanation": "'N' represents the total number of timestamps, reflecting the length of the dynamic benchmark's temporal evolution. This allows for continuous adaptation and mitigation of data contamination, ensuring a faithful evaluation.", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 19}
{"context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n", "topic": "How does StructEval leverage LLMs and knowledge graphs to enhance existing benchmarks, and what is the specific aim of expanding on examined concepts?", "question": "Which of the following best describes the primary distinction of StructEval compared to other LLM-based benchmark rewriting techniques described in the provided text?", "choices": {"A": "It focuses on replacing variables within existing benchmark samples to generate new, related questions.", "B": "It employs LLMs and knowledge graphs to expand upon the core concepts initially assessed by the benchmark.", "C": "It utilizes a contamination detector to identify and rewrite potentially compromised samples, preserving their difficulty.", "D": "It generates new samples that retain the original sample's stylistics and essential knowledge while exploring different cognitive levels."}, "answer": "B", "explanation": "StructEval's key innovation is its use of both LLMs and knowledge graphs specifically to *expand* on the concepts examined by the original benchmark, unlike the other methods which focus on rewriting, replacing, or detecting contamination.", "question_token_count": 27, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 19}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "The significance of understanding and mitigating data contamination in LLM benchmarking, particularly given the rapid development of LLMs.", "question": "An LLM exhibits a high score on a benchmark that includes syntactically transformed versions of its training data. Considering the ongoing debate regarding the distinction between memorization and reasoning in LLMs, which of the following best describes the most significant risk associated with interpreting this benchmark result?", "choices": {"A": "The benchmark accurately reflects the LLM\u2019s improved ability to generalize to novel syntactic structures.", "B": "The high score primarily indicates the LLM\u2019s capacity to recall information rather than its ability to reason effectively.", "C": "Syntactic transformations are inherently irrelevant to assessing an LLM\u2019s performance in real-world applications.", "D": "The benchmark is inherently flawed and should be discarded due to its reliance on rephrased data."}, "answer": "B", "explanation": "A high score on a benchmark containing syntactically transformed training data may be misleading if it primarily reflects the LLM\u2019s ability to recognize and recall patterns from the training data, rather than demonstrating genuine reasoning capabilities. This is the central argument of the provided text.", "question_token_count": 55, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 19}
{"context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n", "topic": "The definition and significance of \"Correctness\" as a primary criterion for evaluating the quality of dynamic benchmarks.", "question": "Within the context of dynamic benchmark evaluation, what does the equation represent in assessing the \"Correctness\" of a benchmark?", "choices": {"A": "The computational complexity of the transformation process applied to the dataset.", "B": "The degree of alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values.", "C": "The frequency with which the oracle (G(\u22c5)) successfully identifies errors in the transformed dataset.", "D": "The scoring function's (\ud835\udcae(\u22c5)) sensitivity to minor discrepancies between transformed outputs and ground truth."}, "answer": "B", "explanation": "The equation directly quantifies correctness as the expected alignment between transformed data outputs and ground truth, utilizing an oracle and a scoring function.", "question_token_count": 25, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 1, "question_groundedness_score": 10, "avg_answer_token_count": 18}
{"context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n", "topic": "The core vulnerability of static benchmarking methods in LLM evaluation as training datasets increase in size.", "question": "Why are static benchmarking methods increasingly unsuitable for evaluating Large Language Models as training datasets expand?", "choices": {"A": "The increased computational cost of static benchmarks makes them impractical with larger datasets.", "B": "Static benchmarks rely on fixed datasets, increasing the likelihood that evaluation data has been incorporated into the model's training.", "C": "Dynamic benchmarks are inherently superior and render static methods obsolete as datasets grow.", "D": "Larger training datasets reduce the impact of contamination on model performance, mitigating the vulnerability of static benchmarks."}, "answer": "B", "explanation": "As training datasets grow, the probability of overlap between the training and evaluation data increases, making static benchmarks, which use fixed datasets, more susceptible to inflated performance due to contamination.", "question_token_count": 18, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 18}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "Analyze the limitations of static benchmarks in the context of rapidly evolving LLMs and discuss the potential risks associated with data contamination.", "question": "How does the inherent adaptability of large language models fundamentally undermine the reliability of conventional, static benchmarks, and what specific vulnerabilities arise from this dynamic interplay?", "choices": {"A": "Static benchmarks become increasingly irrelevant as models improve, but the primary risk is reduced interpretability of model performance.", "B": "The constant retraining of LLMs on readily available data inevitably leads to benchmark overfitting, diminishing their ability to accurately assess true generalization capabilities.", "C": "The rapid evolution of LLMs primarily impacts the cost of benchmark creation, rather than their fundamental validity as performance indicators.", "D": "Dynamic benchmarks are sufficient to completely eliminate the risk of data contamination, rendering static benchmarks obsolete."}, "answer": "B", "explanation": "The text states that LLMs continue training on all available data, leading to contamination risks. This means that benchmarks become less effective as models improve, and the primary issue is the inability to accurately assess generalization capabilities.", "question_token_count": 30, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 6, "avg_answer_token_count": 23}
{"context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n", "topic": "Explain the role of the interviewer LLM in LLM-as-an-Interviewer and how it utilizes existing static benchmarks.", "question": "How does the interviewer LLM in LLM-as-an-Interviewer contribute to a more comprehensive evaluation than simply utilizing the original static benchmarks?", "choices": {"A": "By directly repeating the questions from the benchmarks to ensure consistency.", "B": "By generating follow-up questions and providing feedback based on the examined LLM\u2019s responses to the initial benchmark queries.", "C": "By exclusively focusing on the most challenging questions within the static benchmarks.", "D": "By replacing the static benchmarks entirely with newly generated questions."}, "answer": "B", "explanation": "The interviewer LLM\u2019s value lies in its ability to go beyond the static benchmarks by engaging in a multi-turn dialogue, probing deeper into the LLM\u2019s understanding and reasoning.", "question_token_count": 28, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 15}
{"context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n", "topic": "Describe the approach taken by Auto-Dataset to generate new samples, differentiating between the types of samples created and the underlying principles guiding their generation.", "question": "Which of the following best describes the core distinction in Auto-Dataset's sample generation strategy compared to other LLM-based benchmark augmentation techniques?", "choices": {"A": "Auto-Dataset focuses on identifying and removing contaminated samples from existing benchmarks, preserving their original difficulty.", "B": "Auto-Dataset generates samples by replacing variables within existing benchmark examples to create new, related instances.", "C": "Auto-Dataset creates two distinct types of samples: one mirroring the original style and knowledge, and another exploring related questions at varying cognitive complexities.", "D": "Auto-Dataset expands upon existing benchmark concepts using LLMs and knowledge graphs to formulate a series of interconnected questions."}, "answer": "C", "explanation": "Auto-Dataset's unique approach lies in generating two types of samples \u2013 one preserving the original characteristics and another exploring related questions at different cognitive levels. This distinguishes it from methods like contamination removal, variable replacement, or concept expansion.", "question_token_count": 27, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 10, "avg_answer_token_count": 23}
{"context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "Analyze the implications of creating a dynamic benchmarking dataset from scratch when the initial seed dataset \ud835\udc9f is empty.", "question": "Considering a dynamic benchmarking framework for LLMs, what is the most significant challenge introduced when the initial seed dataset (\ud835\udc9f) is empty, necessitating the construction of the entire dynamic benchmark from scratch?", "choices": {"A": "The absence of any baseline data makes it impossible to evaluate the LLM's performance against a known standard, rendering the benchmark meaningless.", "B": "The transformation function (T) must be carefully designed to ensure it generates diverse and representative data without introducing unintended biases or patterns.", "C": "The infinite timestamp number (N) becomes problematic as there is no initial data to transform, leading to a potentially unbounded and computationally infeasible process.", "D": "Establishing a reliable evaluation metric becomes difficult because the transformation function\u2019s impact on the generated data is unknown, making it challenging to interpret the LLM's responses."}, "answer": "A", "explanation": "Starting with an empty seed dataset fundamentally breaks the benchmarking process because there's no foundation for comparison or evaluation. The LLM's performance cannot be meaningfully assessed without any initial data to serve as a reference point.", "question_token_count": 41, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 28}
{"context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n", "topic": "The privacy and security risks associated with the continuous data collection and updating processes inherent in dynamic LLM benchmarks.", "question": "What is the most significant ethical challenge introduced by the adaptive nature of dynamic LLM benchmarks, according to the provided text?", "choices": {"A": "The risk of perpetuating biases through reliance on outdated data sources.", "B": "The potential for misuse of benchmark results to inflate model performance.", "C": "The continuous collection and updating of data leading to privacy and security vulnerabilities.", "D": "The lack of transparency in evaluation criteria used to assess LLM performance."}, "answer": "C", "explanation": "The text explicitly states that dynamic benchmarks \"raise privacy and security concerns regarding the continual collection and updating of data.\" This is the core ethical challenge directly linked to their adaptive nature.", "question_token_count": 24, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 14}
{"context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n", "topic": "The potential for static LLM benchmarks to perpetuate biases due to reliance on outdated or biased data sources.", "question": "How does the reliance on specific data sources in static LLM benchmarks most critically threaten the development of equitable AI systems?", "choices": {"A": "By exclusively using data from high-resource languages, limiting the model's ability to generalize to diverse linguistic contexts.", "B": "By inadvertently embedding biases present in historical datasets, leading to skewed model outputs that disadvantage marginalized groups.", "C": "By prioritizing data volume over data quality, resulting in models that are susceptible to noise and irrelevant information.", "D": "By failing to account for evolving societal norms, causing models to perpetuate outdated and potentially harmful stereotypes."}, "answer": "B", "explanation": "The text specifically mentions that static benchmarks \"can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources.\" Option B directly addresses this point by highlighting the embedding of biases from historical datasets, which is a subtle but crucial implication of relying on specific data sources.", "question_token_count": 23, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 20}
{"context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n", "topic": "Describe the hybrid generation techniques, such as LatestEval, DARG, and C2LEVA, and explain how they attempt to address the challenges of data contamination in LLM benchmarking.", "question": "Which of the following best describes the fundamental distinction in approach between LatestEval, DARG, and C2LEVA regarding the mitigation of data contamination in LLM benchmarking?", "choices": {"A": "LatestEval relies solely on temporal cutoff, while DARG utilizes graph-based perturbations and C2LEVA integrates all three contamination-free construction methods.", "B": "DARG focuses on extracting and perturbing reasoning graphs, LatestEval combines temporal cutoff with LLM generation, and C2LEVA uses a single, unified approach to contamination-free construction.", "C": "LatestEval prioritizes data privacy through label protection, DARG employs post-hoc detection of contaminated models, and C2LEVA balances correctness with scalability.", "D": "All three methods primarily focus on increasing the transparency of the evaluation process by revealing label information and model assumptions."}, "answer": "B", "explanation": "The correct answer accurately reflects the distinct strategies employed by each technique. LatestEval uses temporal cutoff and LLM generation, DARG uses graph-based perturbations, and C2LEVA combines all three approaches.", "question_token_count": 34, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 29}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "Describe the specific strategy employed by AntiLeak-Bench to generate queries related to newly emerged knowledge.", "question": "Which technique does AntiLeak-Bench specifically utilize to construct evaluation queries and minimize data contamination concerns in large language models?", "choices": {"A": "It dynamically collects questions from online coding competition platforms.", "B": "It generates queries centered on information that emerged after the model\u2019s knowledge cutoff date.", "C": "It continuously updates its question pool with problems sourced from recent math competitions.", "D": "It designs academic writing tasks based on the most recent arXiv papers."}, "answer": "B", "explanation": "AntiLeak-Bench distinguishes itself by specifically generating queries related to knowledge that was not available prior to the model's training cutoff, thereby preventing contamination.", "question_token_count": 25, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 1, "question_groundedness_score": 1, "avg_answer_token_count": 14}
{"context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n", "topic": "Explain how analyzing model behavior under conditions like masked inputs, partial completions, or preference for original test cases can contribute to post-hoc data contamination detection.", "question": "Why is analyzing a language model\u2019s behavior when presented with masked inputs, partial completions, or a preference for original test cases valuable for post-hoc data contamination detection?", "choices": {"A": "These conditions force the model to rely on its general knowledge rather than memorized training data, highlighting inconsistencies arising from contamination.", "B": "These conditions increase computational efficiency, allowing for faster identification of overlapping data between training and test sets.", "C": "These conditions directly reveal the n-gram overlap between training and test data, providing a definitive measure of contamination.", "D": "These conditions simplify model interpretability by isolating specific memorized segments within the model's parameters."}, "answer": "A", "explanation": "Analyzing model behavior under these conditions reveals inconsistencies when the model relies on memorized training data, a hallmark of contamination. The other options are incorrect because they describe unrelated benefits or processes.", "question_token_count": 34, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 21}
{"context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n", "topic": "Practical mitigation tools that future research should prioritize in addressing data contamination within LLM benchmarks.", "question": "Considering the identified limitations of both static and dynamic LLM benchmarking approaches, which category of mitigation tools would represent the most strategically valuable area of future research investment?", "choices": {"A": "Developing adversarial filtering techniques to remove contaminated data from training sets.", "B": "Enhancing the robustness of dynamic benchmarks through improved randomization and controlled leakage.", "C": "Focusing on methods to detect and quantify the presence of contaminated data within benchmark datasets.", "D": "Creating synthetic benchmark datasets that are entirely disjoint from any publicly available training corpora."}, "answer": "B", "explanation": "The paper highlights the increasing vulnerability of static methods and the challenges with dynamic methods. While all options have merit, the most strategically valuable area is enhancing dynamic benchmarks, as this directly addresses the core issue of unreliability and reproducibility while remaining adaptable to evolving training data.", "question_token_count": 31, "answer_correctness_score": 9, "explanation_validity_score": 8, "question_clarity_score": 6, "question_groundedness_score": 8, "avg_answer_token_count": 15}
{"context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n", "topic": "Evaluate the tradeoffs between the different dynamic benchmark construction processes (temporal cutoff, rule-based, LLM-based, and hybrid) in terms of data generation cost, interpretability, and potential for bias.", "question": "Which of the following best describes the primary factor that necessitates additional validation mechanisms for LLM-based dynamic benchmark generation, despite the potential for creating novel evaluation data points?", "choices": {"A": "The reliance on newly released information inherent in temporal cutoff approaches.", "B": "The computational expense associated with rule-based data generation.", "C": "The lack of inherent transparency and traceability within LLMs used for transformation.", "D": "The tendency for hybrid approaches to combine biases from multiple sources."}, "answer": "C", "explanation": "LLM-based generation's strength lies in creating novel data, but it introduces a challenge: LLMs are often \"black boxes,\" making it difficult to understand *how* they transform data. This lack of transparency requires additional validation measures to ensure correctness.", "question_token_count": 33, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 13}
{"context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n", "topic": "Summarize how KIEval generates follow-up questions and its reliance on the evaluated model\u2019s responses to an initial question.", "question": "How does KIEval determine the subsequent inquiries within its evaluation process?", "choices": {"A": "By randomly generating follow-up questions based on a predefined set of topics.", "B": "By leveraging the evaluated model's response to an initial question from a static benchmark.", "C": "By soliciting human input to guide the generation of follow-up questions.", "D": "By analyzing external knowledge sources to formulate questions related to the initial topic."}, "answer": "B", "explanation": "KIEval's method is centered on using the model\u2019s response to the initial question to guide the generation of subsequent questions, ensuring the evaluation is contextually driven.", "question_token_count": 15, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 10, "avg_answer_token_count": 15}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "How the difficulty of reasoning tasks within these evaluation frameworks (S3Eval, DyVal, NPHardEval, Xie et al.) is controlled through modifications to graph structure.", "question": "Which of the following best describes the primary method by which DyVal and NPHardEval control the difficulty of reasoning tasks for LLMs?", "choices": {"A": "By varying the complexity of the SQL queries used to interact with randomly generated tables.", "B": "By adjusting the number of nodes and edges within randomly generated directed acyclic graphs or by altering the size of graphs used as inputs for NP problems.", "C": "By employing rule-based conversions to transform randomly generated graphs into natural language descriptions.", "D": "By constructing puzzles with random reasoning graphs, automatically adjusting the puzzle's logic and constraints."}, "answer": "B", "explanation": "DyVal and NPHardEval specifically control difficulty through graph structure modification. DyVal uses nodes and edges in DAGs, while NPHardEval uses graph size for NP problems like TSP.", "question_token_count": 29, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 20}
{"context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n", "topic": "Define scalability in the context of dynamic benchmarking, explaining its significance and the relationship between dataset size and statistical errors.", "question": "In the context of dynamic benchmarking, how does the provided scalability metric balance the pursuit of larger, more statistically robust datasets against the practical constraints of transformation costs?", "choices": {"A": "It prioritizes minimizing transformation costs, even at the expense of dataset size, to ensure rapid benchmark generation.", "B": "It aims to maximize the ratio of transformed dataset size to the cost of transformation, representing the efficiency of data generation.", "C": "It solely focuses on maximizing the size of the transformed dataset, regardless of the associated costs, to eliminate statistical errors.", "D": "It utilizes a fixed transformation cost to ensure all dynamic benchmarks have equivalent computational resources."}, "answer": "B", "explanation": "The equation provided explicitly defines scalability as the proportion of data generated per unit cost, directly reflecting a balance between dataset size and transformation costs.", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 21}
{"context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n", "topic": "How TRUCE leverages confidential computing and secure multi-party computation to enable private benchmarking and maintain the confidentiality of test data and model parameters.", "question": "In the context of private benchmarking, what is the primary advantage of TRUCE's utilization of confidential computing and secure multi-party computation over traditional encryption methods for protecting test data and model parameters?", "choices": {"A": "TRUCE eliminates the need for robust key management practices, simplifying deployment and reducing operational overhead.", "B": "TRUCE inherently prevents minor text variations from defeating decontamination methods, providing a higher level of security.", "C": "TRUCE enables benchmarking without exposing the underlying data or model parameters, even to the benchmarking service provider.", "D": "TRUCE significantly reduces computational overhead compared to encryption, allowing for faster and more efficient benchmarking."}, "answer": "C", "explanation": "TRUCE's core benefit lies in its ability to protect data even from the entity performing the benchmarking, a level of privacy not typically afforded by encryption alone. Confidential computing and secure multi-party computation allow computations on encrypted data without revealing the data itself.", "question_token_count": 38, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 9, "avg_answer_token_count": 20}
{"context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n", "topic": "Discuss the practical significance of monitoring both external and internal diversity when performing data transformations, and how these metrics could inform the transformation process.", "question": "Considering the provided definitions of external and internal diversity in data transformations, which of the following best describes how monitoring these metrics could most effectively guide the transformation process?", "choices": {"A": "Maximizing both external and internal diversity ensures the transformed dataset is entirely independent of the original seed data and exhibits minimal trial-to-trial variation, guaranteeing optimal generalization.", "B": "Prioritizing high external diversity while maintaining moderate internal diversity indicates a transformation process that effectively captures novel patterns while ensuring relative stability and reproducibility across trials.", "C": "Minimizing both external and internal diversity is crucial for preserving the core characteristics of the original seed data and ensuring consistent results across multiple transformation attempts, prioritizing fidelity over innovation.", "D": "Focusing solely on external diversity is sufficient to assess the quality of a transformation, as internal diversity is merely a byproduct of the transformation process and does not directly impact the utility of the transformed dataset."}, "answer": "B", "explanation": "Monitoring both external and internal diversity allows for a nuanced understanding of the transformation's impact. High external diversity suggests novelty, while controlled internal diversity indicates stability and reproducibility.", "question_token_count": 32, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 34}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The implications of current dynamic benchmarks not fully satisfying the proposed criteria, and what this suggests for future design and standardization.", "question": "The paper concludes that current dynamic benchmarks for LLMs \"do not fully satisfy these proposed criteria, implying the imperfection of current design.\" What is the most likely consequence of this imperfection for the advancement of LLM development and research?", "choices": {"A": "It will accelerate the adoption of static benchmarks due to their perceived reliability, despite the data contamination issues.", "B": "It risks hindering the accurate assessment of LLM capabilities, potentially leading to misguided development efforts and an overestimation of model performance.", "C": "It necessitates a complete abandonment of dynamic benchmarking in favor of alternative evaluation methodologies.", "D": "It will primarily impact commercial LLM deployment, with limited consequences for academic research and model development."}, "answer": "B", "explanation": "The paper explicitly states that current dynamic benchmarks are imperfect. This imperfection most directly impacts the ability to accurately assess LLMs, potentially leading to flawed development and inflated performance metrics.", "question_token_count": 47, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 21}
{"context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n", "topic": "Explain the concept of in-distribution contamination in the context of LLM training and its potential impact on model performance.", "question": "How does the utilization of Large Language Models (LLMs) for benchmark rewriting introduce the risk of in-distribution contamination during LLM training, and what is the primary consequence of this phenomenon?", "choices": {"A": "It primarily increases computational costs associated with training, as the LLMs require significant resources to generate new samples.", "B": "It can lead to an overestimation of model performance on evaluation datasets due to the LLMs inadvertently incorporating elements of those datasets into the training data.", "C": "It exclusively affects the stylistic diversity of the generated samples, limiting the model\u2019s ability to generalize to different writing styles.", "D": "It guarantees a more robust and generalizable model, as the LLMs effectively augment the training data with diverse variations of existing examples."}, "answer": "B", "explanation": "In-distribution contamination arises when LLMs used to rewrite benchmarks introduce elements from the original benchmarks into the training data. This leads to inflated performance metrics because the model has effectively \"seen\" aspects of the evaluation data during training.", "question_token_count": 37, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 26}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "How do MMLU, BBH, and AGI Eval benchmarks assess an LLM's knowledge across multiple domains?", "question": "Which characteristic distinguishes MMLU, BBH, and AGI Eval from other knowledge benchmarks mentioned in the text?", "choices": {"A": "They primarily focus on retrieving real-world information using datasets like NaturalQuestions and TriviaQA.", "B": "They specifically evaluate an LLM\u2019s ability to solve multi-step math problems.", "C": "They cover multiple domains, assessing internal knowledge across a diverse range of subjects.", "D": "They target technical and long-context challenges, utilizing open-domain evaluations."}, "answer": "C", "explanation": "MMLU, BBH, and AGI Eval are explicitly stated to cover \"multi-domain tasks\" and assess \"internal knowledge,\" setting them apart from benchmarks focused on math problems, information retrieval, or technical challenges.", "question_token_count": 23, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 16}
{"context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n", "topic": "The primary challenges hindering the reliability and reproducibility of dynamic benchmarking approaches for LLMs.", "question": "Given the documented challenges with dynamic benchmarking approaches for LLMs, which of the following is the *most* likely foundational impediment to achieving consistent reliability and reproducibility?", "choices": {"A": "The inherent computational expense associated with real-time data sampling and evaluation.", "B": "The absence of universally accepted, standardized protocols for dynamic benchmark design and execution.", "C": "The difficulty in isolating the impact of specific LLM architectural innovations on benchmark performance.", "D": "The tendency for dynamic benchmarks to inadvertently favor models trained on highly curated datasets."}, "answer": "B", "explanation": "The text states that dynamic approaches \"face challenges in reliability and reproducibility\" and that \"standardized dynamic evaluation\" is needed. This implies a lack of standardization is the root cause. While other options might be factors, the lack of standardized protocols is the most directly stated and fundamental challenge.", "question_token_count": 33, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 8, "avg_answer_token_count": 15}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "Explain how NaturalQuestions and TriviaQA contribute to the evaluation of an LLM's knowledge capabilities.", "question": "Which characteristic best distinguishes the evaluation focus of NaturalQuestions and TriviaQA compared to benchmarks like MMLU and BBH?", "choices": {"A": "They primarily assess reasoning capabilities across diverse domains.", "B": "They emphasize retrieval of factual information from the real world.", "C": "They evaluate a model's ability to perform complex mathematical calculations.", "D": "They focus on understanding nuanced language and contextual relationships."}, "answer": "B", "explanation": "NaturalQuestions and TriviaQA are specifically designed to test an LLM's ability to retrieve real-world information, whereas MMLU and BBH evaluate broader multi-domain knowledge.", "question_token_count": 26, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 11}
{"context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n", "topic": "How a higher correctness score, as defined by the formula, indicates the maintenance of correctness in a dynamic benchmark.", "question": "According to the provided evaluation criteria, what does a higher correctness score fundamentally signify regarding a dynamic benchmark's utility in assessing LLMs?", "choices": {"A": "Increased computational efficiency in generating transformed datasets.", "B": "A stronger correlation between the benchmark's results and the LLM's inherent capabilities.", "C": "Preservation of the fidelity of ground truth relationships throughout iterative transformations.", "D": "Reduced reliance on oracle functions for evaluating the benchmark's accuracy."}, "answer": "C", "explanation": "The text explicitly states, \"A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\" This implies that the benchmark's transformations are preserving the underlying relationships represented by the ground truth.", "question_token_count": 27, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 13}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "The crucial role of addressing data contamination to ensure benchmarks accurately measure an LLM's ability to handle novel and unseen data.", "question": "Which of the following best describes the primary concern regarding the inclusion of syntactically transformed data in LLM benchmarks, as discussed in the provided text?", "choices": {"A": "It definitively demonstrates an LLM's superior reasoning capabilities compared to models trained on unaltered data.", "B": "It introduces ambiguity in differentiating between an LLM's recall of memorized information and its genuine reasoning ability.", "C": "It simplifies the benchmarking process by providing a readily available source of test data.", "D": "It is universally accepted as a reliable measure of an LLM's ability to generalize to completely novel data."}, "answer": "B", "explanation": "The text explicitly states the debate around syntactic transformations and the difficulty in distinguishing between memorization and reasoning. Option B accurately reflects this concern. The other options misrepresent the core issue.", "question_token_count": 30, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 19}
{"context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n", "topic": "Outline the optimal design principles for dynamic benchmarking proposed in the text and discuss their potential implications for improving benchmark quality.", "question": "What is the most significant challenge identified by the authors regarding current dynamic benchmarking methods for LLMs, and how does addressing it most directly contribute to the reliability of benchmark results?", "choices": {"A": "The primary challenge is the lack of explicit data contamination in the training data.", "B": "The lack of standardized criteria for evaluating dynamic benchmarks.", "C": "The inherent limitations of enhancing static benchmarks.", "D": "The difficulty in collecting both static and dynamic benchmarking methods."}, "answer": "B", "explanation": "The abstract explicitly states a \"critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks.\" This gap directly impacts the reliability of benchmark results because without clear evaluation standards, it's difficult to determine if a dynamic benchmark is effectively mitigating data contamination and providing an accurate assessment of LLM performance.", "question_token_count": 35, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 8, "avg_answer_token_count": 12}
{"context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n", "topic": "Explain how MMLU-CF generates novel multiple-choice samples, specifically outlining the processes of shuffling answer choices and replacing incorrect options.", "question": "What is the primary mechanism by which MMLU-CF generates novel multiple-choice question samples?", "choices": {"A": "It utilizes a complex algorithm to identify and replace semantically similar incorrect options with distractors derived from external knowledge sources.", "B": "It randomly shuffles the order of answer choices and substitutes incorrect options with alternatives generated by a separate LLM.", "C": "It randomly shuffles answer choices and replaces incorrect options with the phrase \"None of the other choices.\"", "D": "It generates entirely new question stems and answer choices based on a predefined template, ensuring minimal overlap with existing datasets."}, "answer": "C", "explanation": "MMLU-CF's novel sample generation relies on shuffling answer choices and replacing incorrect options with a specific placeholder.", "question_token_count": 20, "answer_correctness_score": 10, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 22}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The fundamental problem of data contamination in LLMs and its impact on the reliability of traditional evaluation methods.", "question": "Dynamic benchmarking methods aim to mitigate data contamination in LLM evaluation by continuously updating or regenerating datasets. However, these methods inherently rely on assumptions about the temporal relationship between training and evaluation data. Which of the following best characterizes the fundamental limitation of these dynamic approaches in fully resolving the data contamination problem?", "choices": {"A": "Dynamic methods guarantee that no data overlap exists between training and evaluation sets, rendering contamination impossible.", "B": "The rapid evolution of LLM training data makes it practically impossible to ensure complete temporal separation, and contamination can still arise from models trained on data generated shortly before the benchmark is updated.", "C": "Dynamic methods are computationally expensive, limiting their scalability to large LLMs and complex tasks.", "D": "The effectiveness of dynamic methods is solely dependent on the quality of the data regeneration process, and flawed regeneration can introduce new biases."}, "answer": "B", "explanation": "The core challenge lies in the continuous nature of LLM training. Even with updates, there\u2019s a window of opportunity for models to be trained on data very close to the benchmark release, making complete separation difficult.", "question_token_count": 59, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 24}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "Detail how LiveBench constructs its benchmark by utilizing recent data sources and its update frequency.", "question": "What is the defining characteristic of LiveBench\u2019s methodology for benchmark construction?", "choices": {"A": "It utilizes exclusively academic papers published on arXiv to ensure relevance to current research.", "B": "It leverages a combination of diverse data sources, including math competitions from the preceding year, with updates occurring every few months.", "C": "It focuses on generating queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date.", "D": "It continuously collects coding problems from online platforms, updating its question pool on a daily basis."}, "answer": "B", "explanation": "LiveBench distinguishes itself by collecting questions based on the latest information sources, exemplified by math competitions from the past 12 months, and updating them every few months.", "question_token_count": 15, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 20}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "Describe the types of mathematical problems evaluated by GSM8K, MATH, AIME 2024, and CNMO 2024 benchmarks.", "question": "Given the benchmarks GSM8K, MATH, AIME 2024, and CNMO 2024 primarily assess a model\u2019s capacity to solve \"complex,\" \"diverse,\" and \"intricate\" mathematical problems, which characteristic most likely distinguishes them from simpler math evaluation benchmarks?", "choices": {"A": "They focus exclusively on problems requiring symbolic manipulation and algebraic simplification.", "B": "They emphasize multi-step reasoning and the integration of multiple mathematical concepts.", "C": "They prioritize problems solvable using only elementary arithmetic operations and basic geometry.", "D": "They are designed to evaluate the model's ability to memorize and reproduce known mathematical formulas."}, "answer": "B", "explanation": "The text explicitly states that these benchmarks test models on complex, diverse, and intricate tasks. This implies a need for multi-step reasoning and the integration of various concepts, distinguishing them from benchmarks focused on simpler arithmetic or formula memorization.", "question_token_count": 58, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 14}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The limitations of static benchmarking methods in mitigating data contamination, specifically concerning the challenges of tracing training data for LLMs.", "question": "Given the inherent challenges of tracing training data for Large Language Models (LLMs), what is the most significant impediment to effectively utilizing static benchmarking methods for robust performance evaluation, despite efforts to mitigate contamination?", "choices": {"A": "The computational cost of continuously updating benchmark datasets to minimize overlap with training data.", "B": "The lack of standardized metrics for evaluating the effectiveness of data encryption techniques applied to benchmark datasets.", "C": "The fundamental conflict between the need for publicly accessible benchmarks and the likelihood of LLMs being trained on that data, rendering performance assessments potentially misleading.", "D": "The limited adoption of dynamic benchmarking methods due to their complexity and the difficulty in regenerating original benchmarks."}, "answer": "C", "explanation": "The core issue is the inherent conflict between open benchmarking and LLM training practices. LLMs are trained on vast internet data, increasing the probability of benchmark data being present in the training set, which compromises evaluation validity.", "question_token_count": 41, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 21}
{"context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n", "topic": "Differentiate between rule-based and LLM-based generation approaches for creating novel evaluation data points in dynamic benchmarks, outlining the advantages and disadvantages of each.", "question": "In the context of dynamic LLM benchmarks, how do the inherent properties of rule-based and LLM-based data generation methods influence the validation strategies required to ensure the reliability of the benchmark?", "choices": {"A": "Rule-based generation necessitates extensive manual validation due to its limited data diversity, while LLM-based generation benefits from inherent interpretability.", "B": "LLM-based generation provides greater data diversity but demands supplementary validation mechanisms due to its reduced interpretability, whereas rule-based generation inherently supports interpretability.", "C": "Both rule-based and LLM-based generation methods require minimal validation efforts due to the dynamic nature of the benchmarks.", "D": "Rule-based generation is favored for its scalability, while LLM-based generation is preferred for its interpretability in dynamic benchmarks."}, "answer": "B", "explanation": "The text explicitly states that rule-based methods are inherently interpretable but may lack diversity, requiring more manual validation. LLM-based methods offer diversity but necessitate additional validation due to reduced interpretability.", "question_token_count": 36, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 25}
{"context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n", "topic": "How graph complexity, as exemplified by DyVal, can be applied to evaluate the complexity of reasoning problems in LLM benchmarks.", "question": "How does the application of graph complexity, as utilized by DyVal, contribute to the evaluation of LLM reasoning benchmarks, particularly concerning the differentiation between performance degradation caused by data contamination versus increased task difficulty?", "choices": {"A": "By providing a domain-specific metric that directly correlates with LLM architecture, allowing for targeted optimization strategies.", "B": "By offering a generalizable measure of reasoning complexity, enabling consistent assessment across diverse LLM benchmarks and differentiating between data contamination and task difficulty.", "C": "By solely focusing on the variance of complexity measurements across trials, ensuring the stability of the dynamic benchmarking method.", "D": "By simplifying the reasoning problem into a linear equation, thereby isolating the impact of data contamination on LLM performance."}, "answer": "B", "explanation": "DyVal\u2019s use of graph complexity aims to provide a generalizable measure of reasoning difficulty, which is crucial for distinguishing between performance drops due to data contamination and those arising from genuinely more complex reasoning tasks.", "question_token_count": 40, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 23}
{"context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n", "topic": "The specific types of datasets used in LLM pre-training and fine-tuning that contribute most significantly to the risk of evaluation data overlap.", "question": "Considering the scale and opacity of LLM training data, which two categories of datasets pose the greatest and most difficult-to-mitigate risk of evaluation data overlap?", "choices": {"A": "Human-annotated datasets used for fine-tuning and retrieval-based detection corpora.", "B": "Web-scraped datasets used for pre-training and proprietary datasets.", "C": "Synthetic datasets used for fine-tuning and retrieval-based detection corpora.", "D": "Web-scraped datasets used for pre-training and human-annotated datasets used for fine-tuning."}, "answer": "D", "explanation": "The text explicitly states that web-scraped datasets (pre-training) and human-annotated/synthetic datasets (fine-tuning) contribute significantly to the risk, due to their scale and potential resemblance to evaluation tasks. The sheer scale and opacity make mitigation difficult.", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 16}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "How the lack of a systematic survey of dynamic benchmarking methods has created a gap in the field and the implications of this gap.", "question": "What is the most significant consequence of the absence of a systematic survey of dynamic benchmarking methods for Large Language Models (LLMs), as highlighted in the provided text?", "choices": {"A": "It primarily limits the detection of data contamination within LLMs during training.", "B": "It hinders the standardization and rigorous evaluation of dynamic benchmarks themselves, impeding the overall progress in LLM development.", "C": "It mainly affects the efficiency of post-hoc contamination detection strategies.", "D": "It restricts the availability of human-crafted datasets for evaluating LLM performance."}, "answer": "B", "explanation": "The passage explicitly states that the lack of a systematic survey prevents the evaluation of dynamic benchmarks and implies that this hinders progress in the field. This goes beyond simply detecting contamination or affecting dataset availability.", "question_token_count": 33, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 10, "avg_answer_token_count": 16}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "Describe the differences in the types of coding challenges assessed by HumanEval, MBPP, and SWE-Bench, and explain the significance of using platforms like Codeforces and datasets like Aider.", "question": "Which of the following best describes the primary distinction between SWE-Bench and the combined assessment of HumanEval and MBPP?", "choices": {"A": "SWE-Bench primarily focuses on debugging existing code, whereas HumanEval and MBPP assess the generation of code from natural language descriptions.", "B": "SWE-Bench is designed to evaluate more advanced coding challenges compared to the relatively simpler synthesis and debugging tasks presented by HumanEval and MBPP.", "C": "SWE-Bench utilizes real-time competitive coding environments like Codeforces, unlike HumanEval and MBPP which rely on static datasets.", "D": "SWE-Bench assesses a broader range of programming languages than HumanEval and MBPP, which are primarily focused on Python."}, "answer": "B", "explanation": "SWE-Bench addresses more advanced challenges compared to the code synthesis and debugging focus of HumanEval and MBPP.", "question_token_count": 25, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 27}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "Describe the limitations of using canary strings to mitigate data contamination, specifically addressing how a malicious developer could circumvent this method.", "question": "How does the intentional leakage of benchmark datasets by a model developer fundamentally undermine the utility of canary string detection for data contamination?", "choices": {"A": "Canary strings are ineffective against deliberate data poisoning attacks, as the malicious data is designed to appear as legitimate training data.", "B": "Canary string detection relies on the assumption of random data exposure, which is violated when a developer intentionally incorporates benchmark data into training.", "C": "Canary strings only detect accidental contamination and are easily bypassed by developers who understand their function and can modify the data accordingly.", "D": "The presence of canary strings alerts malicious developers to the use of benchmark datasets, allowing them to avoid incorporating those specific strings into leaked data."}, "answer": "B", "explanation": "The core issue is that canary strings rely on *unintentional* memorization of benchmark data. A malicious developer, aware of the canary strings, can deliberately incorporate the benchmark data into training without using the specific canary strings, thus circumventing the detection mechanism.", "question_token_count": 26, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 26}
{"context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n", "topic": "The scope of the survey's coverage regarding technical details of specific benchmarking methods and its implications for practitioners.", "question": "Given the stated limitations of the survey, which of the following best characterizes the primary impediment to its direct utility for practitioners aiming to implement LLM benchmarking methodologies?", "choices": {"A": "The lack of standardized dynamic evaluation protocols.", "B": "The omission of specific implementation guidelines for individual benchmarking methods.", "C": "The vulnerability of static methods to data contamination as training datasets grow.", "D": "The need for further refinement and validation of dynamic benchmarking criteria."}, "answer": "B", "explanation": "The text explicitly states that the survey \"focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods,\" directly impacting practitioners seeking implementation guidance.", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 12}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The distinction between static and dynamic benchmarking approaches for LLMs, including the motivations for transitioning to dynamic methods.", "question": "Which of the following best encapsulates the primary driver for the shift from static to dynamic benchmarking approaches in the evaluation of Large Language Models (LLMs)?", "choices": {"A": "Static benchmarks provide a more consistent and reproducible evaluation environment, reducing variability in LLM performance.", "B": "Data contamination in LLMs, stemming from their training on vast internet datasets, renders static benchmarks susceptible to inflated performance assessments.", "C": "Dynamic benchmarks offer a simpler and more cost-effective means of evaluating LLMs compared to the complex datasets required for static benchmarks.", "D": "Static benchmarks are inherently limited in their ability to assess the creative and generative capabilities of LLMs, a gap dynamic benchmarks effectively address."}, "answer": "B", "explanation": "The primary driver for the shift is the risk of data contamination. LLMs are trained on massive datasets, increasing the likelihood that benchmark data is included in the training set, leading to inaccurate performance evaluations.", "question_token_count": 31, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 23}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "A critical analysis of the strengths and weaknesses of existing dynamic benchmarking methods, considering their alignment with the proposed evaluation criteria.", "question": "According to the paper, what is the primary shortcoming of current dynamic benchmarking methods regarding their alignment with the proposed evaluation criteria?", "choices": {"A": "They consistently outperform static benchmarks in detecting data contamination.", "B": "They fully satisfy the proposed evaluation criteria, indicating a robust design.", "C": "They do not fully satisfy the proposed criteria, implying imperfection in their current design.", "D": "Their effectiveness is solely dependent on the availability of large, un-contaminated datasets."}, "answer": "C", "explanation": "The paper explicitly states that \"existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design.\" This highlights a key limitation.", "question_token_count": 26, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 15}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "The definition and example of syntactic contamination in the context of LLM training and testing, specifically focusing on rephrasing training data with prefixes.", "question": "Which of the following best encapsulates the central philosophical challenge posed by syntactic contamination, as discussed in the provided text?", "choices": {"A": "The difficulty in definitively proving that an LLM is reasoning rather than merely recalling memorized information after syntactic rephrasing of training data.", "B": "The increased computational cost associated with generating synthetic test data that avoids syntactic overlap with the training set.", "C": "The inherent bias introduced by prefix strings, which disproportionately impact the performance of smaller LLMs.", "D": "The lack of standardized methods for detecting and quantifying syntactic contamination across different NLP applications."}, "answer": "A", "explanation": "The text explicitly states the debate revolves around distinguishing between memorization and reasoning when syntactic transformations are used. Option A captures this core challenge.", "question_token_count": 24, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 21}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "How NPHardEval assesses LLMs' reasoning abilities using graphs and NP problems like the Traveling Salesman Problem (TSP).", "question": "Why might NPHardEval choose to evaluate LLMs on NP-hard problems like the Traveling Salesman Problem (TSP) instead of problems with known polynomial-time solutions?", "choices": {"A": "To ensure the LLM demonstrates the ability to find provably optimal solutions to complex problems.", "B": "To assess the LLM's ability to approximate solutions and leverage heuristic strategies, even when exact solutions are computationally infeasible.", "C": "To specifically test the LLM\u2019s knowledge of graph theory and combinatorial optimization algorithms.", "D": "To evaluate the LLM's performance against the theoretical lower bounds of computational complexity."}, "answer": "B", "explanation": "Evaluating LLMs on NP-hard problems like TSP allows for the assessment of their ability to handle problems where finding an optimal solution is computationally expensive, reflecting real-world scenarios where approximations are often necessary.", "question_token_count": 35, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 18}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "Explain the concept of exact contamination and provide examples of common scenarios where it occurs.", "question": "Which of the following scenarios best exemplifies exact contamination in the context of LLM training and evaluation?", "choices": {"A": "A test question is rephrased using a synonym substitution tool before being included in the training dataset.", "B": "A code snippet used in a benchmark implementation is directly copied into the training corpus.", "C": "A test example is slightly modified by adding extra whitespace characters before being included in the training data.", "D": "A test question is paraphrased using a different sentence structure while retaining the original meaning."}, "answer": "B", "explanation": "Exact contamination is defined as the presence of an identical data point in both the training and test datasets. Option B directly describes this scenario, with a code snippet from a benchmark implementation being copied verbatim into the training corpus.", "question_token_count": 20, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 19}
{"context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n", "topic": "Describe how Benchmark Self-Evolving utilizes a multi-agent framework to extend existing static benchmarks.", "question": "How does the multi-agent framework in Benchmark Self-Evolving contribute to the evolution of static benchmarks?", "choices": {"A": "By replacing static benchmarks entirely with dynamically generated ones.", "B": "By distributing tasks like planning, generation, verification, and evaluation across specialized LLM agents.", "C": "By utilizing human interviewers to iteratively refine the static benchmarks.", "D": "By employing a single LLM to paraphrase queries from existing static benchmarks."}, "answer": "B", "explanation": "Benchmark Self-Evolving leverages multiple agents, each responsible for a specific step in the benchmark creation process, to dynamically extend existing static benchmarks.", "question_token_count": 20, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 2, "question_groundedness_score": 5, "avg_answer_token_count": 14}
{"context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n", "topic": "The inherent limitations of a survey-based analysis regarding the rapid evolution of LLM development and benchmarking techniques.", "question": "Given the documented limitations of survey-based analysis in rapidly evolving fields, which of the following best characterizes the primary challenge in applying such an approach to the assessment of LLM benchmarking techniques?", "choices": {"A": "The lack of granular technical detail hinders practical implementation, requiring further specialized research.", "B": "The inherent temporal lag between survey completion and technological advancement renders findings susceptible to obsolescence.", "C": "The reliance on standardized criteria limits the ability to evaluate novel or unconventional benchmarking methods.", "D": "The difficulty in achieving reproducibility compromises the reliability of the survey\u2019s conclusions."}, "answer": "B", "explanation": "The text explicitly states that the rapid evolution of LLMs and benchmarking techniques means recent methods or tools may not be fully covered, highlighting the issue of obsolescence due to the temporal lag.", "question_token_count": 36, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 17}
{"context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n", "topic": "Analyze the significance of the lack of standardized criteria for evaluating dynamic benchmarks and how this impacts the field.", "question": "How does the absence of standardized evaluation criteria for dynamic benchmarks in LLM assessment most critically impede progress within the field?", "choices": {"A": "It prevents the direct comparison of different LLMs, as benchmark results become incomparable.", "B": "It limits the ability to effectively quantify the reduction of data contamination achieved by various dynamic benchmarking techniques.", "C": "It discourages researchers from developing novel dynamic benchmarking methodologies due to uncertainty regarding their merit.", "D": "It renders static benchmarks obsolete, hindering the comprehensive evaluation of LLMs."}, "answer": "B", "explanation": "The lack of standardized criteria makes it difficult to determine which dynamic benchmarks are truly effective at reducing data contamination, thus hindering the overall progress of the field. Options A, C, and D are all potential consequences, but B is the most direct and critical impediment.", "question_token_count": 23, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 18}
{"context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n", "topic": "Summarize the key contributions of the survey described in the text to the broader research landscape of data contamination in LLMs.", "question": "Within the current landscape of LLM benchmarking, what is the primary contribution of the described survey concerning data contamination mitigation?", "choices": {"A": "Providing a comprehensive collection of static and dynamic benchmarking methods, accessible via a GitHub repository.", "B": "Demonstrating the inherent limitations of static benchmarks and advocating for a complete shift to dynamic evaluation strategies.", "C": "Identifying a lack of standardized criteria for evaluating dynamic benchmarks and proposing optimal design principles to address this gap.", "D": "Presenting a detailed analysis of existing data contamination risks and proposing novel methods for static benchmark enhancement."}, "answer": "C", "explanation": "The survey's key contribution lies in highlighting the absence of standardized evaluation criteria for dynamic benchmarks and offering design principles to improve them. This is a critical advancement in the field.", "question_token_count": 23, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 1, "question_groundedness_score": 8, "avg_answer_token_count": 20}
{"context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n", "topic": "Evaluate existing dynamic benchmarks for LLMs, considering the design principles outlined in the text, and identify any shortcomings or areas for improvement.", "question": "Given the limitations of static benchmarks and the identified absence of standardized evaluation criteria for dynamic benchmarks, which of the following represents the most significant impediment to reliable assessment of LLM performance in the presence of potential data contamination?", "choices": {"A": "The inherent difficulty in completely isolating LLM training data from evaluation datasets.", "B": "The lack of consensus on appropriate methods for generating synthetic data for dynamic benchmarks.", "C": "The computational expense associated with dynamically constructing and evaluating benchmark datasets.", "D": "The limited availability of diverse and challenging datasets suitable for assessing LLM generalization capabilities."}, "answer": "A", "explanation": "The abstract explicitly states a \"critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks.\" This absence of standardized evaluation is the most significant impediment, as it prevents a reliable and consistent assessment of LLM performance despite efforts to mitigate data contamination.", "question_token_count": 43, "answer_correctness_score": 6, "explanation_validity_score": 4, "question_clarity_score": 6, "question_groundedness_score": 8, "avg_answer_token_count": 15}
{"context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n", "topic": "The importance of standardized evaluation criteria for dynamic benchmarking algorithms used to assess Large Language Models (LLMs).", "question": "Within the context of dynamic benchmarking algorithm evaluation, what is the most critical implication of relying on an \"oracle\" function (denoted as \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) ) to determine the ground truth for assessing correctness?", "choices": {"A": "The oracle's computational cost is the primary factor determining the scalability of the dynamic benchmarking process.", "B": "The oracle's inherent subjectivity introduces a potential source of bias that must be carefully mitigated to ensure reliable benchmark results.", "C": "The oracle's accuracy and consistency directly influence the validity of the benchmark by providing the reference against which transformed outputs are evaluated.", "D": "The oracle's ability to handle complex data types is essential for benchmarking LLMs across diverse application domains."}, "answer": "C", "explanation": "The oracle provides the ground truth, and its accuracy is paramount to the correctness score. If the oracle is flawed, the benchmark will be flawed, regardless of the scoring function.", "question_token_count": 61, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 22}
{"context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n", "topic": "Discuss the advantages of the coordinated approach used in BENCHAGENTS, particularly the incorporation of human-in-the-loop feedback, and its impact on benchmark quality and scalability.", "question": "Which of the following best encapsulates the synergistic advantage of BENCHAGENTS' coordinated multi-agent framework in benchmark creation?", "choices": {"A": "The framework's reliance on specialized LLM agents for each stage \u2013 planning, generation, verification, and evaluation \u2013 inherently guarantees benchmark quality, obviating the need for human intervention.", "B": "The coordinated structure primarily enables scalability by distributing the benchmark creation workload across multiple agents, with human feedback serving as a secondary quality control mechanism.", "C": "The integration of human-in-the-loop feedback within the coordinated multi-agent framework allows for iterative refinement and calibration of benchmarks, fostering both diversity and quality while maintaining scalability.", "D": "The core benefit of BENCHAGENTS lies in its ability to surpass static benchmarks through automated generation, with the multi-agent coordination merely facilitating this process and human feedback being largely superfluous."}, "answer": "C", "explanation": "The correct answer highlights the synergistic relationship between the coordinated structure, human feedback, and the resulting benefits of diversity, quality, and scalability. The coordinated framework allows for iterative refinement through human feedback, leading to superior benchmarks.", "question_token_count": 26, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 34}
{"context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n", "topic": "The purpose and benefits of label protection strategies, as demonstrated in benchmarks like GLUE, SuperGLUE, and OpenAI\u2019s HumanEval, in preventing models from learning or memorizing test answers.", "question": "Which of the following best encapsulates the primary rationale for employing label protection strategies in benchmarks such as GLUE, SuperGLUE, and HumanEval?", "choices": {"A": "To enhance the computational efficiency of model evaluation processes.", "B": "To safeguard evaluation data from unauthorized access and potential misuse through encryption.", "C": "To prevent models from directly learning or memorizing the solutions contained within the test sets.", "D": "To comply with legal and licensing requirements regarding data distribution and reuse."}, "answer": "C", "explanation": "Label protection strategies are specifically designed to prevent models from memorizing test answers, thereby preserving the integrity of the benchmark evaluation. This ensures that performance reflects true generalization ability, not rote learning.", "question_token_count": 31, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 14}
{"context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n", "topic": "Why is interpretability of the transformation process crucial in dynamic benchmarking, and what are the cost implications of a lack of interpretability?", "question": "What primary economic consequence arises from a lack of transparency in the data transformation process within dynamic benchmarking frameworks?", "choices": {"A": "Increased reliance on static benchmarks to validate dynamic results.", "B": "Higher costs associated with manual validation and error correction.", "C": "Reduced need for temporal cutoff approaches in data collection.", "D": "Accelerated adoption of purely rule-based dynamic benchmarking methods."}, "answer": "B", "explanation": "The text explicitly states that a lack of interpretability leads to increased manual validation, driving up costs. The other options are not directly supported by the provided text.", "question_token_count": 21, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 11}
{"context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n", "topic": "Explain the relationship between the size of training datasets and the probability of data contamination in LLM benchmarks, as described in the text, and why this renders traditional benchmarks outdated.", "question": "How does the relationship between training dataset size and the probability of data contamination impact the utility of traditional LLM benchmarks, according to the provided analysis?", "choices": {"A": "Increased training dataset size directly increases the probability of data contamination, making traditional benchmarks more reliable.", "B": "Traditional benchmarks remain effective regardless of training dataset size due to robust label protection methods.", "C": "As training dataset size increases, the probability of data contamination decreases, rendering traditional static benchmarks increasingly ineffective.", "D": "The relationship is negligible; data contamination primarily stems from model architecture, not training data size."}, "answer": "C", "explanation": "The text explicitly states that Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121, meaning as |\ud835\udc9ftrain| increases, Prcontam decreases, and traditional benchmarks become outdated.", "question_token_count": 29, "answer_correctness_score": 8, "explanation_validity_score": 4, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 19}
{"context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "Explain the limitations of post-hot detection methods as described in the text, specifically addressing the challenges related to data access and assumptions about model behavior.", "question": "What is the combined impact of restricted training data access and potentially flawed assumptions about model behavior on the efficacy of post-hot detection methods?", "choices": {"A": "Both limitations undermine the reliability of post-hot detection by preventing accurate overlap identification and by relying on behavioral characteristics that may not generalize across different models.", "B": "Limited data access primarily affects the accuracy of overlap detection, while flawed assumptions mainly influence the model's ability to generalize to unseen data.", "C": "Restricted data access is the more significant limitation, as it fundamentally prevents the detection of contamination regardless of model behavior.", "D": "Flawed assumptions about model behavior are the primary obstacle, as even with full data access, incorrect assumptions will lead to inaccurate detection."}, "answer": "A", "explanation": "The text explicitly states that both restricted data access and flawed assumptions contribute to the limitations of post-hot detection. Option A accurately reflects this combined impact. Option B separates the limitations, Option C overemphasizes data access, and Option D prioritizes assumptions incorrectly.", "question_token_count": 27, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 9, "question_groundedness_score": 10, "avg_answer_token_count": 26}
