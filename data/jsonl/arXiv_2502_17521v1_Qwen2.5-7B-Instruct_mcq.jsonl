{"context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n", "topic": "VarBench's Strategy in Replacing Variables in Existing Benchmark Samples", "question": "How does VarBench's strategy of replacing variables in existing benchmark samples differ fundamentally from other benchmark rewriting techniques like Auto-Dataset and StructEval?", "choices": {"A": "By directly rewriting stylistic elements and knowledge retention in new samples.", "B": "Through prompting LLMs to rewrite samples while preserving difficulty levels but altering variable values.", "C": "By expanding on examined concepts through knowledge graphs to create extended questions.", "D": "Using a contamination detector to identify and remove contaminated samples before rewriting."}, "answer": "B", "explanation": "VarBench's approach focuses on replacing variables rather than rewriting stylistic elements or expanding concepts, distinguishing it from other techniques.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "question_token_count": 29, "avg_answer_token_count": 14}
{"context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n", "topic": "Differentiating Performance Drops Due to Data Contamination vs. Increased Task Complexity", "question": "When evaluating the performance of an LLM on a transformed dataset, which of the following scenarios would indicate a performance drop primarily due to increased task complexity rather than data contamination?", "choices": {"A": "The performance drop is consistent across multiple runs with the same transformed dataset.", "B": "The performance drop occurs despite the dataset being cleaned and verified for quality.", "C": "The performance drop is observed in tasks that have been formally proven to be more complex through an established complexity metric.", "D": "The performance drop is minimal and occurs only in edge cases of the dataset."}, "answer": "C", "explanation": "The correct answer lies in recognizing that a consistent performance drop across multiple runs suggests a stable issue rather than variability due to contamination. A drop observed in tasks proven to be more complex indicates a genuine increase in task difficulty rather than contamination issues. Minimal and occasional drops can be attributed to various factors including implementation issues or other confounding variables.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 6, "question_token_count": 34, "avg_answer_token_count": 17}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The role of encryption and post-hoc detection in reducing data contamination.", "question": "How does encryption and post-hoc detection complement each other in mitigating data contamination in LLM benchmarking?", "choices": {"A": "Encryption ensures data remains confidential during transmission, while post-hoc detection helps identify contamination after training.", "B": "Encryption prevents data leakage during training, whereas post-hoc detection allows for real-time monitoring of contamination.", "C": "Encryption guarantees data integrity before training, and post-hoc detection provides a means to validate the results after training.", "D": "Encryption secures data during the training phase, while post-hoc detection offers a way to retrospectively assess contamination in the benchmarking process."}, "answer": "D", "explanation": "The correct answer is D. Encryption is crucial for securing data during the training phase to prevent unauthorized access or data leakage. Post-hoc detection, on the other hand, is employed to retrospectively assess the benchmarking process and identify any contamination that may have occurred after the training phase. While A, B, and C touch on aspects related to data security and validation, they do not accurately capture the complementary roles of encryption and post-hoc detection in the broader context of LLM benchmarking.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "question_token_count": 19, "avg_answer_token_count": 22}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The necessity of transparent benchmarking in the context of LLM development.", "question": "What critical criterion does the paper propose for evaluating dynamic benchmarks that addresses a key limitation of existing methods in mitigating data contamination?", "choices": {"A": "Transparency in the benchmarking process.", "B": "Continuous updating of benchmark datasets based on the timestamps of LLM training.", "C": "Regeneration of benchmark data to reconstruct original benchmarks.", "D": "Criteria for assessing the robustness of dynamic benchmarks against data contamination."}, "answer": "D", "explanation": "The question requires a deep understanding of the paper's proposed criteria for evaluating dynamic benchmarks. While options B and C describe methods for mitigating data contamination, they do not directly address the evaluation criteria. Option A is too broad and does not specifically relate to the evaluation aspect. Option D correctly identifies the focus of the proposed criteria, which aims to evaluate the robustness of dynamic benchmarks against data contamination.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 6, "question_token_count": 25, "avg_answer_token_count": 11}
{"context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n", "topic": "Definition and Importance of Scalability in Dynamic Benchmarking", "question": "What does the scalability metric in dynamic benchmarking primarily measure, and how is it represented mathematically?", "choices": {"A": "The efficiency of transforming small datasets into larger ones while minimizing costs.", "B": "The proportion of data that can be generated per unit cost, represented as E[\u2225Ti(\ud835\udc9f)\u2225/\u2225\ud835\udc9f\u2225]E, parallel, T, subscript, i, left parenthesis, \ud835\udc9f, right parenthesis, parallel, divided by, parallel, \ud835\udc9f, parallel, end parallel, start_script E end_script.", "C": "The ratio of the total cost to the size of the transformed dataset.", "D": "The accuracy of the benchmarking results relative to the original dataset size."}, "answer": "B", "explanation": "The scalability metric primarily measures the efficiency of transforming small datasets into larger ones while minimizing costs, which is mathematically represented as the expectation over the transformation space, E[\u2225Ti(\ud835\udc9f)\u2225/\u2225\ud835\udc9f\u2225], where \u2225Ti(\ud835\udc9f)\u2225 represents the size of the transformed dataset and \u2225\ud835\udc9f\u2225 represents the size of the original dataset.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 10, "question_token_count": 19, "avg_answer_token_count": 28}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "Purpose and Methods of Constructing Reliable Benchmarks to Evaluate Language Models", "question": "What critical issue does LiveBench aim to address through its methodology of collecting questions based on the latest information sources?", "choices": {"A": "It ensures the model's responses are not contaminated by outdated data.", "B": "It evaluates the model's ability to handle complex mathematical concepts.", "C": "It tests the model's performance on a variety of coding problems.", "D": "It assesses the model's proficiency in generating academic writing tasks."}, "answer": "A", "explanation": "LiveBench focuses on collecting questions based on the latest information to avoid contamination from outdated data. While other benchmarks may test various aspects like coding or academic writing, LiveBench specifically aims to mitigate data contamination.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "question_token_count": 23, "avg_answer_token_count": 12}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "Potential methods to counteract the effectiveness of canary strings by malicious actors attempting to leak benchmarking data.", "question": "What strategy might a malicious actor employ to effectively use canary strings against their intended purpose, thereby leaking benchmarking data without detection?", "choices": {"A": "Incorporating canary strings into the model's own training data in a manner that mimics legitimate usage.", "B": "Modifying the canary strings to become indistinguishable from other common tokens used in the model's training dataset.", "C": "Using a different type of token altogether that is not recognized as a canary string.", "D": "Randomizing the placement of canary strings in the output to avoid consistent patterns."}, "answer": "B", "explanation": "The correct answer is B because modifying the canary strings to be indistinguishable from other common tokens would make it difficult for model developers to identify and filter out the contaminated data, thus allowing the malicious actor to leak benchmarking data.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 4, "question_token_count": 26, "avg_answer_token_count": 19}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "Examine a model's performance in answering short questions in Chinese by assessing its factuality using C-SimpleQA.", "question": "Since the provided context does not discuss the evaluation of models using C-SimpleQA, what type of benchmark would be most appropriate for assessing a model's factuality in answering short questions in Chinese?", "choices": {"A": "Instruction Following benchmarks", "B": "Reasoning benchmarks", "C": "Coding benchmarks", "D": "None of the above"}, "answer": "D", "explanation": "The context focuses on various types of benchmarks but does not mention C-SimpleQA. A domain expert would recognize that none of the listed benchmarks directly relate to the specific task of evaluating factuality in short Chinese questions.", "answer_correctness_score": 1, "explanation_validity_score": 1, "question_clarity_score": 2, "question_groundedness_score": 2, "question_token_count": 39, "avg_answer_token_count": 4}
{"context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n", "topic": "Structure and Components of the Seed Dataset in Static Benchmarks", "question": "In the context of a seed dataset for a static benchmark, how does the scoring function \\(\\mathcal{S}(\\cdot)\\) primarily differ in its evaluation criteria compared to the characteristics of the expected outputs \\(\\mathcal{Y}\\)?", "choices": {"A": "The scoring function assesses the semantic similarity between the model's outputs and the expected outputs, while the expected outputs specify the exact solutions or responses required.", "B": "The scoring function measures the syntactic correctness of the model's outputs, whereas the expected outputs provide the actual data used to train the models.", "C": "The scoring function evaluates the diversity of the model's outputs, while the expected outputs define the set of acceptable responses.", "D": "The scoring function calculates the computational efficiency of the model's outputs, whereas the expected outputs provide the theoretical limits for performance."}, "answer": "A", "explanation": "The scoring function \\(\\mathcal{S}(\\cdot)\\) typically measures how well the model's outputs align with the expected outputs in terms of correctness and relevance. While the expected outputs specify the desired outcomes, the scoring function quantitatively evaluates this alignment.", "answer_correctness_score": 10, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 10, "question_token_count": 43, "avg_answer_token_count": 25}
{"context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n", "topic": "Fairness, accountability, and privacy considerations in the design of benchmarking frameworks.", "question": "How can a benchmarking framework be designed to ensure fairness, accountability, and privacy while mitigating the risk of bias and misuse?", "choices": {"A": "By using exclusively static benchmarks to avoid privacy concerns.", "B": "Through the incorporation of diverse and regularly updated datasets, transparent methodologies, and robust governance structures.", "C": "Implementing strict access controls on all benchmarking data and prohibiting public sharing of results.", "D": "Limiting the scope of benchmarking to closed, private domains where data can be freely collected and shared without oversight."}, "answer": "B", "explanation": "The correct answer requires a deep understanding of the multifaceted issues involved in benchmarking frameworks and the need for a balanced approach. Option B addresses the key aspects of fairness, accountability, and privacy by ensuring the use of diverse and up-to-date datasets, transparent methodologies, and robust governance, which are essential for mitigating risks of bias and misuse. Options A, C, and D are either overly restrictive or fail to address multiple dimensions of the problem.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "question_token_count": 25, "avg_answer_token_count": 17}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "Importance of addressing contamination for assessing generalization and robustness in LLMs.", "question": "What is the primary reason for considering syntactic transformations as a form of contamination in LLM benchmarking, despite the debate surrounding their nature?", "choices": {"A": "To enhance the model's syntactic recall capabilities.", "B": "To ensure the model's reasoning capability is adequately tested.", "C": "Because syntactic transformations are easier to implement than semantic changes.", "D": "To align the test data more closely with the training data."}, "answer": "B", "explanation": "The correct answer is B because the context states that syntactic transformations are considered contamination primarily to test the model's reasoning capability, not just its recall of memorized information. This distinction is crucial for evaluating the model's true reasoning abilities rather than its capacity to reproduce learned patterns.", "answer_correctness_score": 8, "explanation_validity_score": 8, "question_clarity_score": 6, "question_groundedness_score": 10, "question_token_count": 26, "avg_answer_token_count": 11}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "Assessing a model\u2019s capacity to handle diverse and intricate math tasks, as seen in recent challenges like AIME 2024 and CNMO 2024.", "question": "In the context of evaluating a model's capacity to handle diverse and intricate math tasks as seen in recent challenges like AIME 2024 and CNMO 2024, which of the following statements best reflects the nature of the challenge posed by these competitions?", "choices": {"A": "These challenges primarily test a model's ability to recall and apply basic mathematical formulas.", "B": "These challenges require a model to solve multi-step problems that involve creative thinking and understanding of advanced mathematical concepts.", "C": "These challenges focus on testing a model's proficiency in arithmetic operations and simple problem-solving techniques.", "D": "These challenges are designed to assess a model's speed in performing calculations rather than its depth of mathematical understanding."}, "answer": "B", "explanation": "The recent challenges like AIME 2024 and CNMO 2024 are known for their complexity and require models to work through multi-step, intricate math problems that often demand creative thinking and understanding of advanced mathematical concepts. Therefore, option B accurately captures the nature of these challenges.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 1, "question_groundedness_score": 10, "question_token_count": 51, "avg_answer_token_count": 19}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The need for dynamic benchmarks to mitigate issues associated with static benchmarks.", "question": "What is the primary reason for transitioning from static benchmarks to dynamic benchmarks in the evaluation of LLMs?", "choices": {"A": "To ensure consistent performance metrics across different models.", "B": "To adapt to the rapid evolution of LLMs and reduce the risk of data contamination.", "C": "To simplify the benchmarking process by standardizing evaluation methods.", "D": "To reduce the computational resources required for benchmarking."}, "answer": "B", "explanation": "Dynamic benchmarks are designed to adapt to the changing capabilities of LLMs and mitigate the risk of data contamination, which is a significant issue with static benchmarks as LLMs evolve rapidly and continue to train on new data.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 9, "question_groundedness_score": 10, "question_token_count": 21, "avg_answer_token_count": 12}
{"context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n", "topic": "Human Effort Required in Data Collection and Benchmark Updates", "question": "How does the reliance on human effort in data collection and benchmark updates impact the long-term reliability and validity of language model evaluations?", "choices": {"A": "It enhances the accuracy of evaluations through meticulous human oversight.", "B": "It significantly increases the risk of data contamination and outdated information.", "C": "It reduces the need for verification processes due to automated rule-based generation.", "D": "It diminishes the importance of using recent competition data for evaluation."}, "answer": "B", "explanation": "The context emphasizes the continuous human involvement required for data collection and benchmark updates, highlighting the risk of data contamination and the need for verification. These factors suggest that relying heavily on human effort can introduce inaccuracies over time, making option B the most accurate response.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 6, "question_token_count": 26, "avg_answer_token_count": 13}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The importance of maintaining the separation between training and testing data in LLM evaluation.", "question": "Why is the separation between training and testing data critical in the evaluation of Large Language Models (LLMs), and what are the potential risks of violating this separation?", "choices": {"A": "To prevent overfitting and ensure the model's generalizability, thereby providing a true assessment of its performance.", "B": "To avoid data contamination, which can lead to inflated and misleading performance metrics.", "C": "To comply with ethical standards in AI research and development.", "D": "To simplify the model training process and reduce computational costs."}, "answer": "B", "explanation": "The correct answer is B, as data contamination can lead to models performing well on seen data but poorly on unseen data, providing an inaccurate representation of their true capabilities.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "question_token_count": 33, "avg_answer_token_count": 15}
{"context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n", "topic": "The need for standardized criteria in evaluating both static and dynamic benchmarks for LLMs.", "question": "What is a key challenge in the current development of dynamic benchmarks for LLMs, and how does the lack of standardized criteria exacerbate this issue?", "choices": {"A": "Ensuring transparency and avoiding high assumptions about contaminated models.", "B": "Balancing the correctness of benchmarks with their scalability.", "C": "Controlling complexity in dynamic benchmarks to ensure efficient evaluation.", "D": "Addressing data contamination issues inherent in static benchmarks."}, "answer": "B", "explanation": "The correct answer is B. The context mentions that while dynamic benchmarks address challenges like transparency and high assumptions about contaminated models, they introduce new issues such as balancing correctness with scalability. Standardized criteria for dynamic benchmarks are crucial to tackle this challenge effectively.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 8, "question_token_count": 30, "avg_answer_token_count": 11}
{"context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n", "topic": "Proposed Optimal Design Principles for Dynamic Benchmarking", "question": "According to the proposed optimal design principles for dynamic benchmarking, which of the following is NOT a key aspect that should be considered to ensure robustness against data contamination?", "choices": {"A": "Incorporating real-time feedback mechanisms to adapt benchmarks dynamically.", "B": "Ensuring the benchmarking process is transparent and reproducible.", "C": "Using fixed datasets to maintain consistency across different benchmarking periods.", "D": "Developing standardized metrics to evaluate the performance of LLMs under varying conditions."}, "answer": "C", "explanation": "The correct answer requires a deep understanding of the proposed design principles. While incorporating real-time feedback mechanisms, ensuring transparency, and developing standardized metrics are crucial, using fixed datasets contradicts the principle of dynamic adaptation.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 1, "question_groundedness_score": 2, "question_token_count": 32, "avg_answer_token_count": 12}
{"context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n", "topic": "Limitations of Existing Static Benchmarking Methods", "question": "What is a critical limitation of existing static benchmarking methods in the context of mitigating data contamination risks for Large Language Models (LLMs)?", "choices": {"A": "They do not adequately account for the diverse sources of data contamination.", "B": "They fail to provide a standardized framework for evaluating the effectiveness of mitigation strategies.", "C": "They are too complex to implement and interpret.", "D": "They do not effectively capture the nuances of real-world data usage scenarios."}, "answer": "B", "explanation": "The correct answer is B. Existing static benchmarking methods often lack standardized criteria for evaluating the success of data contamination mitigation strategies, which makes it difficult to compare and improve upon different methods.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 6, "question_token_count": 27, "avg_answer_token_count": 13}
{"context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n", "topic": "Methods to mitigate data contamination in LLM benchmarks, including temporal cutoff, LLM-based, and graph-based generation.", "question": "How does C2LEVA ensure the contamination-free construction of bilingual evaluation datasets?", "choices": {"A": "By exclusively using temporal cutoff to filter out contaminated samples.", "B": "Through a hybrid approach combining temporal cutoff, LLM-based, and graph-based generation techniques.", "C": "Utilizing only LLM-based generation to ensure sample faithfulness.", "D": "Applying graph-based perturbation exclusively to extract reasoning graphs from benchmarks."}, "answer": "B", "explanation": "C2LEVA incorporates all three methods\u2014temporal cutoff, LLM-based generation, and graph-based generation\u2014to construct contamination-free bilingual evaluation datasets. This comprehensive approach ensures a thorough mitigation of contamination risks.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "question_token_count": 17, "avg_answer_token_count": 14}
{"context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n", "topic": "The necessity and challenges of developing fair and transparent benchmarks for LLMs.", "question": "What is the primary reason for the increased difficulty in developing fair and transparent benchmarks for LLMs despite the existence of various detection methods?", "choices": {"A": "The proprietary nature of training data makes it hard to assess performance accurately.", "B": "The scale and complexity of the training corpora increase the risk of data contamination.", "C": "The reliance on human-annotated and synthetic datasets for fine-tuning introduces new biases.", "D": "The use of web-scraped data in pre-training phases ensures a diverse dataset."}, "answer": "B", "explanation": "The difficulty arises primarily from the scale and complexity of the training corpora, which makes it challenging to entirely exclude evaluation data, even with existing detection methods. This challenge is exacerbated by the proprietary nature of training data, but the core issue lies in the sheer size and diversity of the training datasets.", "answer_correctness_score": 10, "explanation_validity_score": 9, "question_clarity_score": 7, "question_groundedness_score": 9, "question_token_count": 27, "avg_answer_token_count": 17}
{"context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n", "topic": "Challenges and Limitations of Current Dynamic Benchmarking Methods", "question": "What is the primary gap identified in the current dynamic benchmarking methods that the authors aim to address through their proposed design principles?", "choices": {"A": "The lack of standardized criteria for evaluating the effectiveness of dynamic benchmarks.", "B": "The reliance on static benchmarks that cannot adapt to changing data conditions.", "C": "The insufficient data contamination mitigation strategies in dynamic benchmarks.", "D": "The inability to fully automate the dynamic benchmarking process."}, "answer": "A", "explanation": "The question probes the authors' identification of the main gap in current dynamic benchmarking methods, requiring a deep understanding of the context provided. It challenges the respondent to distinguish between various issues discussed and focus on the most critical one highlighted in the paper.", "answer_correctness_score": 10, "explanation_validity_score": 8, "question_clarity_score": 6, "question_groundedness_score": 10, "question_token_count": 25, "avg_answer_token_count": 12}
{"context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n", "topic": "Advantages and Collision Probability of Rule-Based Generation Method", "question": "How does the rule-based generation method ensure a low collision probability in producing test cases, and what theoretical underpinning might support this claim?", "choices": {"A": "By using a deterministic rule set that minimizes the chance of generating identical test cases through strict adherence to predefined structures.", "B": "Through the implementation of randomization techniques that introduce variability while maintaining unique characteristics of each test case.", "C": "By employing sophisticated machine learning algorithms to predict and avoid generating similar test cases.", "D": "It relies on manual verification and review processes to eliminate duplicate cases after generation."}, "answer": "A", "explanation": "The correct answer is A. The rule-based generation method ensures low collision probability by strictly following a deterministic rule set, thereby minimizing the chance of generating identical test cases. This is supported by the theoretical underpinning of formal language theory and combinatorial mathematics, which ensure that each test case is uniquely identifiable based on the defined rules.", "answer_correctness_score": 9, "explanation_validity_score": 8, "question_clarity_score": 6, "question_groundedness_score": 4, "question_token_count": 29, "avg_answer_token_count": 18}
{"context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n", "topic": "Analyze the implications of a low correctness score in the context of evaluating LLMs and discuss strategies to improve the correctness of dynamic benchmarks.", "question": "How might a low correctness score in a dynamic benchmark impact the evaluation of LLMs, and what strategies can be employed to enhance the accuracy of such benchmarks?", "choices": {"A": "It would indicate unreliable benchmark results, potentially misleading the evaluation of LLM performance, and strategies to improve could include enhancing the oracle function's reliability and rigorously validating the ground truth annotations.", "B": "It would suggest overfitting to training data, and strategies might involve increasing the size of the training dataset and fine-tuning the models extensively.", "C": "It would highlight the need for more sophisticated scoring functions, suggesting the development of complex algorithms to better align model outputs with ground truths.", "D": "It would reveal the benchmarks' sensitivity to noise, recommending the implementation of robust error handling mechanisms and regularization techniques."}, "answer": "A", "explanation": "A low correctness score in a dynamic benchmark suggests that the generated datasets may not accurately reflect real-world scenarios, leading to potentially misleading evaluations of LLMs. Enhancing the reliability of the oracle function and rigorously validating ground truth annotations are effective strategies to improve the correctness of dynamic benchmarks. Options B, C, and D address different aspects of model evaluation but do not directly relate to the issue of correctness in dynamic benchmarks as described.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 6, "question_token_count": 32, "avg_answer_token_count": 28}
{"context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n", "topic": "Components of the Scalability Formula", "question": "What does the term \\(\\parallel T_i(\\mathcal{D}) \\parallel\\) represent in the scalability formula, and how does it contribute to the overall scalability measure?", "choices": {"A": "The size of the original dataset.", "B": "The size of the transformed dataset.", "C": "The cost associated with the transformation process.", "D": "The ratio of the original dataset size to the transformed dataset size."}, "answer": "B", "explanation": "\\(\\parallel T_i(\\mathcal{D}) \\parallel\\) represents the size of the transformed dataset, which is a crucial component in the scalability formula as it directly affects the overall measure of scalability. The formula balances the size of the transformed dataset with the cost of the transformation process to ensure an optimal dynamic benchmark.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 5, "question_groundedness_score": 10, "question_token_count": 33, "avg_answer_token_count": 9}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "Criteria for evaluating the effectiveness of dynamic benchmarks in LLM evaluation.", "question": "According to the context, what key criterion must a dynamic benchmark satisfy to ensure robust evaluation of LLMs while minimizing the risk of data contamination?", "choices": {"A": "It must update benchmark datasets based on the timestamps of LLM training.", "B": "It must regenerate benchmark data to reconstruct original benchmarks.", "C": "It must be transparent and traceable to the exact training data of the LLM.", "D": "It must incorporate a comprehensive set of evaluative criteria that consider the dynamic nature of LLM training data."}, "answer": "D", "explanation": "The correct answer is D, as the context emphasizes the need for a set of criteria to evaluate dynamic benchmarks comprehensively. The other options represent specific methods mentioned in the text but do not encapsulate the broader requirement for robust evaluation criteria.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 8, "question_token_count": 29, "avg_answer_token_count": 15}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "Understanding the structure and objectives of knowledge benchmarks, including datasets such as NaturalQuestions, TriviaQA, and MMLU.", "question": "What is a distinguishing characteristic of the NaturalQuestions dataset compared to TriviaQA and MMLU in terms of the type of knowledge retrieval it emphasizes?", "choices": {"A": "NaturalQuestions focuses on real-world information retrieval.", "B": "NaturalQuestions is designed for multi-domain tasks across various domains.", "C": "NaturalQuestions specializes in technical and long-context challenges.", "D": "NaturalQuestions requires models to solve multi-step math problems."}, "answer": "A", "explanation": "NaturalQuestions (Kwiatkowski et al., 2019) is known for its focus on real-world, commonsense questions that require common sense reasoning and contextual understanding, distinguishing it from TriviaQA and MMLU which focus more on factual and multi-domain knowledge retrieval.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 8, "question_token_count": 31, "avg_answer_token_count": 11}
{"context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n", "topic": "Evaluation Criteria Using Scoring Functions", "question": "What aspect of the scoring function \\( \\mathcal{S}(\\cdot) \\) challenges even domain experts in evaluating the quality of an LLM's outputs?", "choices": {"A": "Its ability to accurately capture the complexity of human language and reasoning.", "B": "Its capacity to handle edge cases and rare scenarios that may not be present in the training data.", "C": "Its reliance on subjective interpretations of expected outputs.", "D": "Its deterministic nature in providing a single score for each output."}, "answer": "A", "explanation": "The scoring function must consider the intricacies and subtleties of natural language and reasoning, which can vary significantly across different contexts and tasks. This requires a deep understanding of both the model's outputs and the expected results, making it a highly challenging aspect for even domain experts.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 3, "question_token_count": 29, "avg_answer_token_count": 14}
{"context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n", "topic": "Balancing correctness and scalability in dynamic LLM benchmarks.", "question": "What critical issue does the proposed research highlight in the development of dynamic LLM benchmarks, and how might it impact their effectiveness?", "choices": {"A": "Balancing correctness and scalability can lead to inefficiencies in evaluation.", "B": "Lack of standardized criteria hinders the development of reliable dynamic benchmarks.", "C": "Insufficient transparency in labeling poses significant challenges.", "D": "High assumptions about contaminated models affect post-hoc detection accuracy."}, "answer": "A", "explanation": "The question requires a deep understanding of the challenges identified in developing dynamic LLM benchmarks, particularly focusing on the balance between correctness and scalability, which is a subtle yet critical aspect of the research findings.", "answer_correctness_score": 10, "explanation_validity_score": 8, "question_clarity_score": 8, "question_groundedness_score": 9, "question_token_count": 25, "avg_answer_token_count": 12}
{"context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n", "topic": "Understanding the limitations of label protection in research and its impact on performance metrics.", "question": "How does label protection affect the evaluation of performance metrics in machine learning research, and what is a potential consequence of relying solely on centralized evaluation systems?", "choices": {"A": "It enhances the reproducibility of results by providing standardized metrics.", "B": "It restricts the ability to perform detailed error analysis and hinders reproducibility.", "C": "It improves the accuracy of performance metrics by reducing bias.", "D": "It has no significant impact on the evaluation process."}, "answer": "B", "explanation": "The context states that label protection limits transparency and independent verification, forcing reliance on centralized evaluation systems which can impede detailed error analysis and reproducibility.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "question_token_count": 30, "avg_answer_token_count": 12}
{"context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n", "topic": "Auto-Dataset Methodology for Generating New Benchmark Samples", "question": "How does Auto-Dataset ensure diversity and avoid in-distribution contamination compared to other methodologies?", "choices": {"A": "By employing LLMs to generate new samples that either retain the stylistics and essential knowledge of the original or present related questions at different cognitive levels.", "B": "By defining strict rules for sample generation and using a contamination detector to identify and remove contaminated samples.", "C": "By rewriting samples from existing static benchmarks to ensure they remain uncontaminated.", "D": "By utilizing knowledge graphs to expand on examined concepts and generate new questions."}, "answer": "A", "explanation": "The correct answer involves understanding that Auto-Dataset uses LLMs to create new samples that either retain the original style and knowledge or present related questions at varying cognitive levels, thereby avoiding the limitations of predefined rules and reducing the risk of contamination. Other methods might rely on rewriting existing samples or using knowledge graphs, but they do not capture the essence of Auto-Dataset's approach as described.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 10, "question_token_count": 18, "avg_answer_token_count": 20}
{"context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n", "topic": "StructEval Approach in Expanding Examined Concepts via Knowledge Graphs", "question": "How does StructEval approach the expansion of examined concepts in benchmarks through the integration of knowledge graphs?", "choices": {"A": "By prompting LLMs to generate stylistically similar yet semantically varied samples.", "B": "By employing LLMs to rewrite samples from static benchmarks, maintaining their difficulty levels.", "C": "By using knowledge graphs to extend questions from the original benchmark, thereby exploring related concepts at varying cognitive levels.", "D": "By detecting contaminated samples in static benchmarks and prompting LLMs to rewrite them."}, "answer": "C", "explanation": "StructEval specifically uses LLMs and knowledge graphs to expand on the original benchmark questions by developing a series of extended questions that explore related concepts at different cognitive levels, as described in the provided context.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "question_token_count": 20, "avg_answer_token_count": 17}
{"context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n", "topic": "Strategies and limitations in detecting and mitigating data contamination in LLM training processes.", "question": "What is the primary reason that makes it difficult to entirely exclude evaluation data during LLM training, despite the existence of retrieval-based detection methods?", "choices": {"A": "The sheer size and diversity of the training corpora.", "B": "The proprietary nature of the training data.", "C": "The lack of comprehensive retrieval-based detection methods.", "D": "The ease of accessing and using large-scale datasets for training."}, "answer": "A", "explanation": "The correct answer is A. Despite the existence of retrieval-based detection methods, the sheer size and diversity of training corpora make it practically impossible to ensure complete exclusion of evaluation data. The other options are plausible but less directly related to the core challenge.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "question_token_count": 28, "avg_answer_token_count": 11}
{"context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n", "topic": "The importance of ethical guidelines in data usage and model transparency within AI benchmarking.", "question": "How do ethical guidelines address the dual challenges of data bias and model transparency in AI benchmarking?", "choices": {"A": "By mandating regular audits of data sources and model performance metrics to ensure fairness and accountability.", "B": "Through the establishment of transparent data policies and the implementation of robust validation techniques to prevent bias.", "C": "By limiting the use of static benchmarks and encouraging the exclusive use of dynamic benchmarks to enhance adaptability.", "D": "Ensuring that all models undergo rigorous testing and approval processes before being used in benchmarks."}, "answer": "B", "explanation": "The correct answer lies in recognizing the comprehensive nature of ethical guidelines, which involve both data policy transparency and the prevention of bias through validation techniques. Options A, C, and D address specific aspects but do not fully capture the breadth of ethical guidelines discussed.", "answer_correctness_score": 10, "explanation_validity_score": 9, "question_clarity_score": 7, "question_groundedness_score": 6, "question_token_count": 19, "avg_answer_token_count": 19}
{"context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n", "topic": "Categorization of dynamic benchmarks and their respective characteristics.", "question": "What characteristic distinguishes hybrid approaches in dynamic benchmarking from the other categories?", "choices": {"A": "They rely solely on manually crafted rules.", "B": "They use a combination of rule-based and LLM-based generation techniques.", "C": "They focus exclusively on collecting new data from recent releases.", "D": "They are less transparent and harder to validate."}, "answer": "B", "explanation": "Hybrid approaches incorporate elements from both rule-based and LLM-based generation, offering a balanced approach but requiring validation mechanisms to ensure reliability.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "question_token_count": 14, "avg_answer_token_count": 11}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "Methods for regenerating benchmark data to reduce contamination in LLM evaluation.", "question": "What is the primary advantage of regenerating benchmark data over simply updating benchmarks based on the timestamps of LLM training, in the context of mitigating data contamination?", "choices": {"A": "Regenerating benchmark data allows for the reconstruction of the original problem space, ensuring that the model is evaluated on fresh data that was not used during its training.", "B": "Regenerating benchmark data provides real-time updates, making it more responsive to the latest trends and developments in the field.", "C": "Regenerating benchmark data is less computationally intensive than updating benchmarks based on timestamps.", "D": "Regenerating benchmark data ensures that the evaluation process remains transparent, allowing for public scrutiny of the model's performance."}, "answer": "A", "explanation": "Regenerating benchmark data offers the primary advantage of ensuring that the model is evaluated on fresh data that was not used during its training, thus providing a more accurate and reliable measure of performance. This approach helps in mitigating data contamination by creating a new and independent set of problems for the model to solve.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "question_token_count": 31, "avg_answer_token_count": 24}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "Evaluation Framework for LLMs on NP-Hard Problems", "question": "Which of the following frameworks uses a combination of randomly generated graphs and natural language descriptions to evaluate LLMs' reasoning abilities for NP-hard problems?", "choices": {"A": "S3Eval", "B": "DyVal", "C": "NPHardEval", "D": "Xie et al.'s method for Knights and Knaves puzzles"}, "answer": "B", "explanation": "DyVal (Zhu et al., 2024a) constructs directed acyclic graphs (DAGs) with varying numbers of nodes and edges, transforms these graphs into natural language descriptions, and then evaluates the LLM's reasoning capability by querying for the root node value. This approach combines graph generation and natural language processing to assess the model's ability to reason about complex structures.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "question_token_count": 29, "avg_answer_token_count": 6}
{"context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n", "topic": "The potential perpetuation of biases through the use of static benchmarks and methods to mitigate these biases.", "question": "How might static benchmarks inadvertently perpetuate biases, and what measures can be taken to mitigate this risk?", "choices": {"A": "By relying on outdated or biased data sources, thus reinforcing existing prejudices in AI systems.", "B": "Through the lack of adaptability, making them unsuitable for diverse user groups.", "C": "By being too transparent, allowing adversaries to exploit weaknesses in the evaluation process.", "D": "Due to their reliance on continuous data collection, leading to potential privacy violations."}, "answer": "A", "explanation": "The correct answer is A. Static benchmarks can perpetuate biases if they are based on outdated or biased data sources. This can reinforce existing prejudices in AI systems, even if the intention was to mitigate bias.", "answer_correctness_score": 8, "explanation_validity_score": 9, "question_clarity_score": 9, "question_groundedness_score": 9, "question_token_count": 21, "avg_answer_token_count": 16}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "Investigating the assessment of technical and long-context challenges through datasets like AlpacaEval and ArenaHard.", "question": "How do AlpacaEval and ArenaHard contribute uniquely to assessing a model's ability to handle technical and long-context challenges compared to other datasets mentioned in the context?", "choices": {"A": "By providing a more diverse range of technical and long-context problems than traditional math benchmarks.", "B": "Through their focus on open-domain evaluations that require deep integration of knowledge across multiple domains.", "C": "By offering controlled environments with well-defined tasks unlike the more open-ended nature of ControlBench and FRAMES.", "D": "They emphasize natural language understanding over problem-solving capabilities, making them distinct from math and knowledge benchmarks."}, "answer": "B", "explanation": "AlpacaEval and ArenaHard are designed to test models' abilities to handle complex, open-domain scenarios rather than structured math problems or straightforward knowledge retrieval. This makes them unique in their emphasis on integrating diverse knowledge and handling intricate, context-rich tasks.", "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 10, "question_token_count": 32, "avg_answer_token_count": 19}
{"context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n", "topic": "Transparency issues in benchmarking and the need for safeguards against misuse of benchmarking results.", "question": "What subtle ethical risk might arise from the dynamic nature of benchmarks, despite their intended adaptability?", "choices": {"A": "Increased computational costs for model training.", "B": "Heightened potential for continuous bias amplification.", "C": "Simplified data management processes.", "D": "Enhanced user trust in AI systems."}, "answer": "B", "explanation": "The dynamic nature of benchmarks introduces ongoing data collection and updating, which can lead to continuous bias amplification if not managed properly. This risk is subtle yet critical, as it can perpetuate existing biases over time without proper oversight.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "question_token_count": 19, "avg_answer_token_count": 8}
{"context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n", "topic": "The planning, generation, verification, and evaluation phases in multi-agent benchmark creation, as described in BENCHAGENTS.", "question": "What is the primary role of each specialized LLM agent during the benchmark creation process as described in BENCHAGENTS?", "choices": {"A": "Planning - defines the scope and structure of the benchmark.", "B": "Generation - creates initial test cases and scenarios based on the defined scope.", "C": "Verification - assesses the accuracy and consistency of the generated test cases.", "D": "Evaluation - conducts the final assessment through multi-agent collaboration and feedback."}, "answer": "D", "explanation": "The question requires a deep understanding of the roles assigned to different LLM agents in the BENCHAGENTS process. It challenges the respondent to comprehend the distinct responsibilities of each agent without simply regurgitating the text.", "answer_correctness_score": 10, "explanation_validity_score": 6, "question_clarity_score": 8, "question_groundedness_score": 10, "question_token_count": 24, "avg_answer_token_count": 13}
{"context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n", "topic": "Importance of Standardized Evaluation Tools in Model Performance Assessment", "question": "What is the primary advantage of using static benchmarks as standardized evaluation tools in assessing model performance, particularly when compared to other forms of evaluation methods?", "choices": {"A": "They provide a subjective and flexible framework for evaluating models.", "B": "They allow for a consistent and comparable measurement of model capabilities across diverse tasks.", "C": "They enable models to adapt to new and unforeseen challenges dynamically.", "D": "They facilitate real-time interaction and immediate feedback between models and evaluators."}, "answer": "B", "explanation": "The correct answer lies in recognizing that static benchmarks offer a standardized and consistent method for evaluating model performance across various tasks, making them comparable and objective.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 4, "question_token_count": 29, "avg_answer_token_count": 14}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "Analyzing the role of recent extensions in refining knowledge benchmarks, such as MMLU-Redux and MMLU-Pro.", "question": "How do MMLU-Redux and MMLU-Pro contribute to the refinement of knowledge benchmarks in a manner distinct from other recent extensions?", "choices": {"A": "By focusing exclusively on technical and long-context challenges.", "B": "Through enhanced multi-domain testing and detailed performance tracking.", "C": "By integrating control and framing challenges into the assessment framework.", "D": "Utilizing open-domain evaluations to gauge broad understanding and applicability."}, "answer": "B", "explanation": "MMLU-Redux and MMLU-Pro specifically refine knowledge benchmarks by providing more detailed and comprehensive multi-domain testing compared to other recent extensions, which often focus on different types of challenges like technical skills or long-context problems.", "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 9, "question_token_count": 29, "avg_answer_token_count": 11}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "Strategies for mitigating data contamination in static benchmarking of LLMs.", "question": "What critical factor makes the risk of data contamination particularly high in the context of LLM benchmarking, and how does this challenge traditional separation principles?", "choices": {"A": "The extensive training data collection from the internet increases the likelihood of contamination, violating the principle of separating training and test data.", "B": "The use of dynamic benchmarking methods decreases the risk of contamination by updating datasets.", "C": "The transparency of static benchmarks allows for easier detection of contamination.", "D": "The rapid evolution of LLM architectures makes it difficult to track training data sources."}, "answer": "A", "explanation": "This question requires a deep understanding of the unique challenges posed by LLMs in benchmarking due to their extensive training data collection from the internet. It probes the critical factor of increased contamination risk and the violation of traditional separation principles.", "answer_correctness_score": 9, "explanation_validity_score": 7, "question_clarity_score": 6, "question_groundedness_score": 8, "question_token_count": 28, "avg_answer_token_count": 17}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "Specific Examples of Up-to-Date Data Sources Used in LiveBench", "question": "What specific types of data sources does LiveBench utilize to ensure up-to-date information for its benchmarks?", "choices": {"A": "Recent math competitions from the past 12 months", "B": "Newly published arXiv papers", "C": "Human-written coding problems from online platforms", "D": "Daily updated forecasting questions from prediction markets"}, "answer": "A", "explanation": "LiveBench (White et al., 2024) is described as collecting questions based on the latest information source, but the text does not specify the exact types of data sources used. Given the context, options A, B, and C are plausible sources that align with the benchmark's aim to use recent data, while option D is less likely as LiveBench focuses on questions rather than forecasting questions.", "answer_correctness_score": 10, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 10, "question_token_count": 21, "avg_answer_token_count": 8}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "Comparison Between Different LLM Evaluation Techniques", "question": "What key feature distinguishes S3Eval from other graph-based evaluation techniques like DyVal and NPHardEval?", "choices": {"A": "S3Eval uses natural language descriptions derived from SQL tables.", "B": "S3Eval evaluates reasoning skills using random SQL queries on SQL tables.", "C": "S3Eval assesses the reasoning capabilities using randomly generated DAGs.", "D": "S3Eval tests LLMs on well-known P and NP problems."}, "answer": "B", "explanation": "S3Eval specifically uses SQL queries on randomly generated tables, which is a distinct feature compared to the graph-based techniques that use DAGs, random P/NP problems, or Knights and Knaves puzzles.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "question_token_count": 23, "avg_answer_token_count": 14}
{"context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n", "topic": "The impact of training corpus size on the effectiveness of static benchmarks in LLM evaluation.", "question": "How does the increase in training corpus size affect the effectiveness of static benchmarks in evaluating large language models (LLMs), and what mathematical relationship is suggested for this impact?", "choices": {"A": "It has no effect; static benchmarks remain equally reliable regardless of training corpus size.", "B": "It improves the effectiveness; larger corpora lead to fewer contaminations.", "C": "It decreases the effectiveness; the probability of contamination increases proportionally with the product of training and test set sizes.", "D": "It enhances the effectiveness by reducing the reliance on human annotation."}, "answer": "C", "explanation": "The question invites a deep understanding of the mathematical relationship described in the context and its implications on benchmark reliability. The answer requires recognizing the inverse relationship between contamination probability and the ratio of training to test set sizes.", "answer_correctness_score": 10, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 10, "question_token_count": 34, "avg_answer_token_count": 17}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "Impact of contamination on the validity of LLM benchmarks.", "question": "How does syntactic contamination primarily affect the evaluation of LLMs in benchmarking?", "choices": {"A": "By allowing LLMs to recall memorized training data rather than demonstrating true reasoning.", "B": "By enhancing the LLM's ability to handle unseen data.", "C": "By improving the accuracy of the LLM on syntactically similar tasks.", "D": "By reducing the computational resources needed for benchmarking."}, "answer": "A", "explanation": "Syntactic contamination involves rephrasing test data using prefix strings, derived from the training data. This form of contamination can make it appear that an LLM is recalling memorized information rather than demonstrating true reasoning capability, thereby overestimating its performance.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "question_token_count": 15, "avg_answer_token_count": 13}
{"context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "Quality Assessment of Dynamic Benchmarks", "question": "What critical limitation does dynamic benchmarking aim to overcome compared to static benchmarking, and how does it achieve this through its definition?", "choices": {"A": "Dynamic benchmarking overcomes the limitation of restricted access to the training dataset by modifying the dataset during evaluation.", "B": "It addresses the issue of data contamination by using a transformation function that changes the dataset at each timestamp.", "C": "Dynamic benchmarking aims to provide a more transparent evaluation by always starting from a non-empty seed dataset.", "D": "It ensures faithfulness in evaluation by allowing infinite timestamps for modification."}, "answer": "B", "explanation": "Dynamic benchmarking overcomes the limitation of static benchmarking by introducing a transformation function \\( T(\\cdot) \\) that modifies the dataset during evaluation, thus avoiding data contamination and providing a flexible evaluation framework.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 10, "question_token_count": 25, "avg_answer_token_count": 19}
{"context": "###### Safety\nSafety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content. Datasets such as RealToxicityPrompts\u00a0(Gehman et\u00a0al., 2020) and ToxiGen\u00a0(Hartvigsen et\u00a0al., 2022) assess resilience against producing harmful outputs.\n\nBy providing a controlled environment to measure these aspects, safety benchmarks play a critical role in guiding the development of models that are not only powerful outputs but also responsible and trustworthy for real-world applications.\n\n\n###### Language\nLanguage benchmarks assess the LLMs\u2019 proficiency in specific languages.\n\nGLUE\u00a0(Wang, 2018) and SuperGLUE\u00a0(Wang et\u00a0al., 2019) cover tasks from sentiment analysis to language inference, while CLUE\u00a0(Xu et\u00a0al., 2020) targets Chinese language.\n\nTypo-fixing\u00a0(Suzgun et\u00a0al., 2022) is also widely used.\n\n\n###### Reading Comprehension\nReading comprehension tasks test a model\u2019s ability to extract and infer information from text. Benchmarks like SQuAD\u00a0(Rajpurkar et\u00a0al., 2018), QuAC\u00a0(Choi et\u00a0al., 2018), and BoolQ\u00a0(Clark et\u00a0al., 2019) challenge models to understand passages and draw logical conclusions.\n", "topic": "The design and purpose of reading comprehension benchmarks, such as SQuAD, QuAC, and BoolQ, in assessing LLMs' ability to extract and infer information from text.", "question": "What critical skill do benchmarks like SQuAD, QuAC, and BoolQ primarily assess in large language models (LLMs), and how does this relate to their broader application?", "choices": {"A": "The ability to generate non-toxic and ethically aligned content.", "B": "Proficiency in specific languages, as measured by GLUE and SuperGLUE.", "C": "The capacity to extract and infer information from text, crucial for practical applications.", "D": "The robustness against producing harmful outputs, as evaluated by datasets like RealToxicityPrompts."}, "answer": "C", "explanation": "While safety benchmarks ensure ethical behavior and robustness, reading comprehension benchmarks like SQuAD, QuAC, and BoolQ are specifically designed to test a model's ability to extract and infer information from text, which is fundamental for various practical applications requiring text understanding.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "question_token_count": 37, "avg_answer_token_count": 16}
{"context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n", "topic": "Explain the mathematical formula for measuring correctness in dynamic benchmarks and interpret its components.", "question": "What does the formula for measuring the correctness of dynamic benchmarks represent, and what are the key components involved?", "choices": {"A": "It represents the expected alignment between the transformed dataset's outputs and their corresponding ground truth values.", "B": "It measures the accuracy of the transformation process in generating the dataset.", "C": "It calculates the probability that the benchmark results are reliable.", "D": "It assesses the consistency between the benchmark's output and the ground truth."}, "answer": "A", "explanation": "The formula quantifies the correctness of the dynamic benchmark by comparing the outputs of the transformations to their ground truth values using a scoring function. The key components are the input/output pairs of the transformation, the ground truth provided by an oracle, and the scoring function that evaluates the alignment.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "question_token_count": 22, "avg_answer_token_count": 15}
{"context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n", "topic": "Analysis of model behavior under different conditions to detect data contamination, such as through masked inputs and partial completions.", "question": "How can post-hoc detection methods reveal model behavior under conditions that indicate data contamination, and what techniques are employed to ensure robustness in identifying such behaviors?", "choices": {"A": "By analyzing model responses to masked inputs and partial completions to detect memorization patterns and by comparing performance across different datasets.", "B": "Through direct overlap detection using n-gram matching and embedding-based similarity to identify exact matches and near-matches in the test and train sets.", "C": "By evaluating the model's preference for original versus paraphrased test cases and using improved mapping metrics to assess embedding-based similarity.", "D": "Using post-hoc detection to monitor label protection mechanisms and ensuring transparency in performance metrics."}, "answer": "A", "explanation": "The question requires a deep understanding of the nuances in post-hoc detection methods, particularly how they analyze model behavior under specific conditions to detect data contamination. It challenges the respondent to recognize the importance of various techniques such as masked inputs, partial completions, and embedding-based similarity, while also understanding their role in ensuring robust identification of contamination.", "answer_correctness_score": 10, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 9, "question_token_count": 30, "avg_answer_token_count": 23}
{"context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n", "topic": "Value and Future Potential of Continuous Collection of Benchmarking Methods through GitHub Repository", "question": "What is the primary challenge in the transition from static to dynamic benchmarking methods for LLMs as highlighted in the document, and how does the proposed solution address this challenge?", "choices": {"A": "The primary challenge is the lack of standardized criteria for evaluating dynamic benchmarks, and the proposed solution is to establish a series of optimal design principles for dynamic benchmarking.", "B": "The primary challenge is the inherent limitations of enhancing static benchmarks, and the proposed solution is to focus solely on improving static benchmarks further.", "C": "The primary challenge is the difficulty in collecting enough data for dynamic benchmarks, and the proposed solution is to create a GitHub repository to continuously gather various benchmarking methods.", "D": "The primary challenge is the reliance on Internet-derived training corpora, and the proposed solution is to reduce the size of the training corpora used for LLMs."}, "answer": "A", "explanation": "The question requires a deep understanding of the document's main argument and identifies the critical gap in the current benchmarking practices. It challenges the respondent to recognize the importance of standardization in dynamic benchmarking and the innovative approach of using a GitHub repository for continuous collection and improvement of benchmarking methods.", "answer_correctness_score": 9, "explanation_validity_score": 7, "question_clarity_score": 1, "question_groundedness_score": 1, "question_token_count": 34, "avg_answer_token_count": 31}
{"context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n", "topic": "The potential benefits and challenges of dynamic benchmarking methods for LLMs.", "question": "What key challenge does dynamic benchmarking for LLMs face that limits its current utility despite showing promise?", "choices": {"A": "Lack of standardized evaluation procedures", "B": "Insufficient coverage of recent methods and tools", "C": "High computational costs", "D": "Difficulty in achieving consistent results across different datasets"}, "answer": "A", "explanation": "Dynamic benchmarking shows promise but faces challenges in reliability and reproducibility, which can hinder its current utility despite its potential benefits.", "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 7, "question_groundedness_score": 9, "question_token_count": 20, "avg_answer_token_count": 7}
{"context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n", "topic": "Use of Query Templates with Placeholder Variables in GSM-Symbolic for Benchmark Creation", "question": "How does the use of placeholder variables in GSM-Symbolic's query templates impact the robustness of the generated benchmarks compared to rule-based generation methods, and what are the potential implications for data contamination in LLM evaluations?", "choices": {"A": "It enhances robustness by allowing for a more varied set of problem instances, reducing the likelihood of data contamination due to repeated problem instances.", "B": "It increases the risk of data contamination as placeholders may reuse previously seen problem structures, but it also reduces the collision probability of generated test cases.", "C": "It has no impact on robustness or data contamination since both methods rely on predefined rules and templates.", "D": "It decreases the collision probability significantly but does not address the issue of data contamination in benchmark creation."}, "answer": "A", "explanation": "The correct answer is A. Using placeholder variables in GSM-Symbolic allows for a broader range of problem instances, reducing the chance that any particular problem instance is repeated. This diversity in problem instances makes the benchmarks more robust and less prone to data contamination. Rule-based generation, on the other hand, might generate similar problem instances due to limited rule variations, increasing the risk of contamination.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "question_token_count": 43, "avg_answer_token_count": 24}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "Challenges faced by static benchmarks as LLMs evolve and the impact on benchmark reliability.", "question": "What critical issue arises when using static benchmarks for evaluating the performance of rapidly evolving LLMs, and what is the proposed solution to address this issue?", "choices": {"A": "Static benchmarks may no longer accurately reflect the capabilities of newer models due to their rapid evolution, leading to potential data contamination, but no solution has been proposed yet.", "B": "Static benchmarks may become too easy for advanced LLMs, leading to unreliable results, and dynamic benchmarks have been proposed to adapt to the evolving models.", "C": "Static benchmarks are insufficient for assessing the general-purpose task-solving abilities of LLMs, necessitating the development of new benchmark types.", "D": "Static benchmarks are effective for evaluating LLMs as they remain consistent over time, and no issues arise during rapid model evolution."}, "answer": "B", "explanation": "The correct answer is B. Static benchmarks may become too easy for advanced LLMs, leading to unreliable results. Dynamic benchmarks have been proposed to adapt to the evolving models and address the issue of data contamination.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "question_token_count": 30, "avg_answer_token_count": 28}
{"context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n", "topic": "Explain the mechanism of using a \"No Derivatives\" license in combination with public key encryption to secure test data, and discuss its effectiveness in preventing automated reuse.", "question": "How does combining public key encryption with a \"No Derivatives\" license enhance the security of test data against automated reuse, and what challenges might still exist despite this combination?", "choices": {"A": "Public key encryption ensures data confidentiality, while the \"No Derivatives\" license prevents unauthorized modifications, thus safeguarding against automated reuse.", "B": "The combination allows for secure transmission but fails to address the issue of data being easily extracted through advanced decontamination techniques.", "C": "Both mechanisms ensure data integrity and prevent any form of reuse or modification, making automated attacks ineffective.", "D": "Public key encryption alone is sufficient to prevent automated reuse, making the \"No Derivatives\" license redundant in this context."}, "answer": "B", "explanation": "While public key encryption protects the confidentiality of the data, the \"No Derivatives\" license is intended to prevent unauthorized derivative works. However, advanced decontamination techniques can bypass these protections, making the combination less effective than initially thought.", "answer_correctness_score": 8, "explanation_validity_score": 9, "question_clarity_score": 7, "question_groundedness_score": 9, "question_token_count": 34, "avg_answer_token_count": 23}
{"context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n", "topic": "Definition and Components of Static Benchmarks", "question": "What critical aspect of a static benchmark can most significantly influence the perceived performance of a language model, even if the underlying scoring function is robust and well-defined?", "choices": {"A": "The quality and diversity of the input prompts (\ud835\udcb3).", "B": "The precision and coverage of the expected outputs (\ud835\udcb4).", "C": "The complexity of the scoring function (\ud835\udcae(\u22c5)).", "D": "The size and representativeness of the seed dataset (\ud835\udc9f)."}, "answer": "A", "explanation": "The choice of input prompts can dramatically affect the model's perceived performance because the same scoring function might yield different results based on the nature and variety of the inputs. High-quality, diverse prompts can better showcase the model's capabilities, while poorly chosen prompts might obscure its true abilities.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 1, "question_groundedness_score": 4, "question_token_count": 32, "avg_answer_token_count": 13}
{"context": "###### Safety\nSafety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content. Datasets such as RealToxicityPrompts\u00a0(Gehman et\u00a0al., 2020) and ToxiGen\u00a0(Hartvigsen et\u00a0al., 2022) assess resilience against producing harmful outputs.\n\nBy providing a controlled environment to measure these aspects, safety benchmarks play a critical role in guiding the development of models that are not only powerful outputs but also responsible and trustworthy for real-world applications.\n\n\n###### Language\nLanguage benchmarks assess the LLMs\u2019 proficiency in specific languages.\n\nGLUE\u00a0(Wang, 2018) and SuperGLUE\u00a0(Wang et\u00a0al., 2019) cover tasks from sentiment analysis to language inference, while CLUE\u00a0(Xu et\u00a0al., 2020) targets Chinese language.\n\nTypo-fixing\u00a0(Suzgun et\u00a0al., 2022) is also widely used.\n\n\n###### Reading Comprehension\nReading comprehension tasks test a model\u2019s ability to extract and infer information from text. Benchmarks like SQuAD\u00a0(Rajpurkar et\u00a0al., 2018), QuAC\u00a0(Choi et\u00a0al., 2018), and BoolQ\u00a0(Clark et\u00a0al., 2019) challenge models to understand passages and draw logical conclusions.\n", "topic": "The importance and methods of evaluating LLMs' safety through benchmark datasets.", "question": "Why are safety benchmarks critical for the development of LLMs, and what specific method does RealToxicityPrompts employ to evaluate LLMs?", "choices": {"A": "They ensure LLMs produce non-toxic and ethically aligned content, using controlled environments to assess resilience against harmful outputs.", "B": "They enhance the language proficiency of LLMs by testing their ability to understand and generate content in multiple languages.", "C": "They improve the reading comprehension skills of LLMs by challenging them to extract and infer information from text.", "D": "They solely focus on the efficiency and speed of LLMs in processing large volumes of data."}, "answer": "A", "explanation": "RealToxicityPrompts evaluates LLMs by measuring their resilience against producing harmful outputs, which is a key aspect of safety benchmarks. This method directly addresses the robustness of LLMs in generating non-toxic content.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 8, "question_token_count": 30, "avg_answer_token_count": 21}
{"context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n", "topic": "How hybrid approaches integrate different elements of dynamic benchmark construction.", "question": "How do hybrid approaches in dynamic benchmark construction typically integrate the strengths of rule-based and LLM-based methods?", "choices": {"A": "By combining predefined rules with LLM-generated data to validate each other's accuracy.", "B": "By alternating between rule-based and LLM-based methods to ensure comprehensive coverage.", "C": "By using LLMs to generate data and rules to filter out biased or irrelevant information.", "D": "By manually validating LLM outputs against rule-based criteria to enhance reliability."}, "answer": "A", "explanation": "The correct answer requires understanding that hybrid approaches leverage both predefined rules and LLM-generated data, but the question asks for a deeper integration rather than just validation or coverage. Option A captures this integration most accurately.", "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 4, "question_token_count": 21, "avg_answer_token_count": 15}
{"context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n", "topic": "The limitations of the current survey due to the rapid evolution of LLM development and unaccounted recent methods.", "question": "How does the rapid evolution of LLM development and the potential unaccounted recent methods impact the comprehensiveness and future relevance of the current survey's findings?", "choices": {"A": "The rapid evolution means that the survey might have overlooked critical new methods, reducing its comprehensiveness and future relevance.", "B": "Recent developments will likely be incorporated into future surveys, so the current findings remain robust.", "C": "The unaccounted recent methods suggest that the survey's findings are broadly applicable but require periodic updates.", "D": "The survey's focus on high-level concepts ensures its comprehensiveness and future relevance despite the rapid evolution."}, "answer": "A", "explanation": "The correct answer is A because the rapid evolution of LLM development implies that many new and potentially crucial methods may not have been included in the survey. This makes the survey less comprehensive and reduces its future relevance as newer, more relevant methods continue to emerge.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "question_token_count": 32, "avg_answer_token_count": 21}
{"context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n", "topic": "Template-Based MMLU-CF Generation Process for Multiple-Choice Questions", "question": "How does MMLU-CF ensure the authenticity and diversity of its generated multiple-choice questions?", "choices": {"A": "By shuffling answer choices and replacing incorrect options with \"None of the other choices.\"", "B": "Using predefined rules to minimize collision probability.", "C": "Applying query templates with placeholder variables that are dynamically filled.", "D": "Relying on temporal cutoffs to incorporate recent competition data."}, "answer": "A", "explanation": "MMLU-CF ensures the authenticity and diversity of its generated questions through the process of shuffling answer choices and replacing incorrect options with \"None of the other choices,\" which maintains the integrity of the multiple-choice format while introducing variability. Options A, C, and D describe other methods but do not align with the specific approach used by MMLU-CF as detailed in the context.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "question_token_count": 20, "avg_answer_token_count": 13}
{"context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n", "topic": "Understanding and Mitigating In-distribution Contamination in LLM Benchmarks", "question": "How does Auto-Dataset ensure that the generated new samples retain both the stylistics and essential knowledge of the original samples while also presenting related questions at different cognitive levels?", "choices": {"A": "By employing LLMs to generate new samples that preserve the original style and knowledge but also introduce variations in complexity.", "B": "Through the use of knowledge graphs to expand on the original concepts, thereby creating related questions at varying cognitive levels.", "C": "By leveraging a contamination detector to first identify contaminated samples before prompting LLMs to rewrite them.", "D": "Utilizing LLMs to replace variables in the original samples, thus generating new samples that are structurally similar but conceptually distinct."}, "answer": "A", "explanation": "The correct answer involves understanding that Auto-Dataset generates new samples by preserving the original style and knowledge but also introduces different cognitive levels through varied question formulations. The other options either refer to different techniques or miss the key aspect of retaining both style and knowledge.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 10, "question_token_count": 34, "avg_answer_token_count": 22}
{"context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n", "topic": "Privacy and security concerns associated with dynamic benchmarks and strategies to address them.", "question": "What is a crucial step in designing dynamic benchmarks to address privacy and security concerns, considering the continuous collection and updating of data?", "choices": {"A": "Implementing strong encryption and anonymization techniques for data.", "B": "Establishing strict access controls and auditing mechanisms.", "C": "Regularly conducting independent audits of the benchmarking process.", "D": "Publishing detailed documentation on the data sources and collection methods."}, "answer": "A", "explanation": "The correct approach involves a multi-layered strategy that includes robust data protection measures and oversight to ensure privacy and security.", "answer_correctness_score": 10, "explanation_validity_score": 8, "question_clarity_score": 4, "question_groundedness_score": 8, "question_token_count": 26, "avg_answer_token_count": 11}
{"context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "Structure of a Dynamic Benchmark Dataset", "question": "What is the primary reason for employing a transformation function \\( T(\\cdot) \\) in a dynamic benchmark dataset and how does it impact the evaluation of LLMs?", "choices": {"A": "To ensure the static benchmark dataset remains unchanged during evaluation, avoiding any form of data contamination.", "B": "To modify the data set dynamically to prevent possible data contamination, ensuring a fair evaluation of LLMs.", "C": "To facilitate easier access to the training dataset, thus enhancing the transparency of the evaluation process.", "D": "To increase the memorization capacity of the LLMs being evaluated, thereby improving their performance metrics."}, "answer": "B", "explanation": "The transformation function \\( T(\\cdot) \\) is crucial because it modifies the data set during benchmarking to avoid possible data contamination, which impacts the evaluation of LLMs by ensuring fairness and integrity of the results. Option A is incorrect because the function's purpose is to change the dataset, not keep it unchanged. Option C is incorrect as the function's main goal is not related to accessing the training dataset but rather to preventing contamination. Option D is incorrect because increasing memorization is not the goal; instead, the focus is on maintaining the fidelity of the evaluation.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 9, "question_token_count": 33, "avg_answer_token_count": 19}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "Strategies for Identifying and Mitigating Data Contamination in LLM Training Data", "question": "In the context of mitigating data contamination in LLM training data, which of the following scenarios would be classified as syntactic contamination, assuming no exact duplicates exist?", "choices": {"A": "A test example containing a typo that matches a training example exactly upon correcting the typo.", "B": "A test example identical to a training example except for minor punctuation differences.", "C": "A test example that, through a series of morphological variations, closely mirrors a training example's lexical meaning but maintains distinct wording.", "D": "A test example where a code snippet from a benchmark implementation is included in the training corpus verbatim."}, "answer": "C", "explanation": "Syntactic contamination involves finding a test data point in the training dataset after applying syntactic transformations such as punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning. Option C accurately describes this scenario, as it involves transforming a training example through these methods to create a test example with a similar meaning but different wording.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "question_token_count": 31, "avg_answer_token_count": 20}
{"context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n", "topic": "Data Contamination Risk in Live Benchmarks Using Recent Competition Problems", "question": "How does the risk of data contamination in live benchmarks using recent competition problems affect the reliability of LLM evaluations, and what method could mitigate this issue?", "choices": {"A": "It has no significant impact on reliability since recent problems are sufficiently different.", "B": "It can lead to biased results due to reused problems, and rule-based generation can mitigate this by synthesizing new test cases.", "C": "It improves reliability by ensuring that LLMs are tested on current trends.", "D": "It decreases the need for continuous human updates to the benchmark systems."}, "answer": "B", "explanation": "The context highlights the risk of data contamination in live benchmarks using recent competition problems, which can lead to biased results. Rule-based generation can mitigate this issue by synthesizing new test cases, thereby reducing the likelihood of reusing previously seen problems.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 8, "question_token_count": 30, "avg_answer_token_count": 17}
{"context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n", "topic": "The importance of complexity control in the design of efficient LLM benchmarks.", "question": "What is the primary reason for the inefficiency in certain dynamic benchmarks despite their advantages over static benchmarks?", "choices": {"A": "Insufficient complexity control leading to overly simplistic or redundant sample generation.", "B": "Lack of standardized evaluation criteria hindering comparison across different benchmarks.", "C": "Over-reliance on human annotation for reliability, increasing costs and time.", "D": "High computational requirements for real-time generation of large datasets."}, "answer": "A", "explanation": "The context mentions that \"We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\" This indicates that complexity control is crucial for efficiency but is often overlooked in certain dynamic benchmarks.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 8, "question_token_count": 21, "avg_answer_token_count": 14}
{"context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n", "topic": "Dynamic benchmark generation using multi-agent frameworks, as demonstrated in Benchmark Self-Evolving and BENCHAGENTS.", "question": "How does BENCHAGENTS utilize multi-agent frameworks to enhance the dynamic benchmark generation process compared to traditional static benchmarks?", "choices": {"A": "By splitting the benchmark creation process into distinct phases such as planning, generation, verification, and evaluation, each managed by specialized LLM agents.", "B": "Through the continuous evolution of benchmarks by adding new tasks and subtasks based on agent collaboration and feedback.", "C": "By employing a human-in-the-loop approach to provide immediate feedback and guide the benchmark's development.", "D": "Utilizing a single LLM to generate a wide range of questions and tasks, reducing the complexity of the benchmarking process."}, "answer": "A", "explanation": "BENCHAGENTS uses a multi-agent framework that divides the benchmark creation into multiple stages, each handled by different agents, which allows for more structured and scalable benchmark generation. This approach is fundamentally different from traditional static benchmarks that do not evolve dynamically.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 10, "question_token_count": 24, "avg_answer_token_count": 23}
{"context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "Definition and Rationale Behind Dynamic Benchmarking", "question": "What is the primary rationale behind using dynamic benchmarking over static benchmarking in evaluating Large Language Models (LLMs)?", "choices": {"A": "To adapt the benchmarking process to changes in the LLM's performance over time by dynamically altering the evaluation datasets.", "B": "To reduce the computational resources required for benchmarking by limiting the size of the datasets used.", "C": "To simplify the evaluation process by using a fixed, static dataset that remains unchanged throughout the testing period.", "D": "To ensure complete access to the training dataset, thereby avoiding legal and privacy constraints."}, "answer": "A", "explanation": "The primary rationale behind dynamic benchmarking is its ability to adapt to changes in the LLM's performance over time by dynamically altering the evaluation datasets. This approach addresses the limitations of static benchmarking, particularly the challenges posed by legal and privacy constraints and varying assumptions about model behavior.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "question_token_count": 22, "avg_answer_token_count": 19}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "Examples of Exact and Syntactic Contamination in LLM Training and Testing Data", "question": "Which of the following scenarios exemplifies syntactic contamination in the context of LLM training and testing data?", "choices": {"A": "An exact match of a test example found in the training dataset.", "B": "A test example with slightly altered punctuation but retains its original meaning, found in the training dataset.", "C": "A new test example created by combining two different training examples.", "D": "A test example with completely different wording but identical meaning, found in the training dataset."}, "answer": "B", "explanation": "Syntactic contamination refers to situations where a test data point can be found in the training dataset after undergoing syntactic transformations such as punctuation normalization, whitespace modification, or synonym substitution, while preserving the original meaning. Therefore, option B correctly describes syntactic contamination.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 5, "question_groundedness_score": 9, "question_token_count": 20, "avg_answer_token_count": 16}
{"context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n", "topic": "Discuss the role of the oracle function \ud835\udca2\u2062(\u22c5) in ensuring the correctness of dynamic benchmarks and provide examples of what this function could be.", "question": "What are two different possible implementations of the oracle function \\( \\mathcal{G}(\\cdot) \\) that could be used in ensuring the correctness of a dynamic benchmark, and what are the implications of each implementation?", "choices": {"A": "A domain-specific annotator and a machine learning model trained on ground truth data.", "B": "A random number generator and a pre-defined set of rules.", "C": "An expert human evaluator and a neural network model.", "D": "A statistical model and a deterministic algorithm."}, "answer": "A", "explanation": "The correct implementation of \\( \\mathcal{G}(\\cdot) \\) depends on the nature of the data and the context. A domain-specific annotator or a machine learning model trained on ground truth data can ensure high accuracy but may be costly and time-consuming. On the other hand, a neural network model or an expert human evaluator might offer more flexibility and adaptability but could introduce biases or inconsistencies. A random number generator or pre-defined rules would be simplistic and may not capture complex patterns in the data.", "answer_correctness_score": 10, "explanation_validity_score": 8, "question_clarity_score": 6, "question_groundedness_score": 6, "question_token_count": 41, "avg_answer_token_count": 12}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "Assessment of LLM Reasoning Using Directed Acyclic Graphs (DAGs)", "question": "How does the evaluation process of LLMs using directed acyclic graphs (DAGs) ensure a controlled assessment of reasoning capabilities?", "choices": {"A": "By constructing DAGs with varying numbers of nodes and edges to adjust the difficulty level and then converting them into natural language descriptions.", "B": "By synthesizing random tables and querying LLMs for specific values within the tables.", "C": "By generating random graphs and evaluating LLMs on well-known P and NP problems like the Traveling Salesman Problem.", "D": "By automatically constructing Knights and Knaves puzzles with random reasoning graphs and querying LLMs."}, "answer": "A", "explanation": "The correct answer involves understanding the multi-step process of creating DAGs, adjusting their complexity, converting them into natural language, and querying the LLMs to test reasoning. Option A accurately describes this process, whereas the other options pertain to different evaluation methods (tables, NP-hard problems, and puzzles).", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 10, "question_token_count": 25, "avg_answer_token_count": 21}
{"context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n", "topic": "Methods for ensuring fairness in dynamic benchmarks while minimizing data contamination.", "question": "How can dynamic benchmarks ensure fairness and minimize data contamination without relying on extensive manual validation?", "choices": {"A": "By employing rule-based or manually crafted transformations.", "B": "Through the use of advanced explainability tools and human-in-the-loop validation.", "C": "Implementing hybrid approaches combining various dynamic benchmark techniques.", "D": "Utilizing temporal cutoff for data collection and analysis."}, "answer": "B", "explanation": "The correct answer requires understanding the nuances of ensuring fairness and minimizing data contamination in dynamic benchmarks, which goes beyond the scope of the provided context. The question encourages reflection on the limitations of the current methods and the need for more sophisticated techniques.", "answer_correctness_score": 10, "explanation_validity_score": 8, "question_clarity_score": 7, "question_groundedness_score": 8, "question_token_count": 18, "avg_answer_token_count": 12}
{"context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n", "topic": "ITD's Use of a Contamination Detector for Sample Rewriting", "question": "How does the contamination detector in ITD's method ensure that the rewritten samples retain their difficulty levels, and what are the potential challenges in achieving this?", "choices": {"A": "By prompting an LLM to rewrite samples after identification, without altering the difficulty.", "B": "Through direct modification of the original sample's content to remove contamination.", "C": "By using a contamination detector to identify contaminated samples, and then prompting an LLM to rewrite them while preserving the original difficulty.", "D": "The contamination detector automatically adjusts the difficulty level based on the identified contamination."}, "answer": "C", "explanation": "The correct answer is C because ITD first identifies contaminated samples using a detector and then prompts an LLM to rewrite them, ensuring the difficulty remains intact. This requires a deep understanding of the interaction between the detector and the rewriting process. Options A and B oversimplify the process, while D incorrectly suggests automatic adjustment.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 7, "question_token_count": 31, "avg_answer_token_count": 17}
{"context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n", "topic": "Transition from Static to Dynamic Benchmarking Methods in LLMs", "question": "What is a critical challenge in adopting dynamic benchmarking methods for LLMs, and how does it relate to the limitations of current static benchmarks?", "choices": {"A": "Ensuring the standardized evaluation criteria for dynamic benchmarks is crucial because current static benchmarks lack such standards.", "B": "The primary challenge lies in the computational cost of dynamically updating benchmarks frequently.", "C": "Dynamic benchmarks are less reliable than static benchmarks due to their complexity.", "D": "There is no significant difference between static and dynamic benchmarks in terms of mitigating data contamination."}, "answer": "A", "explanation": "The answer requires a deep understanding of the context, particularly the need for standardized criteria in dynamic benchmarks and the limitations of current static benchmarks.", "answer_correctness_score": 9, "explanation_validity_score": 8, "question_clarity_score": 7, "question_groundedness_score": 9, "question_token_count": 28, "avg_answer_token_count": 16}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "Definition and examples of syntactic contamination in LLM datasets.", "question": "How does syntactic contamination affect the evaluation of LLMs in applications that primarily depend on syntactic information?", "choices": {"A": "It provides a more accurate assessment of the model's reasoning capabilities.", "B": "It may lead to overestimating the model's ability to handle unseen data by leveraging memorized information.", "C": "It enhances the model's ability to generalize across different contexts.", "D": "It has no significant impact on the evaluation process."}, "answer": "B", "explanation": "The correct answer is B because syntactic contamination can lead to models recalling memorized information rather than demonstrating true reasoning capabilities, thus potentially overestimating their performance.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "question_token_count": 20, "avg_answer_token_count": 14}
{"context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n", "topic": "Discuss the advantages and disadvantages of employing robust encryption methods versus label protection in maintaining the integrity of model evaluation, using specific examples from the literature.", "question": "What are the key differences between employing robust encryption methods and label protection in maintaining the integrity of model evaluation, and what are their respective advantages and disadvantages as highlighted in the literature?", "choices": {"A": "Robust encryption ensures data confidentiality but introduces computational overhead and relies on strong key management; label protection hides true answers to prevent model learning, but may still allow model exposure through other means.", "B": "Robust encryption prevents data leakage and model exposure but requires complex key management; label protection mitigates risk of data contamination but does not protect the integrity of the test set itself.", "C": "Robust encryption and label protection both prevent model exposure but differ in implementation complexity and reliance on key management; robust encryption adds computational load while label protection does not.", "D": "Robust encryption and label protection both require strong key management but differ in their impact on model learning; robust encryption can be bypassed with minor text variations, while label protection can be compromised if test labels are leaked."}, "answer": "A", "explanation": "This question requires a deep understanding of both encryption methods and label protection, their advantages, and their limitations as described in the literature. It asks for a nuanced comparison that reflects the complexities and trade-offs involved in choosing between these two approaches.", "answer_correctness_score": 9, "explanation_validity_score": 8, "question_clarity_score": 6, "question_groundedness_score": 9, "question_token_count": 36, "avg_answer_token_count": 36}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "Techniques for continuously updating benchmark datasets to minimize data contamination.", "question": "What is a key challenge in implementing continuous updates to benchmark datasets for LLMs to ensure they are not contaminated, and how does this differ from traditional static benchmarking methods?", "choices": {"A": "Ensuring that the latest training data is excluded from the benchmark dataset.", "B": "Guaranteeing that the benchmark dataset remains static over time to maintain consistency.", "C": "Regularly refreshing the benchmark dataset to reflect recent training data while avoiding contamination.", "D": "Limiting the size of the benchmark dataset to reduce complexity."}, "answer": "C", "explanation": "The key challenge is to refresh the benchmark dataset regularly to reflect recent training data without including it in the dataset, which is a significant departure from static benchmarking where datasets remain unchanged over time.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 8, "question_token_count": 34, "avg_answer_token_count": 15}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The concept of contamination detectors and their role in quantifying contamination risks in benchmarks.", "question": "How do contamination detectors contribute to the development of robust benchmarks for evolving LLMs?", "choices": {"A": "By identifying outdated aspects of the benchmarks that no longer accurately reflect model performance.", "B": "By quantifying the risk of data contamination and helping to update benchmarks dynamically.", "C": "By assessing the ability of LLMs to follow instructions and generate code.", "D": "By evaluating the overall performance of LLMs across multiple tasks and scenarios."}, "answer": "B", "explanation": "Contamination detectors specifically aim to measure and address the risk of data contamination in benchmarks, which is crucial as LLMs evolve and new data is continually incorporated. This ensures that benchmarks remain relevant and accurately reflect the true capabilities of the models.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "question_token_count": 17, "avg_answer_token_count": 15}
{"context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n", "topic": "Mathematical Representation of Scalability", "question": "What does the term \\(\\left\\lVert T_i(\\mathcal{D}) \\right\\rVert / \\left\\lVert \\mathcal{D} \\right\\rVert\\) in the scalability equation represent, and how does the cost function \\( \\textsf{Cost}(\\cdot) \\) influence the overall measure of scalability?", "choices": {"A": "The ratio of the size of the transformed dataset to the original dataset, with no impact from the cost function.", "B": "The proportion of data that can be generated per unit cost, with the cost function affecting the overall measure.", "C": "The efficiency of the transformation process, independent of the cost function.", "D": "The total cost of the transformation process divided by the size of the original dataset."}, "answer": "B", "explanation": "The term \\(\\left\\lVert T_i(\\mathcal{D}) \\right\\rVert / \\left\\lVert \\mathcal{D} \\right\\rVert\\) represents the ratio of the size of the transformed dataset to the original dataset. However, the overall measure of scalability is influenced by the cost function \\( \\textsf{Cost}(\\cdot) \\), which accounts for the monetary, time, or manual effort costs associated with the transformation process.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "question_token_count": 62, "avg_answer_token_count": 19}
{"context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n", "topic": "Techniques Employed by LLMs to Rewrite Benchmark Samples", "question": "What is the primary method used by the ITD technique to ensure the rewritten samples maintain their original difficulty levels while addressing contamination?", "choices": {"A": "Prompting LLMs to rewrite samples while preserving stylistics.", "B": "Expanding on the original concepts using knowledge graphs.", "C": "Utilizing a contamination detector to identify and prompt LLMs to rewrite samples.", "D": "Identifying and replacing variables in the samples."}, "answer": "C", "explanation": "The correct answer is C, as the ITD technique specifically mentions using a contamination detector to identify contaminated samples and prompting an LLM to rewrite them while preserving their difficulty levels. This requires understanding the unique approach of ITD in handling contaminated samples compared to other techniques.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 1, "question_groundedness_score": 8, "question_token_count": 26, "avg_answer_token_count": 12}
{"context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n", "topic": "Analyzing the Impact of Data Contamination on LLM Evaluation Through Dynamic Benchmarking Metrics", "question": "How does the collision rate and repeat trials metric together influence the reliability of dynamic benchmarking in evaluating LLMs' true capabilities, considering the risk of data contamination?", "choices": {"A": "By measuring the overlap of transformed datasets, collision rate highlights potential contamination, while repeat trials indicate the ease of regenerating existing transformations.", "B": "They ensure that each benchmark trial is unique, thereby completely eliminating data contamination risks.", "C": "These metrics help in assessing the robustness of the benchmark by identifying both the extent of overlap and the variability of generated test cases.", "D": "Collision rate alone is sufficient to ensure the benchmark's effectiveness in LLM evaluation."}, "answer": "C", "explanation": "The correct answer requires understanding that both collision rate and repeat trials are interrelated and necessary for comprehensive evaluation. Collision rate identifies overlap, which indicates potential contamination, while repeat trials measure the variability and uniqueness of test cases.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "question_token_count": 32, "avg_answer_token_count": 21}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "Test a model's intuitive reasoning skills using benchmarks like PIQA, SIQA, HellaSwag, and WinoGrande.", "question": "What unique aspects of human reasoning are tested by benchmarks like PIQA, SIQA, HellaSwag, and WinoGrande, and how do they differ from other types of language comprehension benchmarks?", "choices": {"A": "These benchmarks test a model\u2019s ability to apply everyday knowledge and logical reasoning in scenarios that require nuanced understanding and inference, distinguishing them from coding and instruction-following benchmarks which focus more on procedural tasks.", "B": "They assess a model\u2019s capacity for creative problem-solving and complex logical reasoning, setting them apart from benchmarks that primarily test fact retrieval or syntactic understanding.", "C": "These benchmarks evaluate a model\u2019s skill in debugging code and synthesizing code from instructions, making them different from reasoning benchmarks that focus on common sense and everyday knowledge application.", "D": "They measure a model\u2019s proficiency in understanding and executing detailed step-by-step instructions, distinguishing them from reasoning benchmarks that deal with broader, more abstract concepts."}, "answer": "B", "explanation": "The correct answer is B, as PIQA, SIQA, HellaSwag, and WinoGrande specifically target intuitive reasoning, logical deduction, and the application of common sense in scenarios that require nuanced understanding and inference. This contrasts with coding and instruction-following benchmarks, which focus more on procedural tasks and step-by-step execution.", "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 9, "question_token_count": 42, "avg_answer_token_count": 33}
{"context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n", "topic": "Multi-round interaction techniques in evaluating LLMs, including their application in LLM-as-an-Interviewer and TreeEval.", "question": "Which of the following best describes the key difference between the interactive evaluation techniques (like LLM-as-an-Interviewer and TreeEval) and the multi-agent evaluation techniques (like Benchmark Self-Evolving and BENCHAGENTS)?", "choices": {"A": "Interactive techniques focus on evaluating models through structured, multi-round interactions designed to simulate human-like questioning, while multi-agent techniques involve automated agents working together to create dynamic benchmarks.", "B": "Interactive techniques involve multiple agents working together to create benchmarks, while multi-agent techniques focus on evaluating models through structured, multi-round interactions.", "C": "Interactive techniques are less effective than multi-agent techniques because they rely on human-like questioning rather than automated processes.", "D": "Interactive techniques generate follow-up questions based on the model's response, whereas multi-agent techniques do not involve follow-up questioning."}, "answer": "A", "explanation": "Option A accurately captures the essence of the distinction between interactive and multi-agent evaluation techniques. Interactive methods such as LLM-as-an-Interviewer and TreeEval are designed to simulate a human-like conversation to evaluate the model, while multi-agent methods like Benchmark Self-Evolving and BENCHAGENTS leverage automation and cooperation among multiple agents to create and evolve benchmarks.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "question_token_count": 45, "avg_answer_token_count": 26}
{"context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n", "topic": "The need for further refinement and validation of dynamic benchmarking criteria in real-world applications.", "question": "What is the key challenge that future research should address to improve the reliability of dynamic benchmarking criteria in LLMs?", "choices": {"A": "Ensuring that benchmarking criteria are static and unchanging.", "B": "Developing more sophisticated data collection methods for static benchmarks.", "C": "Standardizing and validating dynamic benchmarking criteria in diverse real-world applications.", "D": "Reducing the size of training datasets to minimize contamination."}, "answer": "C", "explanation": "The context emphasizes the need for further refinement and validation of dynamic benchmarking criteria in real-world applications, highlighting the current challenges in reliability and reproducibility. Option C directly addresses this need by focusing on standardizing and validating these criteria across different scenarios.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "question_token_count": 23, "avg_answer_token_count": 12}
{"context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "Transformation Function in Dynamic Benchmarking", "question": "What is the primary role of the transformation function \\( T(\\cdot) \\) in dynamic benchmarking?", "choices": {"A": "To modify the evaluation data set dynamically to mitigate potential data contamination.", "B": "To ensure full access to the training dataset is maintained.", "C": "To provide a static evaluation environment that remains constant over time.", "D": "To enhance the memorization capabilities of the LLM being evaluated."}, "answer": "A", "explanation": "The transformation function \\( T(\\cdot) \\) is crucial in dynamic benchmarking as it adapts the dataset during evaluation to avoid contamination, which aligns with the description of modifying the data set to avoid possible data contamination.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "question_token_count": 20, "avg_answer_token_count": 13}
{"context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n", "topic": "The benefits and limitations of LLM-based generation approaches in creating novel evaluation data points.", "question": "How does the transparency of LLMs influence the reliability and interpretability of data generated for evaluating LLMs?", "choices": {"A": "Transparency in LLMs enhances interpretability, but limits the scale of generated data.", "B": "Transparency in LLMs is crucial for ensuring the reliability and interpretability of generated data, necessitating additional validation mechanisms.", "C": "Transparency in LLMs reduces the need for manual validation, thus increasing efficiency.", "D": "Transparency in LLMs has no significant impact on the reliability or interpretability of generated data."}, "answer": "B", "explanation": "The correct answer is B because LLMs' transparency is essential for ensuring reliability and interpretability, often requiring additional validation mechanisms like explainability tools or human-in-the-loop validation to maintain trustworthiness.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 9, "question_token_count": 22, "avg_answer_token_count": 19}
{"context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n", "topic": "The comprehensiveness of the survey in covering high-level concepts versus detailed technical implementation guidelines.", "question": "What aspect does the survey emphasize less compared to its coverage of high-level concepts?", "choices": {"A": "Comprehensive technical implementation guidelines", "B": "Detailed analysis of static benchmarking methods", "C": "Evaluation of dynamic benchmarking methods' reliability", "D": "Overview of recent advancements in LLM benchmarking"}, "answer": "A", "explanation": "The survey focuses more on high-level concepts and less on providing detailed technical implementation guidelines, which limits its applicability for practitioners needing in-depth implementation details.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 1, "question_groundedness_score": 9, "question_token_count": 17, "avg_answer_token_count": 7}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The limitations and challenges associated with current dynamic benchmarking methods.", "question": "What is a significant limitation of current dynamic benchmarking methods in mitigating data contamination in large language models?", "choices": {"A": "They rely too heavily on timestamps for updating benchmark datasets, which may not accurately reflect the latest training data.", "B": "They do not provide clear criteria for evaluating the effectiveness of dynamic benchmarks themselves.", "C": "They overemphasize post-hoc contamination detection rather than preventing contamination during the training phase.", "D": "They are too complex to implement, requiring significant computational resources and expertise."}, "answer": "B", "explanation": "The correct answer is B, as the paper explicitly states that no existing work discusses criteria for evaluating dynamic benchmarks themselves, indicating a significant gap in the current methodology.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 9, "question_groundedness_score": 8, "question_token_count": 20, "avg_answer_token_count": 18}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "Impact of Graph Size on LLM Performance in NP-Hard Problems", "question": "How does the variation in graph size affect an LLM's performance in solving NP-hard problems, as evidenced by the evaluation frameworks discussed in the context?", "choices": {"A": "Larger graphs enhance LLMs' ability to generalize and handle complex problem structures effectively.", "B": "Smaller graphs are more challenging for LLMs due to the increased complexity in reasoning.", "C": "Graph size has no significant impact on LLM performance since LLMs are equally adept at handling all sizes.", "D": "The optimal graph size for LLM performance varies depending on the specific NP-hard problem being evaluated."}, "answer": "A", "explanation": "This question challenges the domain expert to consider the intricacies of how different graph sizes influence LLM performance in NP-hard problems. It requires a deep understanding of the evaluation frameworks discussed and the nuances of graph complexity.", "answer_correctness_score": 6, "explanation_validity_score": 4, "question_clarity_score": 8, "question_groundedness_score": 9, "question_token_count": 30, "avg_answer_token_count": 19}
{"context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n", "topic": "Evaluating the effectiveness of post-hoc detection methods like CONSTAT in identifying contamination across various benchmarks.", "question": "How does the CONSTAT method by Dekoninck et al. (2024) contribute to the evaluation of model performance across different benchmarks in detecting data contamination, and what challenges might arise in its application?", "choices": {"A": "By comparing model performance consistency across benchmarks, it helps identify contamination but may overlook subtle shifts in model behavior.", "B": "It enhances data integrity by directly removing contaminated samples before training but faces difficulties in large-scale datasets.", "C": "It relies on token-level n-gram matching to detect overlaps, ensuring high accuracy but limiting scalability.", "D": "It uses embedding-based similarity to detect contamination, providing a flexible approach that can handle variations in benchmark datasets."}, "answer": "A", "explanation": "The correct answer is A. While CONSTAT compares model performance consistency across benchmarks to detect contamination, it might miss subtle changes in model behavior that could indicate contamination. Options B, C, and D describe other aspects of post-hoc detection methods but do not capture the unique contribution and limitations of the CONSTAT method in the context of evaluating model performance across different benchmarks.", "answer_correctness_score": 10, "explanation_validity_score": 9, "question_clarity_score": 7, "question_groundedness_score": 10, "question_token_count": 42, "avg_answer_token_count": 21}
{"context": "###### Safety\nSafety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content. Datasets such as RealToxicityPrompts\u00a0(Gehman et\u00a0al., 2020) and ToxiGen\u00a0(Hartvigsen et\u00a0al., 2022) assess resilience against producing harmful outputs.\n\nBy providing a controlled environment to measure these aspects, safety benchmarks play a critical role in guiding the development of models that are not only powerful outputs but also responsible and trustworthy for real-world applications.\n\n\n###### Language\nLanguage benchmarks assess the LLMs\u2019 proficiency in specific languages.\n\nGLUE\u00a0(Wang, 2018) and SuperGLUE\u00a0(Wang et\u00a0al., 2019) cover tasks from sentiment analysis to language inference, while CLUE\u00a0(Xu et\u00a0al., 2020) targets Chinese language.\n\nTypo-fixing\u00a0(Suzgun et\u00a0al., 2022) is also widely used.\n\n\n###### Reading Comprehension\nReading comprehension tasks test a model\u2019s ability to extract and infer information from text. Benchmarks like SQuAD\u00a0(Rajpurkar et\u00a0al., 2018), QuAC\u00a0(Choi et\u00a0al., 2018), and BoolQ\u00a0(Clark et\u00a0al., 2019) challenge models to understand passages and draw logical conclusions.\n", "topic": "The structure and goals of safety benchmarks in measuring LLMs' resilience against producing harmful outputs.", "question": "What is the primary objective of safety benchmarks in the context of Large Language Models (LLMs)?", "choices": {"A": "To measure the LLM's ability to produce non-toxic and ethically aligned content under controlled environments.", "B": "To evaluate the LLM's proficiency in specific languages like English and Chinese.", "C": "To test the LLM's capacity for language translation between different languages.", "D": "To assess the LLM's performance in reading comprehension tasks."}, "answer": "A", "explanation": "The primary objective of safety benchmarks is to ensure that LLMs generate non-toxic and ethically aligned content. This is directly stated in the context: \"Safety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content.\"", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 9, "question_groundedness_score": 10, "question_token_count": 20, "avg_answer_token_count": 15}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "Advancements in dynamic benchmarking techniques for LLMs.", "question": "How does the proposed framework for evaluating dynamic benchmarks differ from traditional static benchmarks in terms of addressing data contamination, and what are the specific criteria outlined for dynamic benchmarks?", "choices": {"A": "It evaluates the dynamic benchmarks based on their ability to adapt and update in real-time to minimize contamination, whereas static benchmarks are evaluated based on their initial design.", "B": "It emphasizes the importance of continuous improvement and adaptation through dynamic updates, while static benchmarks rely on predefined datasets.", "C": "It introduces a set of criteria for evaluating dynamic benchmarks that includes real-time adaptability, transparency, and robustness against contamination, distinguishing it from static benchmarks that focus on initial validation.", "D": "It relies solely on post-hoc contamination detection methods to ensure dynamic benchmarks are free from contamination, similar to static benchmarks."}, "answer": "C", "explanation": "The correct answer is C. The proposed framework for evaluating dynamic benchmarks introduces specific criteria that emphasize real-time adaptability, transparency, and robustness against contamination. This distinguishes it from traditional static benchmarks, which focus more on initial validation and predefined datasets.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "question_token_count": 33, "avg_answer_token_count": 28}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "Evaluation Methods for LLMs Using Random SQL Queries", "question": "How might an evaluation method that uses random SQL queries differ fundamentally from those using graph-based approaches, and what implication does this have for assessing LLMs' reasoning abilities?", "choices": {"A": "It measures LLMs' capacity to handle structured data versus logical reasoning tasks, indicating that graph-based methods better capture complex logical dependencies.", "B": "It focuses solely on LLMs' database query optimization skills rather than their general reasoning capabilities.", "C": "It assesses LLMs' ability to perform arithmetic operations within database contexts, which is less relevant for evaluating logical reasoning.", "D": "It evaluates LLMs' proficiency in natural language processing rather than their reasoning skills."}, "answer": "A", "explanation": "The question requires a deep understanding of the fundamental differences between table-based and graph-based evaluation methods. While SQL query evaluations test LLMs' ability to manipulate structured data, graph-based methods assess more complex logical reasoning tasks, making them better suited for evaluating broader reasoning abilities.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "question_token_count": 34, "avg_answer_token_count": 21}
{"context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n", "topic": "Evaluate the potential vulnerabilities of encryption-based methods to minor text variations and suggest strategies to mitigate these risks.", "question": "What strategic approach can be taken to mitigate the risk of minor text variations compromising encryption-based methods for securing evaluation data?", "choices": {"A": "Implementing strict access controls and regular key rotation to minimize the risk of private keys being exposed.", "B": "Utilizing hash functions and checksums to detect and prevent unauthorized alterations to encrypted texts.", "C": "Enhancing encryption algorithms to be more resilient against small changes in plaintext.", "D": "Developing a robust labeling system where labels are obfuscated and only accessible to authorized evaluators."}, "answer": "B", "explanation": "The correct answer is B because hash functions and checksums can help detect any unauthorized modifications to the encrypted data, thereby providing an additional layer of security. While options A, C, and D are valid strategies, they do not directly address the issue of minor text variations in the encrypted data, which is a focus of the question.", "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 4, "question_groundedness_score": 9, "question_token_count": 24, "avg_answer_token_count": 18}
{"context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n", "topic": "Adherence to Mathador Games Rules in Mathador-LM for Dynamic Math Benchmark Generation", "question": "How does Mathador-LM ensure the validity and uniqueness of its generated queries compared to template-based methods?", "choices": {"A": "By strictly following the rules of Mathador games, thus minimizing collisions and ensuring adherence to game-specific constraints.", "B": "Through frequent human verification of each generated query.", "C": "By using complex algorithms to predict and avoid duplicate queries.", "D": "By relying on recent mathematical competition problems for inspiration."}, "answer": "A", "explanation": "Mathador-LM's unique approach lies in its strict adherence to predefined rules from the Mathador games, which minimizes collisions and ensures that each query adheres to specific game constraints, unlike template-based methods which may introduce variability through placeholders and randomization.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 8, "question_token_count": 22, "avg_answer_token_count": 13}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "Debate surrounding syntactic transformations as a form of data contamination.", "question": "What is the primary argument against considering syntactic transformations as a form of data contamination in the context of evaluating language models?", "choices": {"A": "Syntactic transformations do not genuinely represent new information and can be easily distinguished from memorized information.", "B": "Language models are unable to reason about syntactic structures, making syntactic transformations irrelevant.", "C": "Such transformations are merely stylistic and do not affect the model's reasoning capabilities.", "D": "The presence of syntactic transformations in test data does not necessarily indicate that the model has learned the information but could be a result of the model's ability to rephrase or manipulate existing data."}, "answer": "D", "explanation": "The correct answer requires a nuanced understanding of the debate around syntactic transformations. Option A is incorrect because the text suggests it is challenging to distinguish between memorized information and reasoning capabilities. Option B is too simplistic and overlooks the complexity of the issue. Option C is too narrow and misses the broader point of the debate. Option D captures the essence of the debate, acknowledging the difficulty in attributing the model's performance solely to reasoning versus manipulation.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "question_token_count": 24, "avg_answer_token_count": 22}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The impact of data contamination on the evaluation of large language models (LLMs).", "question": "How does data contamination impact the evaluation of large language models (LLMs), and what are the proposed methods to mitigate its effects?", "choices": {"A": "It leads to an inflated and misleading assessment of LLM performance, and mitigating it requires methods like continuous updating of benchmark datasets and regenerating benchmark data.", "B": "It enhances the performance of LLMs by providing more diverse training data, and there are no significant methods to mitigate its effects.", "C": "It has minimal impact on LLM evaluation, and existing static benchmarks are sufficient to ensure reliable performance metrics.", "D": "It primarily affects the computational efficiency of LLMs during training and does not significantly impact their evaluation."}, "answer": "A", "explanation": "The correct answer is A because data contamination inflates LLM performance, and methods like continuous updates and regeneration of benchmark data help mitigate this issue. Options B, C, and D are incorrect as they either misrepresent the impact of data contamination or fail to acknowledge the proposed mitigation strategies.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 9, "question_groundedness_score": 10, "question_token_count": 27, "avg_answer_token_count": 24}
{"context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n", "topic": "Quantifying the Reliability of Dynamic Benchmarks in Reflecting True LLM Capabilities", "question": "How does the concept of \"collision\" in dynamic benchmarking affect the reliability of a benchmark in reflecting true LLM capabilities, and what metrics are proposed to evaluate this aspect?", "choices": {"A": "It decreases reliability because collisions indicate potential data contamination and overlap, leading to less diverse test cases.", "B": "It increases reliability as it ensures consistent results across different transformations.", "C": "It has no impact since collisions only affect the training phase, not the evaluation phase.", "D": "It enhances reliability by providing insights into the benchmark's ability to generate novel variations and detect contamination."}, "answer": "A", "explanation": "Collision refers to the extent of overlap between different transformations of the benchmark dataset. High collision rates indicate more overlap, which can limit the benchmark's ability to generate novel and diverse test cases. Metrics like Collision Rate and Repeat Trials are proposed to quantify this overlap and the number of trials needed to regenerate an existing dataset. These metrics help ensure the benchmark remains effective despite potential training data contamination.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "question_token_count": 34, "avg_answer_token_count": 17}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "Evaluate a model's ability to generate and debug code using benchmark tests like HumanEval, MBPP, and SWE-Bench.", "question": "Which of the following benchmarks would best evaluate a model's capability to generate and debug complex software systems, including handling edge cases and integrating with existing codebases?", "choices": {"A": "HumanEval", "B": "MBPP", "C": "SWE-Bench", "D": "C-Eval"}, "answer": "C", "explanation": "SWE-Bench is designed to address more advanced challenges in software engineering, including complex code synthesis and debugging tasks that require handling edge cases and integrating with existing codebases, making it the most suitable benchmark among the options provided.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "question_token_count": 32, "avg_answer_token_count": 3}
