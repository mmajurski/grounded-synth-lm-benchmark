{"context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n", "topic": "Comparative analysis of different LLM-based rewriting techniques in addressing contamination and enhancing benchmark dataset quality.", "question": "How does the integration of contamination detection with LLM-based rewriting in ITD uniquely address benchmark contamination compared to other rewriting methods such as Auto-Dataset and StructEval, and what trade-off does it manage that those methods do not explicitly confront?", "choices": {"A": "ITD uniquely identifies contaminated samples before rewriting to maintain difficulty levels, thereby reducing contamination risk without sacrificing sample challenge, whereas Auto-Dataset and StructEval do not detect contamination and may inadvertently preserve it while focusing on stylistic or conceptual expansion.", "B": "ITD replaces variables in samples to enhance diversity, unlike Auto-Dataset which only retains original stylistics, and StructEval which focuses solely on increasing cognitive complexity without contamination concerns.", "C": "ITD relies exclusively on knowledge graphs to generate new questions, enabling structural expansion, while Auto-Dataset and StructEval focus on contamination detection and rewriting to reduce contamination.", "D": "ITD generates multiple new samples at varying cognitive levels without contamination detection, unlike Auto-Dataset which detects contamination before rewriting, and StructEval which rewrites samples to maintain difficulty."}, "answer": "A", "explanation": "ITD is distinct in using a contamination detector to first identify problematic samples in static benchmarks, then rewriting them with an LLM while preserving difficulty levels, explicitly managing the trade-off between contamination removal and challenge retention. In contrast, Auto-Dataset and StructEval do not perform contamination detection but focus on stylistic retention and conceptual expansion respectively, potentially preserving contamination.", "question_token_count": 48, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 38}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "Ethical and trust considerations in the deployment and enforcement of contamination detection methods in model evaluation.", "question": "In the context of contamination detection in LLM evaluation, why does reliance on canary strings introduce ethical and trust challenges that complicate the enforcement of contamination mitigation, despite their technical effectiveness?", "choices": {"A": "Because canary strings can be easily detected and removed by models during training, making them ineffective in practice.", "B": "Because the effectiveness of canary strings depends on the honesty and cooperation of model developers, allowing potential manipulation if developers deliberately leak benchmark data.", "C": "Because canary strings increase the computational cost of evaluation, leading developers to avoid their use for economic reasons.", "D": "Because canary strings cause models to underperform by forcing them to ignore legitimate training data, raising fairness concerns."}, "answer": "B", "explanation": "The core ethical and trust challenge is that canary strings only work if developers recognize and act upon them honestly; if developers intentionally leak benchmarking data to artificially inflate scores, the method fails, revealing a dependence on developer integrity rather than purely technical enforcement.", "question_token_count": 36, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 23}
{"context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n", "topic": "The transparency and assumption issues inherent in static benchmarking approaches, such as label protection and post-hoc contamination detection.", "question": "How do transparency issues such as label protection and assumption-heavy post-hoc contamination detection fundamentally limit the reliability of static LLM benchmarks, and why do these limitations motivate the development of dynamic benchmarking despite its own challenges?", "choices": {"A": "Transparency issues obscure contamination evidence, making it difficult to verify dataset purity, while post-hoc detection relies on unverifiable assumptions; these limitations undermine static benchmark validity and motivate dynamic benchmarks that can proactively prevent contamination, albeit with challenges in correctness and scalability.", "B": "Transparency issues in static benchmarks primarily help protect proprietary data, and post-hoc contamination detection ensures complete elimination of contamination, making dynamic benchmarks unnecessary.", "C": "Label protection and post-hoc detection enhance static benchmark reliability by providing secure mechanisms to detect contamination, thus reducing the need for dynamic benchmarking.", "D": "Static benchmarks' transparency and contamination detection methods are fully reliable, but dynamic benchmarking is preferred due to its lower computational costs."}, "answer": "A", "explanation": "Transparency issues like label protection prevent open verification of contamination, and post-hoc contamination detection depends on assumptions that may not hold, thereby compromising the reliability of static benchmarks as contamination risk grows with training data scale. This motivates dynamic benchmarking, which tries to avoid contamination proactively but introduces new challenges such as balancing evaluation correctness with scalability.", "question_token_count": 42, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 32}
{"context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "Compare and contrast static benchmarking with dynamic benchmarking in terms of transparency, faithfulness, and ability to detect data contamination.", "question": "How does dynamic benchmarking fundamentally enhance transparency, faithfulness, and data contamination detection in LLM evaluation compared to static benchmarking, considering the role of dataset transformation over time?", "choices": {"A": "By repeatedly modifying the evaluation dataset through transformation functions, dynamic benchmarking reduces contamination risk and increases transparency by preventing static overlap with training data, thereby enabling more faithful and adaptive performance assessment.", "B": "By using a fixed dataset that remains unchanged, dynamic benchmarking ensures consistent evaluation conditions, which increases transparency and faithfulness but does not address contamination detection effectively.", "C": "Dynamic benchmarking primarily relies on assumptions about lower perplexity for contaminated data, similar to static benchmarking, thus offering no significant improvement in transparency or faithfulness.", "D": "Dynamic benchmarking discards the need for any dataset during evaluation, which eliminates contamination but sacrifices transparency and faithfulness due to lack of concrete data."}, "answer": "A", "explanation": "Dynamic benchmarking applies transformation functions to the benchmark dataset over multiple timestamps, which adapts the evaluation data and prevents overlap with training sets, addressing contamination. This evolving dataset enhances transparency by making the evaluation process more open and faithful by reflecting realistic model challenges, unlike static benchmarking\u2019s fixed dataset that is vulnerable to contamination and less transparent.", "question_token_count": 33, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 8, "avg_answer_token_count": 31}
{"context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n", "topic": "Comparative analysis of rule-based versus LLM-assisted transformations in terms of transparency, traceability, and reliability.", "question": "In the context of dynamic benchmarking transformations, how does the inherent interpretability of rule-based methods fundamentally affect the scalability and reliability of data validation compared to LLM-assisted transformations, and what implications does this have for the necessity of supplementary validation mechanisms?", "choices": {"A": "Rule-based methods\u2019 inherent interpretability reduces the need for costly manual validation, enhancing scalability and reliability, whereas LLM-assisted transformations lack transparency, necessitating additional explainability tools or human-in-the-loop validation to ensure correctness.", "B": "LLM-assisted transformations are inherently more interpretable than rule-based methods, enabling automatic validation without supplementary mechanisms, thereby improving scalability and reliability.", "C": "Both rule-based and LLM-assisted transformations require equal levels of manual validation and explainability tools, making their scalability and reliability comparable.", "D": "Rule-based methods hinder scalability due to their complexity, while LLM-assisted transformations are fully transparent and require no additional validation, leading to higher reliability."}, "answer": "A", "explanation": "Rule-based transformations are inherently interpretable, making manual verification simpler and less costly, thus scalable and reliable. In contrast, LLM-assisted transformations depend on model transparency and traceability, which are often limited, requiring extra explainability tools or human oversight to maintain correctness.", "question_token_count": 48, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 2, "question_groundedness_score": 9, "avg_answer_token_count": 31}
{"context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n", "topic": "The architecture and functionality of the Benchmark Self-Evolving framework for dynamic extension of static benchmarks via multi-agent collaboration.", "question": "How does the multi-agent architecture of the Benchmark Self-Evolving framework fundamentally enhance the extensibility and quality of static benchmarks, and what role does agent specialization play in this process?", "choices": {"A": "By allowing multiple agents to independently create benchmarks without coordination, increasing diversity but risking quality; agent specialization is minimal and agents perform redundant tasks.", "B": "By decomposing benchmark creation into specialized tasks managed by dedicated agents that coordinate through iterative collaboration, enabling scalable, diverse, and high-quality benchmark extension.", "C": "By replacing human evaluators entirely with a single agent that generates and verifies benchmarks sequentially, simplifying the pipeline but limiting scalability.", "D": "By focusing solely on human-in-the-loop feedback without agent collaboration, ensuring quality but limiting the dynamic extension of benchmarks."}, "answer": "B", "explanation": "The Benchmark Self-Evolving framework uses a multi-agent system where each agent is specialized for a distinct role\u2014such as planning, generation, verification, and evaluation\u2014which work collaboratively to dynamically extend static benchmarks. This specialization combined with coordinated interaction enables scalable, diverse, and high-quality benchmark creation that surpasses the limitations of static benchmarks.", "question_token_count": 36, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 2, "question_groundedness_score": 4, "avg_answer_token_count": 27}
{"context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n", "topic": "Discuss the role and selection of the diversity measurement function \u0398(\u00b7) and how metrics like N-gram statistics or BLEU scores can be used to quantify diversity between datasets.", "question": "In the context of measuring dataset diversity, what are the conceptual implications of choosing a function \u0398(\u00b7) based on N-gram statistics or BLEU scores for quantifying both external and internal diversity between datasets, and how might the inherent properties of these metrics affect the sensitivity and reliability of diversity assessments?", "choices": {"A": "N-gram and BLEU-based functions primarily capture lexical overlap and sequence similarity, which may limit sensitivity to semantic or structural diversity, potentially underestimating true diversity between datasets.", "B": "These metrics are designed to measure semantic similarity and thus provide a comprehensive assessment of both lexical and conceptual diversity across datasets.", "C": "Using N-gram or BLEU metrics ensures perfect sensitivity to any form of dataset variation, making them ideal for all types of diversity measurement without need for complementary metrics.", "D": "N-gram and BLEU scores emphasize document length differences rather than content variation, leading to overestimation of diversity between datasets."}, "answer": "A", "explanation": "N-gram and BLEU metrics focus on lexical and sequential overlap, which makes them sensitive to surface-level similarities but less so to deeper semantic or structural differences; this affects the reliability of diversity measurement by potentially overlooking meaningful variation, thus influencing how external and internal diversity are quantified.", "question_token_count": 58, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 29}
{"context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "Explore the implications of starting a dynamic benchmarking process with an empty seed dataset and how a benchmark can be created from scratch.", "question": "In the context of dynamic benchmarking for large language models, what are the primary theoretical and practical implications of initiating the benchmarking process with an empty seed dataset, and how does the transformation function T(\u22c5) facilitate the creation of a valid benchmark from scratch over time?", "choices": {"A": "Starting with an empty seed dataset implies that the benchmark lacks initial evaluation samples, requiring the transformation function T(\u22c5) to iteratively generate or select new data points to construct meaningful and contamination-free evaluation datasets at each timestamp, thereby enabling the dynamic benchmark to evolve and maintain validity without relying on pre-existing data.", "B": "An empty seed dataset means the benchmarking process can only rely on static datasets from external sources, and the transformation function T(\u22c5) only filters these datasets without generating new data, limiting the benchmark\u2019s adaptability and freshness.", "C": "Initiating with an empty seed dataset allows the benchmark to bypass contamination issues entirely, as no prior data exists, and the transformation function T(\u22c5) serves solely to shuffle existing samples to simulate data evolution.", "D": "Starting with an empty seed dataset means the benchmark cannot proceed until a minimal static dataset is manually introduced, as the transformation function T(\u22c5) requires pre-existing data to modify and cannot create datasets independently."}, "answer": "A", "explanation": "The correct answer reflects that an empty seed dataset necessitates the transformation function to actively generate or select new evaluation data over time, building the benchmark dynamically without initial data, ensuring ongoing relevance and avoiding contamination.", "question_token_count": 53, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 2, "question_groundedness_score": 9, "avg_answer_token_count": 47}
{"context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n", "topic": "Challenges highlighted by Yang et al. (2023) demonstrating how minor text variations can defeat advanced decontamination methods, emphasizing the need for robust encryption.", "question": "How do minor text variations undermine advanced decontamination methods in protecting evaluation data, and why does this vulnerability specifically reinforce the necessity for employing robust encryption techniques despite their computational and key management challenges?", "choices": {"A": "Minor text variations create entirely new semantic content that decontamination methods cannot recognize, making encryption necessary to prevent any form of data leakage.", "B": "Minor text variations evade pattern-based decontamination filters by appearing as novel data, thus enabling contamination that only encryption can reliably prevent by restricting data access.", "C": "Minor text variations increase the volume of data exponentially, overwhelming decontamination methods, so encryption is used primarily to reduce data size and complexity.", "D": "Minor text variations confuse encryption algorithms, so stronger encryption is required to correct errors caused by these variations during decontamination."}, "answer": "B", "explanation": "Minor text variations do not alter semantic meaning significantly but evade detection by decontamination methods that rely on pattern matching or similarity heuristics, allowing contaminated data to pass through unnoticed; thus, encryption is necessary to secure evaluation data by preventing unauthorized access entirely, despite introducing overhead and requiring strong key management.", "question_token_count": 39, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 3, "question_groundedness_score": 6, "avg_answer_token_count": 28}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "The implications of evaluation frameworks like S3Eval, DyVal, and NPHardEval for future development of more robust and generalizable reasoning benchmarks for LLMs.", "question": "How do evaluation frameworks like S3Eval, DyVal, and NPHardEval collectively inform the design principles for future reasoning benchmarks that aim to robustly and generalizably assess large language models' reasoning capabilities?", "choices": {"A": "By emphasizing static, pre-defined problem sets that focus on memorization of known solutions to test LLMs\u2019 reasoning depth.", "B": "By demonstrating the importance of randomized, structured problem generation with controllable difficulty and diverse reasoning domains to prevent overfitting and promote generalizable reasoning assessment.", "C": "By focusing solely on natural language understanding without incorporating structured data or combinatorial problem-solving components.", "D": "By limiting evaluation to only polynomial-time solvable problems to ensure computational feasibility and avoid NP-hard complexity in reasoning assessment."}, "answer": "B", "explanation": "The correct answer highlights that these frameworks employ randomized and structured problem generation across diverse domains (tables, graphs, NP problems) with controllable difficulty to prevent overfitting and enable robust, generalizable reasoning assessment for LLMs. The other options misrepresent key elements: static problems encourage memorization (A), ignoring structured data reduces reasoning scope (C), and excluding NP-hard problems limits assessment of complex reasoning abilities (D).", "question_token_count": 44, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 8, "avg_answer_token_count": 23}
{"context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n", "topic": "The limitations of static benchmarking methods in the context of large-scale LLM training data and why these methods are becoming outdated.", "question": "Why do static benchmarking methods become increasingly unreliable for large-scale LLM evaluation as training data grows, and how does the relationship expressed by the contamination probability formula Pr_contam \u221d |D_train| \u22c5 |D_test|^{-1} explain this phenomenon?", "choices": {"A": "Because larger training datasets increase the chance that test data overlaps with training data, the formula shows contamination probability rises proportionally with training set size and inversely with test set size, undermining static benchmarks\u2019 validity.", "B": "Because larger test datasets dilute contamination effects, the formula indicates contamination probability decreases with training set size and increases with test set size, making static benchmarks more reliable as data grows.", "C": "Because contamination depends solely on model architecture rather than dataset sizes, the formula implies contamination probability is constant regardless of training or test set sizes, so static benchmarks remain reliable.", "D": "Because contamination probability is independent of dataset sizes, the formula suggests static benchmarks become unreliable due to model overfitting unrelated to training or test data overlap."}, "answer": "A", "explanation": "The contamination probability formula demonstrates that as the size of the training data increases, the likelihood that some test examples appear in training data also increases proportionally, while a smaller test set size further amplifies this risk. This overlap causes static benchmarks to become outdated and unreliable for evaluating LLMs trained on vast datasets, necessitating dynamic benchmarking methods.", "question_token_count": 51, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 9, "question_groundedness_score": 10, "avg_answer_token_count": 35}
{"context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n", "topic": "Comparative analysis of encryption methods and label protection approaches in mitigating risks of data leakage and contamination in machine learning evaluations.", "question": "In the context of safeguarding machine learning evaluation data from leakage and contamination, how do encryption methods fundamentally differ from label protection approaches in their mechanisms and inherent vulnerabilities, and what are the principal trade-offs associated with each in ensuring evaluation integrity?", "choices": {"A": "Encryption methods secure the test data by encoding it to prevent unauthorized access but depend heavily on key management and incur computational overhead, while label protection hides only the true test labels to prevent models from learning answers, offering simpler implementation but risking exposure of raw data.", "B": "Encryption methods prevent access to both test data and labels by using licensing restrictions without computational costs, whereas label protection encrypts the entire dataset, making it computationally expensive but fully secure against key compromise.", "C": "Encryption methods solely focus on hiding the test labels from public access to maintain evaluation integrity, whereas label protection encrypts the entire test dataset to prevent accidental inclusion in training sets, both methods having equal vulnerability to key exposure.", "D": "Encryption methods rely on obscuring test labels through secure multi-party computation with no risk of key exposure, while label protection methods depend on public key encryption that introduces computational overhead and risk of unauthorized data access."}, "answer": "A", "explanation": "Encryption methods encode the test data itself, preventing unauthorized access but requiring strong key management and adding computational overhead, with vulnerability if keys are compromised. Label protection keeps the test labels hidden to prevent models from memorizing answers, thus maintaining evaluation integrity with simpler overhead but without encrypting the entire data, risking exposure of the raw test inputs.", "question_token_count": 47, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 44}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "The pedagogical and practical implications of using competitive programming platforms (like Codeforces) as benchmarks to measure dynamic problem-solving skills of models.", "question": "How do competitive programming platforms like Codeforces uniquely contribute to evaluating dynamic problem-solving skills in language models compared to traditional static coding benchmarks, and what are the key pedagogical implications of incorporating such platforms as evaluation tools?", "choices": {"A": "They provide static code synthesis tasks that emphasize debugging skills without requiring adaptability, thus primarily reinforcing incremental learning rather than dynamic problem solving.", "B": "They simulate real-time, algorithmically complex challenges that require adaptive, creative problem-solving strategies, thereby offering a more realistic assessment of a model\u2019s ability to handle evolving and multifaceted coding problems.", "C": "They focus on instruction-following by presenting step-by-step directives, which primarily test language comprehension rather than problem-solving agility.", "D": "They evaluate factual recall of coding syntax and semantics, similar to reasoning benchmarks, without significantly challenging a model\u2019s ability to generate novel solutions under time constraints."}, "answer": "B", "explanation": "Competitive programming platforms like Codeforces uniquely test dynamic problem-solving by presenting real-time, complex algorithmic challenges that require models to adapt and creatively solve problems beyond static code generation or debugging tasks. This has important pedagogical implications, as it encourages training and evaluation strategies that emphasize flexibility, creativity, and comprehensive coding proficiency in realistic, evolving scenarios.", "question_token_count": 43, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 30}
{"context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n", "topic": "Strategies for balancing novelty, fairness, and contamination risks in the construction of dynamic benchmarks.", "question": "In constructing dynamic benchmarks for LLM evaluation, how does the interpretability of transformation methods fundamentally influence the balance between ensuring novelty, maintaining fairness, and mitigating contamination risks?", "choices": {"A": "High interpretability in rule-based transformations facilitates manual validation, reducing contamination risk but may limit novelty, whereas low interpretability in LLM-based generation increases novelty but necessitates additional validation to manage fairness and contamination.", "B": "Low interpretability in temporal cutoff methods increases contamination risk but allows for greater novelty and fairness without requiring human-in-the-loop validation.", "C": "Hybrid approaches rely exclusively on interpretable transformations, thereby eliminating contamination risk but potentially compromising fairness and novelty.", "D": "LLM-based generation methods inherently guarantee fairness and contamination avoidance due to their generative capabilities, regardless of interpretability concerns."}, "answer": "A", "explanation": "Interpretability allows easier validation of transformed data, reducing contamination risk and enabling fairness checks; rule-based transformations offer this but may restrict novelty. Conversely, LLM-based methods generate novel data but with opaque processes needing extra validation mechanisms. Temporal cutoff methods limit contamination by using new data but may lack novelty. Hybrids aim to balance these factors. Thus, interpretability directly affects how well these trade-offs can be managed.", "question_token_count": 33, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 7, "avg_answer_token_count": 28}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "Investigate how contamination can lead to overestimation of LLM capabilities and the subsequent risks this poses for real-world applicability and trustworthiness of AI systems.", "question": "How does data contamination in benchmarking large language models lead to an overestimation of their true capabilities, and what are the primary risks this poses for the models\u2019 real-world applicability and trustworthiness?", "choices": {"A": "Contamination causes models to rely on memorized training data during evaluation, inflating performance metrics and risking deployment decisions based on misleading benchmarks that do not reflect genuine reasoning or generalization.", "B": "Contamination improves models\u2019 reasoning abilities by exposing them to diverse syntactic variations, enhancing their robustness and trustworthiness in real-world scenarios.", "C": "Contamination solely affects the speed of inference without impacting evaluation metrics, thus posing no significant risks to real-world applicability or trustworthiness.", "D": "Contamination leads to underestimation of model capabilities by making benchmarks artificially difficult, which results in overly cautious deployment decisions that limit innovation."}, "answer": "A", "explanation": "Contamination means the test data overlaps or is very similar to training data, causing models to appear more capable by recalling memorized information rather than demonstrating true reasoning or generalization. This inflates benchmark scores, misleading researchers and practitioners about model robustness and applicability, which can result in poor deployment decisions and erosion of trust.", "question_token_count": 39, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 29}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The implications of continuous LLM training on all available data for the integrity and relevance of benchmark results.", "question": "How does continuous training of LLMs on all available data compromise the integrity of static benchmarks, and why do dynamic benchmarks coupled with contamination detectors provide a more robust evaluation framework?", "choices": {"A": "Continuous training on all data leads to overfitting on benchmark tasks, causing static benchmarks to underestimate model generalization; dynamic benchmarks and contamination detectors address this by regularly introducing novel tasks and identifying leaked data.", "B": "Continuous training on all data can result in LLMs having prior exposure to benchmark datasets, causing data contamination that artificially inflates performance on static benchmarks; dynamic benchmarks and contamination detectors mitigate this by adapting tests over time and detecting such contamination, preserving evaluation validity.", "C": "Continuous training on all data increases model size and complexity, making static benchmarks computationally infeasible; dynamic benchmarks and contamination detectors reduce model size and simplify evaluation to counteract this.", "D": "Continuous training on all data causes static benchmarks to become outdated due to shifts in language usage; dynamic benchmarks and contamination detectors update linguistic content but do not address contamination risks."}, "answer": "B", "explanation": "Continuous training on all available data risks the model having seen static benchmark data during training, leading to data contamination that inflates benchmark scores and undermines their validity. Dynamic benchmarks continually evolve to present novel challenges, and contamination detectors identify when benchmark data has leaked into training, together ensuring more reliable assessment of true model capabilities.", "question_token_count": 35, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 6, "avg_answer_token_count": 39}
{"context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "Discuss how the transformation function \\(T(\\cdot)\\) in dynamic benchmarking modifies datasets over time to mitigate data contamination and enable faithful evaluation.", "question": "In the context of dynamic benchmarking for large language models, how does the transformation function \\( T(\\cdot) \\) modify the original static dataset over time to mitigate data contamination, and why is this modification essential for maintaining a faithful evaluation?", "choices": {"A": "\\( T(\\cdot) \\) generates entirely new datasets unrelated to the original to ensure no overlap with training data, which is essential to prevent any form of data contamination and maintain evaluation integrity.", "B": "\\( T(\\cdot) \\) incrementally augments the original static dataset with new samples at each timestamp, ensuring the dataset grows but does not change existing data, which helps in maintaining evaluation transparency.", "C": "\\( T(\\cdot) \\) applies time-indexed transformations to the static dataset producing a sequence of modified datasets \\(\\mathcal{D}_t\\) that evolve over time, preventing repeated exposure of the same data to the model and thus mitigating contamination, which is critical for transparent and faithful evaluation.", "D": "\\( T(\\cdot) \\) filters out difficult samples from the static dataset over time to simplify the evaluation, ensuring that only easy instances remain, which helps in reducing evaluation complexity and contamination risk."}, "answer": "C", "explanation": "The transformation function \\( T(\\cdot) \\) does not create unrelated datasets nor simply augment or filter data; instead, it modifies the original dataset at each timestamp to produce evolving datasets \\(\\mathcal{D}_t\\), which prevents models from memorizing static test sets and encountering repeated data. This dynamic evolution is essential to mitigate contamination and preserve the transparency and faithfulness of evaluation over time.", "question_token_count": 47, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 43}
{"context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n", "topic": "Behavioral post-hoc detection methods analyzing model memorization through masked inputs and partial completions.", "question": "How do behavioral post-hoc detection methods using masked inputs and partial completions enhance the identification of model memorization and data contamination compared to traditional exact n-gram overlap techniques, and what fundamental limitation of exact matching do these behavioral approaches address?", "choices": {"A": "They exploit embedding-based similarity to detect semantic overlaps missed by exact matching, addressing the limitation that exact matching cannot capture paraphrased or semantically similar content.", "B": "They analyze model performance variations under controlled input perturbations, revealing memorization patterns that exact n-gram overlap misses due to its inability to detect approximate or partial memorization.", "C": "They improve transparency by exposing training labels directly during evaluation, overcoming the limitation that exact matching depends on protected labels and centralized evaluation systems.", "D": "They rely on statistical frequency analysis of token occurrences to identify contamination, compensating for exact matching\u2019s failure to consider token distributions in training data."}, "answer": "B", "explanation": "Behavioral post-hoc detection methods probe model responses to masked or partially completed inputs to expose memorization that does not require exact textual overlap, thus overcoming the fundamental limitation of exact n-gram matching which can yield false negatives by missing approximate or paraphrased memorization.", "question_token_count": 47, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 30}
{"context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n", "topic": "Critical evaluation of the assumption that increasing dataset size always improves benchmarking quality in the context of scalability.", "question": "In the context of evaluating dynamic benchmarking scalability, why is simply increasing the size of the dataset not always indicative of improved benchmarking quality?", "choices": {"A": "Because larger datasets invariably increase statistical errors, degrading benchmark reliability.", "B": "Because scalability also depends on the cost-efficiency of data generation, making larger datasets potentially impractical or inefficient.", "C": "Because the original dataset size must remain fixed to preserve benchmarking validity regardless of transformations.", "D": "Because smaller datasets are preferred to minimize transformation costs even at the expense of statistical accuracy."}, "answer": "B", "explanation": "Although larger datasets reduce statistical errors, scalability measures the ratio of dataset size increase relative to the cost of generating that data. Without considering cost, increasing dataset size might be inefficient or impractical, meaning simply having a larger dataset does not necessarily improve benchmarking quality.", "question_token_count": 27, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 18}
{"context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "Interpret the importance of quality criteria in assessing existing dynamic benchmarks and how partial or full support for features influences benchmark utility.", "question": "In the context of dynamic benchmarking for large language models, how does partial versus full support of quality criteria features impact the utility and reliability of a benchmark, and why is adherence to these criteria critical for faithful model evaluation?", "choices": {"A": "Partial support generally maintains benchmark integrity, but full support is only necessary for specific model types, as some criteria are optional depending on the task.", "B": "Partial support of quality criteria features may lead to incomplete or biased evaluations, reducing benchmark reliability, whereas full support ensures comprehensive and transparent assessments, which are essential for faithful dynamic benchmarking.", "C": "Full support of quality criteria often complicates benchmark design without significant benefits, while partial support strikes an optimal balance between usability and evaluation depth.", "D": "The level of feature support is irrelevant to benchmark utility as long as the dataset is large and diverse; quality criteria mainly affect computational efficiency rather than evaluation accuracy."}, "answer": "B", "explanation": "Partial support of quality criteria can cause gaps in evaluation coverage or transparency, undermining reliability; full support ensures that benchmarks accurately and transparently measure model performance over time, which is fundamental to faithful dynamic benchmarking.", "question_token_count": 44, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 4, "avg_answer_token_count": 31}
{"context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n", "topic": "Potential impacts of contamination in benchmark datasets on the validity and reliability of model evaluation results.", "question": "How does contamination in benchmark datasets undermine the validity and reliability of model evaluation results, and what are the inherent challenges in using LLM-based rewriting methods to mitigate this contamination without compromising the original benchmark\u2019s difficulty and knowledge representation?", "choices": {"A": "Contamination causes models to memorize test data, inflating evaluation scores, and LLM rewriting methods must balance removing contamination while preserving sample difficulty and essential knowledge, which is challenging because rewriting can inadvertently alter these attributes.", "B": "Contamination introduces random noise that decreases model performance, and LLM rewriting methods can perfectly eliminate contamination without affecting benchmark characteristics due to their generative abilities.", "C": "Contamination primarily affects training efficiency rather than evaluation, and LLM rewriting methods focus on increasing sample diversity rather than addressing contamination or preserving difficulty.", "D": "Contamination causes evaluation to be stricter by introducing harder samples, and LLM rewriting methods simplify samples to reduce difficulty, which improves reliability but may reduce validity."}, "answer": "A", "explanation": "Contamination leads to models effectively seeing test samples during training, causing inflated evaluation results that do not reflect true generalization; rewriting methods aim to remove or alter contaminated samples but must carefully preserve original difficulty and knowledge to maintain benchmark validity, a balance that is difficult to achieve because rewriting may unintentionally simplify or distort samples.", "question_token_count": 44, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 3, "question_groundedness_score": 8, "avg_answer_token_count": 32}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "Distinguishing exact contamination from syntactic contamination and their respective detection challenges.", "question": "How do exact contamination and syntactic contamination differ in terms of their impact on evaluation validity and the technical challenges involved in detecting them within large language model benchmarks?", "choices": {"A": "Exact contamination involves identical data points causing straightforward detection and direct validity compromise, while syntactic contamination requires recognizing semantically equivalent but syntactically altered data, making detection complex and potentially leading to subtle validity breaches.", "B": "Exact contamination is more difficult to detect because it involves paraphrased data points, whereas syntactic contamination is easier as it relies on exact duplicates that are easily found by string matching.", "C": "Both exact and syntactic contamination equally compromise evaluation validity, but syntactic contamination is irrelevant since it does not affect model performance or benchmarking outcomes.", "D": "Exact contamination only affects code benchmarks, while syntactic contamination exclusively affects natural language benchmarks, so their detection methods and impacts do not overlap."}, "answer": "A", "explanation": "Exact contamination occurs when identical data points exist in both training and test sets, allowing straightforward detection via string matching and direct compromise of evaluation validity. Syntactic contamination involves semantically equivalent but syntactically modified data points, making detection technically challenging and potentially causing subtle, less obvious validity issues.", "question_token_count": 31, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 32}
{"context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n", "topic": "The impact of proprietary training data practices on the transparency and verifiability of contamination in LLM benchmarks.", "question": "How does the proprietary nature of LLM training data most critically impact the detection and mitigation of contamination in evaluation benchmarks, and what are the broader implications for assessing true model performance?", "choices": {"A": "It limits access to training corpora details, preventing the community from verifying overlaps with evaluation data, thereby undermining transparency and the fairness of benchmark assessments.", "B": "It primarily restricts the use of retrieval-based detection methods, which rely on proprietary algorithms rather than data visibility, reducing contamination detection effectiveness.", "C": "Proprietary data practices encourage larger training datasets that inherently reduce contamination risk by diluting evaluation overlaps, enhancing performance assessments.", "D": "Keeping training data proprietary mainly affects model fine-tuning stages but has little effect on pre-training contamination risks or overall benchmark reliability."}, "answer": "A", "explanation": "The key issue with proprietary training data is that it restricts community access to the datasets, making it impossible to verify whether evaluation data appears in training sets. This opacity impedes contamination detection and mitigation, which in turn undermines the transparency and fairness of benchmark results and true performance assessment.", "question_token_count": 35, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 26}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "Comparative analysis of different temporal cutoff benchmarks such as LiveBench, AntiLeak-Bench, AcademicEval, LiveCodeBench, LiveAoPSBench, and Forecastbench.", "question": "How do the update frequencies and data source selections of temporal cutoff benchmarks like LiveBench, AntiLeak-Bench, AcademicEval, LiveCodeBench, LiveAoPSBench, and Forecastbench collectively influence their effectiveness in mitigating data contamination while ensuring domain-specific evaluation relevance?", "choices": {"A": "Benchmarks with more frequent updates and diverse data sources, such as Forecastbench's daily forecasting data, provide better contamination mitigation and domain relevance by continuously challenging models with truly novel information.", "B": "Benchmarks using static, infrequently updated datasets focused on single domains, like LiveBench's math competitions updated every few months, are more effective at contamination mitigation than those with daily updates.", "C": "AntiLeak-Bench's approach of generating queries strictly about knowledge unknown before the cutoff is less effective than LiveAoPSBench's continuous collection from community forums due to limited data source diversity.", "D": "AcademicEval's reliance on the latest academic papers is less relevant for contamination mitigation because academic research rarely introduces new knowledge post-cutoff compared to coding challenges like LiveCodeBench."}, "answer": "A", "explanation": "The effectiveness of temporal cutoff benchmarks in preventing contamination and maintaining evaluation relevance depends heavily on how frequently they update and the nature of their data sources; more frequent updates and diverse, dynamic sources like Forecastbench\u2019s daily data better ensure models face genuinely new information, while less frequent updates may risk stale evaluations.", "question_token_count": 54, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 7, "question_groundedness_score": 10, "avg_answer_token_count": 37}
{"context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n", "topic": "Discuss potential challenges and limitations in accurately measuring diversity between datasets and how these might affect the interpretation of \u0398(\u00b7).", "question": "What are the primary challenges in accurately measuring dataset diversity using a function like \u0398(\u00b7), and how might these challenges influence the interpretation of external and internal diversity metrics in transformed datasets?", "choices": {"A": "The main challenges involve metric sensitivity to dataset size and the inability of \u0398(\u00b7) to capture semantic differences, which can lead to underestimating true diversity and misleading conclusions about dataset variation.", "B": "The main challenges are that \u0398(\u00b7) always overestimates diversity due to random noise in datasets, causing external diversity to be conflated with internal diversity and thus invalidating all diversity measurements.", "C": "The main challenges are limited computational resources for calculating \u0398(\u00b7), which force approximations that generally overstate internal diversity but have little effect on external diversity interpretation.", "D": "The main challenges involve the incompatibility of \u0398(\u00b7) with transformed datasets, as it only works on original seed datasets, making all diversity measurements irrelevant and incomparable."}, "answer": "A", "explanation": "The correct answer highlights that diversity metrics like \u0398(\u00b7) can be sensitive to factors such as dataset size, n-gram overlap, or metric design that fail to capture semantic or structural differences, which may cause under- or overestimation of diversity and thus affect the interpretation of both internal and external diversity values in transformed datasets.", "question_token_count": 36, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 35}
{"context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n", "topic": "The necessity and components of standardized dynamic evaluation protocols to improve consistency and trustworthiness in LLM benchmarking.", "question": "Considering the increasing vulnerability of static benchmarking methods to data contamination and the current reliability and reproducibility challenges faced by dynamic approaches, what is the primary rationale for advocating standardized dynamic evaluation protocols in LLM benchmarking, and which key component is essential to ensure their effectiveness?", "choices": {"A": "To reduce computational overhead by limiting evaluation frequency; the essential component is a fixed evaluation dataset.", "B": "To maintain consistency and trustworthiness in benchmarking results despite evolving model training data; the essential component is reproducible and well-defined dynamic evaluation procedures.", "C": "To increase model complexity assessment by introducing random test sets; the essential component is continuous model retraining during evaluation.", "D": "To eliminate human involvement in benchmarking by automating all test generation; the essential component is fully autonomous test generation algorithms."}, "answer": "B", "explanation": "The principal reason for advocating standardized dynamic evaluation protocols is to address the contamination vulnerability of static benchmarks while overcoming dynamic methods' current issues with reliability and reproducibility, thereby ensuring consistent and trustworthy evaluation results. The critical component for effectiveness is having reproducible and clearly defined dynamic evaluation procedures that can be standardized across benchmarking efforts.", "question_token_count": 50, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 6, "avg_answer_token_count": 23}
{"context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n", "topic": "Critical evaluation of the trade-offs between automation and interpretability in dynamic benchmark data generation.", "question": "In the context of dynamic benchmark data generation, how do the interpretability properties of rule-based transformations compare to LLM-assisted transformations in terms of validation cost and correctness assurance, and what mechanisms are typically required to mitigate the challenges posed by LLM-generated data?", "choices": {"A": "Rule-based transformations are less interpretable and require more extensive manual validation, whereas LLM-assisted transformations are fully transparent and thus reduce validation costs without additional mechanisms.", "B": "Rule-based transformations are inherently interpretable, reducing manual validation costs and easing correctness assurance, while LLM-assisted transformations are less transparent and typically require explainability tools or human-in-the-loop validation to ensure reliability.", "C": "Both rule-based and LLM-assisted transformations have similar interpretability levels, so validation costs and correctness assurance mechanisms do not differ significantly between the two.", "D": "LLM-assisted transformations are inherently interpretable and cost-effective because their generative process is fully traceable, unlike rule-based methods which often require complex explainability tools to validate correctness."}, "answer": "B", "explanation": "Rule-based transformations are inherently interpretable, reducing the need for costly manual validation and simplifying correctness assurance; in contrast, LLM-assisted transformations lack transparency, necessitating additional mechanisms such as explainability tools or human-in-the-loop validation to maintain reliability.", "question_token_count": 50, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 34}
{"context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n", "topic": "Explore the role of ethical guidelines in governing data usage, model transparency, and the broader societal impact of AI benchmarks.", "question": "How can ethical guidelines in AI benchmarking reconcile the inherent tension between ensuring model transparency and protecting user privacy, while simultaneously mitigating bias and preventing misuse of evaluation results?", "choices": {"A": "By prioritizing transparency above all, allowing full access to data and model internals regardless of privacy risks to guarantee unbiased evaluation.", "B": "By implementing strict data governance that anonymizes data, designing benchmarks with fairness criteria, and establishing accountability measures to balance transparency, privacy, and bias mitigation.", "C": "By focusing solely on dynamic benchmarks to continuously update data and relying on automated bias detection without transparency to users to prevent misuse.", "D": "By restricting access to benchmarking results to only trusted parties to avoid misuse, even if it limits transparency and user awareness."}, "answer": "B", "explanation": "The correct approach involves balancing transparency with privacy through data anonymization, fairness-driven benchmark design, and accountability frameworks to mitigate bias and prevent misuse, as opposed to extremes that sacrifice privacy, transparency, or fairness.", "question_token_count": 32, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 4, "question_groundedness_score": 9, "avg_answer_token_count": 26}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The human effort and interdisciplinary collaboration involved in constructing and maintaining robust benchmarking suites for LLMs.", "question": "In the context of benchmarking large language models, why is the human effort combined with interdisciplinary collaboration critical to addressing the limitations of static benchmarks, and how do contamination detectors and dynamic benchmarks specifically mitigate these limitations?", "choices": {"A": "Because static benchmarks become obsolete as models improve, human and interdisciplinary input is needed to update tasks; contamination detectors identify overlap between training data and benchmarks, while dynamic benchmarks continuously introduce new challenges to prevent overfitting.", "B": "Because static benchmarks are inherently biased, human and interdisciplinary collaboration is necessary to eliminate all subjective elements; contamination detectors remove biased samples, and dynamic benchmarks replace human judgment entirely.", "C": "Because static benchmarks are expensive to run, human and interdisciplinary teams focus on reducing costs; contamination detectors compress data, and dynamic benchmarks simplify tasks to save resources.", "D": "Because static benchmarks do not measure model speed, human effort ensures timing tests are included; contamination detectors check computational efficiency, and dynamic benchmarks vary model runtime environments."}, "answer": "A", "explanation": "The correct answer highlights that static benchmarks lose challenge as models improve and risk contamination from training data, requiring human and interdisciplinary efforts to design and maintain benchmarks. Contamination detectors quantify data overlap to preserve validity, while dynamic benchmarks adapt tasks over time to maintain meaningful evaluation.", "question_token_count": 41, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 4, "avg_answer_token_count": 35}
{"context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n", "topic": "Critical analysis of how static benchmarks can reveal strengths and weaknesses in language models\u2019 abilities such as commonsense reasoning, factual knowledge retrieval, and toxicity detection.", "question": "How do static benchmarks function as diagnostic tools to reveal specific strengths and weaknesses in language models\u2019 abilities such as commonsense reasoning, factual knowledge retrieval, and toxicity detection, and what are the inherent limitations of their static and predefined nature in accurately assessing these nuanced capabilities?", "choices": {"A": "By providing a fixed dataset with expected outputs and a scoring function, static benchmarks enable consistent measurement of model performance across tasks, but their predefined nature limits adaptability to evolving language use and contextual subtleties, potentially obscuring nuanced understanding.", "B": "Static benchmarks dynamically generate new prompts during evaluation to test model adaptability in real-time, ensuring comprehensive assessment of reasoning and toxicity detection without limitations.", "C": "Static benchmarks primarily focus on model training rather than evaluation, thus revealing strengths and weaknesses only indirectly and lacking a formal scoring system to quantify performance.", "D": "The static nature of benchmarks allows them to fully capture the complexity of commonsense reasoning and toxicity detection through exhaustive datasets that require no updates or contextual interpretation."}, "answer": "A", "explanation": "Static benchmarks offer a fixed set of inputs, expected outputs, and scoring criteria that allow for consistent, repeatable evaluation of model abilities, revealing specific strengths and weaknesses by measuring performance against a standardized baseline. However, their static and predefined design limits their ability to adapt to language evolution and contextual nuances, which are critical for tasks like commonsense reasoning and toxicity detection, thereby constraining the accuracy of their assessment in these areas.", "question_token_count": 53, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 2, "question_groundedness_score": 6, "avg_answer_token_count": 33}
{"context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n", "topic": "Development and practical application of mitigation tools designed to minimize data contamination effects during LLM benchmarking.", "question": "Considering the increasing vulnerability of static benchmarking methods to data contamination as LLM training datasets expand, what are the primary challenges that practical mitigation tools must overcome to ensure reliable and reproducible dynamic benchmarking in real-world LLM evaluation?", "choices": {"A": "They must balance contamination detection sensitivity with consistent reproducibility, while adapting to evolving datasets and maintaining standardized evaluation protocols.", "B": "They need to prioritize speed over accuracy, focusing on rapid contamination detection without concern for reproducibility or standardization.", "C": "Their main challenge is to replace static benchmarks entirely, eliminating the need for any consistency in evaluation metrics.", "D": "They should focus solely on preventing contamination by excluding any data sources used in training from benchmark datasets, without considering dynamic adaptation."}, "answer": "A", "explanation": "Practical mitigation tools face the challenge of maintaining a balance between sensitivity to contamination and the reproducibility of results, particularly because dynamic benchmarking involves evolving datasets and methodologies. Standardized protocols are necessary to ensure consistency across evaluations, which is why option A correctly captures these nuanced requirements. Options B, C, and D either disregard reproducibility, oversimplify the role of benchmarks, or ignore the need for dynamic adaptation.", "question_token_count": 42, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 22}
{"context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n", "topic": "The approach employed by KIEval to generate follow-up questions grounded in static benchmark responses for deeper evaluation.", "question": "How does KIEval\u2019s strategy of generating follow-up questions based on an evaluated model\u2019s responses to static benchmark questions fundamentally differ from other interactive evaluation methods such as LLM-as-an-Interviewer and TreeEval, and what key advantage does this confer in assessing large language models?", "choices": {"A": "KIEval generates follow-up questions by paraphrasing the initial benchmark questions before evaluation, which allows it to maintain consistency across models; this differs from other methods that dynamically generate new topics, providing more standardized assessment.", "B": "KIEval grounds follow-up questions specifically on the evaluated model\u2019s actual responses to static benchmark questions, enabling targeted probing of the model\u2019s reasoning and understanding; unlike LLM-as-an-Interviewer and TreeEval, which either paraphrase queries or generate questions from topics, KIEval\u2019s approach yields deeper, response-sensitive evaluation.", "C": "KIEval uses a multi-agent framework to plan and verify follow-up questions, ensuring diverse perspectives in evaluation; this contrasts with single-agent methods like LLM-as-an-Interviewer, thereby improving benchmark scalability.", "D": "KIEval begins by generating initial questions on a given topic and then creates follow-up subtopics regardless of the evaluated model\u2019s response, which allows it to cover broader knowledge areas compared to response-driven methods."}, "answer": "B", "explanation": "KIEval\u2019s unique approach lies in generating follow-up questions directly based on how the evaluated model responds to static benchmark questions, which allows it to probe the model\u2019s understanding and reasoning more precisely and responsively. This is distinct from LLM-as-an-Interviewer, which paraphrases queries and conducts multi-turn questioning, and TreeEval, which generates questions and subtopics based on previous topics rather than the model\u2019s actual answers. The key advantage is that KIEval\u2019s method enables a more nuanced and targeted evaluation grounded in the model\u2019s demonstrated outputs.", "question_token_count": 56, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 48}
{"context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n", "topic": "The range of task categories covered by static benchmarks and their significance in comprehensive model assessment.", "question": "Why is it critical for static benchmarks to cover a broad range of task categories such as math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension when evaluating large language models?", "choices": {"A": "Because covering diverse task categories ensures a comprehensive evaluation that captures the multifaceted capabilities and potential weaknesses of models, preventing overfitting to a narrow skill set.", "B": "Because including many tasks allows for quicker benchmarking by testing unrelated skills simultaneously, reducing overall evaluation time.", "C": "Because focusing on a broad range of tasks guarantees that models will perform equally well on all types of problems without bias.", "D": "Because a large variety of task categories simplifies the scoring function design by averaging performance across unrelated benchmarks."}, "answer": "A", "explanation": "The critical importance lies in achieving a comprehensive and multidimensional assessment that reflects the varied abilities of language models, ensuring that evaluations reveal both strengths and weaknesses across different domains rather than being skewed by proficiency in a limited area.", "question_token_count": 42, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 23}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "Reflect on the broader challenges in creating robust benchmarking frameworks that can accurately measure LLM abilities without bias introduced by contamination.", "question": "Which fundamental challenge most critically undermines the validity of benchmarking frameworks for large language models when addressing contamination, especially syntactic contamination?", "choices": {"A": "The inability to distinguish between a model's memorized recall of training data and its genuine reasoning capability during inference.", "B": "The computational expense of continuously updating benchmarks to include the latest training data.", "C": "The inherent variability in human-annotated benchmark data leading to inconsistent evaluation metrics.", "D": "The lack of standardized metrics to measure model performance across different NLP tasks."}, "answer": "A", "explanation": "The core issue with contamination, particularly syntactic contamination, is that it makes it difficult to tell if an LLM is recalling memorized data or demonstrating true reasoning, which critically undermines benchmark validity by inflating performance scores and misrepresenting generalization.", "question_token_count": 25, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 17}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The design and function of contamination detectors as tools to identify and quantify risks of benchmark dataset contamination in the context of LLM training data.", "question": "In the context of large language model (LLM) benchmarking, how do contamination detectors fundamentally contribute to maintaining the integrity of static benchmark datasets, and what is a primary limitation they face as LLMs continue to train on increasingly overlapping data?", "choices": {"A": "They identify exact matches between benchmark questions and LLM training data to flag contamination; however, they struggle with detecting semantic or paraphrased overlaps as models learn from diverse data variations.", "B": "They dynamically rewrite benchmark questions to avoid overlap with training data, but their main limitation is that they cannot quantify contamination risk accurately.", "C": "They prevent contamination by restricting access to benchmark datasets during model training, but they are limited because static benchmarks inherently cannot adapt to new data.", "D": "They measure LLM performance degradation due to contamination after deployment, but their limitation is that they cannot detect contamination prior to training or evaluation phases."}, "answer": "A", "explanation": "Contamination detectors primarily function by detecting overlaps\u2014often exact or near-exact matches\u2014between benchmark data and LLM training sets to quantify contamination risk, preserving benchmark validity. However, as LLMs are exposed to vast and diverse training data, contamination can occur through semantic similarity or paraphrasing, which these detectors find challenging to identify, representing a significant limitation.", "question_token_count": 48, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 4, "avg_answer_token_count": 29}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "The spectrum of reasoning benchmarks from intuitive everyday reasoning (PIQA, SIQA, HellaSwag, WinoGrande) to academic challenge sets (ARC, OpenBookQA, CommonsenseQA), and how they test different facets of model reasoning.", "question": "How do intuitive everyday reasoning benchmarks like PIQA, SIQA, HellaSwag, and WinoGrande fundamentally differ from academic challenge sets such as ARC, OpenBookQA, and CommonsenseQA in the facets of reasoning they assess, and what implications does this have for evaluating the reasoning capabilities of language models?", "choices": {"A": "Intuitive benchmarks primarily test a model\u2019s ability to perform logical deduction using formal symbolic rules, whereas academic sets focus on memorization of factual knowledge without requiring inference.", "B": "Intuitive benchmarks assess a model\u2019s quick, commonsense judgments across diverse real-world scenarios, emphasizing multiple perspectives of everyday knowledge, while academic sets require integration of background knowledge with logical reasoning to solve more complex, knowledge-intensive problems.", "C": "Both intuitive and academic benchmarks test identical reasoning skills, but intuitive sets use simpler language to make the tasks easier, and academic sets use complex language to increase difficulty.", "D": "Intuitive benchmarks evaluate a model\u2019s ability to follow detailed instructions step-by-step, whereas academic challenge sets measure proficiency in generating and debugging code."}, "answer": "B", "explanation": "Intuitive everyday reasoning benchmarks evaluate a model\u2019s ability to apply commonsense and everyday knowledge rapidly and flexibly across multiple perspectives. In contrast, academic challenge sets push models to synthesize explicit background knowledge with logical inference to produce plausible answers, thus assessing deeper, knowledge-integrated reasoning. This distinction implies that evaluating a model\u2019s reasoning requires multiple types of benchmarks to fully capture both intuitive and formal reasoning capabilities.", "question_token_count": 65, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 35}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "Assess how reliance on syntactic information in certain NLP applications influences the interpretation of contamination and benchmarking outcomes.", "question": "How does the reliance on syntactic information in certain NLP applications alter the interpretation of syntactic contamination and affect the reliability of benchmarking outcomes for large language models?", "choices": {"A": "It makes syntactic contamination less relevant because syntactic rephrasing does not impact models that focus on semantics, thus benchmarks remain reliable.", "B": "It increases the impact of syntactic contamination by making benchmarks more susceptible to memorization effects, thereby inflating performance metrics and undermining the evaluation of true reasoning capabilities.", "C": "It reduces the significance of syntactic contamination since models trained on syntactic variations generalize better, leading to more robust benchmarking outcomes.", "D": "It has no effect on contamination interpretation or benchmarking reliability, as syntactic changes do not alter the underlying data distribution relevant for model evaluation."}, "answer": "B", "explanation": "Reliance on syntactic information heightens the influence of syntactic contamination because benchmarks that involve syntactic manipulations become vulnerable to models exploiting memorized patterns rather than demonstrating true reasoning, thus inflating performance and compromising the validity of benchmarking outcomes.", "question_token_count": 30, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 8, "avg_answer_token_count": 27}
{"context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n", "topic": "The limitations imposed by label protection on transparency, independent verification, and reproducibility in model performance evaluation.", "question": "How does label protection fundamentally affect the scientific rigor of model performance evaluation, and why does this necessitate reliance on centralized systems despite the availability of post-hoc contamination detection techniques?", "choices": {"A": "Label protection reduces transparency and prevents independent verification, forcing reliance on centralized evaluation systems that hinder detailed error analysis and reproducibility, because post-hoc detection can only identify contamination after evaluation but cannot replace open access to labels for thorough validation.", "B": "Label protection enhances scientific rigor by ensuring all researchers use the same centralized evaluation systems, which improves reproducibility and eliminates the need for post-hoc contamination detection.", "C": "Label protection allows researchers to freely perform detailed error analysis and independent verification, as centralized evaluation systems provide open access to protected labels for transparency.", "D": "Label protection only limits access to test labels but has no impact on reproducibility or error analysis, since post-hoc detection fully compensates for any transparency issues by identifying data contamination."}, "answer": "A", "explanation": "Label protection limits transparency and independent verification by restricting access to test labels, which forces reliance on centralized evaluation systems that impede detailed error analysis and reproducibility. Although post-hoc detection techniques help identify data contamination, they cannot substitute for open label access necessary for thorough independent evaluation and scientific rigor.", "question_token_count": 35, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 34}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "The methodology and impact of transforming directed acyclic graphs (DAGs) into natural language descriptions for reasoning evaluation in DyVal.", "question": "How does the rule-based conversion of directed acyclic graphs (DAGs) into natural language descriptions in DyVal impact the fidelity of reasoning evaluation, and what key challenges does this transformation introduce for assessing an LLM\u2019s ability to accurately deduce the root node\u2019s value?", "choices": {"A": "It ensures perfect structural fidelity since natural language precisely encodes all graph dependencies, allowing straightforward root value deduction without ambiguity.", "B": "It introduces ambiguity and potential loss of structural detail, requiring the LLM to reconstruct implicit logical dependencies from language, which complicates accurate root node value deduction.", "C": "It simplifies the reasoning task by linearizing graph dependencies, thus making it easier for the LLM to identify the root node value through sequential reasoning.", "D": "It replaces graph-based reasoning with purely symbolic logic expressions, bypassing natural language interpretation and thus isolating the LLM\u2019s symbolic reasoning capabilities."}, "answer": "B", "explanation": "The rule-based conversion translates DAGs into natural language descriptions that may not explicitly encode every structural detail, forcing LLMs to infer implicit dependencies from language. This transformation can introduce ambiguity and increase reasoning complexity, making the accurate deduction of the root node\u2019s value more challenging and a more rigorous test of the model\u2019s reasoning capabilities.", "question_token_count": 52, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 4, "avg_answer_token_count": 27}
{"context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n", "topic": "Ethical and methodological considerations arising from the opacity of LLM training data in assessing model performance claims.", "question": "How does the opacity of large language model (LLM) training data primarily complicate the ethical and methodological assessment of model performance claims?", "choices": {"A": "It prevents researchers from knowing the exact model architecture, which hinders reproducibility of results.", "B": "It obscures the degree of overlap between training and evaluation data, increasing the risk of data contamination and inflating performance metrics.", "C": "It restricts access to the fine-tuning algorithms used, making it impossible to compare models fairly.", "D": "It limits understanding of the preprocessing techniques applied to training data, which affects tokenization consistency during evaluation."}, "answer": "B", "explanation": "The opacity of LLM training data mainly creates ethical and methodological challenges by hiding whether evaluation datasets have been seen during training, which leads to data contamination and potential inflation of performance claims, undermining fair and reliable benchmarking.", "question_token_count": 27, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 21}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "The design principles and rationale behind using randomly generated SQL tables and queries in S3Eval to evaluate LLM reasoning abilities over tabular data.", "question": "What is the primary rationale for using randomly generated SQL tables and queries in S3Eval when evaluating the reasoning abilities of large language models over tabular data?", "choices": {"A": "To ensure that LLMs are tested on a wide variety of unpredictable schemas and query structures, preventing memorization and encouraging genuine relational reasoning.", "B": "To simplify the evaluation process by using only standard, repetitive table schemas that LLMs can easily recognize and parse.", "C": "To focus the evaluation exclusively on the LLM\u2019s ability to recall fixed SQL query templates rather than generalizing to novel queries.", "D": "To limit the complexity of SQL queries and tables so that the evaluation measures only surface-level pattern matching abilities."}, "answer": "A", "explanation": "Random generation in S3Eval creates diverse and unpredictable SQL tables and queries, which prevents LLMs from relying on memorization or fixed patterns and instead requires them to apply true relational reasoning and logic execution skills.", "question_token_count": 31, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 24}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The importance of benchmark dataset curation and human involvement in crafting reliable static benchmarks like HumanEval.", "question": "Considering the inherent risk of data contamination in static benchmarks like HumanEval due to their public availability and extensive internet scraping by LLMs, why does human involvement in the curation and design of these benchmarks remain critical to ensuring reliable evaluation of LLM capabilities?", "choices": {"A": "Because human curation enables the creation of nuanced, diverse, and carefully structured tasks that automated methods cannot yet replicate, maintaining benchmark validity despite contamination risks.", "B": "Because human involvement primarily serves to encrypt benchmark data, preventing models from accessing the test sets during training.", "C": "Because human designers can directly control and restrict the training data LLMs access, ensuring no contamination occurs.", "D": "Because human curation allows benchmarks to be updated dynamically in real-time, eliminating the need for static datasets."}, "answer": "A", "explanation": "Human involvement is essential in crafting benchmarks like HumanEval because humans can design sophisticated, diverse, and meaningful evaluation tasks that capture nuanced model abilities. This complexity is not easily replicated by automated or dynamic methods. Although contamination risk exists, the quality and validity of the benchmark depend heavily on expert human design, which maintains the benchmark's evaluative power and relevance.", "question_token_count": 50, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 23}
{"context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n", "topic": "The absence of standardized criteria for evaluating dynamic benchmarks and the consequences for benchmarking consistency.", "question": "How does the absence of standardized criteria for evaluating dynamic benchmarks fundamentally impact the consistency and reliability of benchmarking large language models, and why is establishing such standards critical for advancing LLM evaluation methodologies?", "choices": {"A": "It leads to inconsistent benchmarking results and hinders comparability across studies, making it difficult to reliably assess model performance and contamination risks.", "B": "It primarily affects the speed of benchmarking procedures but does not influence the validity of performance comparisons between models.", "C": "It only limits the diversity of dynamic benchmarks but has no effect on the accuracy or fairness of model evaluations.", "D": "It causes static benchmarks to become obsolete, eliminating the need for any traditional evaluation methods."}, "answer": "A", "explanation": "Without standardized criteria, dynamic benchmarks can vary widely in design and evaluation metrics, resulting in inconsistent and non-comparable results that undermine the reliability of LLM performance assessments; establishing standards is therefore essential to ensure valid, fair, and reproducible benchmarking practices.", "question_token_count": 37, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 22}
{"context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n", "topic": "Examination of how benchmarks like GLUE, SuperGLUE, and OpenAI\u2019s HumanEval employ label protection to prevent models from memorizing true answers during training.", "question": "How does label protection specifically safeguard benchmark evaluation integrity in contrast to encryption methods, and why is this approach particularly favored in benchmarks like GLUE, SuperGLUE, and OpenAI\u2019s HumanEval?", "choices": {"A": "Label protection prevents models from accessing test inputs by encrypting them, whereas encryption hides only the answers; it is favored because it reduces computational overhead.", "B": "Label protection hides the true test answers from public access to prevent models from memorizing them during training, maintaining evaluation integrity; it is favored because it directly blocks answer exposure without requiring complex key management.", "C": "Label protection uses confidential computing to keep both test data and model parameters secret, whereas encryption only secures test data; it is favored because it enables private benchmarking.", "D": "Label protection allows open access to test labels but restricts training data usage, whereas encryption fully restricts data access; it is favored because it simplifies model training procedures."}, "answer": "B", "explanation": "Label protection specifically withholds the true test answers from public access, preventing models from memorizing or learning them during training, thereby preserving the validity and integrity of evaluation results. Unlike encryption, which secures the data itself but requires strong key management and incurs overhead, label protection directly blocks answer exposure, which is why benchmarks like GLUE, SuperGLUE, and HumanEval adopt this approach.", "question_token_count": 40, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 33}
{"context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n", "topic": "The role of interpretability in ensuring correctness and reducing verification costs in dynamic benchmarking data transformations.", "question": "In the context of dynamic benchmarking data transformations, how does the interpretability of rule-based approaches fundamentally differ from that of LLM-assisted transformations in terms of ensuring correctness and minimizing verification costs?", "choices": {"A": "Rule-based transformations are inherently interpretable, enabling direct correctness verification and thus lower verification costs, whereas LLM-assisted transformations rely on opaque generative processes requiring additional explainability tools or human-in-the-loop validation, increasing verification complexity and costs.", "B": "LLM-assisted transformations are inherently interpretable due to model transparency, which eliminates the need for human verification, whereas rule-based transformations require extensive manual checks due to their rigid structure.", "C": "Both rule-based and LLM-assisted transformations have equal interpretability, but LLM-assisted transformations reduce verification costs by automating correctness checks without human intervention.", "D": "Rule-based transformations are less interpretable than LLM-assisted ones because rule-based methods generate complex, opaque data, necessitating more manual validation and thus higher verification costs."}, "answer": "A", "explanation": "Rule-based transformations are designed with explicit, transparent rules making them inherently interpretable, which simplifies correctness verification and reduces verification costs. Conversely, LLM-assisted transformations produce data through less transparent generative processes, necessitating additional mechanisms such as explainability tools or human oversight to verify correctness, thereby increasing verification complexity and cost.", "question_token_count": 37, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 36}
{"context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n", "topic": "The definition and interpretation of the cost function in scalability evaluation, encompassing monetary cost, time, and manual effort.", "question": "How does the interpretation of the cost function in scalability evaluation critically influence the balance between dataset size and resource expenditure in dynamic benchmarking, and why is considering multiple cost dimensions (monetary, time, manual effort) essential for accurately quantifying scalability?", "choices": {"A": "Because the cost function only accounts for monetary expense, scalability is best measured by the largest possible dataset size regardless of time or manual effort, ensuring maximal data generation.", "B": "The cost function's multidimensional nature ensures that scalability reflects a practical trade-off, where generating larger datasets must be balanced against diverse resource costs, preventing misleading evaluation based solely on dataset size.", "C": "Since time and manual effort are negligible compared to monetary cost, they can be omitted from the cost function without affecting the accuracy of scalability measurement.", "D": "The cost function is irrelevant to scalability since the primary goal is to maximize dataset size, and resource expenditure should be considered separately in benchmarking evaluation."}, "answer": "B", "explanation": "The cost function is integral to scalability measurement because it normalizes dataset size by the associated resource costs, which are multidimensional. Considering monetary cost, time, and manual effort ensures that scalability evaluation reflects realistic trade-offs in dynamic benchmarking, avoiding overemphasis on dataset size alone.", "question_token_count": 48, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 31}
{"context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n", "topic": "Potential strategies or theoretical frameworks to develop more generalizable complexity metrics for diverse benchmark datasets.", "question": "Considering the challenges in creating complexity metrics that generalize across domains, which theoretical approach best addresses the trade-off between domain specificity and broad applicability to ensure stable complexity measurement in dynamic benchmark datasets?", "choices": {"A": "Developing a unified complexity metric based solely on graph-theoretic properties applicable to all reasoning and non-reasoning tasks.", "B": "Employing a modular framework that combines domain-specific complexity components with a meta-metric capturing statistical variance across trials to quantify stability.", "C": "Relying exclusively on empirical performance drops of LLMs as a proxy for dataset complexity, disregarding mathematical formulations of complexity.", "D": "Utilizing a single heuristic complexity score derived from seed dataset characteristics without accounting for transformations or variance in trials."}, "answer": "B", "explanation": "Option B best addresses the trade-off by integrating domain-specific insights through modular components while incorporating a meta-level statistical measure (variance) to assess stability across dynamic transformations, thus supporting generalizability without losing necessary task nuances.", "question_token_count": 37, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 5, "avg_answer_token_count": 24}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "The limitations of canary strings as a mitigation method, including reliance on model developers' awareness and integrity.", "question": "Considering the reliance of canary strings on model developers' awareness and integrity, which of the following best explains why this mitigation method may fail to prevent data contamination in large language models?", "choices": {"A": "Because canary strings can be easily removed by automated preprocessing before training.", "B": "Because if developers deliberately choose to ignore canary strings, the method cannot detect intentional data leakage.", "C": "Because canary strings only detect contamination in dynamic benchmarking datasets, not static ones.", "D": "Because canary strings degrade the overall performance of the model when used excessively."}, "answer": "B", "explanation": "The main limitation of canary strings is that their effectiveness depends on developers noticing and responding to them; if developers intentionally ignore or bypass these markers to leak data, the mitigation fails.", "question_token_count": 37, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 17}
{"context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n", "topic": "The role and mechanisms of encryption methods in securing evaluation data to prevent unauthorized access and contamination of training sets.", "question": "Considering the limitations of encryption methods in securing evaluation data, such as reliance on strong key management and susceptibility to key exposure, how does the integration of confidential computing and secure multi-party computation, as exemplified by TRUCE, fundamentally enhance the protection of test data compared to traditional public key encryption approaches?", "choices": {"A": "By enabling encryption of test data with a more complex public key system that is immune to key exposure.", "B": "By allowing test data and model parameters to remain confidential during evaluation without exposing raw data or keys to any single party.", "C": "By replacing encryption with license agreements that legally prevent unauthorized data use, removing the need for cryptographic protections.", "D": "By simplifying key management through centralized control of all encryption keys used in the evaluation process."}, "answer": "B", "explanation": "TRUCE leverages confidential computing and secure multi-party computation to enable private benchmarking, which means that test data and model parameters are kept confidential during evaluation without any single party gaining access to raw data or private keys, addressing the vulnerabilities of traditional encryption that depend on key secrecy.", "question_token_count": 60, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 21}
{"context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n", "topic": "Practical strategies for designing dynamic benchmarks that maximize scalability according to the given quantitative criterion.", "question": "In the context of dynamic benchmarking scalability defined as the expected ratio of transformed dataset size to transformation cost, which strategy most effectively maximizes scalability when transformation costs include significant manual effort?", "choices": {"A": "Increasing the complexity of transformations to generate larger datasets regardless of cost increase.", "B": "Prioritizing transformations that yield moderate dataset size growth but minimal manual effort.", "C": "Minimizing the size of the transformed dataset to reduce overall transformation cost.", "D": "Focusing solely on reducing monetary costs while ignoring time and manual effort factors."}, "answer": "B", "explanation": "Since scalability is the expected proportion of data generated per unit cost, when manual effort is a significant cost factor, strategies that moderate dataset size growth but minimize manual effort optimize scalability best. Increasing complexity raises cost disproportionately, minimizing dataset size reduces output, and ignoring manual effort misses key cost components.", "question_token_count": 35, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 8, "avg_answer_token_count": 15}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "The integration of background knowledge with logical reasoning in academic reasoning benchmarks and its significance in evaluating language model understanding.", "question": "Why is the integration of background knowledge with logical reasoning crucial in academic reasoning benchmarks like ARC and OpenBookQA for evaluating language model understanding?", "choices": {"A": "Because it tests a model\u2019s ability to memorize facts without requiring inference.", "B": "Because it ensures that models can generate plausible answers by combining factual knowledge with logical deduction.", "C": "Because it focuses solely on evaluating a model\u2019s intuitive reasoning skills without external knowledge.", "D": "Because it simplifies the reasoning task by limiting it to straightforward fact retrieval."}, "answer": "B", "explanation": "Academic reasoning benchmarks require models not only to recall background information but also to apply logical reasoning to that knowledge to arrive at plausible answers, thereby assessing deeper comprehension beyond memorization or intuition alone.", "question_token_count": 28, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 9, "question_groundedness_score": 10, "avg_answer_token_count": 16}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "The role and importance of benchmark extensions like MMLU-Redux and MMLU-Pro in refining the assessment of LLM knowledge capabilities.", "question": "How do benchmark extensions such as MMLU-Redux and MMLU-Pro fundamentally enhance the evaluation of large language model knowledge capabilities compared to the original MMLU benchmark?", "choices": {"A": "By expanding the range of multi-step mathematical problem-solving tasks beyond the original MMLU scope.", "B": "By refining and increasing the precision of multi-domain knowledge assessments through improved question quality and difficulty calibration.", "C": "By focusing exclusively on open-domain question answering rather than multi-domain knowledge evaluation.", "D": "By introducing technical and long-context challenges unrelated to knowledge retrieval capabilities."}, "answer": "B", "explanation": "MMLU-Redux and MMLU-Pro are designed to refine the original MMLU benchmark's multi-domain knowledge evaluation by enhancing question quality, coverage, and difficulty calibration, thereby providing a more precise and nuanced assessment of LLM knowledge capabilities.", "question_token_count": 36, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 17}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "The implications of temporal cutoff benchmarks for assessing LLM capabilities in dynamic knowledge domains like mathematics, coding, academic writing, and forecasting.", "question": "Why is employing a temporal cutoff date critical when constructing benchmarks for evaluating large language models in rapidly evolving domains like mathematics and coding, and what are the primary implications of this approach for assessing model capabilities?", "choices": {"A": "It prevents data contamination by ensuring evaluation data postdates the model\u2019s knowledge cutoff, allowing fair assessment of the model\u2019s ability to generalize to unseen, new information.", "B": "It allows models to be trained continuously on the latest data, ensuring the benchmarks reflect the most current knowledge the model possesses.", "C": "It simplifies benchmark creation by limiting datasets to a single fixed time period, reducing the complexity of data collection and evaluation.", "D": "It guarantees that the model\u2019s performance on older data is prioritized over newer data, highlighting foundational knowledge rather than recent developments."}, "answer": "A", "explanation": "Employing a temporal cutoff ensures that evaluation datasets contain information unavailable to the model during training, thus preventing data contamination and enabling an accurate measurement of the model\u2019s ability to apply its learned knowledge to truly novel content, which is especially important in fast-changing domains.", "question_token_count": 40, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 27}
{"context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n", "topic": "The necessity and implementation of explainability tools and human-in-the-loop validation to enhance the interpretability of LLM-assisted transformations.", "question": "Why are explainability tools and human-in-the-loop validation particularly essential for ensuring the correctness of LLM-assisted data transformations in dynamic benchmarking, unlike rule-based transformations?", "choices": {"A": "Because LLM-assisted transformations generate data that inherently lacks transparency and traceability, making errors difficult to detect without additional interpretability mechanisms.", "B": "Because rule-based transformations are more prone to errors, so they require less interpretability support compared to LLM-assisted transformations.", "C": "Because explainability tools can directly modify the outputs of LLMs to correct errors without human intervention.", "D": "Because human-in-the-loop validation is only necessary when using temporal cutoff data collection, not in LLM-assisted transformations."}, "answer": "A", "explanation": "LLM-assisted transformations lack inherent transparency and traceability, unlike rule-based methods, which makes it difficult to verify correctness solely by inspecting the transformation logic; thus, explainability tools and human validation are critical to ensure reliability and reduce costly manual verification.", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 23}
{"context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n", "topic": "The application and implications of graph complexity metrics, such as those proposed by DyVal, for evaluating reasoning problem complexity in benchmarks.", "question": "How does the application of graph complexity metrics, such as those proposed by DyVal, improve the evaluation of reasoning problem complexity in dynamic benchmarks, and what role does the stability of these metrics play in interpreting performance variations of large language models on transformed datasets?", "choices": {"A": "Graph complexity metrics provide a domain-specific measure tailored to reasoning problems, and high stability ensures that performance variations are solely due to data contamination rather than task complexity changes.", "B": "By generalizing complexity measurement through graph representations, DyVal enables consistent assessment across different reasoning tasks, while stability\u2014measured as low variance in complexity across trials\u2014indicates reliable complexity estimates that help distinguish genuine task difficulty increases from data contamination effects.", "C": "DyVal\u2019s graph complexity metrics focus on syntactic complexity of input data, and high stability implies that all performance drops in LLMs are artifacts of model limitations, not changes in task complexity.", "D": "The use of graph complexity metrics allows dynamic benchmarks to ignore complexity fluctuations, and stability reflects the average performance of LLMs rather than complexity variance, thus simplifying interpretation of model results."}, "answer": "B", "explanation": "DyVal\u2019s graph complexity metrics offer a generalized approach to measuring reasoning problem complexity beyond domain-specific methods, using graph representations. Stability, defined as the variance of complexity measurements across trials, is crucial because low variance means the complexity metric is reliable and consistent, enabling researchers to distinguish whether performance drops stem from increased task difficulty or from data contamination.", "question_token_count": 51, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 39}
{"context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n", "topic": "Critical analysis of the trade-offs between dynamic dataset transformations, complexity stability, and benchmarking validity in evaluating LLMs.", "question": "In the context of dynamic benchmarking for large language models, how does the stability of complexity metrics influence the interpretation of performance drops, and why is this stability difficult to achieve across diverse datasets?", "choices": {"A": "Stability ensures that performance drops can be attributed to data contamination rather than increased task complexity, but achieving it is difficult because complexity metrics are often domain-specific and do not generalize well.", "B": "Stability allows for ignoring task complexity when interpreting performance drops, and it is easily achieved through existing universal complexity metrics.", "C": "Stability guarantees that all dynamic dataset transformations reduce task complexity, simplifying performance analysis, but it is hard to implement due to lack of computational resources.", "D": "Stability indicates that performance drops only occur due to random variance in LLM responses, and difficulty arises because complexity metrics are too simplistic and underestimate complexity changes."}, "answer": "A", "explanation": "Stability in complexity metrics is crucial to distinguish whether performance drops result from contamination or increased complexity; however, this is challenging because current complexity measures are often specialized for certain domains and lack general applicability across datasets, leading to variance and instability in complexity assessments.", "question_token_count": 38, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 30}
{"context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n", "topic": "The implications of data contamination and benchmarking methodology on the validity and trustworthiness of LLM performance assessments.", "question": "How does the transition from static to dynamic benchmarking address the challenges of data contamination in LLM performance evaluation, and why is the establishment of standardized criteria critical to ensuring the validity and trustworthiness of these dynamic benchmarks?", "choices": {"A": "Dynamic benchmarking continuously updates test data to reduce overlap with training sets, mitigating contamination effects; standardized criteria are essential to objectively evaluate dynamic benchmarks\u2019 effectiveness and maintain consistent trust in LLM assessments.", "B": "Dynamic benchmarking relies on fixed datasets that exclude known contaminated samples, which fully eliminates contamination; standardized criteria are less important since contamination is already removed.", "C": "Static benchmarking is preferable because it uses unchanging datasets that ensure repeatability, whereas dynamic benchmarking introduces variability that undermines trust regardless of criteria.", "D": "Data contamination only affects training efficiency, not benchmarking validity; therefore, dynamic benchmarking and standardization efforts have minimal impact on trustworthiness."}, "answer": "A", "explanation": "The transition to dynamic benchmarking is designed to reduce contamination by continuously refreshing evaluation data, limiting overlap with the model's training corpus. However, without standardized criteria to assess dynamic benchmarks, it is difficult to gauge their effectiveness or compare results across studies, risking inconsistent or untrustworthy performance evaluations.", "question_token_count": 42, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 30}
{"context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n", "topic": "Reflection on the limitations of current LLM benchmarking surveys, particularly regarding coverage of recent methods, technical depth, and real-world validation of criteria.", "question": "How do the limitations highlighted in current LLM benchmarking surveys, particularly regarding coverage of recent methods, technical depth, and real-world validation, affect the reliability and future evolution of benchmarking practices?", "choices": {"A": "They undermine benchmarking reliability by allowing contamination risks to go unaddressed and hinder the adoption of dynamic methods due to lack of practical validation, necessitating ongoing refinement and standardization.", "B": "They have minimal impact since static benchmarking methods remain fully reliable despite rapid developments and do not require further refinement.", "C": "They primarily affect only the theoretical understanding of benchmarking, without influencing practical applications or tool development.", "D": "They suggest that current benchmarking surveys are complete and need no further updates or validation for real-world deployment."}, "answer": "A", "explanation": "The limitations\u2014such as incomplete coverage of recent methods, preliminary dynamic benchmarking criteria, and lack of fine-grained technical details\u2014reduce the reliability of benchmarking by leaving contamination risks and dynamic evaluation challenges insufficiently addressed, thus indicating that benchmarking practices must continue evolving with refined, validated approaches for real-world use.", "question_token_count": 37, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 2, "avg_answer_token_count": 24}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The concept of data contamination in LLM benchmarking datasets, its sources, and its potential impact on the validity of model evaluation.", "question": "How does data contamination in LLM benchmarking datasets arise, and why does it fundamentally compromise the validity of model evaluation, necessitating the development of contamination detectors and dynamic benchmarks?", "choices": {"A": "Data contamination occurs when benchmark datasets include errors or mislabeled examples, leading to unreliable evaluation results, which is why contamination detectors and dynamic benchmarks aim to correct these labeling mistakes.", "B": "Data contamination arises because LLMs are frequently trained on all publicly available data, including benchmark datasets, causing models to have prior exposure to test examples; this prior exposure inflates performance scores and undermines the fairness and reliability of the evaluation, prompting the need for contamination detectors and dynamic benchmarks to detect and mitigate this overlap.", "C": "Data contamination happens when benchmarks are too difficult for current LLMs, causing artificially low scores and necessitating contamination detectors and dynamic benchmarks to simplify evaluation tasks.", "D": "Data contamination is the result of intentional manipulation of benchmark datasets by model developers, and contamination detectors and dynamic benchmarks serve to prevent such tampering to maintain evaluation integrity."}, "answer": "B", "explanation": "The correct answer identifies that data contamination stems from the overlap between training data and benchmark datasets, leading to prior exposure of test examples to models; this inflates evaluation results and compromises validity, which is why contamination detectors and dynamic benchmarks are developed to quantify and reduce such contamination.", "question_token_count": 34, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 9, "avg_answer_token_count": 41}
{"context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n", "topic": "Discuss the ethical implications of transparency in benchmarking results and how misuse of these results can affect trust and fairness in AI model assessments.", "question": "How can misuse of transparent benchmarking results, such as selective evaluation criteria or artificial performance inflation, undermine the ethical principles of fairness and trust in AI model assessments?", "choices": {"A": "It can lead to biased assessments that advantage some models unfairly and erode stakeholder confidence in evaluation integrity.", "B": "It primarily affects the technical accuracy of models without impacting fairness or trust in assessments.", "C": "Misuse of benchmarking results only impacts privacy and security concerns, not fairness or trust.", "D": "Transparency in benchmarking inherently prevents any misuse, ensuring fairness and trust automatically."}, "answer": "A", "explanation": "Misuse like selectively choosing biased criteria or inflating results distorts fair comparison and damages trust in benchmarks, thereby violating fairness and accountability principles.", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 18}
{"context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n", "topic": "Describe the mathematical formulation of external diversity using expectation notation and interpret its components in the context of dataset transformation.", "question": "In the mathematical formulation of external diversity expressed as \ud835\udd3c_i=1^N \u0398(\ud835\udc9f_i, \ud835\udc9f), what is the precise interpretation of the expectation operator and the function \u0398 in the context of dataset transformation, and how do they collectively characterize the diversity of the transformed datasets relative to the original seed dataset?", "choices": {"A": "The expectation operator averages the diversity scores computed by \u0398 over all transformed datasets \ud835\udc9f_i, where \u0398 quantifies the difference between each transformed dataset and the original dataset \ud835\udc9f, thereby measuring how varied the transformations are relative to the seed dataset.", "B": "The expectation operator selects the maximum diversity score from \u0398 applied to pairs of transformed datasets \ud835\udc9f_i and \ud835\udc9f, where \u0398 measures similarity, thus focusing on the closest transformed dataset to the seed dataset.", "C": "The expectation operator sums all internal pairwise diversity scores between transformed datasets \ud835\udc9f_i and \ud835\udc9f_j, while \u0398 measures the difference between transformed datasets only, ignoring the original dataset \ud835\udc9f.", "D": "The expectation operator calculates the variance of the transformation function \u0398 over the original dataset \ud835\udc9f, where \u0398 quantifies the internal consistency of each transformed dataset \ud835\udc9f_i independently."}, "answer": "A", "explanation": "The expectation operator \ud835\udd3c_i=1^N averages over all N transformed datasets, applying the function \u0398 to measure the difference between each transformed dataset \ud835\udc9f_i and the original seed dataset \ud835\udc9f. This average quantifies the overall external diversity, reflecting how much on average the transformations differ from the original data.", "question_token_count": 66, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 40}
{"context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n", "topic": "Challenges and limitations of existing complexity measurement metrics and their lack of generalizability across different domains.", "question": "Why do existing complexity measurement metrics often fail to generalize across different domains, and how does this limitation affect the reliability of dynamic benchmarking methods in distinguishing between data contamination and increased task complexity in LLM evaluations?", "choices": {"A": "Because complexity metrics are typically designed for specific data structures or reasoning types, their domain specificity limits applicability; this causes instability in benchmarking results, making it difficult to attribute performance drops accurately to contamination or complexity changes.", "B": "Because complexity metrics are universally applicable but computationally expensive, their use is often avoided; this leads to reliance on data contamination as the sole explanation for performance drops in dynamic benchmarks.", "C": "Because existing complexity metrics are designed to measure only dataset size, they overlook task difficulty; hence, performance drops are always attributed to increased complexity rather than contamination.", "D": "Because complexity metrics primarily focus on model architecture rather than dataset characteristics, they fail to detect contamination, which undermines the ability to identify true complexity changes in benchmarks."}, "answer": "A", "explanation": "The core reason existing complexity metrics fail to generalize is their design for particular domains or data types, limiting cross-domain applicability. This domain specificity leads to high variance in measured complexity across trials, causing instability in benchmarking. Consequently, it becomes challenging to discern whether performance drops in LLMs arise from data contamination or genuine increases in task complexity, reducing the reliability of dynamic benchmarks.", "question_token_count": 40, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 35}
{"context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n", "topic": "The function and significance of the scoring function in quantifying alignment between transformed outputs and ground truth values.", "question": "In the context of quantifying the correctness of dynamic benchmarks, what is the fundamental role of the scoring function when assessing the alignment between transformed outputs and their ground truth values, and why is this role critical for ensuring reliable benchmarking of large language models?", "choices": {"A": "It generates the ground truth outputs against which the transformed dataset is compared, ensuring an objective reference for correctness.", "B": "It quantitatively measures the degree of alignment or similarity between transformed outputs and ground truth values, enabling nuanced evaluation of correctness beyond binary matching.", "C": "It transforms the input data into a new format to facilitate easier comparison with ground truth, thereby simplifying the evaluation process.", "D": "It filters out incorrect outputs from the transformed dataset to maintain only fully correct instances, ensuring benchmark precision."}, "answer": "B", "explanation": "The scoring function's fundamental role is to measure how well the transformed outputs align with the ground truth values provided by the oracle; this measurement is often nuanced, capturing degrees of similarity or correctness rather than a simple yes/no match. This enables reliable and meaningful assessment of benchmark quality, ensuring that evaluations of LLMs based on these benchmarks are valid and not misleading.", "question_token_count": 49, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 23}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The conceptual foundations and practical implementations of dynamic benchmarking approaches to reduce contamination in LLM evaluation.", "question": "What is the primary conceptual advantage of dynamic benchmarking approaches over static benchmarks in mitigating data contamination in LLM evaluation, and how do dynamic methods practically implement this advantage to maintain benchmark integrity?", "choices": {"A": "Dynamic benchmarking primarily avoids data contamination by encrypting benchmark datasets, making them inaccessible during model training; practically, this is implemented through cryptographic protections on static datasets.", "B": "Dynamic benchmarking continuously updates or regenerates benchmark datasets based on LLM training timelines, thereby minimizing overlap with training data; practically, this involves timestamp-based dataset refreshing and reconstruction to ensure benchmarks remain unseen by models.", "C": "Dynamic benchmarking relies on post-hoc detection and removal of contaminated model outputs after evaluation, ensuring benchmark integrity without altering the original static datasets.", "D": "Dynamic benchmarking uses manually curated, secret benchmark datasets that are never publicly released, thus preventing contamination through restricted access rather than dataset modification."}, "answer": "B", "explanation": "The primary conceptual advantage of dynamic benchmarking is its proactive approach to minimizing contamination by continuously adapting benchmark data to avoid overlap with training datasets, unlike static benchmarks which remain fixed and vulnerable. Practically, this is achieved through methods like timestamp-based updating and regeneration of benchmarks, ensuring the models are evaluated on data they have not seen during training.", "question_token_count": 36, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 8, "avg_answer_token_count": 31}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "The significance of open-domain evaluation benchmarks such as AlpacaEval and ArenaHard in testing LLMs\u2019 ability to handle diverse, unrestricted queries.", "question": "How do open-domain evaluation benchmarks like AlpacaEval and ArenaHard fundamentally differ from traditional domain-specific benchmarks, and why are they essential for assessing the true generalization capabilities of large language models?", "choices": {"A": "They focus on multi-step math problems rather than factual recall, essential for testing complex reasoning in LLMs.", "B": "They test LLMs on unrestricted, diverse queries across multiple domains, thereby evaluating flexibility and robustness beyond fixed-domain knowledge.", "C": "They evaluate LLMs on long-context technical tasks exclusively, which domain-specific benchmarks do not cover.", "D": "They primarily assess a model\u2019s ability to retrieve real-world factual information more accurately than specialized datasets."}, "answer": "B", "explanation": "Open-domain benchmarks like AlpacaEval and ArenaHard differ by challenging models with a wide variety of unpredictable, unrestricted queries that span multiple domains, rather than focusing narrowly on specific tasks like math problem-solving or factual recall. This approach is essential to gauge an LLM's adaptability, generalization, and robustness in real-world scenarios, testing its capacity to handle diverse inputs rather than just specialized knowledge.", "question_token_count": 38, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 21}
{"context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n", "topic": "The importance of correctness as a foundational criterion in evaluating the quality and reliability of dynamic benchmarking algorithms for large language models.", "question": "In the context of dynamic benchmarking algorithms for large language models, why is the correctness criterion considered foundational, and how does the incorporation of an oracle function and a scoring function collectively ensure the reliability of the benchmark evaluation?", "choices": {"A": "Because correctness guarantees that the generated dataset outputs exactly match the original inputs, the oracle function serves to reconstruct inputs, and the scoring function measures input similarity, ensuring the benchmark evaluates data fidelity.", "B": "Because correctness ensures that the benchmark outputs align with ground truth values as verified by the oracle, while the scoring function quantifies this alignment, thereby preventing misleading evaluations by providing an objective measure of dataset reliability.", "C": "Because correctness focuses on maximizing the diversity of outputs, the oracle function generates varied data points, and the scoring function evaluates the novelty of outputs, ensuring the benchmark challenges the language model effectively.", "D": "Because correctness validates the speed of dataset generation, the oracle function assesses computational efficiency, and the scoring function measures runtime performance to ensure scalable benchmarking."}, "answer": "B", "explanation": "Correctness is foundational because it guarantees that the dynamic benchmark outputs correspond accurately to ground truth, preventing false confidence in model evaluations. The oracle function provides the authoritative ground truth for comparison, and the scoring function quantifies how closely the benchmark outputs match this truth, collectively ensuring the benchmark's reliability and validity.", "question_token_count": 43, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 36}
{"context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n", "topic": "The role division and interaction among specialized LLM agents in the BENCHAGENTS system, including planning, generation, verification, and evaluation phases.", "question": "In the BENCHAGENTS multi-agent framework for automated benchmark creation, how does the division of labor among specialized LLM agents\u2014specifically planning, generation, verification, and evaluation\u2014contribute to the system\u2019s ability to produce scalable, diverse, and high-quality benchmarks, and what role does human-in-the-loop feedback play in enhancing this multi-agent collaboration?", "choices": {"A": "The division isolates tasks so each agent can independently optimize its phase without interaction, while human feedback replaces the need for verification and evaluation by agents.", "B": "The specialized agents sequentially build upon each other\u2019s outputs, enabling iterative refinement and quality control, while human-in-the-loop feedback guides and corrects agents to maintain diversity and high benchmark standards.", "C": "Each agent works in parallel on the entire benchmark creation process to maximize speed, and human feedback is only used post hoc to assess final benchmark quality without influencing agent collaboration.", "D": "The agents randomly alternate roles in each iteration to prevent bias, relying primarily on human feedback to direct the process rather than predefined agent specializations."}, "answer": "B", "explanation": "The correct answer highlights that BENCHAGENTS splits the benchmark creation process into sequential phases handled by specialized agents\u2014planning, generation, verification, and evaluation\u2014allowing iterative refinement and quality control. Human-in-the-loop feedback is crucial to guide and correct the agents, ensuring diversity and high-quality outputs.", "question_token_count": 72, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 32}
{"context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n", "topic": "Critically assess the trade-offs between adaptability in dynamic benchmarks and the potential privacy concerns they introduce in the context of AI evaluation.", "question": "In the context of AI evaluation, how do dynamic benchmarks' adaptability features fundamentally conflict with privacy concerns, and what ethical design principles can best reconcile this trade-off to ensure fair and secure assessments?", "choices": {"A": "Dynamic benchmarks continuously collect and update data, increasing privacy risks; implementing strict data minimization and transparency protocols can mitigate these risks while preserving adaptability.", "B": "Dynamic benchmarks avoid privacy concerns by using only synthetic data; therefore, no additional ethical design principles are necessary.", "C": "The adaptability of dynamic benchmarks inherently eliminates bias and privacy issues, making privacy protection less critical in their design.", "D": "Privacy concerns in dynamic benchmarks are negligible because data collection is infrequent, so ethical principles focus primarily on transparency of results rather than data handling."}, "answer": "A", "explanation": "Dynamic benchmarks improve evaluation relevance through continual data updates, which raises privacy risks due to ongoing data collection. Ethical design principles such as data minimization and transparency are essential to balance adaptability with privacy protection and fairness.", "question_token_count": 39, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 6, "avg_answer_token_count": 25}
{"context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n", "topic": "Describe the mathematical formulation of internal diversity using expectation notation and explain how it captures variation between different transformation trials.", "question": "How does the mathematical formulation of internal diversity using expectation notation over dataset pairs explicitly capture the variation between different transformation trials, and why is it important that the expectation excludes pairs where i = j?", "choices": {"A": "It computes the average similarity between each transformed dataset and the original seed dataset, including identical datasets, to measure variation.", "B": "It calculates the expected diversity measure between all pairs of distinct transformed datasets, excluding identical pairs (i = j), to quantify variation solely between different transformation trials.", "C": "It measures the diversity between each transformed dataset and a fixed reference dataset, ignoring pairwise comparisons among transformed datasets.", "D": "It averages the diversity measure over repeated transformations of the same dataset (i = j) to capture the variation within individual transformation trials."}, "answer": "B", "explanation": "The formulation of internal diversity as an expectation over pairs (i, j) with i \u2260 j explicitly measures the diversity between distinct transformed datasets, thus capturing variation between different transformation trials; excluding i = j avoids trivial zero differences from identical pairs.", "question_token_count": 39, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 26}
{"context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n", "topic": "The challenges and implications of publicly available benchmarking algorithms on the validity of LLM performance assessment.", "question": "How do the Collision Rate and Repeat Trials metrics collectively influence the reliability of dynamic benchmarks in assessing LLM capabilities when the benchmarks are publicly available and potentially included in LLM training data?", "choices": {"A": "They measure the frequency and overlap of benchmark data variations to ensure benchmarks generate sufficiently novel and diverse test cases despite training contamination, preserving evaluation validity.", "B": "They assess the overall accuracy of LLMs on the benchmark tasks by comparing performance across different training datasets contaminated with benchmark data.", "C": "They quantify the computational resources required to perform multiple benchmark transformations and the efficiency of retraining LLMs on contaminated data.", "D": "They identify which specific benchmark tasks have been memorized by LLMs during training, allowing exclusion of these tasks from evaluation."}, "answer": "A", "explanation": "Collision Rate measures the overlap between independently transformed benchmark datasets, indicating potential contamination, while Repeat Trials estimate how many transformations are needed to reproduce existing data, reflecting the benchmark\u2019s capacity for novel test case generation. Together, these metrics evaluate whether the benchmark can still produce diverse, uncorrupted challenges that reliably assess LLM capabilities despite public availability and contamination risk.", "question_token_count": 35, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 25}
{"context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n", "topic": "Propose methods to empirically estimate external and internal diversity metrics in practical scenarios and how to validate these estimates.", "question": "When empirically estimating external and internal diversity metrics for transformed datasets, what are the critical methodological steps to ensure reliable measurement and validation of these metrics, particularly regarding the choice of the diversity function \u0398(\u00b7) and the sampling of transformation trials?", "choices": {"A": "Select a diversity function \u0398(\u00b7) that is insensitive to dataset differences and use a single transformation trial to minimize variance in estimates.", "B": "Use multiple transformation trials to generate diverse datasets, compute \u0398(\u00b7) between datasets accordingly, and validate \u0398(\u00b7) by correlating its scores with domain-expert assessments or benchmark diversity measures.", "C": "Compute \u0398(\u00b7) only between the seed dataset and one transformed dataset, assuming this single comparison suffices for both external and internal diversity estimation.", "D": "Ignore the choice of \u0398(\u00b7) as any metric will yield similar diversity estimates, and focus solely on increasing the number of transformation trials without validation."}, "answer": "B", "explanation": "Reliable estimation requires multiple transformed datasets to capture variation (for both external and internal diversity), a carefully chosen \u0398(\u00b7) that meaningfully reflects dataset differences, and validation of this metric via correlation with expert judgment or benchmark measures to ensure it accurately captures diversity. Options A, C, and D fail to ensure meaningful measurement or validation.", "question_token_count": 47, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 6, "avg_answer_token_count": 30}
{"context": "###### Safety\nSafety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content. Datasets such as RealToxicityPrompts\u00a0(Gehman et\u00a0al., 2020) and ToxiGen\u00a0(Hartvigsen et\u00a0al., 2022) assess resilience against producing harmful outputs.\n\nBy providing a controlled environment to measure these aspects, safety benchmarks play a critical role in guiding the development of models that are not only powerful outputs but also responsible and trustworthy for real-world applications.\n\n\n###### Language\nLanguage benchmarks assess the LLMs\u2019 proficiency in specific languages.\n\nGLUE\u00a0(Wang, 2018) and SuperGLUE\u00a0(Wang et\u00a0al., 2019) cover tasks from sentiment analysis to language inference, while CLUE\u00a0(Xu et\u00a0al., 2020) targets Chinese language.\n\nTypo-fixing\u00a0(Suzgun et\u00a0al., 2022) is also widely used.\n\n\n###### Reading Comprehension\nReading comprehension tasks test a model\u2019s ability to extract and infer information from text. Benchmarks like SQuAD\u00a0(Rajpurkar et\u00a0al., 2018), QuAC\u00a0(Choi et\u00a0al., 2018), and BoolQ\u00a0(Clark et\u00a0al., 2019) challenge models to understand passages and draw logical conclusions.\n", "topic": "The implications of benchmark-driven development on the future design of large language models with respect to balancing performance, safety, and linguistic competence.", "question": "How does the reliance on safety benchmarks alongside language and reading comprehension benchmarks influence the architectural and training trade-offs in the future design of large language models, particularly in balancing performance, ethical robustness, and linguistic competence?", "choices": {"A": "It encourages a singular focus on maximizing linguistic competence, with safety considered only after performance goals are met, leading to sequential training phases.", "B": "It necessitates integrated training approaches that simultaneously optimize for ethical robustness and linguistic performance, often requiring trade-offs such as reduced model capacity for better safety compliance.", "C": "It results in the separation of models specialized for safety and others for linguistic tasks, avoiding trade-offs by deploying ensemble systems.", "D": "It primarily drives the expansion of model size and data volume, assuming that larger models inherently resolve safety and linguistic competence without explicit benchmark-driven trade-offs."}, "answer": "B", "explanation": "The correct answer reflects that reliance on diverse benchmarks requires integrated optimization strategies balancing competing demands; models must often trade off some performance or capacity to meet stringent safety standards while maintaining linguistic competence, thus influencing architectural and training design decisions.", "question_token_count": 42, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 6, "avg_answer_token_count": 28}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "Critical evaluation of how benchmark evolution reflects the growing complexity and diversity of tasks required to comprehensively assess LLM performance.", "question": "How does the evolution of benchmarks from datasets like GSM8K and NaturalQuestions to more recent challenges such as AIME 2024 and ControlBench reflect the increasing complexity and diversity required for comprehensive evaluation of large language models, and what critical implications does this have for assessing true LLM capabilities?", "choices": {"A": "The evolution shows a shift from single-step, domain-specific tasks to multi-step, multi-domain, and long-context challenges, indicating that comprehensive evaluation demands models capable of intricate reasoning, multi-faceted knowledge integration, and context management.", "B": "The evolution primarily focuses on increasing dataset size and frequency of updates, which mainly improves LLM training speed rather than their reasoning or knowledge integration abilities.", "C": "The benchmark progression reflects a trend toward simpler tasks to ensure broader accessibility, which implies that LLM capabilities are best assessed through straightforward, domain-limited evaluations.", "D": "The development of newer benchmarks emphasizes memorization of facts over reasoning, suggesting that true LLM capabilities are best measured by their recall accuracy alone."}, "answer": "A", "explanation": "The correct answer (A) captures how benchmark evolution incorporates more complex, diverse tasks requiring multi-step reasoning, multi-domain knowledge, and context handling, thus reflecting the critical need for comprehensive, nuanced LLM assessment beyond simple memorization or isolated tasks.", "question_token_count": 58, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 33}
{"context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n", "topic": "The methodology and workflow of the LLM-as-an-Interviewer framework, including query paraphrasing and multi-turn evaluation strategies.", "question": "In the LLM-as-an-Interviewer evaluation framework, what is the fundamental purpose of the initial query paraphrasing step before engaging in multi-turn follow-up questioning, and how does this design choice enhance the robustness and granularity of the LLM evaluation compared to static benchmark assessments?", "choices": {"A": "To simplify questions for the evaluated model, ensuring easier answer retrieval and thereby improving evaluation accuracy.", "B": "To introduce linguistic variability and avoid overfitting to static benchmark phrasing, enabling the interviewer LLM to probe deeper understanding through diverse follow-up interactions.", "C": "To shorten the evaluation process by converting complex questions into concise ones, thus allowing more questions to be asked within time constraints.", "D": "To standardize the questions into a fixed format that all evaluated models must answer identically, ensuring uniformity in assessment."}, "answer": "B", "explanation": "The initial paraphrasing step serves to reformulate existing static benchmark questions to introduce variability and prevent the evaluated model from exploiting memorized or rigid phrasing patterns. This linguistic diversity allows the interviewer LLM to engage in multi-turn interactions that explore nuanced aspects of the model\u2019s understanding, providing a more robust and fine-grained evaluation than static, fixed questions.", "question_token_count": 55, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 10, "avg_answer_token_count": 24}
{"context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n", "topic": "The integration of human-in-the-loop feedback within multi-agent frameworks to enhance benchmark quality, diversity, and scalability.", "question": "How does the integration of human-in-the-loop feedback within multi-agent evaluation frameworks fundamentally enhance the scalability, diversity, and quality of benchmark creation compared to purely automated multi-agent systems?", "choices": {"A": "It allows human reviewers to directly edit benchmark questions, ensuring manual control over diversity and quality without impacting scalability.", "B": "It introduces iterative human oversight that guides multi-agent coordination, enabling dynamic benchmark evolution that balances automated generation with human judgment to improve quality and diversity at scale.", "C": "It replaces multi-agent planning and verification steps with human decision-making, sacrificing scalability for higher quality benchmarks.", "D": "It primarily increases the speed of benchmark generation by automating human feedback through additional LLM agents, without affecting the benchmark\u2019s diversity or quality."}, "answer": "B", "explanation": "The integration of human-in-the-loop feedback does not simply add manual editing or replace automated steps but strategically incorporates human judgment to guide multi-agent collaboration, thereby enabling dynamic, scalable, and diverse benchmark evolution that maintains high quality. This iterative oversight allows the system to balance automation with nuanced human insight, enhancing all three aspects simultaneously.", "question_token_count": 36, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 9, "avg_answer_token_count": 25}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "The implications of incorporating multi-domain tasks in knowledge benchmarks for assessing the generalization and adaptability of language models.", "question": "How do multi-domain knowledge benchmarks fundamentally enhance the assessment of language model generalization and adaptability compared to single-domain benchmarks, and what implications does this have for the design of future evaluation frameworks?", "choices": {"A": "By requiring models to retrieve information from a single specialized domain, multi-domain benchmarks simplify evaluation and allow for deeper focus on specific knowledge areas, implying future frameworks should narrow domain scope to improve accuracy.", "B": "Multi-domain benchmarks compel models to integrate and switch between diverse knowledge areas, thus better measuring their broad generalization and adaptability, suggesting future evaluations must incorporate varied and complex contexts to reflect real-world applications.", "C": "They primarily test models on mathematical problem-solving skills within multiple domains, indicating that future benchmarks should emphasize only technical and numerical challenges for effective generalization assessment.", "D": "Multi-domain benchmarks focus on open-ended creative tasks without strict knowledge retrieval, implying that future evaluations should prioritize unrestricted generation over factual accuracy to assess adaptability."}, "answer": "B", "explanation": "Multi-domain benchmarks require language models to handle a variety of subjects and contexts, testing their ability to generalize knowledge and adapt to different domains seamlessly, unlike single-domain benchmarks which assess narrow expertise. This necessitates evaluation frameworks that reflect the complexity and diversity of real-world tasks, encouraging models to develop flexible, integrative reasoning capabilities.", "question_token_count": 37, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 6, "avg_answer_token_count": 34}
{"context": "###### Safety\nSafety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content. Datasets such as RealToxicityPrompts\u00a0(Gehman et\u00a0al., 2020) and ToxiGen\u00a0(Hartvigsen et\u00a0al., 2022) assess resilience against producing harmful outputs.\n\nBy providing a controlled environment to measure these aspects, safety benchmarks play a critical role in guiding the development of models that are not only powerful outputs but also responsible and trustworthy for real-world applications.\n\n\n###### Language\nLanguage benchmarks assess the LLMs\u2019 proficiency in specific languages.\n\nGLUE\u00a0(Wang, 2018) and SuperGLUE\u00a0(Wang et\u00a0al., 2019) cover tasks from sentiment analysis to language inference, while CLUE\u00a0(Xu et\u00a0al., 2020) targets Chinese language.\n\nTypo-fixing\u00a0(Suzgun et\u00a0al., 2022) is also widely used.\n\n\n###### Reading Comprehension\nReading comprehension tasks test a model\u2019s ability to extract and infer information from text. Benchmarks like SQuAD\u00a0(Rajpurkar et\u00a0al., 2018), QuAC\u00a0(Choi et\u00a0al., 2018), and BoolQ\u00a0(Clark et\u00a0al., 2019) challenge models to understand passages and draw logical conclusions.\n", "topic": "The contribution of reading comprehension benchmarks such as SQuAD, QuAC, and BoolQ to testing a model\u2019s capacity for information extraction, inference, and logical reasoning from text.", "question": "How do reading comprehension benchmarks such as SQuAD, QuAC, and BoolQ collectively contribute to assessing a language model\u2019s integrated ability to perform information extraction, inference, and logical reasoning from text?", "choices": {"A": "They evaluate a model\u2019s ability to memorize factual data and reproduce it verbatim from training.", "B": "They measure a model\u2019s capacity to extract explicit information, engage in multi-turn dialogue for context understanding, and determine the truthfulness of statements through logical inference.", "C": "They primarily assess a model\u2019s fluency in generating grammatically correct and stylistically natural text.", "D": "They test a model\u2019s proficiency in translating text between languages and resolving typographical errors."}, "answer": "B", "explanation": "These benchmarks each focus on complementary aspects of reading comprehension: SQuAD emphasizes extracting explicit answers from passages; QuAC involves multi-turn question answering requiring contextual understanding and inference; BoolQ evaluates the model\u2019s ability to make boolean decisions based on logical reasoning about the text. Together, they comprehensively test integrated comprehension skills beyond mere memorization or fluency.", "question_token_count": 41, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 9, "question_groundedness_score": 8, "avg_answer_token_count": 22}
{"context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n", "topic": "The design and operation of TreeEval, focusing on how initial questions lead to follow-up subtopics and questions based on model responses.", "question": "How does TreeEval dynamically generate follow-up subtopics and questions during its evaluation process, and why is this approach significant compared to static benchmark evaluations?", "choices": {"A": "TreeEval generates all follow-up questions before any model interaction, relying on a fixed question tree, which ensures consistency across evaluations but limits adaptability.", "B": "TreeEval produces initial questions and then creates follow-up subtopics and questions based on the examined LLM's previous responses, enabling a multi-turn, adaptive evaluation that mirrors human interviewing.", "C": "TreeEval uses multiple LLM agents to independently generate and verify questions, coordinating their outputs to build a comprehensive fixed benchmark set.", "D": "TreeEval paraphrases existing static benchmark questions and uses a single follow-up question per initial query without considering the examined model's responses."}, "answer": "B", "explanation": "TreeEval starts with an initial question generated on a topic and then dynamically generates follow-up subtopics and questions based on the examined LLM\u2019s prior responses, allowing for an adaptive multi-turn evaluation that better probes model understanding than static benchmarks.", "question_token_count": 30, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 29}
{"context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n", "topic": "The conceptual meaning and calculation of the Collision Rate metric and its role in quantifying overlap between benchmark dataset transformations.", "question": "In the context of dynamic benchmarking for large language models, how does the Collision Rate metric precisely quantify overlap between independently transformed benchmark datasets, and why is this quantification critical for ensuring the benchmark\u2019s reliability in the presence of potential training data contamination?", "choices": {"A": "Collision Rate measures the proportion of identical data samples shared between two transformed datasets, which is critical because high overlap indicates reduced novelty in test cases, undermining the benchmark\u2019s ability to reliably evaluate model capabilities after contamination.", "B": "Collision Rate calculates the total number of unique transformations applied to the benchmark, ensuring that the dataset is diverse enough to prevent any contamination during training.", "C": "Collision Rate assesses the frequency of repeated transformation trials needed to produce a new dataset, thus directly measuring the benchmark\u2019s resilience to contamination by counting unique trial attempts.", "D": "Collision Rate evaluates the percentage of benchmark data excluded from the training set, guaranteeing no overlap exists and thereby fully eliminating contamination risks."}, "answer": "A", "explanation": "Collision Rate specifically quantifies the percentage overlap between two independently transformed versions of the benchmark dataset, highlighting the extent to which data contamination could occur by reusing similar test samples. This is crucial because a high Collision Rate means the benchmark\u2019s dynamic transformations fail to generate novel, diverse test cases, compromising its ability to reliably reflect LLM capabilities when the benchmark is potentially included in training data.", "question_token_count": 48, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 9, "avg_answer_token_count": 32}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "Comparative insights into how different graph-based reasoning tasks (DAG evaluation, NP-hard problems, logic puzzles) test varied aspects of LLM reasoning and problem-solving skills.", "question": "How do the reasoning demands and graph structures of DAG evaluation, NP-hard problem-solving, and logic puzzles differentially challenge large language models\u2019 reasoning abilities, and what distinct aspects of LLM problem-solving does each task primarily assess?", "choices": {"A": "DAG evaluation primarily tests hierarchical causal inference through acyclic graph traversal; NP-hard problems assess combinatorial optimization and scalability with complex cyclic graphs; logic puzzles evaluate rule-based logical deduction on reasoning graphs.", "B": "DAG evaluation focuses on cyclic graph optimization and heuristic search; NP-hard problems emphasize linear graph traversal and shortest path calculation; logic puzzles test probabilistic reasoning in random graphs.", "C": "DAG evaluation measures LLMs\u2019 ability to generate SQL queries from graphs; NP-hard problems test neural network graph embeddings; logic puzzles challenge LLMs to memorize graph isomorphisms.", "D": "DAG evaluation, NP-hard problems, and logic puzzles all primarily assess LLMs\u2019 memorization of graph topologies without requiring actual reasoning or problem-solving skills."}, "answer": "A", "explanation": "DAG evaluation involves acyclic graphs requiring hierarchical value inference from root nodes, testing causal and structural reasoning; NP-hard problems use complex graph structures like those in TSP to assess combinatorial optimization and handling of computational complexity; logic puzzles employ reasoning graphs to test logical deduction and rule-based inference, highlighting distinct reasoning facets in LLMs.", "question_token_count": 44, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 34}
{"context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n", "topic": "The need for and challenges associated with establishing standardized evaluation criteria for dynamic benchmarks in LLM evaluation.", "question": "Why is the establishment of standardized evaluation criteria critical for dynamic benchmarks in large language model (LLM) assessment, and what are the main challenges that complicate this standardization process?", "choices": {"A": "Standardized criteria are crucial to ensure consistent reliability and comparability across benchmarks; the main challenges include balancing correctness with scalability and controlling complexity while addressing contamination and transparency issues.", "B": "Standardized criteria primarily help reduce computational costs; the main challenges are primarily technical limitations in generating enough test samples dynamically.", "C": "Standardized criteria are important only for static benchmarks to prevent data contamination; dynamic benchmarks do not require such criteria due to their evolving nature.", "D": "Standardized criteria mainly facilitate user interface design for benchmark tools; the challenges lie in improving annotation speed and automating scoring without human oversight."}, "answer": "A", "explanation": "Establishing standardized criteria for dynamic benchmarks is essential to guarantee reliable, fair, and scalable evaluation of LLMs amid contamination and privacy concerns that render static benchmarks obsolete. The difficulties stem from achieving a balance between correctness and scalability, controlling evaluation complexity, and ensuring transparency despite the dynamic generation of samples.", "question_token_count": 35, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 28}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "Comparative analysis of the methodologies and challenges involved in coding, instruction following, and reasoning benchmarks for comprehensive language model assessment.", "question": "How do the methodological focuses and evaluative challenges of coding, instruction following, and reasoning benchmarks distinctly influence the assessment of comprehensive language model capabilities, and what implications do these differences have for developing models that perform robustly across these domains?", "choices": {"A": "Coding benchmarks emphasize syntactic and semantic precision in program generation and debugging, instruction following benchmarks prioritize procedural comprehension and execution of detailed directives, and reasoning benchmarks assess multi-dimensional knowledge integration and inference, implying that models must be architected with modular capabilities to handle the distinct cognitive demands of each domain.", "B": "Coding and instruction following benchmarks primarily test linguistic fluency and vocabulary breadth, while reasoning benchmarks focus exclusively on mathematical problem-solving skills, suggesting that improving model size alone suffices for robust cross-domain performance.", "C": "Instruction following benchmarks require models to generate creative outputs without constraints, coding benchmarks test only error detection without synthesis, and reasoning benchmarks evaluate memorization capacity, indicating that a single training dataset can effectively cover all three assessment areas.", "D": "Reasoning benchmarks focus on evaluating syntactic correctness, coding benchmarks assess factual recall, and instruction following benchmarks test random response generation, which means that benchmark results are largely unrelated to underlying model capabilities."}, "answer": "A", "explanation": "The correct answer differentiates the distinct methodological focuses\u2014coding benchmarks require precise code generation and debugging, instruction following demands accurate procedural execution, and reasoning involves complex knowledge integration and inference. These differences imply that comprehensive evaluation demands models designed to address diverse cognitive and functional challenges rather than relying on a single approach or dataset.", "question_token_count": 47, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 6, "avg_answer_token_count": 44}
{"context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n", "topic": "The relationship between benchmark task diversity (e.g., math, coding, reasoning, safety) and the multidimensional evaluation of language model capabilities.", "question": "How does the inclusion of diverse task categories such as math, coding, reasoning, and safety in static benchmarks fundamentally enable a multidimensional evaluation of language model capabilities, and why is this diversity essential for accurate characterization of model performance?", "choices": {"A": "Diverse task categories each assess distinct cognitive and functional abilities, allowing static benchmarks to provide a comprehensive, multidimensional profile of language model strengths and weaknesses, which single-task evaluations cannot capture.", "B": "Including multiple task categories primarily serves to increase the size of the benchmark dataset, thereby improving statistical reliability but not fundamentally affecting the dimensionality of model evaluation.", "C": "Diverse task categories are combined mainly to simplify benchmarking logistics and reduce evaluation time by grouping similar tasks together, without significant impact on the depth of model capability assessment.", "D": "Task diversity in benchmarks is mainly used to test the model\u2019s ability to memorize a wide variety of facts rather than to evaluate different reasoning or functional competencies, making it less relevant for multidimensional evaluation."}, "answer": "A", "explanation": "The inclusion of diverse task categories is crucial because each category targets unique aspects of language model capabilities\u2014from arithmetic and logical reasoning to ethical safety considerations\u2014thereby enabling a multidimensional assessment. This comprehensive approach reveals nuanced strengths and weaknesses that single-domain tests would miss, ensuring a more accurate and holistic characterization of model performance.", "question_token_count": 45, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 8, "avg_answer_token_count": 34}
{"context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n", "topic": "The rationale behind combining multiple contamination-free construction methods, as seen in frameworks like C2LEVA, to enhance benchmark integrity.", "question": "Considering the increasing risk of data contamination in LLM benchmarks as training corpora scale, why does combining multiple contamination-free construction methods\u2014such as temporal cutoff, LLM-based generation, and graph-based perturbation\u2014in frameworks like C2LEVA critically enhance the integrity and reliability of benchmark evaluations?", "choices": {"A": "Because integrating multiple methods allows for simultaneous mitigation of diverse contamination pathways, compensates for individual method limitations, and ensures robustness across different data domains and languages.", "B": "Because using multiple methods primarily reduces the computational cost of generating benchmark samples by distributing workload across different techniques.", "C": "Because the combination mainly facilitates faster annotation by human evaluators due to varied sample generation styles improving annotator engagement.", "D": "Because applying multiple methods guarantees that benchmarks remain static and unchanging, preventing any updates that might introduce contamination."}, "answer": "A", "explanation": "Combining multiple contamination-free methods addresses distinct contamination risks inherent in each approach, creating a more comprehensive and robust defense against overlap with training data. Temporal cutoff prevents use of future data, LLM-based generation creates novel content, and graph-based perturbation ensures logical variation, together enhancing benchmark reliability and bilingual coverage, which single methods alone cannot fully achieve.", "question_token_count": 56, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 10, "avg_answer_token_count": 24}
{"context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n", "topic": "The inherent risks of contamination in LLM evaluation data due to large-scale web-scraped pre-training datasets and their implications for model assessment fairness.", "question": "Considering the inherent contamination risks in large-scale web-scraped pre-training datasets for LLMs, which factor most fundamentally complicates the development of fair and reliable benchmarks for evaluating these models' true performance?", "choices": {"A": "The use of human-annotated fine-tuning datasets that are entirely independent from pre-training data.", "B": "The proprietary nature and opacity of many LLM training datasets preventing verification of overlap with evaluation sets.", "C": "The inability of retrieval-based detection methods to identify any contamination within training data.", "D": "The exclusive reliance on synthetic datasets for post-training fine-tuning that perfectly match evaluation tasks."}, "answer": "B", "explanation": "While retrieval-based detection methods face challenges, they do exist and can identify some contamination, and fine-tuning datasets are not always entirely independent or synthetic. The key fundamental issue is the proprietary opacity of many LLM training datasets, which prevents the community from verifying overlaps and thus undermines the fairness and reliability of benchmarks.", "question_token_count": 41, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 8, "avg_answer_token_count": 18}
{"context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n", "topic": "The conceptual scope and purpose of static benchmarks in evaluating diverse capabilities of large language models.", "question": "How does the formal problem formulation of a static benchmark, defined as a triplet (input prompts, expected outputs, scoring function), enable comprehensive and standardized evaluation of diverse capabilities in large language models, and what are the critical implications of this structure for assessing model performance across varied task categories?", "choices": {"A": "It establishes a fixed dataset against which models are compared, enabling objective scoring of outputs across multiple tasks, but limits evaluation to static responses without dynamic interaction capabilities.", "B": "It allows models to generate any outputs without constraints, focusing only on qualitative human judgment rather than quantitative scoring.", "C": "It uses dynamic prompts that adapt to model responses in real time, thereby capturing interactive and evolving model abilities beyond static datasets.", "D": "It prioritizes only linguistic tasks by scoring outputs based solely on grammatical correctness, ignoring other dimensions like reasoning or coding skills."}, "answer": "A", "explanation": "The triplet structure (input prompts, expected outputs, scoring function) creates a stable, standardized testing ground for LLMs, allowing consistent, objective comparison across diverse tasks by quantifying output quality against known answers; however, this static nature inherently restricts evaluation to fixed scenarios and does not capture models\u2019 dynamic or interactive competencies.", "question_token_count": 57, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 26}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "The potential sources of exact contamination, including verbatim test examples, code snippets, and documentation leaks.", "question": "Which of the following scenarios best exemplifies exact contamination in the training data of a large language model, and why does this type of contamination critically undermine the validity of evaluation benchmarks?", "choices": {"A": "A test question paraphrased with synonyms appears in the training set, causing the model to indirectly memorize the answer through semantic similarity.", "B": "A code snippet from a benchmark\u2019s official implementation is present verbatim in the training corpus, allowing the model to produce identical outputs without genuine generalization.", "C": "Test data points are transformed by punctuation normalization and whitespace changes before appearing in the training data, leading to subtle overlap.", "D": "The training data includes summaries of benchmark datasets rather than the original test examples, resulting in partial contamination that tests model abstraction."}, "answer": "B", "explanation": "Exact contamination occurs when identical data points appear both in training and test datasets, such as verbatim code snippets from benchmark implementations. This enables the model to recall rather than generalize, invalidating performance measurements. Paraphrased or syntactically transformed examples represent syntactic contamination, not exact duplication.", "question_token_count": 36, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 26}
{"context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n", "topic": "Strategies and theoretical approaches to mitigate or detect contamination risks in large-scale LLM training pipelines.", "question": "In the context of large-scale LLM training, what is the primary challenge that retrieval-based contamination detection methods face, and how does this challenge compound the difficulty of ensuring fair and reliable benchmarking?", "choices": {"A": "Retrieval-based methods struggle with the proprietary nature of training data, which prevents access to evaluation sets and thus leads to biased benchmarks.", "B": "The massive scale and complexity of training corpora hinder retrieval-based methods from fully identifying overlaps, thereby increasing contamination risks that undermine benchmark validity.", "C": "These methods cannot detect contamination because fine-tuning datasets are always disjoint from evaluation sets, making contamination irrelevant to benchmarking.", "D": "Retrieval-based detection is ineffective because it only applies to synthetic datasets, while contamination mainly arises from human-annotated data, skewing benchmark fairness."}, "answer": "B", "explanation": "The core difficulty with retrieval-based contamination detection lies in the sheer volume and diversity of training data, which limits comprehensive overlap identification. This incomplete detection allows contamination to persist unnoticed, compromising the fairness and reliability of benchmarks, especially when combined with opaque, proprietary datasets.", "question_token_count": 38, "answer_correctness_score": 10, "explanation_validity_score": 9, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 27}
{"context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n", "topic": "Evaluate how the concepts of external and internal diversity can influence strategies for data augmentation or dataset transformation in machine learning workflows.", "question": "In the context of data augmentation strategies, how does balancing external and internal diversity influence the effectiveness of transformed datasets, and what potential trade-offs arise when optimizing for high external diversity at the expense of internal diversity?", "choices": {"A": "High external diversity ensures transformed datasets are sufficiently novel compared to the seed data, but if internal diversity is low, the augmented datasets may be redundant, limiting generalization gains.", "B": "Prioritizing internal diversity over external diversity guarantees that each augmentation trial is unique, which inherently maximizes overall dataset novelty and model robustness.", "C": "Maximizing external diversity while ignoring internal diversity leads to augmented datasets that are both highly varied among themselves and highly distinct from the original data, optimizing learning outcomes.", "D": "Balancing external and internal diversity is unnecessary because external diversity alone fully captures the utility of data augmentation for improving model performance."}, "answer": "A", "explanation": "High external diversity creates datasets that differ significantly from the original, but without sufficient internal diversity, multiple augmentations may be similar to each other, reducing the benefits of augmentation by limiting variation within the augmented set. Thus, a trade-off exists where maximizing external diversity can lead to low internal diversity, causing redundancy and less effective learning.", "question_token_count": 42, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 7, "avg_answer_token_count": 29}
{"context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n", "topic": "The approach taken by VarBench to identify and replace variables in benchmark samples to produce novel data instances.", "question": "How does VarBench's approach of prompting LLMs to identify and replace variables in existing benchmark samples fundamentally differ from other benchmark rewriting methods, and what is the primary advantage of this approach in generating novel data instances?", "choices": {"A": "VarBench rewrites entire benchmark questions using LLMs to change the question format, which increases stylistic diversity but risks altering difficulty levels.", "B": "VarBench uses LLMs to generate completely new questions unrelated to the original benchmarks, which maximizes novelty but may reduce relevance.", "C": "VarBench specifically targets variable identification and replacement within existing questions, enabling generation of new samples that preserve the original question's structure and difficulty while expanding dataset diversity.", "D": "VarBench relies on knowledge graphs to create extended questions on related concepts, thereby increasing cognitive complexity beyond the original samples."}, "answer": "C", "explanation": "VarBench\u2019s distinctive method involves prompting LLMs to detect variables within benchmark questions and substitute them, producing new examples that maintain the original structure and challenge level. This contrasts with rewriting entire questions or generating unrelated new questions, allowing for increased sample diversity without compromising difficulty or relevance.", "question_token_count": 43, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 10, "avg_answer_token_count": 27}
{"context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n", "topic": "Challenges and considerations in implementing domain-specific annotators or oracles for correctness evaluation in diverse benchmarking contexts.", "question": "Which of the following most fundamentally challenges the implementation of domain-specific annotators or oracles as ground truth evaluators to ensure correctness in dynamic benchmarking of large language models?", "choices": {"A": "The scalability of annotators to cover diverse domains without compromising ground truth reliability.", "B": "The computational cost of scoring functions used to measure alignment between outputs and ground truth.", "C": "The inability of dynamic benchmarks to generate sufficiently complex datasets for evaluation.", "D": "The lack of standardized metrics for measuring correctness across different benchmarks."}, "answer": "A", "explanation": "The core challenge in implementing domain-specific oracles lies in ensuring they can reliably provide ground truth across diverse and potentially complex domains at scale; this affects the correctness guarantee directly. While computational cost and standardization are important, they are secondary to the fundamental issue of oracle reliability and scalability.", "question_token_count": 34, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 15}
{"context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n", "topic": "How high variance in complexity measurements indicates instability in dynamic benchmarking methods and its impact on result reliability.", "question": "Why does a high variance in complexity measurements across different trials of a dynamic benchmarking method indicate instability, and how does this instability affect the reliability of interpreting performance drops in large language model evaluations?", "choices": {"A": "Because high variance means the complexity metric is sensitive to minor changes, indicating that performance drops may be confounded by inconsistent complexity rather than true data contamination, thus reducing the reliability of the benchmark.", "B": "Because high variance shows that the dataset is inherently simple, making performance drops unlikely to be caused by complexity changes, which increases the reliability of contamination detection.", "C": "Because high variance reflects that the dynamic benchmark is too stable, ensuring that performance drops are solely due to contamination, thus improving reliability.", "D": "Because high variance means the complexity metric is consistent and reliable, so performance drops can confidently be attributed to data contamination."}, "answer": "A", "explanation": "A high variance in complexity measurements means the complexity metric fluctuates significantly across trials, reflecting instability in how complexity is assessed. This instability makes it difficult to determine whether observed performance drops result from increased task complexity or genuine data contamination, thereby undermining the reliability of the benchmark's conclusions.", "question_token_count": 38, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 30}
{"context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n", "topic": "The concept and implementation of label protection as a strategy to keep test set answers hidden from public access to preserve evaluation integrity.", "question": "How does label protection uniquely contribute to maintaining evaluation integrity in benchmark datasets compared to encryption methods, and what is its primary advantage in preventing data contamination during model training?", "choices": {"A": "Label protection encrypts the entire test dataset to prevent unauthorized access, thereby stopping any data leakage and ensuring integrity.", "B": "Label protection hides only the true test set answers from public access, preventing models from learning or memorizing these answers and thus maintaining unbiased evaluation.", "C": "Label protection requires distributing private keys to all evaluators so they can securely access both test inputs and answers without risk of exposure.", "D": "Label protection involves modifying test data with minor text variations to defeat decontamination methods and preserve the confidentiality of test answers."}, "answer": "B", "explanation": "Label protection specifically withholds the true answers of test sets from public access, which prevents models from being exposed to or memorizing the answers during training, thereby preserving the integrity of the evaluation. Unlike encryption, which secures the entire dataset, label protection focuses on the critical component\u2014the labels\u2014to directly mitigate contamination risks.", "question_token_count": 33, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 25}
{"context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n", "topic": "Challenges and trade-offs in dynamic benchmarking, specifically balancing evaluation correctness, scalability, and complexity control.", "question": "In the context of dynamic benchmarking for large language models, what is the fundamental trade-off that must be managed to ensure effective evaluation, and why does neglecting complexity control undermine the benchmarking process despite achieving scalability and correctness?", "choices": {"A": "The trade-off between data contamination and model privacy; neglecting complexity control leads to privacy breaches.", "B": "The trade-off between evaluation correctness, scalability, and complexity control; neglecting complexity control results in inefficiencies that compromise practical evaluation despite correctness and scalability.", "C": "The trade-off between static and dynamic benchmark design; neglecting complexity control causes benchmarks to become static and outdated.", "D": "The trade-off between human and LLM-generated annotations; neglecting complexity control causes annotation errors to proliferate uncontrollably."}, "answer": "B", "explanation": "Dynamic benchmarking requires balancing evaluation correctness (reliability), scalability (handling large data and models), and complexity control (ensuring evaluations are efficient and manageable). Neglecting complexity control may allow correctness and scalability but leads to inefficient evaluations, undermining the practical utility of the benchmark.", "question_token_count": 43, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 7, "avg_answer_token_count": 24}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "The potential adversarial scenarios where developers intentionally circumvent canary string detection to inflate benchmark performance.", "question": "Why do canary strings fail to prevent intentional inflation of benchmark performance by model developers, and what does this imply about the limitations of static benchmark datasets in ensuring genuine model generalization?", "choices": {"A": "Because canary strings are too complex for developers to detect, implying that static benchmarks are inherently unreliable for all models.", "B": "Because developers aware of canary strings can intentionally avoid or manipulate them, implying that static benchmarks cannot fully prevent adversarial data leakage or memorization.", "C": "Because canary strings cause models to underperform on benchmarks, implying that their presence discourages honest model training.", "D": "Because canary strings only detect memorization in training data, implying that benchmarks cannot measure model reasoning capabilities."}, "answer": "B", "explanation": "Canary strings rely on developers being aware and responsive to these markers; if developers intentionally circumvent or leak benchmark data to boost scores, canary detection fails, revealing a fundamental limitation of static benchmarks in preventing adversarial contamination and ensuring true generalization.", "question_token_count": 36, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 24}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "The significance of multilingual benchmarking in instruction following and reasoning, exemplified by C-Eval and C-SimpleQA focusing on Chinese language tasks.", "question": "In the context of evaluating language models, why is the inclusion of multilingual benchmarks such as C-Eval and C-SimpleQA, which focus on Chinese instruction following and reasoning tasks respectively, critical for advancing the robustness and generalizability of model capabilities beyond English-centric evaluations?", "choices": {"A": "Because they test models on culturally and linguistically specific instruction and reasoning tasks, revealing limitations that English-only benchmarks may overlook.", "B": "Because they simplify evaluation by translating English benchmarks into Chinese, ensuring uniformity across languages.", "C": "Because they focus exclusively on coding tasks in Chinese, which are fundamentally different from reasoning tasks.", "D": "Because they eliminate the need for reasoning benchmarks by combining instruction and coding evaluations into one dataset."}, "answer": "A", "explanation": "Multilingual benchmarks like C-Eval and C-SimpleQA are essential because they assess model performance on instruction following and reasoning within the linguistic and cultural context of Chinese, uncovering challenges that English-only benchmarks cannot capture, thereby enhancing robustness and generalizability.", "question_token_count": 55, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 20}
{"context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n", "topic": "Critical challenges and future research directions in developing contamination-resilient benchmarking frameworks for LLMs.", "question": "Considering the identified limitations of static benchmarking and the lack of standardized evaluation criteria for dynamic benchmarking in LLM contamination research, which of the following best describes the primary challenge for developing contamination-resilient benchmarking frameworks that future research must address?", "choices": {"A": "Designing dynamic benchmarks that comprehensively cover all possible contamination sources without compromising computational efficiency.", "B": "Establishing universally accepted criteria to evaluate dynamic benchmarks\u2019 effectiveness in detecting and mitigating data contamination while balancing adaptability and reproducibility.", "C": "Creating static benchmarks that can be frequently updated with new data to prevent overlap with LLM training datasets.", "D": "Developing training data curation methods that eliminate all contaminated samples prior to model training, making benchmarking unnecessary."}, "answer": "B", "explanation": "The core challenge highlighted is the absence of standardized evaluation criteria for dynamic benchmarks, which is crucial to reliably assess their contamination resilience. While dynamic benchmarks aim to mitigate contamination better than static ones, without agreed-upon evaluation principles, their effectiveness cannot be consistently measured or compared. Thus, establishing these criteria that balance adaptability to new data and reproducibility of results is paramount. Options A, C, and D either focus on incomplete aspects or unrealistic goals that do not address this fundamental challenge.", "question_token_count": 45, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 21}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "Explore the implications of contaminated benchmarks for policy-making related to AI technologies and the ethical considerations involved.", "question": "How does contamination of benchmarks in large language model evaluation fundamentally undermine the ethical integrity of AI policy-making, and what is the primary consequence of relying on such contaminated benchmarks for regulatory decisions?", "choices": {"A": "It causes policy-makers to underestimate AI capabilities, leading to overly cautious regulation that stifles innovation.", "B": "It results in inflated assessments of AI performance, causing misguided trust and potentially premature or inappropriate deployment decisions.", "C": "It primarily affects only academic research by limiting reproducibility, without significant implications for real-world policy.", "D": "It leads to complete rejection of AI technologies by policy-makers due to perceived unreliability in evaluations."}, "answer": "B", "explanation": "Contaminated benchmarks artificially inflate the apparent performance of AI models, misleading policy-makers into overestimating their true capabilities, which can cause premature or inappropriate deployment and regulatory decisions, thus undermining ethical governance and public trust.", "question_token_count": 37, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 6, "avg_answer_token_count": 21}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The significance of data contamination in large language model benchmarking and its impact on the validity of evaluation results.", "question": "How does data contamination specifically undermine the validity of large language model benchmarking results, and why do dynamic benchmarking approaches potentially offer a more reliable solution compared to static benchmarks?", "choices": {"A": "Data contamination causes models to memorize benchmark data, artificially boosting performance metrics, while dynamic benchmarks reduce contamination risk by continuously updating and regenerating test data based on training timelines.", "B": "Data contamination leads to underestimation of model capabilities due to data omission, and dynamic benchmarks fix this by using fixed, unchanging datasets for consistent evaluation.", "C": "Data contamination affects only the training speed of models without impacting evaluation accuracy, whereas dynamic benchmarks accelerate model training by exposing them to evolving datasets.", "D": "Data contamination is irrelevant in large language model benchmarking because models generalize beyond memorized data; dynamic benchmarks are primarily designed to test new model architectures rather than mitigate contamination."}, "answer": "A", "explanation": "Data contamination results in models having prior exposure to benchmark test data during training, causing inflated, misleading performance that does not reflect true generalization; dynamic benchmarking mitigates this by updating or regenerating benchmarks aligned with training data timelines, thus reducing overlap and improving the fairness and validity of evaluations.", "question_token_count": 33, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 31}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "Critical evaluation of existing literature reviews on LLM benchmarking, focusing on gaps related to dynamic benchmarking and contamination detection.", "question": "Why do existing literature reviews on Large Language Model benchmarking fall short in comprehensively addressing contamination detection and dynamic benchmarking, and what are the implications of these gaps for the future development and standardization of LLM evaluation methods?", "choices": {"A": "Because they primarily focus on static benchmarking and post-hoc contamination detection without systematically reviewing dynamic benchmarking methods or proposing evaluation criteria, leading to a lack of standardized frameworks and potentially perpetuating contamination risks in LLM evaluation.", "B": "Because they extensively cover dynamic benchmarking methods but neglect the importance of static benchmarks, resulting in overemphasis on continuous dataset updates and underappreciation of foundational evaluation techniques.", "C": "Because they prioritize commercial privacy concerns over technical benchmarking challenges, which causes a focus on data protection rather than contamination detection or benchmarking innovation.", "D": "Because they rely exclusively on proprietary datasets inaccessible to the research community, which limits transparency but ensures contamination is completely avoided."}, "answer": "A", "explanation": "Existing reviews concentrate mostly on post-hoc contamination detection and static benchmarks, neglecting emerging dynamic benchmarking approaches and lacking criteria to evaluate these new methods. This omission prevents the establishment of standardized, contamination-resilient benchmarking frameworks, thereby hindering reliable assessment and future advancement in LLM evaluation.", "question_token_count": 42, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 31}
