{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "Differentiate between exact contamination and syntactic contamination, providing specific examples of each.", "question": "An LLM is evaluated on a question-answering benchmark. During analysis, it is discovered that the training dataset includes sentences with minor punctuation differences (e.g., \u201cHello, world!\u201d vs. \u201cHello world!\u201d) compared to questions in the benchmark. Additionally, a code snippet used in the benchmark's evaluation script was directly present in the training corpus. Which categorization accurately describes these two instances of data contamination?", "choices": {"A": "Both instances represent exact contamination.", "B": "The punctuation difference is syntactic contamination, while the code snippet is exact contamination.", "C": "Both instances represent syntactic contamination.", "D": "The punctuation difference is exact contamination, while the code snippet is syntactic contamination."}, "answer": "B", "explanation": "Exact contamination is defined as an exact duplicate. The code snippet is a direct copy. Syntactic contamination involves transformations like punctuation changes. Therefore, the punctuation difference falls into the syntactic category, while the code snippet is exact.", "question_token_count": 85, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 11}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "Explain the rationale behind using a temporal cutoff when evaluating Large Language Models.", "question": "Utilizing a temporal cutoff \u2013 evaluating LLMs on data post-training \u2013 primarily addresses which inherent limitation of these models?", "choices": {"A": "The tendency of LLMs to generate biased or harmful content.", "B": "The potential for LLMs to exhibit data contamination, conflating memorization with reasoning.", "C": "The computational expense associated with continuously updating LLM parameters.", "D": "The difficulty in ensuring LLMs adhere to copyright restrictions on training data."}, "answer": "B", "explanation": "The context explicitly states that using data after the knowledge cutoff date helps \"mitigate data contamination,\" preventing the evaluation of memorized information rather than genuine reasoning. The cited benchmarks all focus on this principle.", "question_token_count": 24, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 14}
{"context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n", "topic": "Interpret the scalability equation as a proportion of data generated per unit cost, explaining its significance in evaluating dynamic benchmarks.", "question": "In the context of dynamic benchmarking, the scalability equation quantifies the relationship between transformed dataset size, original dataset size, and transformation cost. What fundamental principle does this equation represent?", "choices": {"A": "The absolute increase in dataset size resulting from the transformation process.", "B": "The proportion of data generated for each unit of cost incurred during transformation.", "C": "The minimization of statistical errors through the reduction of original dataset size.", "D": "The direct correlation between monetary cost and the complexity of the transformation function."}, "answer": "B", "explanation": "The provided text explicitly states that the equation \"could be interpreted as the proportion of data that can be generated per unit cost.\" This represents the efficiency of the benchmarking process in creating larger datasets without excessive expense.", "question_token_count": 36, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 14}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "Evaluate the utility of Forecastbench's daily updates for evaluating an LLM's forecasting abilities.", "question": "Considering the goal of minimizing data contamination when evaluating LLM forecasting capabilities, what is the primary advantage of Forecastbench\u2019s daily data updates compared to benchmarks updated less frequently (e.g., monthly or quarterly)?", "choices": {"A": "Daily updates ensure the benchmark aligns with the most recent real-world events, reducing the likelihood that the LLM was pre-trained on the specific forecasting questions.", "B": "Daily updates allow for a larger dataset size, which improves the statistical significance of the evaluation results.", "C": "Daily updates decrease the computational cost of running the benchmark, as fewer questions need to be processed at once.", "D": "Daily updates simplify the benchmark creation process, reducing the human effort required to curate the dataset."}, "answer": "A", "explanation": "The core purpose of using post-cutoff data is to avoid contamination. Daily updates minimize the window of opportunity for information about the forecasting questions to leak into the LLM\u2019s training data, making the evaluation more reliable. While larger datasets (B) are generally good, and efficiency (C) and ease of creation (D) are desirable, they don\u2019t directly address the contamination problem.", "question_token_count": 40, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 23}
{"context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n", "topic": "The range of task categories that are typically assessed using static benchmarks, including examples like arithmetic problem-solving and toxicity detection.", "question": "Static benchmarks are employed to assess Large Language Models (LLMs) across a variety of capabilities. Which of the following best represents the *broadest* range of task categories typically evaluated using these benchmarks?", "choices": {"A": "Primarily focused on numerical computation and logical deduction.", "B": "Concentrated on natural language processing tasks such as translation and summarization.", "C": "Encompassing a wide spectrum of skills including mathematical reasoning, linguistic understanding, code generation, and safety evaluations.", "D": "Limited to evaluating factual recall and knowledge retrieval capabilities."}, "answer": "C", "explanation": "The text explicitly states that static benchmarks cover a \u201cbroad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension.\u201d Option C is the only one that accurately reflects this comprehensive scope.", "question_token_count": 41, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 15}
{"context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n", "topic": "The importance of continuous refinement and adaptation of LLM benchmarking methods to address emerging challenges and innovations.", "question": "Given the inherent limitations of both static and dynamic LLM benchmarking methods \u2013 the former\u2019s increasing vulnerability with data scale and the latter\u2019s reproducibility concerns \u2013 what represents the most critical ongoing necessity for the field?", "choices": {"A": "A complete shift towards exclusively dynamic benchmarking techniques, prioritizing adaptability over reproducibility.", "B": "The establishment of universally fixed benchmarking datasets to ensure consistent evaluation across models.", "C": "Continuous refinement and adaptation of benchmarking methodologies to address emerging challenges and innovations.", "D": "Focusing solely on mitigating data contamination within existing static benchmarks, avoiding the complexities of dynamic evaluation."}, "answer": "C", "explanation": "The text explicitly states in the conclusion that future research should focus on standardized dynamic evaluation and practical mitigation tools, and the limitations section notes that methods are still emerging and require further refinement. This highlights the ongoing need for adaptation and improvement rather than settling on a fixed approach.", "question_token_count": 40, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 8, "avg_answer_token_count": 16}
{"context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n", "topic": "Describe the approach taken by TRUCE (Chandran et al., 2024) in leveraging confidential computing and secure multi-party computation for private benchmarking.", "question": "TRUCE (Chandran et al., 2024) utilizes both confidential computing and secure multi-party computation to enable private benchmarking. What is the primary function of combining these two approaches in this context?", "choices": {"A": "To reduce computational overhead associated with encryption, thereby accelerating the benchmarking process.", "B": "To ensure that both test data and model parameters remain confidential during the benchmarking process.", "C": "To enforce a \u201cNo Derivatives\u201d license on the test data, preventing automated crawling and reuse.", "D": "To provide a mechanism for verifying the integrity of the labels used in the benchmark, preventing manipulation."}, "answer": "B", "explanation": "The text explicitly states that TRUCE leverages these technologies \u201censuring that test data and model parameters remain confidential.\u201d This highlights the core purpose of combining these techniques within the TRUCE framework.", "question_token_count": 42, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 8, "avg_answer_token_count": 18}
{"context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "How the transformation function T(\u22c5) modifies a static benchmark dataset in dynamic benchmarking.", "question": "In the context of dynamic benchmarking, what is the primary purpose of the transformation function, T(\u22c5)?", "choices": {"A": "To increase the size of the benchmark dataset over time, ensuring comprehensive evaluation.", "B": "To introduce noise into the benchmark dataset, simulating real-world data variations.", "C": "To modify the benchmark dataset to mitigate potential data contamination issues.", "D": "To simplify the benchmark dataset, reducing computational costs during evaluation."}, "answer": "C", "explanation": "The text explicitly states that the transformation function T(\u22c5) \u201cmodifies the data set during the benchmarking to avoid possible data contamination.\u201d This is the primary reason for its inclusion in the dynamic benchmarking framework.", "question_token_count": 22, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 14}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "Discuss the significance of C-Eval and C-SimpleQA in evaluating LLMs specifically for Chinese language understanding and instruction following.", "question": "Within the landscape of Chinese language Large Language Model (LLM) evaluation, what fundamentally differentiates the assessment goals of C-Eval and C-SimpleQA, despite both operating within the domain of Chinese language processing?", "choices": {"A": "C-Eval primarily measures a model\u2019s ability to accurately recall factual information in Chinese, while C-SimpleQA assesses its capacity to execute complex, multi-step instructions.", "B": "C-Eval focuses on evaluating a model\u2019s adherence to nuanced Chinese instructions, encompassing complex reasoning and task completion, whereas C-SimpleQA concentrates on verifying the factual correctness of responses to concise Chinese queries.", "C": "C-SimpleQA assesses a model\u2019s coding capabilities in Chinese, while C-Eval evaluates its ability to perform logical reasoning in Chinese.", "D": "Both C-Eval and C-SimpleQA are designed to assess identical capabilities \u2013 the accurate translation of English instructions into Chinese outputs."}, "answer": "B", "explanation": "The text explicitly states that C-Eval focuses on Chinese instructions (implying the ability to *follow* them) and C-SimpleQA evaluates the factuality of answers to short questions in Chinese. Therefore, option B accurately captures this distinction, highlighting C-Eval's focus on instruction following and C-SimpleQA's focus on factual correctness.", "question_token_count": 44, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 32}
{"context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n", "topic": "The application of CONSTAT, as proposed by Dekoninck et al. (2024), in detecting data contamination by comparing model performance across benchmarks.", "question": "Considering the application of CONSTAT for data contamination detection, what fundamental principle underlies its effectiveness in identifying compromised model integrity through benchmark performance comparison?", "choices": {"A": "CONSTAT leverages the assumption that contamination will invariably lead to uniformly improved performance across all benchmarks.", "B": "CONSTAT\u2019s efficacy stems from the expectation that contamination will disproportionately affect performance consistency *between* benchmarks rather than absolute performance levels.", "C": "CONSTAT relies on identifying benchmarks where performance degradation directly correlates with the presence of contaminated data.", "D": "CONSTAT operates under the premise that contamination introduces predictable patterns of errors within individual benchmarks, detectable through n-gram analysis."}, "answer": "B", "explanation": "CONSTAT detects contamination by comparing model performance *across* benchmarks. The core idea is that if a model is contaminated with test data, it will perform unusually well on benchmarks containing similar data, but may not generalize well to other, unrelated benchmarks. This inconsistency in performance across benchmarks is the key indicator of contamination.", "question_token_count": 29, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 22}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "The importance of mitigating data contamination for reliable model comparisons and informed deployment decisions.", "question": "Considering an NLP application heavily reliant on syntactic information, and acknowledging the debate surrounding whether syntactically transformed data constitutes \"true\" contamination, what is the most critical consequence of *not* classifying such transformations as contamination in benchmark evaluations?", "choices": {"A": "It may underestimate a model\u2019s ability to generalize to entirely novel semantic concepts.", "B": "It risks overestimating the model's reasoning capabilities and its true performance on tasks prioritizing syntactic accuracy.", "C": "It will inevitably lead to the identification of all memorized training data within the test set.", "D": "It simplifies the benchmarking process, reducing computational costs without significantly impacting result reliability."}, "answer": "B", "explanation": "The text explicitly states that, for applications relying on syntactic information, considering syntactically transformed data as contamination is crucial. Failing to do so will lead to an overestimation of the model\u2019s capabilities *specifically* in relation to syntactic accuracy, as the model is effectively being tested on data it has already \"seen\" in a slightly altered form.", "question_token_count": 47, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 17}
{"context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n", "topic": "The approach StructEval takes to expand upon original benchmark concepts using LLMs and knowledge graphs.", "question": "StructEval distinguishes itself from other LLM-based benchmark rewriting methods by incorporating knowledge graphs. What is the primary benefit of this integration regarding the expanded questions generated?", "choices": {"A": "It allows for the identification and removal of potentially contaminated samples within the original benchmark.", "B": "It enables the creation of questions that probe deeper conceptual understanding by leveraging structured knowledge beyond surface-level rewording.", "C": "It ensures that the difficulty level of the expanded questions remains consistent with the original benchmark samples.", "D": "It focuses on identifying and replacing variables within existing benchmark samples to create new variations."}, "answer": "B", "explanation": "The text states StructEval \"expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\" This indicates a focus on a more comprehensive exploration of concepts, facilitated by the structured knowledge provided by knowledge graphs, going beyond simple rewording or variable substitution.", "question_token_count": 31, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 9, "avg_answer_token_count": 19}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The challenges associated with identifying and mitigating data contamination, including privacy concerns and the difficulty of tracing training data.", "question": "Given the inherent difficulties in tracing the complete training data provenance of contemporary Large Language Models, what is the most significant impediment to definitively establishing the extent of data contamination in benchmark datasets, and how does this limitation fundamentally challenge the validity of performance evaluations?", "choices": {"A": "The lack of standardized data encryption techniques for benchmark datasets prevents accurate post-hoc contamination detection.", "B": "The commercial sensitivity surrounding training data composition hinders comprehensive auditing and verification of data sources.", "C": "The dynamic nature of the internet and constant model retraining render static benchmark datasets perpetually susceptible to contamination.", "D": "The computational cost of exhaustively comparing benchmark data against potential training data sources is prohibitively expensive."}, "answer": "B", "explanation": "The text explicitly states that \u201cdue to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible.\u201d This directly impacts the ability to determine the extent of contamination and therefore undermines the validity of performance evaluations. While other options represent challenges, the inability to trace data is the *most* significant and fundamental impediment.", "question_token_count": 51, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 8, "avg_answer_token_count": 19}
{"context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n", "topic": "The purpose of utilizing static benchmarks in the evaluation of Large Language Models.", "question": "Within the formal definition of a static benchmark \ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.)), what is the primary function of the scoring function \ud835\udcae(\u22c5) beyond merely comparing an LLM's output to expected outputs \ud835\udcb4?", "choices": {"A": "To normalize the output distributions of different LLMs, enabling direct comparison regardless of model scale.", "B": "To provide a standardized, objective metric for quantifying the quality of an LLM\u2019s response, facilitating reliable and reproducible evaluations.", "C": "To dynamically adjust the difficulty of the benchmark based on the LLM\u2019s performance, ensuring continuous challenge.", "D": "To identify and correct biases present in the input prompts \ud835\udcb3, improving the fairness of the benchmark."}, "answer": "B", "explanation": "The context states that the scoring function \ud835\udcae(\u22c5) \"evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4.\" This implies its role is to provide an objective metric for quantifying performance, not just a simple comparison. This is essential for reliable evaluation.", "question_token_count": 50, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 21}
{"context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n", "topic": "The role of contamination detectors, such as the one used in ITD, in improving the quality of LLM benchmarks.", "question": "Within the ITD framework for benchmark rewriting, what is the primary strategic benefit of incorporating a contamination detector prior to prompting an LLM to rewrite samples?", "choices": {"A": "To ensure the LLM generates samples that are stylistically consistent with the original benchmark, preserving its inherent characteristics.", "B": "To proactively identify and eliminate samples already present in the LLM's training data, thus enhancing the benchmark's novelty and reducing inflated performance metrics.", "C": "To facilitate the expansion of examined concepts within the original benchmark by leveraging knowledge graphs and LLM-generated extensions.", "D": "To automatically adjust the difficulty level of the rewritten samples, ensuring they remain comparable to the original benchmark's cognitive demands."}, "answer": "B", "explanation": "The text explicitly states that ITD utilizes a contamination detector to identify contaminated samples *before* rewriting them. Contamination refers to the presence of benchmark data in the LLM's training set. Removing these samples prevents the LLM from simply recalling answers rather than demonstrating genuine understanding, thus improving the benchmark\u2019s quality and providing more reliable performance metrics.", "question_token_count": 30, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 24}
{"context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n", "topic": "The approach used by MMLU-CF to create novel multiple-choice questions by manipulating answer choices and introducing a \"None of the other choices\" option.", "question": "MMLU-CF generates novel multiple-choice questions primarily through which mechanism?", "choices": {"A": "Randomly generating entirely new questions using a large language model.", "B": "Creating dynamic math benchmarks by filling placeholder variables in query templates.", "C": "Shuffling answer choices and introducing \u201cNone of the other choices\u201d as a potential option.", "D": "Adhering to predefined game rules to generate evaluation queries with varying input numbers."}, "answer": "C", "explanation": "The text explicitly states that MMLU-CF generates novel samples \u201cby shuffling answer choices and randomly replacing incorrect options with \u2018None of the other choices.\u2019\u201d", "question_token_count": 16, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 15}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "How contaminated benchmarks can lead to misleading conclusions about progress in LLM research and policy-making.", "question": "A critical consequence of employing contaminated benchmarks in Large Language Model (LLM) research, extending beyond inflated performance metrics, is the potential to distort which critical processes?", "choices": {"A": "The accurate assessment of an LLM's capacity for genuine reasoning and generalization to novel data.", "B": "The efficient allocation of computational resources during model training and deployment.", "C": "The optimization of syntactic parsing algorithms within NLP applications.", "D": "The development of more robust methods for identifying and removing adversarial examples."}, "answer": "A", "explanation": "The text explicitly states that contaminated benchmarks \u201ccan lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making\u201d and undermines \u201cthe validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability.\u201d This directly links to the ability to assess true reasoning capabilities.", "question_token_count": 33, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 14}
{"context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n", "topic": "Discuss the vulnerabilities highlighted by Yang et al. (2023) regarding decontamination methods and the implications for encryption robustness.", "question": "Yang et al. (2023) demonstrated that even advanced decontamination methods are susceptible to minor text variations. Considering this finding, what is the most critical implication for the continued robustness of encryption methods used to protect evaluation datasets?", "choices": {"A": "Encryption guarantees complete data security regardless of the effectiveness of decontamination protocols.", "B": "The vulnerability of decontamination necessitates stronger encryption algorithms with increased key lengths.", "C": "If minor text variations can bypass decontamination, encryption\u2019s effectiveness is predicated on the inability to create functionally equivalent, yet distinct, encrypted inputs.", "D": "The computational overhead of encryption is a more significant concern than vulnerabilities revealed by decontamination research."}, "answer": "C", "explanation": "Yang et al.\u2019s finding indicates that even if data is encrypted, subtle alterations can render decontamination ineffective. This means that an attacker could potentially create slightly modified versions of the encrypted data that bypass decontamination and still allow for data contamination. Therefore, the effectiveness of encryption relies on the assumption that such functionally equivalent variations cannot be created without breaking the encryption itself.", "question_token_count": 47, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 8, "avg_answer_token_count": 21}
{"context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n", "topic": "Detail the key insights regarding data contamination highlighted in the analysis, and explain why traditional benchmarks are becoming less effective.", "question": "Considering the formula Pr<sub>contam</sub> \u221d |\ud835\udc9f<sub>train</sub>| \u22c5 |\ud835\udc9f<sub>test</sub>|<sup>-1</sup>, how does the increasing size of LLM training datasets fundamentally alter the validity of traditional, static benchmarks?", "choices": {"A": "Larger training datasets necessitate more complex, hand-crafted benchmarks to accurately reflect model capabilities.", "B": "The probability of contamination increases with training dataset size relative to the testing dataset size, rendering static benchmarks less representative of true generalization ability.", "C": "Static benchmarks remain valid, but require post-hoc adjustments to account for potential data overlap between training and testing sets.", "D": "Increasing training dataset size primarily impacts evaluation speed, but does not significantly affect the validity of static benchmarks."}, "answer": "B", "explanation": "The text explicitly states that \"the probability of contamination increases with Pr<sub>contam</sub> \u221d |\ud835\udc9f<sub>train</sub>| \u22c5 |\ud835\udc9f<sub>test</sub>|<sup>-1</sup>, rendering traditional benchmarks outdated for models trained on web-scale data.\" This directly links the formula to the decreasing validity of static benchmarks.", "question_token_count": 59, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 22}
{"context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n", "topic": "Explain what Repeat Trials measures and how it relates to the ability of a dynamic benchmark to generate novel test cases.", "question": "A dynamic benchmark reports a high 'Repeat Trials' value. What does this most directly indicate regarding its ability to mitigate the risk of data contamination during LLM evaluation?", "choices": {"A": "The benchmark\u2019s transformations are highly susceptible to producing overlapping data, increasing the likelihood of data contamination.", "B": "The benchmark requires a substantial number of transformation attempts to reproduce existing variations, suggesting a high degree of novelty in generated test cases.", "C": "The benchmark\u2019s collision rate is likely to be low, indicating minimal overlap between transformed datasets, but its computational cost is high.", "D": "The benchmark primarily focuses on evaluating LLMs against a fixed set of test cases, limiting its ability to adapt to evolving model capabilities."}, "answer": "B", "explanation": "A high 'Repeat Trials' value, as defined in the text, means it takes many attempts to regenerate an existing transformed dataset. This directly implies that the benchmark is good at creating *novel* variations, and therefore less prone to generating redundant data that could contaminate the LLM's training.", "question_token_count": 33, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 24}
{"context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n", "topic": "The inherent trade-offs between consistency and vulnerability to contamination in static LLM benchmarking methods as training datasets expand.", "question": "As LLM training datasets continue to expand, what is the primary inherent limitation that increasingly compromises the validity of static benchmarking methods?", "choices": {"A": "Static methods lack the flexibility to adapt to the evolving capabilities of LLMs, resulting in outdated evaluation metrics.", "B": "The increased scale of training data amplifies the probability of overlap between the benchmark dataset and the training dataset, leading to inflated performance scores.", "C": "Static methods are computationally expensive and cannot efficiently process the vast amount of data required for comprehensive evaluation.", "D": "Dynamic benchmarking methods consistently outperform static methods, rendering static methods obsolete for large-scale LLMs."}, "answer": "B", "explanation": "The text explicitly states that static methods become \u201cmore vulnerable to contamination as training datasets grow.\u201d This contamination arises from the increasing likelihood of overlap between the benchmark data and the LLM's training data, leading to artificially inflated performance metrics.", "question_token_count": 26, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 6, "avg_answer_token_count": 22}
{"context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n", "topic": "Differentiate between the concepts of data overlap and data contamination in the context of dynamic benchmarking.", "question": "A dynamic benchmarking system exhibits a high Collision Rate. Which of the following best describes the primary risk this presents to the validity of evaluating Large Language Models (LLMs)?", "choices": {"A": "The LLM will be unable to generalize to unseen data due to insufficient diversity in the benchmark.", "B": "The LLM\u2019s performance on the benchmark may be artificially inflated because the training data contains information similar to the test data.", "C": "The benchmark\u2019s transformation process is computationally inefficient, leading to increased evaluation costs.", "D": "The benchmark is unable to accurately measure the LLM\u2019s ability to handle complex reasoning tasks."}, "answer": "B", "explanation": "A high Collision Rate indicates significant overlap between different transformations of the benchmark dataset. This overlap means that data the LLM is being tested on may have been, or is highly similar to, data it was trained on, leading to an artificially inflated performance score. This is the core concept of data contamination.", "question_token_count": 34, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 19}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "Analyze the role of competitive platforms like Codeforces and datasets like Aider in probing dynamic problem-solving capabilities of LLMs.", "question": "Considering the stated role of platforms like Codeforces and datasets like Aider in probing dynamic problem-solving capabilities of LLMs, which characteristic of these resources is *most* crucial for assessing this specific ability, beyond simply evaluating code correctness?", "choices": {"A": "The large volume of pre-written code solutions available for comparison and analysis.", "B": "The time-constrained nature of challenges and the iterative refinement process required for optimal solutions.", "C": "The focus on specific programming languages commonly used in academic settings.", "D": "The detailed documentation and tutorials provided to assist users in understanding complex algorithms."}, "answer": "B", "explanation": "Dynamic problem solving necessitates the ability to adapt and improve solutions under pressure, which is precisely what the time constraints and iterative nature of platforms like Codeforces enforce. Simply having correct code isn\u2019t enough; the model must demonstrate the ability to *quickly* and *repeatedly* refine its approach. The other options relate to code availability, language focus, or learning resources, but don't directly address the dynamic aspect.", "question_token_count": 47, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 15}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "Explain how benchmarks like ARC, OpenBookQA, and CommonsenseQA go beyond basic reasoning to require integration of background knowledge.", "question": "What fundamentally distinguishes reasoning benchmarks such as ARC, OpenBookQA, and CommonsenseQA from assessments solely focused on deductive or inductive reasoning?", "choices": {"A": "They prioritize the ability to generate and debug code, mirroring real-world software development challenges.", "B": "They necessitate the application of pre-existing, real-world knowledge alongside logical inference to formulate plausible responses.", "C": "They primarily assess a model\u2019s capacity to follow complex, multi-step instructions accurately.", "D": "They evaluate a model\u2019s performance on Chinese language tasks, focusing on instruction comprehension and factual recall."}, "answer": "B", "explanation": "The text explicitly states that ARC, OpenBookQA, and CommonsenseQA \"push models further by requiring the integration of background knowledge with logical reasoning.\" This means they go beyond simply applying logical rules; they require the model to draw upon its understanding of the world.", "question_token_count": 29, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 19}
{"context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n", "topic": "How does KIEval utilize responses from an evaluated model to inform the generation of subsequent questions?", "question": "KIEval distinguishes itself from other interactive LLM evaluation frameworks by utilizing the evaluated model\u2019s responses to inform the generation of subsequent questions. What is the *primary* input driving this follow-up question generation process within KIEval?", "choices": {"A": "The initial question from a static benchmark, ensuring consistency across evaluations.", "B": "The previous topic being assessed, allowing for a broad exploration of related concepts.", "C": "The examined LLM\u2019s response to the preceding question, enabling a dynamically tailored evaluation.", "D": "A pre-defined tree structure of subtopics, guiding the evaluation towards specific areas of expertise."}, "answer": "C", "explanation": "The text explicitly states that \u201cKIEval generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\u201d This directly indicates that the model\u2019s response is the primary driver of follow-up question generation. Options A, B, and D describe approaches used by other frameworks (static benchmarks, TreeEval's topic-based approach) or general evaluation strategies, but not KIEval\u2019s specific method.", "question_token_count": 46, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 8, "avg_answer_token_count": 16}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "Analyze the implications of documentation leaks as a source of data contamination in LLM development.", "question": "A large language model demonstrates unexpectedly high performance on a specialized task for which limited public training data exists. Investigation reveals that detailed documentation for a proprietary software library used in that task was inadvertently included in the model's training corpus. According to the provided definitions, this scenario most accurately represents which type of data contamination?", "choices": {"A": "Exact contamination, as the documentation constitutes a direct duplication of information not generally available.", "B": "Syntactic contamination, as the model has learned to generate outputs consistent with the style and terminology found in the documentation, even without verbatim replication.", "C": "Semantic contamination, as the model has absorbed the underlying meaning and concepts from the documentation, impacting its general knowledge base.", "D": "Implementation contamination, as the documentation likely contains code snippets that influenced the model's code generation capabilities."}, "answer": "B", "explanation": "The scenario describes a situation where the model's performance is boosted by knowledge gleaned from documentation, but not through direct duplication. The documentation provides a source of information that enables the model to generate outputs *consistent* with the documentation's style and terminology, representing syntactic contamination.", "question_token_count": 61, "answer_correctness_score": 8, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 22}
{"context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n", "topic": "Analyze the limitations of encryption methods concerning key management and computational overheads, as outlined in the text.", "question": "In the context of securing evaluation data with encryption, what represents a primary vulnerability that undermines its effectiveness, despite its theoretical strengths?", "choices": {"A": "The inherent inability of encryption algorithms to adapt to minor text variations.", "B": "Dependence on robust key management and the introduction of computational overheads.", "C": "The potential for models to learn and memorize test labels, even with encrypted data.", "D": "The limitations of secure multi-party computation in private benchmarking scenarios."}, "answer": "B", "explanation": "The text explicitly states that while encryption methods protect against data leakage, they \u201cdepend on strong key management\u201d and \u201cintroduce extra computational overheads,\u201d and are vulnerable if \u201cencryption is compromised or private key is exposed.\u201d This highlights these factors as key weaknesses.", "question_token_count": 27, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 14}
{"context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n", "topic": "The relationship between the cognitive levels defined by Bloom et al. (1956) and the generation of benchmark questions by Auto-Dataset.", "question": "Auto-Dataset utilizes Bloom et al.\u2019s (1956) cognitive levels to enhance benchmark generation. What is the primary function of generating questions at *different* cognitive levels within this framework?", "choices": {"A": "To increase the stylistic diversity of the generated benchmark, mirroring natural language variations.", "B": "To mitigate the risk of in-distribution contamination by introducing novel phrasing of existing concepts.", "C": "To systematically assess an LLM\u2019s ability to handle tasks requiring varying degrees of cognitive complexity.", "D": "To expand the benchmark\u2019s coverage of concepts by leveraging knowledge graphs and external information sources."}, "answer": "C", "explanation": "The text explicitly states that Auto-Dataset generates questions at different cognitive levels (Bloom et al., 1956). This is intended to create a more comprehensive assessment of the LLM\u2019s capabilities, not simply to diversify style or reduce contamination.", "question_token_count": 39, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 18}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The authors' contribution to the field, which includes a systematic survey of LLM benchmarking methods and the proposal of a new set of criteria for evaluating dynamic benchmarks.", "question": "Considering the identified lack of comprehensive evaluation criteria for dynamic benchmarks, what is the primary significance of the authors' proposed set of criteria within the broader context of LLM development?", "choices": {"A": "The criteria solely serve to identify and categorize existing dynamic benchmarks, offering no practical improvement to the benchmarking process.", "B": "The criteria provide a standardized framework for assessing the effectiveness and limitations of dynamic benchmarks, guiding future design and standardization efforts.", "C": "The criteria primarily address the issue of data contamination in static benchmarks, offering a solution applicable to all LLM evaluation methods.", "D": "The criteria are intended to demonstrate the superiority of dynamic benchmarks over static benchmarks, advocating for a complete shift in methodology."}, "answer": "B", "explanation": "The text explicitly states the authors propose a set of criteria \"to bridge this gap\" and that they \"hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\" This directly supports option B. The other options misrepresent the authors\u2019 intent or the scope of their contribution.", "question_token_count": 34, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 23}
{"context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n", "topic": "The limitations of existing complexity metrics, such as graph complexity (DyVal), in evaluating benchmark datasets.", "question": "A key challenge in evaluating dynamic benchmarks, as highlighted in the text, is the limited generalizability of existing complexity metrics. Which of the following best encapsulates the core issue with metrics like DyVal\u2019s graph complexity in this context?", "choices": {"A": "Such metrics are computationally expensive and impractical for large-scale datasets.", "B": "They often fail to accurately capture complexity changes introduced by dynamic transformations across diverse reasoning tasks.", "C": "The stability equation presented demonstrates that complexity measurements are inherently unreliable.", "D": "Existing metrics are primarily designed to detect data contamination, rather than assess task difficulty."}, "answer": "B", "explanation": "The text explicitly states that existing complexity metrics \"do not generalize well across different applications,\" and provides DyVal as an example. This directly addresses the issue of limited applicability to diverse reasoning tasks and transformations. Options A and C are not directly supported by the text, and Option D misrepresents the purpose of complexity metrics.", "question_token_count": 47, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 15}
{"context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n", "topic": "Describe how LLM-as-an-Interviewer leverages paraphrasing and follow-up questions to evaluate other LLMs.", "question": "In the LLM-as-an-Interviewer framework, the interviewer LLM initially paraphrases queries from static benchmarks. What is the primary purpose of this initial paraphrasing step?", "choices": {"A": "To simplify the benchmark questions for the examined LLM, ensuring a baseline level of understanding.", "B": "To introduce ambiguity into the questions, testing the examined LLM's robustness to poorly defined prompts.", "C": "To mitigate potential biases present in the original benchmark phrasing and assess the examined LLM's understanding of the underlying concepts.", "D": "To increase the length of the queries, forcing the examined LLM to demonstrate its ability to handle longer input sequences."}, "answer": "C", "explanation": "The text states that LLM-as-an-Interviewer \"employs an interviewer LLM that first paraphrases queries from existing static benchmarks.\" While the text doesn't explicitly state the *reason* for paraphrasing, the broader context of evaluation suggests a desire to assess the model's understanding of concepts *independent* of specific phrasing, and to reduce biases in the benchmarks themselves. This aligns with option C.", "question_token_count": 36, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 21}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "Evaluating LLM reasoning capabilities using randomly generated directed acyclic graphs (DAGs) and natural language descriptions as implemented in DyVal.", "question": "In the DyVal framework, the conversion of a randomly generated directed acyclic graph (DAG) into a natural language description is rule-based. Considering this process, which of the following aspects of the DAG is *most* critical to preserve in the natural language description to maintain accurate LLM evaluation of the root node's value?", "choices": {"A": "The aesthetic arrangement of nodes and edges within the visual representation of the DAG.", "B": "The precise numerical values assigned to each node within the DAG, ensuring direct translation into the description.", "C": "The topological relationships between nodes, specifically the direction and existence of edges defining dependencies.", "D": "The total number of nodes and edges in the DAG, providing a quantitative measure of complexity for the LLM."}, "answer": "C", "explanation": "The core principle of DyVal relies on the LLM's ability to follow logical dependencies to determine the root node's value. Topological relationships (direction and existence of edges) directly encode these dependencies. Preserving these relationships in the natural language description is therefore crucial for accurate evaluation. While node values are important, the *relationships* between them are paramount for reasoning. The number of nodes/edges is a complexity metric, not a functional requirement for correct evaluation. Aesthetic arrangement is irrelevant.", "question_token_count": 63, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 18}
{"context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n", "topic": "Discuss how the practice of withholding test labels in benchmarks prevents models from learning or memorizing answers during training.", "question": "Withholding test labels during benchmark evaluations primarily aims to mitigate which critical risk in machine learning model assessment?", "choices": {"A": "Reducing computational overhead associated with evaluation processes.", "B": "Preventing models from exploiting memorization of test data during training.", "C": "Ensuring the confidentiality of model parameters during benchmarking.", "D": "Simplifying the implementation of encryption methods for evaluation data."}, "answer": "B", "explanation": "The text explicitly states that test labels are withheld \"to prevent models from learning or memorizing them during training,\" thus mitigating data contamination risks and ensuring a more accurate evaluation of generalization ability.", "question_token_count": 21, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 9, "question_groundedness_score": 10, "avg_answer_token_count": 11}
{"context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n", "topic": "Explain the role of multi-agent systems in the context of dynamic benchmark construction for LLM evaluation.", "question": "Within multi-agent systems designed for dynamic benchmark construction for LLM evaluation, such as BENCHAGENTS, what is the primary advantage of assigning distinct LLM agents to the phases of planning, generation, verification, and evaluation?", "choices": {"A": "Specialization allows each agent to optimize its performance for a specific sub-task, leading to higher overall benchmark quality and scalability.", "B": "Dividing the process prevents any single LLM agent from becoming overly focused on exploiting weaknesses in the evaluated LLM.", "C": "It ensures redundancy in the benchmark creation process, mitigating the risk of errors in any single phase.", "D": "This division of labor is primarily for computational efficiency, reducing the processing load on individual LLM agents."}, "answer": "A", "explanation": "The text states that BENCHAGENTS leverages a multi-agent framework, splitting benchmark creation into specialized phases handled by individual LLM agents, resulting in scalable, diverse, and high-quality benchmarks. This highlights the benefit of specialization for optimized performance and quality.", "question_token_count": 44, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 21}
{"context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n", "topic": "The core problem that benchmark rewriting techniques like Auto-Dataset, StructEval, ITD, and VarBench aim to address.", "question": "Which fundamental challenge in Large Language Model (LLM) evaluation are techniques like Auto-Dataset, StructEval, ITD, and VarBench primarily designed to mitigate?", "choices": {"A": "The computational expense of evaluating LLMs on large datasets.", "B": "The risk of artificially inflated performance scores due to training data overlap with benchmark samples.", "C": "The difficulty in standardizing the format of LLM inputs and outputs.", "D": "The subjective nature of human evaluation metrics for LLM performance."}, "answer": "B", "explanation": "These techniques aim to address the problem of \"in-distribution contamination,\" where LLMs may have been trained on data similar to the benchmark, leading to unrealistically high scores. The context explicitly states the risk of contamination during training and describes techniques designed to rewrite samples to avoid this.", "question_token_count": 34, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 13}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "Describe the data sources utilized by Forecastbench to generate new forecasting questions daily.", "question": "Forecastbench distinguishes itself by updating forecasting questions daily; from what primary type of data source are these new questions derived?", "choices": {"A": "Newly published academic papers on arXiv.", "B": "Recent coding challenges from platforms like LeetCode.", "C": "Prediction markets reflecting collective forecasts.", "D": "Current math competition problems from the past year."}, "answer": "C", "explanation": "The text explicitly states that Forecastbench updates new forecasting questions daily from \"different data sources, e.g., prediction markets.\" This highlights the reliance on collective forecasts as the primary data input.", "question_token_count": 24, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 8, "avg_answer_token_count": 9}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "Describe how the LiveBench benchmark addresses the problem of data contamination in LLM evaluation.", "question": "LiveBench mitigates data contamination in LLM evaluation primarily by which method?", "choices": {"A": "Utilizing a static dataset of questions curated before a specified knowledge cutoff date.", "B": "Generating queries about information demonstrably unavailable during the model\u2019s training period.", "C": "Continuously collecting and updating questions based on the most recent information sources.", "D": "Focusing on academic writing tasks derived from pre-print publications on arXiv."}, "answer": "C", "explanation": "The text explicitly states that LiveBench \"collects questions based on the latest information source\u2026 with new questions added and updated every few months.\" This continuous updating is the key mechanism for mitigating data contamination.", "question_token_count": 15, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 15}
{"context": "###### Safety\nSafety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content. Datasets such as RealToxicityPrompts\u00a0(Gehman et\u00a0al., 2020) and ToxiGen\u00a0(Hartvigsen et\u00a0al., 2022) assess resilience against producing harmful outputs.\n\nBy providing a controlled environment to measure these aspects, safety benchmarks play a critical role in guiding the development of models that are not only powerful outputs but also responsible and trustworthy for real-world applications.\n\n\n###### Language\nLanguage benchmarks assess the LLMs\u2019 proficiency in specific languages.\n\nGLUE\u00a0(Wang, 2018) and SuperGLUE\u00a0(Wang et\u00a0al., 2019) cover tasks from sentiment analysis to language inference, while CLUE\u00a0(Xu et\u00a0al., 2020) targets Chinese language.\n\nTypo-fixing\u00a0(Suzgun et\u00a0al., 2022) is also widely used.\n\n\n###### Reading Comprehension\nReading comprehension tasks test a model\u2019s ability to extract and infer information from text. Benchmarks like SQuAD\u00a0(Rajpurkar et\u00a0al., 2018), QuAC\u00a0(Choi et\u00a0al., 2018), and BoolQ\u00a0(Clark et\u00a0al., 2019) challenge models to understand passages and draw logical conclusions.\n", "topic": "Explain the importance of safety benchmarks in the development of responsible and trustworthy LLMs, citing specific datasets used for evaluation.", "question": "Which core challenge in LLM development do datasets like RealToxicityPrompts and ToxiGen most directly address when used as safety benchmarks?", "choices": {"A": "Quantifying the models\u2019 ability to accurately translate between multiple languages.", "B": "Mitigating the risk of LLMs generating harmful or ethically misaligned outputs.", "C": "Improving the models\u2019 performance on complex reading comprehension tasks requiring inference.", "D": "Enhancing the models\u2019 capacity to identify and correct grammatical errors in text."}, "answer": "B", "explanation": "The context explicitly states that safety benchmarks, including RealToxicityPrompts and ToxiGen, \u201cassess resilience against producing harmful outputs.\u201d This directly addresses the challenge of ensuring LLMs are not used to generate toxic or unethical content.", "question_token_count": 29, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 9, "question_groundedness_score": 10, "avg_answer_token_count": 15}
{"context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n", "topic": "The critical gap in the field regarding the lack of standardized criteria for evaluating dynamic benchmarks.", "question": "The absence of standardized criteria for evaluating dynamic LLM benchmarks most directly hinders which crucial aspect of responsible AI development?", "choices": {"A": "The reproducibility of research findings and the comparability of LLM performance across different studies.", "B": "The efficient allocation of computational resources during LLM training and deployment.", "C": "The accessibility of LLMs to developers with limited expertise in benchmarking methodologies.", "D": "The reduction of bias in LLM training data and the promotion of fairness in model outputs."}, "answer": "A", "explanation": "The text explicitly states a \u201ccritical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks.\u201d Standardized criteria are essential for ensuring that results are reproducible and that different models can be fairly compared. Without them, it becomes difficult to assess true progress and identify the most effective techniques.", "question_token_count": 23, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 1, "question_groundedness_score": 9, "avg_answer_token_count": 15}
{"context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n", "topic": "The increasing importance of addressing data contamination risks in the training of Large Language Models (LLMs).", "question": "What fundamental challenge currently impedes the reliable assessment of dynamic benchmarks designed to mitigate data contamination in Large Language Models?", "choices": {"A": "The inherent limitations of static benchmarks in capturing the full complexity of LLM capabilities.", "B": "The absence of universally accepted metrics and protocols for verifying the novelty of dynamically generated test data.", "C": "The computational cost associated with generating sufficiently large and diverse dynamic test sets.", "D": "The difficulty in replicating the Internet-derived training corpora used for LLMs to definitively identify contamination sources."}, "answer": "B", "explanation": "The text explicitly states a \u201ccritical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks.\u201d This directly relates to verifying the novelty of the data, ensuring it wasn\u2019t present in the training set, and therefore providing a valid assessment.", "question_token_count": 23, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 18}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "Discuss the challenges of maintaining a temporally-constrained benchmark and ensuring its continued relevance.", "question": "A primary challenge in maintaining temporally-constrained benchmarks, like those described in the context, is the inherent trade-off between ensuring the benchmark remains relevant and avoiding the introduction of undue volatility in evaluation metrics. Which of the following best encapsulates this challenge?", "choices": {"A": "The computational cost of continuously updating the benchmark with new data outweighs the benefits of assessing current knowledge.", "B": "Frequent updates to the benchmark introduce noise and potential bias due to the rapid emergence and evolution of information.", "C": "Maintaining a temporal cutoff necessitates restricting the benchmark to domains with consistently updated information sources, limiting generalizability.", "D": "The dependence on external data sources (e.g., arXiv, LeetCode) introduces inconsistencies in data quality and format."}, "answer": "B", "explanation": "The core issue with temporal benchmarks is that \u201cnew\u201d information isn\u2019t always reliable or representative. Rapidly evolving fields can introduce noise and bias if the benchmark isn't carefully curated. While cost, generalizability, and data quality are concerns, they are secondary to the fundamental problem of ensuring the new data used for evaluation is meaningful and not just fleeting or erroneous.", "question_token_count": 52, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 7, "avg_answer_token_count": 22}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "Explain how open-domain evaluations like AlpacaEval and ArenaHard contribute to a comprehensive assessment of LLM performance.", "question": "AlpacaEval and ArenaHard are categorized within the context as providing what type of evaluation for Large Language Models?", "choices": {"A": "Evaluations focused on solving complex, multi-step mathematical problems.", "B": "Assessments of internal knowledge retrieval using curated question-answering datasets.", "C": "Open-domain evaluations targeting technical and long-context challenges.", "D": "Benchmarks designed to specifically test a model\u2019s ability to follow instructions and maintain control during generation."}, "answer": "C", "explanation": "The text explicitly states that \u201copen-domain evaluations [are] provided by AlpacaEval and ArenaHard\u201d alongside benchmarks targeting technical and long-context challenges. This directly links these evaluations to this category.", "question_token_count": 24, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 15}
{"context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n", "topic": "Summarize the key risks associated with using web-scraped datasets like FineWeb in the pre-training of LLMs regarding potential data contamination.", "question": "What primary challenge does the utilization of extensive, web-scraped datasets\u2014such as FineWeb\u2014pose to the reliable evaluation of Large Language Models (LLMs)?", "choices": {"A": "The inherent bias present in web-scraped data leads to skewed model outputs, irrespective of evaluation dataset overlap.", "B": "The sheer scale and often proprietary nature of these datasets significantly increase the likelihood of unintended overlap between training and evaluation data, hindering accurate performance assessment.", "C": "Retrieval-based detection methods are ineffective against web-scraped data, rendering any performance evaluation unreliable.", "D": "Fine-tuning on human-annotated datasets always introduces contamination, regardless of the pre-training data source."}, "answer": "B", "explanation": "The text explicitly states that the large scale and complexity of training corpora (like FineWeb) make it difficult to entirely exclude evaluation data, and the proprietary nature of many LLM\u2019s training data complicates accurate assessment. This directly addresses the challenge of reliable evaluation.", "question_token_count": 34, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 23}
{"context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n", "topic": "The implications of high variance in complexity measurements across different trials during dynamic benchmarking.", "question": "In the context of dynamic benchmarking for Large Language Models, a high variance in complexity measurements (\u03a8(\u22c5)) across different trials, as described by the stability equation, most directly indicates what potential issue with the benchmarking methodology?", "choices": {"A": "The dynamic transformation process is consistently increasing the difficulty of the seed dataset, leading to expected performance drops.", "B": "The complexity metric \u03a8(\u22c5) is not sensitive enough to detect subtle changes in task difficulty.", "C": "The dynamic transformation process is introducing unpredictable fluctuations in task complexity, undermining the reliability of performance comparisons.", "D": "Data contamination is likely occurring within the seed dataset, causing artificially low complexity measurements."}, "answer": "C", "explanation": "High variance in complexity measurements signifies instability in the benchmarking method. This means the transformations are not consistently producing tasks of a similar difficulty, making it difficult to attribute performance changes to either data contamination or genuine changes in model capability. Option C directly addresses this core implication.", "question_token_count": 45, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 19}
{"context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n", "topic": "Propose potential standardized criteria for evaluating dynamic benchmarks, building upon the identified lack of such criteria.", "question": "Considering the identified absence of standardized criteria for evaluating dynamic benchmarks, and acknowledging the trade-off between correctness and scalability, what singular, overarching criterion should be prioritized in the development of such standards, and why?", "choices": {"A": "Minimizing contamination probability, as the core issue with static benchmarks is data leakage.", "B": "Maximizing evaluation scalability, enabling frequent and comprehensive assessments.", "C": "Ensuring human-level performance consistency, aligning LLM evaluation with human capabilities.", "D": "Balancing complexity control with dynamic content generation, ensuring efficient and informative assessments."}, "answer": "A", "explanation": "The text explicitly states a lack of standardized criteria for dynamic benchmarks and identifies balancing correctness with scalability as a current challenge. Prioritizing minimizing contamination probability directly addresses the fundamental flaw of static benchmarks \u2013 data leakage \u2013 and aligns with the core motivation for moving towards dynamic benchmarks. While scalability and complexity control are important, they are secondary to ensuring the validity of the evaluation itself.", "question_token_count": 43, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 4, "question_groundedness_score": 9, "avg_answer_token_count": 15}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The core problem of data contamination in LLM evaluation, explaining how it arises from training on internet-sourced data and its impact on benchmark results.", "question": "What fundamental characteristic of contemporary Large Language Model (LLM) training exacerbates the problem of data contamination in benchmark evaluations?", "choices": {"A": "The reliance on proprietary datasets inaccessible for contamination analysis.", "B": "The inherent limitations of static benchmarking methodologies.", "C": "The extensive scraping of publicly available Internet data for model training.", "D": "The increasing complexity of LLM architectures and training algorithms."}, "answer": "C", "explanation": "The text explicitly states that LLMs \u201coften scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination.\u201d This scraping process is the primary driver of the problem, as it increases the probability that benchmark data will be included in the training set.", "question_token_count": 25, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 8, "avg_answer_token_count": 11}
{"context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n", "topic": "Discuss the role of both human-annotated and synthetic datasets in exacerbating data contamination during the fine-tuning process.", "question": "Regarding data contamination in Large Language Models, how do both human-annotated and synthetic datasets uniquely contribute to the risk of inflated performance metrics during the fine-tuning phase?", "choices": {"A": "Human-annotated datasets introduce contamination through inherent biases reflecting evaluation task expectations, while synthetic datasets risk direct replication of evaluation data characteristics.", "B": "Synthetic datasets primarily contribute to contamination via their reliance on LLM-generated outputs, whereas human-annotated datasets offer a cleaner signal and mitigate contamination risk.", "C": "Both dataset types equally contribute to contamination by simply increasing the overall volume of training data, making overlap with evaluation sets statistically more likely.", "D": "Human-annotated datasets introduce contamination through potential overlap with the web scraping used in pre-training, while synthetic datasets pose minimal contamination risk due to their artificial nature."}, "answer": "A", "explanation": "The text explicitly states that both human-annotated and synthetic datasets \"may resemble evaluation tasks, further compounding contamination risks.\" It further implies that synthetic datasets have a risk of directly replicating evaluation data, and human-annotated datasets may reflect expectations of the evaluation tasks.", "question_token_count": 34, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 29}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "Compare and contrast the approaches used by PIQA, SIQA, HellaSwag, and WinoGrande to assess intuitive reasoning skills.", "question": "Which of the following best characterizes the shared objective of the PIQA, SIQA, HellaSwag, and WinoGrande benchmarks?", "choices": {"A": "Evaluating a model\u2019s proficiency in generating syntactically correct and logically consistent code solutions.", "B": "Assessing a model\u2019s capacity to follow complex, multi-step instructions with precision and detail.", "C": "Measuring a model\u2019s ability to apply everyday knowledge and intuitive understanding to solve reasoning challenges.", "D": "Determining a model\u2019s capability to recall and integrate factual information from a broad range of knowledge domains."}, "answer": "C", "explanation": "The text explicitly states that PIQA, SIQA, HellaSwag, and WinoGrande \u201care designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives.\u201d This directly aligns with the ability to apply everyday knowledge and intuitive understanding.", "question_token_count": 30, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 18}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "Describe how dynamic benchmarks address the challenges posed by the evolving nature of LLMs and data contamination.", "question": "What primary challenge motivates the development and implementation of dynamic benchmarks in the evaluation of Large Language Models (LLMs)?", "choices": {"A": "The increasing computational cost associated with evaluating LLMs on extensive, static datasets.", "B": "The tendency of static benchmarks to become either trivial for advanced LLMs or compromised by unintentional inclusion of benchmark data in model training sets.", "C": "The inherent subjectivity in assessing the quality of LLM-generated text, requiring continuous adaptation of evaluation criteria.", "D": "The difficulty in creating benchmarks that accurately reflect the diverse range of real-world applications for LLMs."}, "answer": "B", "explanation": "The text explicitly states that static benchmarks \"may become too easy for stronger LLMs or introduce data contamination issues,\" directly linking the need for dynamic benchmarks to these specific challenges. This is the core motivation for their development.", "question_token_count": 23, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 21}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "Evaluate the differences in the types of coding challenges addressed by HumanEval, MBPP, and SWE-Bench.", "question": "Considering the landscape of coding benchmarks, HumanEval and MBPP focus on code synthesis and debugging. SWE-Bench is described as addressing \"more advanced\" challenges. Which of the following best characterizes the nature of these advanced challenges addressed by SWE-Bench relative to HumanEval and MBPP?", "choices": {"A": "SWE-Bench emphasizes larger codebases and collaborative development workflows, simulating real-world software engineering projects.", "B": "SWE-Bench focuses on tasks requiring the integration of external libraries and APIs, demanding a broader understanding of software ecosystems.", "C": "SWE-Bench centers on problems demanding a deeper understanding of algorithmic complexity and optimization techniques.", "D": "SWE-Bench tests the ability to handle more complex and nuanced problem statements, requiring sophisticated logical reasoning and problem decomposition skills."}, "answer": "D", "explanation": "The text states SWE-Bench addresses \"more advanced\" challenges, but doesn't detail *how* they are advanced. However, considering the context of coding benchmarks, the most plausible advancement is increased problem complexity requiring deeper reasoning and problem decomposition, as opposed to merely larger codebases or specific library knowledge. This is a subtle inference, making it a challenging question.", "question_token_count": 58, "answer_correctness_score": 8, "explanation_validity_score": 8, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 23}
{"context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n", "topic": "The limitations imposed by label protection on transparency and independent verification of AI model performance.", "question": "How does the restriction of independent verification, stemming from label protection strategies, most significantly impact the trajectory of advancements in artificial intelligence research?", "choices": {"A": "It accelerates the development cycle by streamlining the evaluation process and reducing the need for extensive error analysis.", "B": "It fosters a greater reliance on proprietary datasets, thereby incentivizing innovation in data acquisition and curation techniques.", "C": "It impedes detailed error analysis, reproducibility of results, and ultimately slows the rate of iterative improvement in model performance.", "D": "It encourages the development of more robust post-hoc detection methods, effectively mitigating the risks associated with data contamination."}, "answer": "C", "explanation": "The text explicitly states that label protection \"limits transparency and independent verification\u2026which can impede detailed error analysis and reproducibility.\" This directly links the limitation to slower progress in iterative improvement. While post-hoc detection is mentioned, it\u2019s presented as a response to the *problem* created by limited verification, not a benefit.", "question_token_count": 27, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 5, "avg_answer_token_count": 22}
{"context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n", "topic": "Describe the unique challenges LLMs face regarding data contamination compared to traditional machine learning models.", "question": "Considering the lifecycle of Large Language Model (LLM) development, what fundamentally distinguishes the data contamination risks faced by LLMs from those encountered in traditional machine learning paradigms?", "choices": {"A": "Traditional models utilize smaller, curated datasets, allowing for rigorous contamination checks, whereas LLMs are pre-trained on massive, web-sourced data with limited oversight.", "B": "Both LLMs and traditional models face identical contamination risks; the difference lies solely in the computational resources required for detection.", "C": "LLMs are less susceptible to contamination due to their ability to generalize from diverse datasets, effectively \"forgetting\" specific training examples.", "D": "The primary difference is that traditional models are evaluated on static datasets, while LLMs are continuously evaluated on evolving web content, making contamination assessment irrelevant."}, "answer": "A", "explanation": "The passage explicitly states that LLMs are pre-trained on massive, diverse datasets scraped from the web, increasing the risk of evaluation data overlap\u2014a scenario less common in traditional ML with curated datasets. This scale and source are the core distinction.", "question_token_count": 34, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 8, "avg_answer_token_count": 28}
{"context": "###### Safety\nSafety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content. Datasets such as RealToxicityPrompts\u00a0(Gehman et\u00a0al., 2020) and ToxiGen\u00a0(Hartvigsen et\u00a0al., 2022) assess resilience against producing harmful outputs.\n\nBy providing a controlled environment to measure these aspects, safety benchmarks play a critical role in guiding the development of models that are not only powerful outputs but also responsible and trustworthy for real-world applications.\n\n\n###### Language\nLanguage benchmarks assess the LLMs\u2019 proficiency in specific languages.\n\nGLUE\u00a0(Wang, 2018) and SuperGLUE\u00a0(Wang et\u00a0al., 2019) cover tasks from sentiment analysis to language inference, while CLUE\u00a0(Xu et\u00a0al., 2020) targets Chinese language.\n\nTypo-fixing\u00a0(Suzgun et\u00a0al., 2022) is also widely used.\n\n\n###### Reading Comprehension\nReading comprehension tasks test a model\u2019s ability to extract and infer information from text. Benchmarks like SQuAD\u00a0(Rajpurkar et\u00a0al., 2018), QuAC\u00a0(Choi et\u00a0al., 2018), and BoolQ\u00a0(Clark et\u00a0al., 2019) challenge models to understand passages and draw logical conclusions.\n", "topic": "Explain how benchmarks like RealToxicityPrompts and ToxiGen contribute to assessing the robustness of LLMs against generating harmful outputs.", "question": "How do benchmarks such as RealToxicityPrompts and ToxiGen primarily contribute to enhancing the reliability of Large Language Models?", "choices": {"A": "By evaluating the models\u2019 ability to accurately translate between multiple languages.", "B": "By quantifying the likelihood of the models generating toxic or unethical outputs under controlled conditions.", "C": "By assessing the models\u2019 proficiency in extracting information and drawing conclusions from complex texts.", "D": "By measuring the models\u2019 capacity to correct grammatical errors and typos in written content."}, "answer": "B", "explanation": "RealToxicityPrompts and ToxiGen are specifically designed to assess resilience against producing harmful outputs, providing a controlled environment to measure these aspects and guide the development of responsible models. This aligns directly with the context provided.", "question_token_count": 27, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 10, "avg_answer_token_count": 16}
{"context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The challenges associated with using static benchmarking schemes for evaluating Large Language Models.", "question": "Within the context of Large Language Model (LLM) evaluation, what fundamental problem does the transformation function, T(\u22c5), in dynamic benchmarking primarily address?", "choices": {"A": "To reduce computational costs associated with evaluating LLMs on large datasets.", "B": "To mitigate the risk of evaluating LLMs on data present in their training set, thereby improving the reliability of benchmark results.", "C": "To standardize the input format across different LLMs, ensuring fair comparison.", "D": "To increase the perplexity of the benchmark dataset, challenging LLMs with more complex inputs."}, "answer": "B", "explanation": "The text explicitly states that static benchmarking schemes face challenges due to data contamination (overlap with the training data). The transformation function T(\u22c5) is introduced specifically to modify the dataset during benchmarking, thereby avoiding this potential contamination and providing a more faithful evaluation.", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 18}
{"context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The formal definition of a dynamic benchmark, including the components \ud835\udc9f and T(\u22c5).", "question": "Within the formal definition of a dynamic benchmark, \u212cdynamic = (\ud835\udc9f, T(\u22c5)), what is the primary purpose of the transformation function, T(\u22c5)?", "choices": {"A": "To ensure the static benchmark dataset \ud835\udc9f remains unchanged throughout the evaluation process.", "B": "To generate a completely new dataset from scratch, independent of any initial static benchmark.", "C": "To modify the dataset \ud835\udc9f during benchmarking to mitigate potential data contamination issues.", "D": "To increase the complexity of the benchmark dataset to better evaluate the LLM's reasoning capabilities."}, "answer": "C", "explanation": "The text explicitly states that the transformation function T(\u22c5) \"modifies the data set during the benchmarking to avoid possible data contamination.\" This is its core function within the dynamic benchmarking framework.", "question_token_count": 36, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 16}
{"context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n", "topic": "The preliminary nature and need for validation of proposed criteria for dynamic LLM benchmarking in real-world applications.", "question": "Considering the limitations outlined regarding proposed criteria for dynamic LLM benchmarking, what represents the most immediate and crucial next step for advancing this methodology?", "choices": {"A": "Development of proprietary datasets to minimize data contamination risks.", "B": "Rigorous empirical validation and refinement of the criteria in diverse real-world applications.", "C": "Prioritization of standardization efforts across all LLM benchmarking techniques, static and dynamic.", "D": "Focus on detailed technical specifications for implementation guidelines to aid practitioners."}, "answer": "B", "explanation": "The text explicitly states that the proposed criteria are a \"first step and may need further refinement and validation in real-world applications.\" This emphasizes the necessity of empirical testing before broader implementation or standardization.", "question_token_count": 27, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 2, "avg_answer_token_count": 14}
{"context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n", "topic": "Strategies for mitigating the risk of harm or disadvantage to specific user groups or research domains through careful benchmark design.", "question": "A benchmarking framework aiming for comprehensive LLM evaluation must proactively address potential harms. Which of the following best encapsulates the core challenge in achieving this goal, as outlined in the provided text?", "choices": {"A": "Prioritizing model performance metrics over considerations of data source bias and privacy.", "B": "Balancing the adaptive benefits of dynamic benchmarks against associated privacy and security risks.", "C": "Solely focusing on mitigating bias within static benchmarks while neglecting the ethical implications of dynamic evaluation.", "D": "Establishing rigid evaluation criteria to prevent artificial inflation of model performance scores."}, "answer": "B", "explanation": "The text explicitly states that both static and dynamic benchmarks present ethical challenges. Static benchmarks risk perpetuating biases from data sources, while dynamic benchmarks introduce privacy and security concerns. The core challenge lies in balancing these competing considerations and ensuring fairness, accountability, and privacy in the overall framework. Option B accurately reflects this balancing act.", "question_token_count": 39, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 17}
{"context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n", "topic": "The broader societal impact of AI benchmarks and the need for ethical guidelines surrounding data usage and model transparency.", "question": "Considering the inherent ethical challenges presented by both static and dynamic LLM benchmarks, which of the following represents the most critical overarching concern for responsible AI development?", "choices": {"A": "Prioritizing continual data collection to ensure dynamic benchmarks remain representative of current language use.", "B": "Mitigating the potential for benchmarks to perpetuate or amplify existing societal biases through data or evaluation criteria.", "C": "Focusing solely on enhancing model transparency to enable easier identification of performance limitations.", "D": "Ensuring the security of benchmarking frameworks to prevent malicious manipulation of evaluation results."}, "answer": "B", "explanation": "The text explicitly states that static benchmarks \"can inadvertently perpetuate biases\" and that transparency must be carefully managed to prevent misuse. While all options represent valid concerns, the potential for benchmarks to reinforce societal biases represents the most fundamental and far-reaching ethical challenge. Addressing bias is crucial for ensuring fairness and avoiding harm to specific user groups.", "question_token_count": 31, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 18}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "Detail the refinements introduced by MMLU-Redux and MMLU-Pro to existing knowledge assessment methodologies.", "question": "Considering the evolution of LLM evaluation benchmarks, what primary impetus likely drove the development of refinements such as MMLU-Redux and MMLU-Pro from the original MMLU benchmark?", "choices": {"A": "To broaden the scope of assessed domains beyond those originally covered in MMLU.", "B": "To address identified limitations or shortcomings in the original MMLU assessment methodology.", "C": "To reduce the computational cost associated with administering the MMLU benchmark.", "D": "To align the benchmark more closely with human-level performance on the assessed tasks."}, "answer": "B", "explanation": "The text states that MMLU-Redux and MMLU-Pro \"refine these assessments further,\" implying the original MMLU was not perfect and had areas for improvement. This suggests the primary driver was to address existing limitations.", "question_token_count": 40, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 8, "avg_answer_token_count": 16}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "Describe the scope and purpose of multi-domain knowledge benchmarks such as MMLU, BBH, and AGI Eval.", "question": "Which characteristic most fundamentally distinguishes benchmarks like MMLU, BBH, and AGI Eval from single-domain knowledge assessments like TriviaQA or NaturalQuestions?", "choices": {"A": "Their reliance on complex mathematical reasoning skills.", "B": "Their assessment of knowledge across a significantly broader range of disciplines.", "C": "Their focus on evaluating a model's ability to generate creative text formats.", "D": "Their emphasis on long-context understanding and information retrieval."}, "answer": "B", "explanation": "MMLU, BBH, and AGI Eval are specifically described as \u201cmulti-domain tasks,\u201d covering knowledge across many disciplines. TriviaQA and NaturalQuestions, while testing knowledge, are focused on retrieving real-world information, not necessarily across a wide array of subjects.", "question_token_count": 31, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 11}
{"context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n", "topic": "Articulate how the proposed metrics (Collision Rate and Repeat Trials) collectively contribute to assessing the robustness of a dynamic benchmark.", "question": "How do the metrics of Collision Rate and Repeat Trials, when considered together, best indicate the robustness of a dynamic benchmark against potential data contamination during LLM training?", "choices": {"A": "A low Collision Rate coupled with a high Repeat Trials value signifies a benchmark generating diverse, novel test cases, resisting contamination.", "B": "A high Collision Rate and a low Repeat Trials value indicate a benchmark that is efficiently reusing existing data, maximizing training efficiency.", "C": "A high Collision Rate coupled with a high Repeat Trials value demonstrates a benchmark's ability to consistently generate similar, predictable test cases.", "D": "Equal values for Collision Rate and Repeat Trials represent an optimal balance between data diversity and computational efficiency."}, "answer": "A", "explanation": "The context states that Collision Rate measures overlap between transformations, while Repeat Trials quantifies the trials needed to regenerate a dataset. A low Collision Rate signifies minimal overlap (diversity), and a high Repeat Trials value indicates the benchmark can generate many unique variations before repeating itself, together signifying robustness against contamination.", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 24}
{"context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The implications of an empty seed dataset \ud835\udc9f on the creation of a dynamic benchmarking dataset.", "question": "Within the framework of dynamic benchmarking, if the initial seed dataset \ud835\udc9f is empty, what is the primary implication regarding the construction of the evaluation dataset \ud835\udc9ft at any given timestamp t?", "choices": {"A": "\ud835\udc9ft is derived solely from modifications applied to a pre-existing, fully populated static benchmark.", "B": "\ud835\udc9ft is generated entirely from scratch, relying on the transformation function Tt(\u22c5) without any initial data.", "C": "\ud835\udc9ft represents a probabilistic sampling of publicly available datasets, guided by the transformation function Tt(\u22c5).", "D": "\ud835\udc9ft is constructed through iterative refinement of the transformation function Tt(\u22c5) based on model performance."}, "answer": "B", "explanation": "The text explicitly states, \"If the seed dataset \ud835\udc9f is empty, the dynamic benchmarking dataset will be created from scratch.\" This directly supports the notion that the evaluation dataset at any timestamp is generated without any initial data, relying solely on the transformation function.", "question_token_count": 39, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 22}
{"context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n", "topic": "The importance of managing transparency in LLM benchmarking to prevent artificially inflated performance metrics or biased evaluation criteria.", "question": "To proactively mitigate the risk of artificially inflated LLM performance metrics or biased evaluation criteria during benchmarking, what systemic adjustment is most crucial to implement within a benchmarking framework?", "choices": {"A": "Prioritizing the use of dynamic benchmarks over static benchmarks to ensure data relevance and adaptability.", "B": "Establishing independent auditing and result verification processes, coupled with comprehensive transparency in benchmark construction and data sourcing.", "C": "Focusing solely on benchmarks utilizing publicly available datasets to minimize potential privacy concerns and data ownership disputes.", "D": "Implementing strict data anonymization protocols to prevent the identification of individual data points within dynamic benchmarks."}, "answer": "B", "explanation": "Independent auditing and transparency are vital to prevent manipulation. While dynamic benchmarks and data privacy are important, they don't directly address the core issue of inflated metrics or biased criteria. Transparency in data sourcing and benchmark construction allows for scrutiny and accountability, preventing the intentional or unintentional skewing of results.", "question_token_count": 33, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 2, "question_groundedness_score": 4, "avg_answer_token_count": 20}
{"context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n", "topic": "The transition from static to dynamic benchmarking methods in LLM evaluation due to concerns about data contamination.", "question": "The transition from static to dynamic benchmarking in LLM evaluation is primarily motivated by concerns over which of the following?", "choices": {"A": "The computational expense of generating sufficiently large static benchmark datasets.", "B": "The inherent subjectivity in evaluating LLM performance using human raters on static benchmarks.", "C": "The potential for LLMs to have been exposed to benchmark data during training, compromising evaluation validity.", "D": "The lack of diversity in tasks represented within typical static benchmark suites."}, "answer": "C", "explanation": "The text explicitly states that data contamination \u2013 the risk of LLMs having been trained on the benchmark data \u2013 is the driving force behind the shift from static to dynamic benchmarking. This directly undermines the validity of the evaluation.", "question_token_count": 22, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 5, "question_groundedness_score": 9, "avg_answer_token_count": 15}
{"context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n", "topic": "Discuss the implications of potential training data contamination for the reliability of LLM benchmarks.", "question": "A dynamic LLM benchmark exhibits a high Collision Rate and a low number of Repeat Trials. What does this combination most directly imply about the benchmark\u2019s utility in evaluating LLM capabilities?", "choices": {"A": "The benchmark is generating a highly diverse set of test cases, ensuring a robust assessment of LLM performance.", "B": "The benchmark is likely susceptible to data contamination, potentially leading to an overestimation of LLM capabilities.", "C": "The benchmark's transformation algorithm is exceptionally efficient, minimizing computational costs.", "D": "The benchmark is primarily useful for evaluating LLMs with limited training data."}, "answer": "B", "explanation": "A high Collision Rate indicates significant overlap between transformed datasets, suggesting the benchmark isn't generating novel test cases. A low number of Repeat Trials means it takes few iterations to reproduce existing variations, further confirming the lack of diversity. This combination strongly suggests the benchmark is vulnerable to data contamination and may provide an inflated view of LLM performance.", "question_token_count": 35, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 10, "avg_answer_token_count": 17}
{"context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n", "topic": "The significance of analyzing model preference for original versus paraphrased test cases as a method for detecting data contamination.", "question": "A machine learning model exhibiting a demonstrably stronger preference for original test cases compared to semantically equivalent paraphrased versions is most indicative of which underlying issue?", "choices": {"A": "Insufficient regularization during model training, leading to overfitting on the training data.", "B": "A bias in the paraphrasing technique, resulting in paraphrases that are not truly semantically equivalent.", "C": "Data contamination, where the model has been exposed to the original test data during training.", "D": "An inherent limitation of the model architecture in generalizing to novel phrasings of known concepts."}, "answer": "C", "explanation": "The text states that analyzing preference for original over paraphrased test cases is a post-hoc method for detecting contamination. A model that has seen the original test data during training will naturally perform better on those exact instances than on paraphrases, revealing memorization rather than generalization.", "question_token_count": 33, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 19}
{"context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n", "topic": "Compare and contrast the strengths and weaknesses of encryption versus label protection as strategies for securing evaluation data.", "question": "Considering the inherent vulnerabilities of each approach, which statement accurately contrasts a primary weakness of encryption versus label protection for securing evaluation data?", "choices": {"A": "Encryption is susceptible to failure if the data itself is compromised, while label protection relies heavily on robust key management practices.", "B": "Encryption introduces computational overhead and is vulnerable if keys are compromised, whereas label protection primarily fails when the evaluation data itself is publicly exposed.", "C": "Label protection prevents data leakage but is computationally expensive, while encryption is less effective against minor text variations used to circumvent decontamination methods.", "D": "Encryption maintains evaluation integrity by preventing model exposure to answers, while label protection is vulnerable to attacks exploiting weaknesses in secure multi-party computation."}, "answer": "B", "explanation": "The text states that encryption's vulnerabilities stem from compromised keys or encryption being broken, and introduces computational overhead. Label protection\u2019s weakness, as implied by the text, is that it doesn\u2019t protect the data itself, and therefore is vulnerable if the data is exposed. Option B accurately captures this contrast.", "question_token_count": 26, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 26}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "Explain the purpose of coding benchmarks in evaluating large language models.", "question": "A comprehensive evaluation of a large language model\u2019s coding abilities would likely utilize a suite of benchmarks, rather than relying on a single assessment. What is the primary rationale for employing multiple, distinct coding benchmarks like HumanEval, MBPP, and SWE-Bench?", "choices": {"A": "To ensure the model demonstrates proficiency across a spectrum of coding challenges, from basic synthesis to complex, real-world problem-solving.", "B": "To provide a statistically significant sample size for accurate performance measurement and reduce the impact of random chance.", "C": "To account for potential biases inherent in any single benchmark dataset, guaranteeing a fairer evaluation.", "D": "To specifically target and identify the types of coding errors a model is most prone to making, facilitating focused debugging efforts."}, "answer": "A", "explanation": "The text explicitly states that benchmarks like HumanEval and MBPP test code synthesis and debugging, while SWE-Bench addresses \"more advanced challenges.\" This demonstrates a progression in complexity and a need to assess a broader range of skills.", "question_token_count": 52, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 8, "avg_answer_token_count": 22}
{"context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n", "topic": "The trade-offs between the adaptability of dynamic benchmarks and the potential for bias in static benchmarks.", "question": "Considering the inherent trade-offs in Large Language Model (LLM) evaluation, which statement most accurately reflects the central ethical challenge presented by both static and dynamic benchmarks?", "choices": {"A": "Static benchmarks are inherently more susceptible to data contamination, while dynamic benchmarks are less prone to reflecting societal biases.", "B": "Dynamic benchmarks prioritize adaptability at the cost of potentially compromising user privacy, while static benchmarks risk perpetuating biases present in their foundational data.", "C": "The primary ethical concern for both benchmark types lies in the potential for malicious actors to manipulate evaluation results for competitive advantage.", "D": "Static benchmarks are ethically superior due to their transparency and immutability, whereas dynamic benchmarks introduce unacceptable levels of uncertainty."}, "answer": "B", "explanation": "The context explicitly states that static benchmarks can perpetuate biases if based on outdated data, and dynamic benchmarks raise privacy concerns due to continual data collection. This option accurately captures the core trade-off between adaptability and potential bias/privacy risks.", "question_token_count": 33, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 24}
{"context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n", "topic": "Describe how Collision Rate is calculated and what a high Collision Rate would indicate about a dynamic benchmark.", "question": "A dynamic benchmark exhibits a consistently high Collision Rate between independently transformed datasets. What does this most likely indicate regarding the benchmark\u2019s efficacy?", "choices": {"A": "The benchmark is effectively preventing data contamination by identifying and removing overlapping test cases.", "B": "The benchmark is generating a diverse range of novel test cases, ensuring robust evaluation of LLM capabilities.", "C": "The benchmark's transformations are producing limited variation, potentially leading to LLMs memorizing the benchmark rather than generalizing.", "D": "The benchmark is optimized for computational efficiency, prioritizing speed over the diversity of generated test cases."}, "answer": "C", "explanation": "A high Collision Rate signifies substantial overlap between transformed datasets. This limits the benchmark's ability to create novel challenges and increases the risk of LLMs simply memorizing the benchmark data instead of demonstrating true generalization ability, as stated in the text.", "question_token_count": 27, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 19}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "Evaluate the specific technical and long-context challenges addressed by ControlBench, FRAMES, and GPQA Diamond.", "question": "ControlBench, FRAMES, and GPQA Diamond are specifically designed to evaluate LLMs on challenges not fully captured by standard benchmarks. Which of the following best characterizes the primary focus of these benchmarks?", "choices": {"A": "Assessing the model's ability to perform complex arithmetic and symbolic manipulation.", "B": "Evaluating the model\u2019s capacity to maintain coherence and accuracy when processing extended sequences of text.", "C": "Measuring the model\u2019s recall of factual information across a wide range of domains.", "D": "Determining the model\u2019s proficiency in generating creative text formats, like poems or code."}, "answer": "B", "explanation": "The context explicitly states these benchmarks target \"technical and long-context challenges.\" Long-context challenges refer to the difficulty LLMs have with maintaining information and coherence over very long input sequences. While the other options represent valid LLM evaluation areas, they are not the *specific* focus of these three benchmarks as stated in the text.", "question_token_count": 40, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 16}
{"context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n", "topic": "Discuss the challenges of ensuring fairness in LLM evaluation using dynamic benchmarks, considering potential data contamination issues.", "question": "Considering the inherent challenges of utilizing LLMs for dynamic benchmark generation, what primary strategy mitigates risks associated with a lack of transparency and traceability in the transformation process?", "choices": {"A": "Implementing temporal cutoff methods to restrict data to newly released information.", "B": "Prioritizing rule-based or manually crafted transformations for inherent interpretability.", "C": "Employing hybrid approaches that combine temporal cutoff with LLM-based generation.", "D": "Increasing the volume of transformed data to statistically validate correctness."}, "answer": "B", "explanation": "The text explicitly states that rule-based or manually crafted transformations are inherently interpretable, reducing the need for extensive manual validation when LLM-assisted transformations lack transparency and traceability. This directly addresses the challenge of ensuring reliability and correctness in LLM-generated benchmarks.", "question_token_count": 34, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 8, "avg_answer_token_count": 13}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "Discuss the importance of preventing data contamination to ensure reliable LLM evaluation benchmarks.", "question": "An LLM evaluation benchmark includes a question: \u201cWhat is the capital of France?\u201d. During analysis, it is discovered that the training data contained the sentence: \u201cThe capital of France is Paris.\u201d, with the only difference being the inclusion of a comma after \u201cFrance\u201d.  What type of data contamination is present?", "choices": {"A": "Exact contamination, as the core information is duplicated verbatim.", "B": "Syntactic contamination, as a minor syntactic transformation (comma inclusion) exists between the training and benchmark data.", "C": "Semantic contamination, as the meaning of the question is present in the training data.", "D": "No contamination, as the slight variation in punctuation indicates no overlap."}, "answer": "B", "explanation": "Syntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation. The inclusion of a comma represents a minor syntactic variation, while the core meaning and information remain identical. This aligns with the definition provided in the text.", "question_token_count": 64, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 16}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "The definition of data contamination in LLM benchmarking and its potential sources.", "question": "Regarding Large Language Model (LLM) benchmarking, the debate surrounding \u201csyntactic contamination\u201d \u2013 where test data is derived from training data via rephrasing \u2013 primarily centers on which challenge to accurate evaluation?", "choices": {"A": "Determining whether observed performance reflects genuine reasoning ability or mere memorization of training data.", "B": "Quantifying the degree to which syntactic variations impact model generalization across diverse linguistic styles.", "C": "Establishing standardized methods for identifying and removing syntactically similar data points from training sets.", "D": "Assessing the computational cost of processing syntactically complex test cases compared to simpler formulations."}, "answer": "A", "explanation": "The text explicitly states that the debate over syntactic contamination arises from the difficulty in distinguishing between an LLM\u2019s ability to recall memorized information and its reasoning capability. This is the core of the issue, making option A the correct answer.", "question_token_count": 44, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 18}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The limitations of current dynamic benchmarks as revealed by the authors' evaluation using their proposed criteria.", "question": "Considering the authors' evaluation of current dynamic benchmarks against their proposed criteria, which fundamental characteristic most significantly hinders the efficacy of these benchmarks in providing a reliable assessment of LLM intelligence?", "choices": {"A": "The inherent difficulty in tracing the exact training data of LLMs, making complete contamination detection impossible.", "B": "The reliance on timestamp-based updates, which are susceptible to manipulation or inaccuracies in training data metadata.", "C": "The failure of existing dynamic benchmarks to comprehensively address all proposed evaluation criteria, indicating an incomplete design.", "D": "The computational cost associated with continuously regenerating benchmark data, limiting scalability and widespread adoption."}, "answer": "C", "explanation": "The text explicitly states that their study \u201creveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design.\u201d This makes option C the most direct and accurate answer. While the other options represent challenges in LLM evaluation, they aren't the primary limitation identified by the authors regarding the *dynamic benchmarks themselves*.", "question_token_count": 36, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 20}
{"context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n", "topic": "The formal definition of stability in dynamic benchmarking and its relationship to variance in complexity measurements.", "question": "In the context of dynamic benchmarking, a high variance in complexity measurements, as formalized by \u03a8(\u22c5), indicates what primary characteristic of the benchmarking method?", "choices": {"A": "Increased sensitivity to data contamination.", "B": "A lack of consistency and reliability in the benchmark's difficulty across trials.", "C": "Improved generalization of complexity metrics across different applications.", "D": "A reduction in the overall complexity of the seed dataset."}, "answer": "B", "explanation": "The text states, \u201chigh variance indicates that the dynamic benchmarking method is not stable.\u201d Stability, in this context, refers to the consistency of complexity measurements. Therefore, high variance directly implies a lack of consistency.", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 11}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "The ongoing debate surrounding whether syntactic transformations should be considered true data contamination.", "question": "Considering the ambiguity surrounding syntactic contamination in LLM benchmarks, what core challenge hinders the accurate assessment of an LLM's capabilities when evaluating performance on syntactically transformed test data?", "choices": {"A": "The inability to definitively separate an LLM's capacity for genuine reasoning from its propensity to recall and reproduce information present in the training dataset.", "B": "The computational expense associated with exhaustively verifying the absence of any overlap between training and testing datasets.", "C": "The inherent limitations of current NLP applications in reliably processing syntactically complex language structures.", "D": "The subjective nature of determining whether a syntactic transformation is substantial enough to constitute genuinely \u201cnew\u201d data."}, "answer": "A", "explanation": "The text explicitly states that a key challenge is \"distinguishing between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference.\" This is the fundamental issue at the heart of the debate over syntactic contamination.", "question_token_count": 35, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 21}
{"context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n", "topic": "VarBench's strategy for generating new benchmark samples by identifying and replacing variables in existing ones.", "question": "VarBench distinguishes itself from other LLM-based benchmark generation techniques by specifically focusing on modifying existing samples through what primary mechanism?", "choices": {"A": "Generating entirely new questions related to the original benchmark's concepts at varying cognitive levels.", "B": "Identifying and replacing specific, identifiable components within existing benchmark samples.", "C": "Detecting and rewriting samples identified as contaminated within static benchmarks while preserving difficulty.", "D": "Expanding upon examined concepts using knowledge graphs to create a series of extended questions."}, "answer": "B", "explanation": "The text explicitly states that \"VarBench prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\" This directly addresses the core mechanism of VarBench.", "question_token_count": 24, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 15}
{"context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n", "topic": "Discuss the relationship between scalability, statistical errors, and the optimal size of a dynamic benchmark dataset.", "question": "Considering a dynamic benchmark aiming for high scalability, which optimization strategy would most directly address the equation\u2019s emphasis on maximizing data generation per unit cost?", "choices": {"A": "Prioritizing transformations that yield the largest possible increase in dataset size, regardless of computational expense.", "B": "Selecting transformation methods that minimize the statistical error rate, even if they require significant manual effort.", "C": "Identifying and implementing transformation processes that offer the highest ratio of transformed data volume to resource expenditure.", "D": "Focusing on transformations that maintain the original dataset\u2019s characteristics to minimize the impact on benchmark results."}, "answer": "C", "explanation": "The equation explicitly defines scalability as the expectation over the transformation space, representing the proportion of data generated per unit cost. Therefore, optimizing for the highest ratio of transformed data volume to resource expenditure directly addresses the equation\u2019s core principle.", "question_token_count": 29, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 19}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "Analyze the limitations of using static benchmarks for evaluating LLMs as they undergo continuous training.", "question": "As Large Language Models (LLMs) undergo continuous training, what fundamental limitation arises when relying solely on static benchmark evaluations?", "choices": {"A": "Static benchmarks accurately reflect a model\u2019s generalization capabilities as its training data expands.", "B": "Continuous training introduces data contamination, rendering static benchmarks increasingly unreliable indicators of true performance.", "C": "Static benchmarks consistently provide a challenging evaluation, irrespective of the LLM\u2019s evolving capabilities.", "D": "The primary limitation is the high cost associated with regularly updating static benchmarks to reflect model advancements."}, "answer": "B", "explanation": "The text explicitly states that \u201cunchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues\u201d as LLMs continue training on all available data. This directly links continuous training to the unreliability of static benchmarks.", "question_token_count": 25, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 17}
{"context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n", "topic": "How the correctness criterion addresses the challenge of non-standardized evaluation criteria for dynamic LLM benchmarks.", "question": "Within the context of dynamic LLM benchmarking, how does the 'Correctness' criterion, as defined by the provided equation, fundamentally mitigate the risk of generating misleading evaluations?", "choices": {"A": "By ensuring that the dynamic benchmark always produces outputs identical to those of a static benchmark, thereby providing a consistent evaluation standard.", "B": "By establishing an expected alignment between transformed dataset outputs and an objective ground truth, quantified via an oracle and a scoring function.", "C": "By prioritizing the complexity of the transformations applied to the dataset, thereby increasing the robustness of the benchmark against adversarial inputs.", "D": "By solely relying on human annotators to validate the outputs of the dynamic benchmark, eliminating the need for automated scoring functions."}, "answer": "B", "explanation": "The 'Correctness' criterion directly addresses the issue of unreliable benchmarks by quantifying the alignment between the benchmark's outputs and an objective ground truth. This is achieved using an oracle function (\ud835\udca2\u2062(\u22c5)) to provide the ground truth and a scoring function (\ud835\udcae\u2062(\u22c5)) to measure the alignment, as explicitly stated in the context.", "question_token_count": 34, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 24}
{"context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n", "topic": "Discuss potential implications of low external diversity in a transformed dataset.", "question": "A data transformation process consistently yields a transformed dataset exhibiting low external diversity relative to the seed dataset. What is the most likely implication of this observation?", "choices": {"A": "The transformation effectively enhances the privacy of sensitive information within the original dataset.", "B": "The transformation process is introducing significant novelty and generalization capabilities to the dataset.", "C": "The transformed dataset may offer limited benefit for tasks requiring representations substantially different from the original data.", "D": "The transformation is successfully mitigating biases present in the seed dataset, resulting in a more balanced representation."}, "answer": "C", "explanation": "Low external diversity signifies minimal change between the transformed and original datasets. This suggests the transformation isn\u2019t creating substantially different representations, limiting its usefulness for tasks that *require* such differences (e.g., generalization to new domains, robustness to perturbations).", "question_token_count": 30, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 17}
{"context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n", "topic": "How static benchmarks contribute to standardized evaluation practices for LLMs across diverse capabilities.", "question": "Within the formal definition of a static benchmark, \ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.)), what is the primary function of \ud835\udcae(\u22c5), the scoring function, beyond merely comparing an LLM's output to the expected output \ud835\udcb4?", "choices": {"A": "To normalize the output length of the LLM to ensure fair comparison across models.", "B": "To quantify the degree of alignment between the LLM's output and the expected output, thereby enabling standardized, quantitative evaluation.", "C": "To identify and penalize instances of hallucination or factual inaccuracies within the LLM's response.", "D": "To dynamically adjust the input prompt \ud835\udcb3 based on the LLM's previous responses, optimizing for higher scores."}, "answer": "B", "explanation": "The text states that the scoring function \ud835\udcae(\u22c5) \"evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4.\" This indicates its core purpose is quantification and standardization of evaluation, rather than length normalization, hallucination detection, or dynamic prompt adjustment.", "question_token_count": 53, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 20}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "How do benchmarks like LiveBench, AntiLeak-Bench, AcademicEval, LiveCodeBench, LiveAoPSBench, and Forecastbench contribute to the reliability of LLM evaluations?", "question": "Which unifying principle underlies the design of LiveBench, AntiLeak-Bench, AcademicEval, LiveCodeBench, LiveAoPSBench, and Forecastbench as methods for evaluating Large Language Models?", "choices": {"A": "They all utilize datasets sourced exclusively from publicly available academic archives, ensuring reproducibility and verifiability of results.", "B": "They all mitigate the risk of data contamination by focusing on information that emerged after the typical knowledge cutoff date of current LLMs.", "C": "They all prioritize evaluating LLMs on tasks requiring complex reasoning and multi-step problem-solving, irrespective of the data source.", "D": "They all aim to assess an LLM\u2019s ability to extrapolate from limited data and generate novel insights beyond its training corpus."}, "answer": "B", "explanation": "The context explicitly states that these benchmarks are designed to prevent data contamination by utilizing data collected *after* the model\u2019s knowledge cutoff date. This ensures the model is evaluated on truly new information, rather than simply recalling memorized data.", "question_token_count": 42, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 5, "question_groundedness_score": 10, "avg_answer_token_count": 24}
{"context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n", "topic": "Synthesize the benefits of combining different approaches in hybrid dynamic benchmark construction.", "question": "What primary advantage does a hybrid approach offer in the construction of dynamic benchmarks for Large Language Models, when contrasted with solely rule-based or LLM-based generation?", "choices": {"A": "Hybrid approaches inherently eliminate the need for interpretability checks due to the controlled nature of rule-based components.", "B": "Hybrid approaches leverage the generative capabilities of LLMs while mitigating concerns regarding data contamination present in temporal cutoff methods.", "C": "Hybrid approaches capitalize on the strengths of different methods, potentially enhancing interpretability and reducing the reliance on purely model-dependent transformations.", "D": "Hybrid approaches consistently outperform both rule-based and LLM-based methods in identifying previously unseen biases within LLMs."}, "answer": "C", "explanation": "The text states that hybrid approaches \"combine the idea of these different approaches,\" and that rule-based transformations are \"inherently interpretable\" while LLM-assisted transformations may require additional mechanisms to ensure reliability. Therefore, a hybrid approach can balance the benefits of both.", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 7, "avg_answer_token_count": 22}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "Compare and contrast the methodologies employed by AntiLeak-Bench and AcademicEval in creating temporally-constrained evaluation datasets.", "question": "Which of the following best encapsulates the primary methodological distinction between AntiLeak-Bench and AcademicEval in their approaches to constructing temporally-constrained evaluation datasets?", "choices": {"A": "AntiLeak-Bench focuses on generating queries regarding knowledge that did not exist prior to the model\u2019s knowledge cutoff, while AcademicEval designs tasks based on recently published academic papers.", "B": "AntiLeak-Bench relies on continuously updated data sources like coding competitions, whereas AcademicEval utilizes static datasets of pre-defined academic prompts.", "C": "AntiLeak-Bench assesses forecasting abilities through prediction markets, while AcademicEval evaluates mathematical reasoning skills via online forums.", "D": "AntiLeak-Bench prioritizes evaluating model performance on live, real-time data streams, while AcademicEval concentrates on assessing the model\u2019s capacity for long-form content generation."}, "answer": "A", "explanation": "The text explicitly states that AntiLeak-Bench \u201cgenerates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date\u201d and that AcademicEval \u201cdesigns academic writing tasks on latest arXiv papers.\u201d This accurately captures the core difference in their methodologies.", "question_token_count": 33, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 30}
{"context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n", "topic": "The characteristics and benefits of rule-based generation methods for creating LLM benchmarks, specifically regarding collision probability.", "question": "Rule-based generation methods for LLM benchmarks are noted for their extremely low collision probability. What primarily enables this characteristic?", "choices": {"A": "The exclusive reliance on temporally recent data sources, minimizing overlap with training datasets.", "B": "The application of predefined rules and constraints during test case synthesis, fostering novelty.", "C": "The extensive use of human verification processes to identify and eliminate duplicate instances.", "D": "The dynamic shuffling of answer choices, ensuring that no two benchmarks are identical in structure."}, "answer": "B", "explanation": "Rule-based generation, as stated in the text, synthesizes new test cases *based on predefined rules*. This inherent structure dictates the generation process, ensuring that each generated instance adheres to these rules and thus avoids collisions. Options A, C, and D represent strategies used in other benchmark creation methods but are not the defining feature of rule-based generation's low collision probability.", "question_token_count": 23, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 8, "avg_answer_token_count": 16}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "Explain the importance of holistic benchmarks in evaluating the performance of evolving Large Language Models (LLMs).", "question": "Why is the development of holistic, dynamic benchmarks particularly crucial as Large Language Models (LLMs) continue to evolve?", "choices": {"A": "Static benchmarks provide a consistently challenging evaluation metric, unaffected by model improvements.", "B": "LLMs are rapidly improving and may memorize static benchmarks, leading to inflated performance scores and data contamination issues.", "C": "Holistic benchmarks primarily focus on coding tasks, which are the most indicative of general intelligence in LLMs.", "D": "Dynamic benchmarks are unnecessary, as LLMs are ultimately limited by their initial training data and cannot surpass those boundaries."}, "answer": "B", "explanation": "The text explicitly states that static benchmarks become too easy for stronger LLMs and introduce data contamination issues as LLMs continue training on available data. This highlights the need for dynamic benchmarks that can adapt to model improvements and mitigate contamination risks.", "question_token_count": 24, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 20}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "Discuss the evolving landscape of LLM benchmarks and the ongoing efforts to create more robust and comprehensive evaluation methods.", "question": "Considering the proliferation of LLM benchmarks \u2013 including iterations like MMLU-Redux and MMLU-Pro \u2013 what primary impetus drives the continuous development and refinement of these evaluation methodologies?", "choices": {"A": "The need to assess LLMs\u2019 proficiency in handling increasingly complex and specialized mathematical problem-solving.", "B": "The recognition that initial benchmarks lacked the nuance to accurately reflect the evolving capabilities of advanced LLMs.", "C": "A desire to standardize evaluation procedures across different LLM architectures and training paradigms.", "D": "The necessity to create benchmarks that are resistant to adversarial attacks and prompt engineering techniques."}, "answer": "B", "explanation": "The text explicitly states that new benchmarks like MMLU-Redux and MMLU-Pro are refinements of existing assessments. This indicates a dissatisfaction with the initial benchmarks' ability to fully capture the capabilities of LLMs, supporting option B as the correct answer. Options A, C, and D, while potentially relevant concerns in LLM development, are not the primary driver highlighted in the provided context.", "question_token_count": 37, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 8, "avg_answer_token_count": 18}
{"context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n", "topic": "The impact of the rapidly evolving landscape of LLM development on the comprehensiveness of current benchmarking surveys.", "question": "Given the inherent limitations of capturing the rapidly evolving landscape of LLM development, how does the comprehensiveness of current benchmarking surveys fundamentally shift over time?", "choices": {"A": "Comprehensiveness remains static, as core evaluation metrics remain consistent despite model advancements.", "B": "Comprehensiveness is perpetually decreasing, as the rate of LLM innovation consistently outpaces the ability of surveys to incorporate new methods and challenges.", "C": "Comprehensiveness oscillates between increases and decreases, contingent upon periodic, large-scale re-evaluations triggered by significant model releases.", "D": "Comprehensiveness is maintained through the continuous refinement of existing static benchmarking methods to account for emergent model capabilities."}, "answer": "B", "explanation": "The text explicitly states that \u201cdue to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered.\u201d This directly supports the idea that comprehensiveness is perpetually decreasing as innovation surpasses the ability of surveys to fully capture the landscape.", "question_token_count": 30, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 6, "avg_answer_token_count": 24}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "Analyzing how random graph synthesis is used in NPHardEval to provide inputs for evaluating LLM performance on TSP.", "question": "Within the NPHardEval framework, employing random graph synthesis for generating Traveling Salesman Problem (TSP) instances primarily serves what purpose when assessing LLM performance?", "choices": {"A": "To ensure the LLM is exposed to a consistent set of TSP instances, facilitating reliable performance comparisons.", "B": "To create TSP instances that mirror real-world scenarios, thereby testing the LLM\u2019s practical problem-solving capabilities.", "C": "To generate a diverse range of TSP instances, mitigating bias from pre-defined examples and thoroughly evaluating the LLM\u2019s reasoning abilities.", "D": "To simplify the TSP instances, allowing for faster evaluation and identification of fundamental reasoning flaws in the LLM."}, "answer": "C", "explanation": "NPHardEval uses random graph synthesis to create varying sizes of graphs for TSP. This is done to assess the LLM\u2019s performance, implying a need to test its reasoning across a diverse set of problem instances, avoiding biases inherent in pre-defined examples.", "question_token_count": 33, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 8, "avg_answer_token_count": 21}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "Evaluating the significance of the 2024 publication dates of the cited works in the context of LLM reasoning evaluation research.", "question": "The concentration of publications evaluating LLM reasoning (Lei et al., 2024; Zhu et al., 2024a; Fan et al., 2024; Xie et al., 2024) all appearing in 2024 most strongly suggests which of the following?", "choices": {"A": "A decline in interest in LLM reasoning evaluation methods prior to 2024 due to inherent limitations in earlier LLM architectures.", "B": "A recent acceleration in LLM capabilities necessitating the development of more sophisticated and challenging evaluation benchmarks.", "C": "A coordinated effort by a specific research consortium to standardize LLM reasoning evaluation methodologies.", "D": "A cyclical pattern in AI research, where reasoning evaluation receives focused attention every ten years."}, "answer": "B", "explanation": "The rapid emergence of these evaluations in 2024 points to a response to recent advancements in LLM capabilities. As LLMs become more powerful, existing benchmarks become insufficient, driving the need for more challenging and nuanced evaluation methods. This is the most plausible explanation given the context.", "question_token_count": 56, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 19}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "Define data contamination and explain its impact on the validity of LLM performance measurements.", "question": "A researcher discovers that several test examples from a benchmark dataset have undergone minor punctuation normalization and whitespace modification before appearing in the LLM's training corpus. According to the provided definitions, what type of data contamination is most likely occurring?", "choices": {"A": "Exact contamination, as any overlap between training and test sets constitutes this type of contamination.", "B": "Syntactic contamination, as the examples exist in the training data after syntactic transformations.", "C": "Semantic contamination, as the meaning of the examples has been slightly altered.", "D": "Implementation contamination, as the examples likely originated from benchmark implementation code."}, "answer": "B", "explanation": "Syntactic contamination is specifically defined as occurring when test data points are found in the training dataset after syntactic transformations like punctuation normalization and whitespace modification, while preserving lexical meaning. This aligns directly with the scenario described in the question.", "question_token_count": 45, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 16}
{"context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n", "topic": "The role of improved mapping metrics in enhancing the accuracy of post-hoc data contamination detection.", "question": "In the context of post-hoc data contamination detection, the adoption of \u201cimproved mapping metrics\u201d represents a direct response to which limitation inherent in earlier methodologies?", "choices": {"A": "The inability to detect contamination resulting from paraphrased or slightly altered test cases.", "B": "The computational expense associated with embedding-based similarity searches.", "C": "The reliance on centralized evaluation systems for performance metrics.", "D": "The challenges in establishing a clear definition of what constitutes \u201cdata contamination.\u201d"}, "answer": "A", "explanation": "The text states that \"exact matching often leads to false negatives,\" prompting the use of more robust techniques like improved mapping metrics. This indicates that improved mapping metrics address the inability of exact matching to identify subtle overlaps or variations, like paraphrased test cases, which would still constitute contamination.", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 10, "avg_answer_token_count": 13}
{"context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n", "topic": "An analysis of the limitations present in currently available dynamic benchmarking methods for LLMs.", "question": "A primary limitation identified within the evolving landscape of LLM benchmarking, shifting from static to dynamic methods to mitigate data contamination, concerns the assessment of dynamic benchmarks themselves. What critical deficiency currently hinders robust evaluation of these dynamically generated benchmarks?", "choices": {"A": "The inherent reliance of dynamic benchmarks on pre-existing static datasets, reintroducing contamination risks.", "B": "The absence of universally accepted, standardized criteria for evaluating their effectiveness and reliability.", "C": "The computational cost associated with generating sufficiently large and diverse dynamic benchmark suites.", "D": "The difficulty in ensuring the generated questions maintain consistent semantic complexity across iterations."}, "answer": "B", "explanation": "The text explicitly states a \"critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks.\" This absence of standardized criteria is the primary deficiency hindering robust evaluation, making option B the correct answer. Options A, C, and D represent potential challenges but are not the central limitation highlighted in the context.", "question_token_count": 46, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 16}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "Describe the types of syntactic transformations that can lead to syntactic contamination while preserving lexical meaning.", "question": "According to the provided definition, which of the following syntactic transformations would *not* lead to syntactic contamination if applied to a test data point and found within the training dataset?", "choices": {"A": "Replacing words with synonyms while maintaining the original sentence structure.", "B": "Normalizing punctuation and whitespace variations within a sentence.", "C": "Changing the sentence from active to passive voice, preserving the core meaning.", "D": "Translating the sentence into a different language and then back into the original language."}, "answer": "D", "explanation": "The definition explicitly states that syntactic contamination occurs with transformations \u201cwhile preserving lexical meaning.\u201d Translation inherently alters lexical meaning, even if the core concept is retained, and therefore would not constitute syntactic contamination as defined in the text.", "question_token_count": 35, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 14}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "Describe the role of \u2131syntactic in identifying syntactic contamination, and provide examples of transformations it encompasses.", "question": "The function \u2131syntactic is central to identifying syntactic contamination in LLMs. Which of the following best characterizes the scope of transformations encompassed by \u2131syntactic?", "choices": {"A": "Primarily focuses on removing punctuation and standardizing whitespace to ensure consistent formatting between training and test datasets.", "B": "Includes alterations that preserve lexical meaning, such as synonym replacement, morphological variations, and syntactic paraphrasing.", "C": "Limited to minor corrections of grammatical errors and spelling mistakes present in the test dataset.", "D": "Exclusively involves the detection of exact string matches between training and test data, disregarding any modifications."}, "answer": "B", "explanation": "The context explicitly defines \u2131syntactic as encompassing \u201cpunctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\u201d This aligns directly with option B. The other options are either too narrow (A, C) or contradict the definition (D).", "question_token_count": 37, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 1, "question_groundedness_score": 10, "avg_answer_token_count": 20}
{"context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n", "topic": "Characterize the four categories of dynamic benchmark construction\u2014temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches\u2014and provide an example of each.", "question": "Which dynamic benchmark construction method inherently offers the highest degree of interpretability, simplifying the verification process and reducing reliance on supplementary validation techniques?", "choices": {"A": "Temporal cutoff, as it mirrors established static benchmark data collection.", "B": "Rule-based generation, due to its reliance on predefined and transparent rules.", "C": "LLM-based generation, leveraging the explainability tools integrated within modern language models.", "D": "Hybrid approaches, combining the strengths of both rule-based and LLM-based methods."}, "answer": "B", "explanation": "The text explicitly states that \u201cRule-based or manually crafted transformations are inherently interpretable,\u201d reducing the need for extensive manual validation. This contrasts with LLM-based methods, which require additional mechanisms for ensuring reliability.", "question_token_count": 27, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 15}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "Understanding how task difficulty is controlled in DyVal through variations in the number of nodes and edges in the generated DAGs.", "question": "In the DyVal framework for evaluating LLM reasoning, the number of nodes and edges in generated directed acyclic graphs (DAGs) are varied. What is the primary rationale behind this manipulation?", "choices": {"A": "To introduce randomness into the evaluation process, ensuring each LLM receives a unique task.", "B": "To directly control the computational resources required to solve the task, limiting evaluation time.", "C": "To modulate the complexity of the reasoning path the LLM must traverse to determine the root node\u2019s value.", "D": "To simulate real-world data dependencies and assess the LLM\u2019s ability to handle incomplete information."}, "answer": "C", "explanation": "The text explicitly states that DyVal \"constructs DAGs with varying numbers of nodes and edges to control task difficulty.\" Increasing the number of nodes and edges increases the length and complexity of the reasoning path, making the task more challenging for the LLM.", "question_token_count": 38, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 18}
{"context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n", "topic": "The role of practical mitigation tools in addressing data contamination issues within LLM benchmarking.", "question": "Considering the identified limitations of both static and dynamic LLM benchmarking methods, what is the primary rationale for the survey's emphasis on the development of practical mitigation tools?", "choices": {"A": "To enhance the consistency of benchmarks, addressing the vulnerability of static methods as training datasets scale.", "B": "To overcome the reproducibility challenges inherent in dynamic approaches and standardize evaluation processes.", "C": "To directly address the core issue of data contamination, acknowledging the shortcomings of existing evaluation strategies.", "D": "To provide practitioners with in-depth implementation guidelines for fine-grained technical details of benchmarking methods."}, "answer": "C", "explanation": "The survey explicitly states the need for \"practical mitigation tools\" in its conclusion, directly responding to the limitations of both static and dynamic approaches. These tools are intended to address the fundamental problem of data contamination, which underlies the issues of consistency, reproducibility, and scalability.", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 18}
