{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "The underlying assumptions about model abilities and limitations that motivate the development of diverse benchmark datasets in AI research.", "question": "How do the design and diversification of benchmark datasets in AI research reflect implicit assumptions about the specific cognitive and functional limitations of large language models, and what does this imply about the challenges in comprehensively evaluating their capabilities?", "answer": "Diverse benchmarks reflect assumptions that LLMs have uneven abilities in reasoning, knowledge retrieval, and long-context management, implying comprehensive evaluation requires targeted tasks to reveal nuanced limitations.", "explanation": "Benchmark datasets are diversified to target distinct facets of model performance\u2014such as multi-step mathematical reasoning and multi-domain factual knowledge\u2014because it is assumed that models have uneven strengths and weaknesses across these areas. This diversification implies that evaluating LLMs comprehensively is challenging due to their complex capabilities, requiring specialized tasks to expose nuanced limitations like long-context understanding, technical accuracy, and reasoning depth.", "question_token_count": 43, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 8, "avg_answer_token_count": 34, "choices": null}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "The evaluation of factual accuracy in language models through datasets like C-SimpleQA, especially in the context of Chinese language short questions.", "question": "How does the C-SimpleQA benchmark uniquely evaluate the factual accuracy of language models in Chinese short-question answering, and why is this focus critical compared to other reasoning datasets that emphasize commonsense or logical inference?", "answer": "By measuring a model\u2019s ability to provide factually correct answers to short Chinese questions, C-SimpleQA uniquely focuses on factual accuracy rather than commonsense or logical inference, addressing critical language-specific challenges in evaluating reliable knowledge retrieval.", "explanation": "C-SimpleQA specifically targets the factual correctness of responses to short questions in Chinese, differentiating it from other reasoning benchmarks that primarily assess commonsense or logical reasoning. This focus is critical because factual accuracy ensures reliable knowledge retrieval rather than inference or reasoning based on background knowledge, and addressing it in Chinese involves unique linguistic and cultural challenges, making it essential for validating model performance in non-English contexts.", "question_token_count": 42, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 46, "choices": null}
{"context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n", "topic": "The differences between traditional model training and LLM pre-training paradigms that contribute to contamination challenges.", "question": "How do the fundamental differences between traditional model training and large language model pre-training paradigms contribute to increased challenges in preventing evaluation data contamination, and what implications do these differences have for the reliability of benchmark assessments?", "answer": "The massive, diverse, and often proprietary nature of LLM pre-training datasets, combined with post-training fine-tuning on related data, increases contamination risks unlike traditional models with clear data separation, complicating fair and reliable benchmark assessments.", "explanation": "Traditional models maintain clear separations between training and evaluation datasets, minimizing contamination risks. In contrast, LLMs are pre-trained on vast, diverse, often web-scraped datasets without transparent boundaries, and undergo fine-tuning on human-annotated or synthetic data resembling evaluation tasks. This scale, diversity, and opacity of training data, combined with proprietary restrictions, make it difficult to exclude evaluation data completely and verify model performance, thereby undermining the fairness and reliability of benchmarks.", "question_token_count": 41, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 7, "question_groundedness_score": 10, "avg_answer_token_count": 45, "choices": null}
{"context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n", "topic": "The role of accountability mechanisms in mitigating ethical risks associated with AI model evaluation and benchmarking.", "question": "How do accountability mechanisms function to mitigate ethical risks such as bias perpetuation, privacy violations, and misuse in AI model evaluation and benchmarking frameworks?", "answer": "By enforcing fairness, transparency, and privacy safeguards that prevent bias, protect data, and discourage misuse in benchmarking processes.", "explanation": "Accountability mechanisms ensure that benchmarking frameworks are designed and implemented with fairness, transparency, and privacy safeguards, thus preventing biased data usage, protecting sensitive information, and discouraging manipulation or selective reporting of model performance, which collectively mitigate ethical risks.", "question_token_count": 29, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 24, "choices": null}
{"context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n", "topic": "The advantages and challenges of rule-based and LLM-based generation methods for creating novel evaluation data points in dynamic benchmarks.", "question": "How do the inherent interpretability characteristics of rule-based and LLM-based generation methods impact the reliability and validation costs of dynamic benchmarks, and what mechanisms are necessary to mitigate challenges associated with each approach?", "answer": "Rule-based methods offer inherent interpretability that lowers validation costs and increases reliability, while LLM-based methods require explainability tools and human oversight to address their lack of transparency and ensure correctness.", "explanation": "Rule-based generation methods are inherently interpretable, making their transformations transparent and reducing the need for extensive manual validation, thus lowering validation costs and enhancing reliability. In contrast, LLM-based generation relies on the model's internal processes, which lack inherent transparency, making correctness harder to verify and increasing validation costs. To mitigate these challenges, LLM-based methods require additional mechanisms such as explainability tools and human-in-the-loop validation to ensure reliability and correctness in dynamic benchmarks.", "question_token_count": 39, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 37, "choices": null}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "Comparative analysis of static versus dynamic benchmarking approaches in terms of contamination risk, transparency, and adaptability.", "question": "How do static and dynamic benchmarking approaches fundamentally differ in their ability to mitigate data contamination risk, maintain transparency, and adapt to the evolving training data of Large Language Models?", "answer": "Static benchmarks have high transparency but high contamination risk and low adaptability, whereas dynamic benchmarks reduce contamination risk and improve adaptability but face challenges in transparency and standardization.", "explanation": "Static benchmarks rely on fixed datasets publicly available for evaluation, which makes them vulnerable to contamination because LLMs can inadvertently train on these datasets, inflating performance metrics; they offer transparency by being openly accessible but lack adaptability since their content does not change post-release. Dynamic benchmarks address contamination by continuously updating or regenerating datasets based on LLM training timestamps or reconstructing benchmarks, thereby reducing overlap with training data; this approach enhances adaptability by evolving alongside models and improves contamination mitigation but poses challenges in maintaining transparency and standardization due to constant changes.", "question_token_count": 34, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 31, "choices": null}
{"context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n", "topic": "The function and necessary properties of scoring functions used to measure alignment between generated outputs and ground truth.", "question": "In the context of dynamic benchmarking correctness evaluation, what key properties must a scoring function possess to reliably measure the alignment between generated outputs and ground truth, and why are these properties essential for ensuring trustworthy and interpretable benchmarking results?", "answer": "Sensitivity to discrepancies, interpretability, comparability, and robustness to noise are key properties that ensure the scoring function reliably measures alignment and supports trustworthy, interpretable benchmarking results.", "explanation": "The scoring function quantifies how well the generated outputs match the ground truth provided by an oracle; therefore, it must be sensitive enough to detect discrepancies, produce scores that are interpretable and comparable across different outputs, and be robust against noise or irrelevant variations to prevent misleading evaluations. These properties ensure the correctness metric accurately reflects the benchmark's fidelity to true data, enabling reliable and meaningful assessments of LLM performance.", "question_token_count": 45, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 8, "avg_answer_token_count": 35, "choices": null}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "The role of lexical meaning preservation in defining syntactic contamination and its significance in evaluation integrity.", "question": "How does the requirement of lexical meaning preservation in syntactic transformations critically define syntactic contamination, and why is this preservation essential to maintaining the validity of language model evaluation benchmarks?", "answer": "Because lexical meaning preservation guarantees semantic equivalence despite syntactic variations, syntactic contamination occurs only when test data's meaning is unchanged but surface form altered, compromising evaluation validity by removing true test independence.", "explanation": "Lexical meaning preservation ensures that despite syntactic changes like punctuation or synonym substitution, the semantic content remains identical; this equivalence means the test data is not truly independent of training data, thus constituting contamination that biases evaluation results and undermines benchmark integrity.", "question_token_count": 33, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 7, "question_groundedness_score": 10, "avg_answer_token_count": 37, "choices": null}
{"context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n", "topic": "The design and methodology behind CONSTAT and similar benchmark comparison techniques for identifying contamination in machine learning models.", "question": "How does the CONSTAT method leverage differences in model performance across benchmarks to detect data contamination, and what are the underlying methodological principles that distinguish it from traditional direct overlap detection techniques?", "answer": "CONSTAT detects contamination by analyzing discrepancies in model performance across benchmarks, using comparative evaluation to reveal potential memorization or prior exposure, thereby leveraging behavioral differences rather than relying only on direct data overlap detection methods.", "explanation": "CONSTAT detects contamination by comparing how a model performs on different benchmarks, identifying suspiciously higher performance on certain datasets that suggests prior exposure or memorization, rather than relying solely on direct data overlap such as n-gram or embedding similarity; this approach uses behavioral evidence of contamination through performance discrepancies, which provides a more robust and nuanced detection method beyond exact or similarity-based matching.", "question_token_count": 36, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 41, "choices": null}
{"context": "###### Safety\nSafety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content. Datasets such as RealToxicityPrompts\u00a0(Gehman et\u00a0al., 2020) and ToxiGen\u00a0(Hartvigsen et\u00a0al., 2022) assess resilience against producing harmful outputs.\n\nBy providing a controlled environment to measure these aspects, safety benchmarks play a critical role in guiding the development of models that are not only powerful outputs but also responsible and trustworthy for real-world applications.\n\n\n###### Language\nLanguage benchmarks assess the LLMs\u2019 proficiency in specific languages.\n\nGLUE\u00a0(Wang, 2018) and SuperGLUE\u00a0(Wang et\u00a0al., 2019) cover tasks from sentiment analysis to language inference, while CLUE\u00a0(Xu et\u00a0al., 2020) targets Chinese language.\n\nTypo-fixing\u00a0(Suzgun et\u00a0al., 2022) is also widely used.\n\n\n###### Reading Comprehension\nReading comprehension tasks test a model\u2019s ability to extract and infer information from text. Benchmarks like SQuAD\u00a0(Rajpurkar et\u00a0al., 2018), QuAC\u00a0(Choi et\u00a0al., 2018), and BoolQ\u00a0(Clark et\u00a0al., 2019) challenge models to understand passages and draw logical conclusions.\n", "topic": "The role and significance of safety benchmarks in evaluating the robustness and ethical alignment of large language models, with examples such as RealToxicityPrompts and ToxiGen.", "question": "How do safety benchmarks like RealToxicityPrompts and ToxiGen fundamentally contribute to ensuring the ethical alignment and robustness of large language models, and why is their role critical compared to other types of benchmarks in guiding the development of trustworthy AI systems?", "answer": "They provide controlled evaluations of models\u2019 resistance to generating harmful or toxic content, ensuring ethical alignment and robustness essential for trustworthy AI beyond mere task proficiency.", "explanation": "Safety benchmarks specifically target a model\u2019s ability to avoid producing harmful or toxic content, providing controlled settings to measure this resilience; this focus on ethical alignment and harm reduction is critical because it directly addresses real-world risks that other benchmarks, such as language proficiency or reading comprehension tests, do not, thereby ensuring models are not only powerful but also responsible and safe for deployment.", "question_token_count": 51, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 10, "avg_answer_token_count": 30, "choices": null}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "The transformation of formal graph structures into natural language descriptions for LLM evaluation and its implications for language understanding and reasoning.", "question": "How does the rule-based transformation of directed acyclic graphs into natural language descriptions for evaluation frameworks like DyVal influence the assessment of LLMs\u2019 reasoning capabilities, and what does this imply about the models\u2019 integration of formal graph reasoning within natural language understanding?", "answer": "It requires LLMs to reconstruct and reason over graph structures from natural language, demonstrating integrated formal reasoning within language understanding rather than isolated symbolic manipulation.", "explanation": "The transformation requires LLMs to interpret natural language that encodes complex graph structures, testing their ability to reconstruct and reason over the underlying graph semantics from linguistic input alone. This challenges models to integrate formal discrete reasoning with language comprehension, meaning their performance reflects true semantic parsing and logical inference capabilities rather than mere symbol manipulation, thus providing a deeper measure of their reasoning integrated within natural language understanding.", "question_token_count": 50, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 29, "choices": null}
{"context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n", "topic": "The importance of transparency in benchmarking processes to prevent misuse such as artificial inflation of model performance or biased selection of evaluation criteria.", "question": "How does transparency in benchmarking processes specifically help prevent the artificial inflation of model performance and the biased selection of evaluation criteria, and why is this crucial for maintaining fairness and accountability in AI evaluations?", "answer": "Transparency enables open documentation and scrutiny of benchmarking methods and criteria, preventing manipulation and selective bias, thus ensuring fairness and accountability in AI evaluations.", "explanation": "Transparency ensures that benchmarking methodologies, data sources, and evaluation criteria are openly documented and scrutinizable, making it harder to manipulate results or selectively choose metrics that favor certain models. This openness fosters accountability by enabling independent verification and discourages unethical practices that could distort model assessments, thereby preserving fairness and preventing harm to particular user groups or research areas.", "question_token_count": 38, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 28, "choices": null}
{"context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n", "topic": "Critical analysis of current dynamic benchmark methods and their limitations.", "question": "What are the primary limitations of current dynamic benchmarking methods for large language models, and how does the absence of standardized evaluation criteria impede the effectiveness of these benchmarks in mitigating data contamination?", "answer": "Lack of standardized evaluation criteria causes inconsistent design and validation of dynamic benchmarks, limiting their reliability and effectiveness in mitigating data contamination.", "explanation": "Current dynamic benchmarking methods face limitations such as inconsistent design principles, variability in implementation, and lack of robust validation frameworks, which undermine their reliability. Without standardized evaluation criteria, it becomes difficult to uniformly assess the quality and contamination-resilience of dynamic benchmarks, leading to challenges in comparing results across studies and ensuring that benchmarks effectively minimize contamination bias.", "question_token_count": 36, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 26, "choices": null}
{"context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n", "topic": "The concept and importance of stability of complexity for trustworthy dynamic benchmarking methodologies.", "question": "Why is measuring the stability of complexity, defined as the variance in complexity across dynamic benchmark trials, critical for interpreting performance drops in large language models, and what are the primary challenges in developing complexity metrics that support this stability assessment across diverse applications?", "answer": "Because stable complexity measurements ensure that performance drops reflect true model issues rather than fluctuating task difficulty, but current complexity metrics are domain-specific and lack generalizability, making stable complexity assessment across diverse benchmarks challenging.", "explanation": "Stability of complexity is crucial because performance drops can result either from increased task complexity or data contamination; without stable complexity measures, it is impossible to disentangle these causes. High variance in complexity across trials undermines trustworthiness of benchmarking results, as performance changes may reflect inconsistent difficulty rather than true model robustness. The main challenges include the domain-specific nature of existing complexity metrics, their limited generalizability, and the inherent difficulty in quantifying complexity consistently across varied tasks, which complicates establishing a reliable, universal complexity measurement function \u03a8(\u00b7).", "question_token_count": 49, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 40, "choices": null}
{"context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n", "topic": "The implications of commercial and data privacy concerns on contamination detection and management in LLM benchmarking.", "question": "How do commercial and data privacy concerns fundamentally complicate the detection and management of data contamination in LLM benchmarking, and what are the implications of these complications for the transparency and reliability of benchmarking results?", "answer": "They restrict data access and transparency, making contamination detection difficult and reducing benchmarking reliability.", "explanation": "Commercial and data privacy constraints limit access to training and test data details, preventing straightforward contamination detection methods like direct data comparison. This lack of transparency hampers reliable identification and management of contamination, which in turn undermines the trustworthiness of benchmark evaluations. Consequently, benchmarks become less reliable, especially static ones, prompting a shift toward dynamic benchmarks despite their own challenges. These complications highlight a critical trade-off between protecting sensitive or proprietary data and ensuring rigorous, transparent evaluation of LLMs.", "question_token_count": 38, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 17, "choices": null}
{"context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n", "topic": "Evaluate how different choices of diversity metrics (e.g., BLEU, N-gram based) might impact the assessment of dataset diversity and the interpretation of results.", "question": "How do different diversity metrics like BLEU scores versus generic N-gram based measures influence the evaluation of external and internal diversity in transformed datasets, and what are the implications of these differences on interpreting the effectiveness of data transformations?", "answer": "BLEU emphasizes lexical overlap and may underrepresent semantic diversity, while generic N-gram measures capture varying lexical variations, so metric choice shapes whether diversity reflects surface similarity or deeper novelty, impacting interpretation of transformation effectiveness.", "explanation": "BLEU scores, relying on n-gram overlap with reference data, primarily capture surface lexical similarity, potentially underestimating semantic or structural diversity, whereas generic N-gram measures can be adapted to capture varying granularities of lexical variation; thus, depending on the metric chosen, assessments of external and internal diversity may emphasize different aspects of dataset variation, affecting conclusions about how effectively transformations introduce novel or diverse data.", "question_token_count": 44, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 42, "choices": null}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "The distinction between benchmarks testing internal knowledge retrieval (e.g., NaturalQuestions, TriviaQA) and those assessing multi-domain knowledge (e.g., MMLU, BBH, AGI Eval) in LLMs.", "question": "How do benchmarks focused on internal knowledge retrieval, such as NaturalQuestions and TriviaQA, fundamentally differ in their evaluation objectives and knowledge representation demands from multi-domain knowledge assessments like MMLU, BBH, and AGI Eval in large language models?", "answer": "Internal knowledge retrieval benchmarks test factual recall from a model\u2019s stored knowledge, while multi-domain assessments evaluate integrative reasoning and application across diverse knowledge domains.", "explanation": "Internal knowledge retrieval benchmarks primarily test a model\u2019s ability to accurately recall and retrieve discrete real-world facts from its internalized knowledge base, emphasizing factual accuracy and memory recall. In contrast, multi-domain knowledge assessments evaluate the model\u2019s capability to integrate, reason across, and apply knowledge from diverse subject areas, reflecting a broader, more complex understanding that goes beyond simple fact retrieval to include domain adaptation and synthesis.", "question_token_count": 50, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 30, "choices": null}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "Strategies and considerations for designing benchmarks that effectively test LLMs on novel and unseen data without contamination.", "question": "Considering the challenges of syntactic contamination and the difficulty in distinguishing between memorization and reasoning in LLMs, what comprehensive strategies should be employed in benchmark design to ensure evaluation on truly novel and unseen data, thereby preserving the validity and reliability of model assessments?", "answer": "Employ multi-layered contamination detection including semantic and syntactic filtering, use strictly out-of-distribution and adversarially vetted data, maintain transparent data provenance, continuously update benchmarks to exclude leaked or memorized content, and design evaluation tasks that emphasize reasoning over pattern matching.", "explanation": "This question probes an expert\u2019s understanding of contamination types and the intricate balance between detecting reused data and assessing true reasoning. It demands insight into benchmark construction methods that prevent contamination, such as rigorous data provenance checks, use of out-of-distribution samples, adversarial filtering, and continuous updating to exclude leaked training data. The answer reflects a synthesis of contamination theory and practical evaluation design to uphold benchmark integrity.", "question_token_count": 50, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 9, "question_groundedness_score": 9, "avg_answer_token_count": 52, "choices": null}
{"context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "Assess how legal and privacy constraints impact the design and implementation of benchmarking methods for large language models.", "question": "How do legal and privacy constraints on training data access influence the development of benchmarking methodologies for large language models, and how does dynamic benchmarking address these challenges?", "answer": "They restrict training data access, hindering contamination detection and static benchmarking, which dynamic benchmarking overcomes by evolving evaluation datasets through transformation functions to ensure fairness and transparency.", "explanation": "Legal and privacy restrictions limit access to training datasets, making it difficult to detect overlap or contamination between training and evaluation data; this undermines traditional static benchmarks. Dynamic benchmarking addresses this by continuously transforming the evaluation dataset over time, avoiding contamination and enabling transparent, adaptive evaluation without requiring full training data access.", "question_token_count": 31, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 33, "choices": null}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "The critical features and evaluation criteria of instruction-following benchmarks like IFEval, InfoBench, and C-Eval, including their focus on step-by-step guidance and multilingual instruction comprehension.", "question": "What are the critical evaluation criteria that distinguish instruction-following benchmarks such as IFEval, InfoBench, and C-Eval, particularly regarding their emphasis on step-by-step guidance and the challenges posed by multilingual instruction comprehension?", "answer": "Ability to accurately comprehend and execute detailed, step-by-step instructions with procedural coherence, alongside robust multilingual understanding exemplified by C-Eval\u2019s focus on Chinese instructions.", "explanation": "Instruction-following benchmarks are designed to assess a model's ability to comprehend and execute complex, detailed directives, requiring evaluation criteria that prioritize accurate understanding and sequential execution of instructions. IFEval and InfoBench simulate real-world scenarios necessitating clear, stepwise guidance, which demands models to process instructions incrementally and maintain procedural coherence. C-Eval introduces an additional layer by focusing on Chinese instructions, highlighting the importance of multilingual comprehension capabilities and the challenges of language-specific syntax, semantics, and cultural context. These benchmarks thus critically evaluate not only the fidelity of instruction execution but also the model's adaptability to linguistic diversity and the ability to maintain clarity and precision in following multi-step instructions.", "question_token_count": 45, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 7, "question_groundedness_score": 10, "avg_answer_token_count": 34, "choices": null}
{"context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n", "topic": "The relationship between benchmark task categories (e.g., coding, reasoning, safety) and the underlying cognitive or computational skills they aim to assess in language models.", "question": "How do the diverse task categories in static benchmarks\u2014such as coding, reasoning, and safety\u2014correspond to distinct cognitive and computational skills in language models, and what implications does this mapping have for interpreting model performance across these categories?", "answer": "Different benchmark categories assess distinct cognitive and computational skills\u2014coding tests program synthesis, reasoning evaluates logical and commonsense inference, and safety measures ethical content understanding\u2014so model performance must be interpreted as reflecting varied underlying abilities specific to each category.", "explanation": "Each benchmark category targets specific cognitive or computational faculties: coding tasks evaluate program synthesis and algorithmic reasoning; reasoning tasks assess logical inference and commonsense understanding; safety tasks require social cognition and ethical judgment to detect harmful content. Recognizing this mapping clarifies that model performance differences across categories reflect varying underlying abilities rather than uniform competence, guiding more nuanced interpretation of evaluation outcomes.", "question_token_count": 46, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 7, "question_groundedness_score": 4, "avg_answer_token_count": 47, "choices": null}
{"context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n", "topic": "The role of transparency and label protection in mitigating contamination risks and improving evaluation reliability.", "question": "How do transparency and label protection mechanisms contribute to mitigating data contamination risks in LLM benchmarking, and why are they essential for ensuring the reliability of evaluation outcomes?", "answer": "They prevent test data leakage and enable contamination detection, thereby preserving evaluation faithfulness and reliability.", "explanation": "Transparency allows clear visibility into benchmark design and data handling, enabling contamination detection and prevention, while label protection prevents test labels from leaking into training data or model exposure. Together, they reduce contamination risk, preserve sample faithfulness and answerability, and uphold evaluation reliability by ensuring that model performance reflects genuine generalization rather than memorization.", "question_token_count": 31, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 19, "choices": null}
{"context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n", "topic": "The impact of robust evaluation data protection strategies on the integrity and fairness of machine learning model assessment and benchmarking.", "question": "How do encryption-based data protection and label withholding mechanisms jointly influence the integrity and fairness of machine learning model benchmarking, and what critical vulnerabilities or trade-offs must be considered to ensure these protections do not inadvertently compromise the assessment's credibility?", "answer": "They collectively enhance evaluation integrity and fairness by preventing data leakage and answer memorization, but their effectiveness depends on secure key management and controlled access, and they involve trade-offs like computational overhead and potential transparency loss that must be managed to maintain credible assessments.", "explanation": "Encryption-based methods protect test data confidentiality by preventing unauthorized access and reuse, thus reducing data contamination risks, while label withholding prevents models from memorizing true answers, maintaining evaluation integrity. However, both approaches rely on secure key management and controlled access, presenting vulnerabilities if compromised. Additionally, encryption introduces computational overhead, and over-restriction may limit transparency and reproducibility. Balancing these factors is essential to avoid undermining fairness or trust in benchmarking results.", "question_token_count": 46, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 50, "choices": null}
{"context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n", "topic": "Potential limitations and trade-offs involved in using LLMs for benchmark data generation versus traditional rule-based methods.", "question": "How do the inherent interpretability differences between rule-based and LLM-based dynamic benchmark data generation methods affect the reliability and cost of benchmark validation, and what mechanisms can mitigate the limitations of LLM-based approaches?", "answer": "Rule-based methods offer transparent, cost-effective validation, while LLM-based methods require explainability tools or human oversight to ensure reliability, increasing validation costs.", "explanation": "Rule-based transformations are inherently interpretable, enabling straightforward manual validation and thus ensuring reliability at lower costs. In contrast, LLM-based transformations lack transparency, requiring additional explainability tools or human-in-the-loop validation to verify correctness, which increases validation complexity and cost. These mitigating mechanisms help compensate for the opacity of LLMs, balancing the trade-off between scalability and reliability.", "question_token_count": 40, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 30, "choices": null}
{"context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n", "topic": "Consider potential challenges or limitations in applying these diversity measures to real-world datasets and how they might affect data augmentation strategies.", "question": "What are the main challenges in applying external and internal diversity measures, such as N-gram or BLEU-based metrics, to real-world datasets for guiding data augmentation strategies, and how might these challenges affect the effectiveness of the augmented data?", "answer": "Metrics like N-gram or BLEU may inadequately capture semantic diversity, be sensitive to dataset size and noise, and computationally expensive, leading to misleading diversity estimates that impair effective data augmentation.", "explanation": "External and internal diversity measures rely on metrics like N-gram or BLEU scores that may not fully capture semantic or contextual diversity, can be sensitive to noise or dataset size disparities, and may incur high computational costs; these limitations can lead to misleading diversity assessments, causing data augmentation strategies to either overestimate diversity and produce redundant data or underestimate it and miss valuable variations, ultimately affecting model generalization and performance.", "question_token_count": 47, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 38, "choices": null}
{"context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n", "topic": "Challenges and limitations of existing complexity metrics in providing domain-general measures for benchmark datasets.", "question": "What are the primary challenges that prevent existing complexity metrics from serving as domain-general measures for benchmark datasets, and how do these challenges impact the interpretation of performance drops in dynamic benchmarking of language models?", "answer": "Domain-specific design of complexity metrics limits generalizability, causing difficulty in reliably attributing performance drops to contamination versus complexity increases.", "explanation": "Existing complexity metrics are typically tailored to specific domains, making them poorly generalizable across diverse applications. This domain specificity arises because complexity often depends on task-specific features and representations, such as graph structures in reasoning tasks. Consequently, complexity measurements vary significantly between domains, limiting their applicability as universal indicators. This leads to difficulty in distinguishing whether performance drops in dynamic benchmarks arise from actual data contamination or from increased task complexity, complicating evaluation stability and reliability.", "question_token_count": 39, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 25, "choices": null}
{"context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n", "topic": "The integration and comparative effectiveness of multiple contamination detection techniques for comprehensive post-hoc analysis in machine learning datasets.", "question": "How can the integration of lexical overlap methods, embedding-based similarity techniques, and behavioral model analyses enhance the detection of training-test contamination in machine learning datasets, and what are the primary challenges in combining these approaches for comprehensive post-hoc evaluation?", "answer": "By leveraging their complementary strengths\u2014lexical overlap for exact matches, embedding similarity for semantic duplicates, and behavioral analysis for memorization patterns\u2014integrated approaches improve contamination detection accuracy; challenges include managing differing detection sensitivities, aligning outputs across methods, and creating cohesive evaluation frameworks.", "explanation": "Lexical overlap methods identify exact data duplicates but suffer from false negatives; embedding-based similarity captures semantic duplicates beyond exact matches; behavioral analyses reveal model memorization patterns undetectable by surface-level checks. Integrating these approaches leverages their complementary strengths to improve contamination detection accuracy. Challenges include harmonizing disparate data representations, balancing false positives and negatives across methods, and designing unified frameworks to interpret combined signals effectively.", "question_token_count": 47, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 8, "avg_answer_token_count": 54, "choices": null}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "The ongoing debate about whether syntactic transformations of test data constitute true contamination and the implications of considering them as such.", "question": "How does considering syntactic transformations of test data as true contamination influence the assessment of a large language model's reasoning capabilities and the validity of benchmarks, and what are the potential trade-offs of this approach in NLP evaluation?", "answer": "It prevents inflated performance from memorized syntactic patterns, enhancing benchmark validity but may underestimate genuine reasoning that relies on syntax, trading off sensitivity for conservatism in evaluation.", "explanation": "Treating syntactic transformations as contamination ensures that benchmarks do not overestimate a model's reasoning ability by preventing it from leveraging memorized syntactic patterns; however, it may also obscure genuine reasoning skills that exploit syntactic cues, leading to a more conservative but arguably more reliable assessment of true generalization.", "question_token_count": 42, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 34, "choices": null}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "The role and significance of multi-step math problem-solving benchmarks such as GSM8K and MATH in evaluating large language models\u2019 reasoning abilities.", "question": "How do multi-step math problem-solving benchmarks like GSM8K and MATH uniquely contribute to assessing the reasoning capabilities of large language models, and why are such benchmarks considered more indicative of advanced cognitive processing than single-step or knowledge-retrieval tasks?", "answer": "They test a model\u2019s ability to perform sequential, multi-step logical reasoning and problem decomposition, providing a more rigorous measure of advanced cognitive processing than single-step or fact-retrieval tasks.", "explanation": "Multi-step math benchmarks require models to perform sequential logical operations and maintain intermediate reasoning steps, thereby testing the model\u2019s ability to decompose complex problems and synthesize solutions over multiple stages. This differs fundamentally from single-step or knowledge-retrieval tasks, which primarily assess memory or direct fact recall without demanding multi-layered reasoning. Hence, these benchmarks provide a more rigorous and revealing evaluation of a model\u2019s internal reasoning architecture and problem-solving capabilities.", "question_token_count": 49, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 38, "choices": null}
{"context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n", "topic": "The challenges and considerations in designing transformation processes that optimize scalability in dynamic benchmarking.", "question": "In the context of dynamic benchmarking, what are the primary challenges in designing transformation processes that maximize scalability, and how can these processes be optimized to balance the trade-off between generating large-scale datasets and minimizing diverse costs such as monetary expense, time, and manual effort?", "answer": "The primary challenges are balancing dataset size expansion with cost constraints, and scalability is optimized by designing efficient, automated transformations that maximize data generation per unit of diverse costs.", "explanation": "The main challenges lie in creating transformations that significantly increase dataset size without incurring prohibitive costs; optimizing scalability requires designing efficient, automated, and low-cost transformations that produce large, diverse datasets while carefully managing resource expenditure across monetary, temporal, and manual effort dimensions.", "question_token_count": 53, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 33, "choices": null}
{"context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n", "topic": "The challenges and considerations in designing scoring functions that accurately measure the quality of language model outputs against expected results.", "question": "What are the primary challenges and key considerations in designing scoring functions for static benchmarks that accurately and fairly evaluate language model outputs across diverse tasks and output types?", "answer": "Handling multiple valid outputs, ensuring semantic rather than surface-level matching, adapting to different task formats, and balancing strictness with tolerance to capture true output quality.", "explanation": "Scoring functions must handle variability in valid outputs, ensuring semantic correctness beyond exact matches, adapt to various task types (e.g., coding, reasoning, language), and balance sensitivity to errors with tolerance for acceptable output diversity; these considerations are essential to produce reliable and meaningful evaluation metrics.", "question_token_count": 31, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 4, "avg_answer_token_count": 32, "choices": null}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "The evolving landscape of LLM evaluation benchmarks and the implications for designing comprehensive assessment frameworks that balance reasoning, knowledge, and context handling.", "question": "How can evaluation frameworks for large language models be designed to effectively integrate and balance the assessment of multi-step reasoning, internalized knowledge retrieval, and long-context handling, given the evolving specialization of benchmarks such as GSM8K, MMLU-Pro, and ControlBench?", "answer": "By creating composite frameworks that combine specialized benchmarks covering reasoning, knowledge, and context, with calibrated metrics that reflect their interplay and relative importance, ensuring balanced, comprehensive, and context-aware assessment of LLM capabilities.", "explanation": "Effective LLM evaluation frameworks must synthesize distinct benchmark focuses\u2014multi-step reasoning from math datasets like GSM8K, broad factual knowledge from refined knowledge benchmarks like MMLU-Pro, and complex instruction or long-context understanding from benchmarks like ControlBench. Integrating these assessments requires addressing their complementary strengths and limitations, ensuring that no single skill dominates the evaluation, and that the framework captures the interplay between reasoning, knowledge, and context complexity to provide a holistic and realistic measure of model capability.", "question_token_count": 54, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 8, "avg_answer_token_count": 41, "choices": null}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "Investigate the assumptions underlying canary string methods and how these assumptions affect their effectiveness and applicability.", "question": "What are the critical assumptions underlying the use of canary strings for detecting data contamination in LLM benchmarks, and how do these assumptions influence the reliability and scope of this mitigation method?", "answer": "Canary strings assume developer awareness and honest response to detection, which limits their reliability and applicability if these assumptions do not hold.", "explanation": "Canary strings rely fundamentally on the assumptions that model developers are aware of these unique tokens and will act upon detecting them, and that the tokens remain unique and identifiable within the dataset. These assumptions affect reliability because if developers intentionally ignore or circumvent canary strings\u2014such as by leaking benchmark data to boost scores\u2014the method fails to detect contamination. Thus, the applicability of canary strings is limited in adversarial or dishonest settings and depends heavily on the integrity and awareness of model trainers.", "question_token_count": 36, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 26, "choices": null}
{"context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n", "topic": "The impact of different template-based generation techniques on the diversity, novelty, and validity of evaluation benchmarks for LLMs.", "question": "How do the differing approaches in template-based generation methods\u2014such as randomized placeholder filling, adherence to domain-specific rules, and modification of multiple-choice options\u2014impact the balance between diversity, novelty, and validity in evaluation benchmarks for large language models?", "answer": "By balancing randomized variation for diversity and novelty with rule adherence for validity, these methods trade off between generating numerous unique test instances and maintaining problem correctness to create effective LLM evaluation benchmarks.", "explanation": "Randomized placeholder filling (e.g., GSM-Symbolic) enhances diversity and novelty by generating numerous unique instances but may risk validity if not carefully constrained; adherence to domain-specific rules (e.g., Mathador-LM) ensures validity by maintaining problem integrity while still allowing novelty through input variation; modification of multiple-choice options (e.g., MMLU-CF) increases novelty and tests robustness but may affect validity if incorrect options are not plausible distractors. Each approach strategically balances these factors to produce reliable and varied benchmarks for LLM evaluation.", "question_token_count": 49, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 36, "choices": null}
{"context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n", "topic": "The principles and advantages of employing multi-agent systems in dynamic benchmark creation for large language model evaluation.", "question": "How do the principles of specialization, coordination, and human-in-the-loop feedback in multi-agent systems collectively enhance the scalability, diversity, and quality of dynamic benchmarks for large language model evaluation compared to traditional static benchmarks?", "answer": "By enabling specialized agents to collaboratively plan, generate, verify, and evaluate benchmarks with iterative human feedback, multi-agent systems produce scalable, diverse, and high-quality dynamic benchmarks surpassing static ones.", "explanation": "Specialization allows distinct LLM agents to handle discrete tasks (planning, generation, verification, evaluation) efficiently; coordination ensures these agents work synergistically to produce coherent, evolving benchmarks; human-in-the-loop feedback provides quality control and iterative refinement. Together, these principles enable the creation of scalable, diverse, and high-quality dynamic benchmarks that adapt beyond the limitations of static benchmarks.", "question_token_count": 43, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 40, "choices": null}
{"context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n", "topic": "The trade-offs between human effort and automated generation in producing up-to-date, contamination-resistant test benchmarks.", "question": "How do rule-based generation methods address the challenges of human effort and data contamination in creating up-to-date benchmarks, and what inherent trade-offs might they introduce compared to manual collection processes?", "answer": "They automate test generation to reduce human effort and contamination risk but may limit problem diversity and real-world complexity compared to manual collection.", "explanation": "Rule-based generation reduces human labor by automating test case synthesis through predefined templates and rules, ensuring extremely low collision probability and minimizing reuse of prior problems, thus limiting contamination. However, this automation may constrain creativity, diversity, or real-world representativeness compared to manual collection, which, although labor-intensive and contamination-prone, can capture more nuanced or varied problem types.", "question_token_count": 37, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 9, "question_groundedness_score": 10, "avg_answer_token_count": 26, "choices": null}
{"context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n", "topic": "Interpretation and implications of variance in complexity measurements across trials as a measure of stability in dynamic benchmarks.", "question": "How does the variance in complexity measurements across different trials serve as an indicator of stability in dynamic benchmarking, and what are the implications of high variance for interpreting performance drops in large language models?", "answer": "Variance quantifies stability by measuring consistency of complexity across trials; high variance signals instability, making it unclear if performance drops stem from increased complexity or data contamination.", "explanation": "Variance in complexity measurements reflects how consistently the complexity of transformed datasets is maintained across trials; low variance means the benchmarking method reliably preserves complexity, making performance drops more attributable to factors like data contamination, while high variance indicates unstable complexity, complicating interpretation by introducing uncertainty about whether performance changes are due to complexity shifts or other causes.", "question_token_count": 38, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 32, "choices": null}
{"context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n", "topic": "Discuss potential methodological strategies to reduce collision and enhance the diversity of test cases in dynamic benchmarks.", "question": "What methodological approaches can be employed in dynamic benchmarking to minimize collision and thereby maximize the generation of novel and diverse test cases, ensuring reliable evaluation of large language models despite potential training data contamination?", "answer": "Employing diverse, orthogonal, stochastic, and modular transformation techniques combined with continuous collision metric monitoring to generate highly distinct benchmark variations and reduce overlap.", "explanation": "Minimizing collision involves designing transformations that produce highly distinct variations of the benchmark dataset, reducing overlap between transformed sets. Approaches include applying diverse and orthogonal transformation techniques (e.g., paraphrasing, adversarial perturbations, domain shifts), using stochastic or randomized transformations to increase uniqueness, incorporating modular transformation pipelines that can be combined in novel ways, and monitoring collision metrics continuously to adapt transformation strategies. These methods collectively enhance benchmark diversity and robustness, preserving the test\u2019s ability to evaluate LLMs effectively even if prior transformed versions appear in training data.", "question_token_count": 38, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 29, "choices": null}
{"context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n", "topic": "The necessity of accounting for task complexity changes when interpreting LLM performance drops in dynamic benchmarking settings.", "question": "Why is it critical to account for changes in task complexity when interpreting performance drops of large language models on dynamically transformed benchmark datasets, and how does the variance of complexity measurements across trials affect the stability and reliability of such benchmarks?", "answer": "Because increased task complexity can cause expected performance drops independent of data contamination, and high variance in complexity measurements across trials indicates instability that makes performance interpretation unreliable.", "explanation": "Performance drops may stem from increased task complexity rather than data contamination; thus, ignoring complexity changes can lead to misattributing causes of degradation. Measuring complexity variance across trials indicates stability\u2014high variance implies unstable benchmarks where performance changes cannot be reliably interpreted, undermining the validity of conclusions drawn from dynamic benchmarking.", "question_token_count": 45, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 31, "choices": null}
{"context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n", "topic": "The relationship between correctness scores and the overall reliability of dynamic benchmarking methods in assessing LLM performance.", "question": "How does the correctness score of a dynamic benchmarking method fundamentally influence the overall reliability of the benchmark in evaluating large language model performance, and what are the potential consequences of neglecting this criterion?", "answer": "Correctness scores ensure benchmark reliability by validating output accuracy; neglecting correctness risks misleading evaluations and false reliability.", "explanation": "The correctness score measures how closely the benchmark\u2019s outputs align with ground truth values, ensuring that the benchmark generates valid, trustworthy data. A high correctness score guarantees that the benchmark\u2019s evaluations of LLMs are accurate and not misleading. Neglecting correctness can result in benchmarks that produce flawed or erroneous datasets, leading to false confidence in LLM performance and unreliable conclusions about their capabilities.", "question_token_count": 37, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 21, "choices": null}
{"context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n", "topic": "The impact of scalability on the overall effectiveness and feasibility of dynamic benchmarking methods in real-world scenarios.", "question": "How does the scalability metric, defined as the ratio of transformed dataset size to original dataset size normalized by transformation cost, influence both the statistical effectiveness and practical feasibility of dynamic benchmarking methods in real-world applications?", "answer": "It determines the balance between generating sufficiently large datasets for statistical reliability and minimizing resource costs to ensure practical feasibility, making scalability essential for effective and usable dynamic benchmarking in real-world scenarios.", "explanation": "The scalability metric quantifies how much benchmark data can be generated per unit cost, directly impacting statistical effectiveness by enabling larger datasets that reduce sampling errors, while also affecting practical feasibility by reflecting the resource expenditure required; thus, a higher scalability means more reliable benchmarks with manageable costs, which is crucial for real-world adoption where resources are limited.", "question_token_count": 41, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 36, "choices": null}
{"context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n", "topic": "The interplay between technical feasibility and ethical imperatives in creating responsible AI evaluation protocols.", "question": "How can the inherent trade-offs between the technical feasibility of dynamic benchmarking and the ethical imperatives of privacy and fairness be reconciled to design responsible and accountable evaluation protocols for large language models?", "answer": "By implementing privacy-preserving data collection, transparent and accountable benchmarking processes, and fairness-focused design that balances adaptability with ethical safeguards.", "explanation": "Dynamic benchmarking allows continual adaptation and relevance but requires ongoing data collection, which raises privacy and security risks; conversely, static benchmarks avoid these risks but risk perpetuating bias and becoming outdated. Reconciling these trade-offs requires designing frameworks that implement privacy-preserving data practices, ensure transparency to prevent misuse, and incorporate fairness and accountability measures to avoid disadvantaging user groups, thus balancing technical adaptability with ethical responsibility.", "question_token_count": 38, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 25, "choices": null}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "The role of syntactic information in NLP applications and why syntactic contamination may be particularly problematic in certain evaluation contexts.", "question": "Why does syntactic contamination pose a uniquely significant challenge in evaluating language models on NLP tasks that rely primarily on syntactic information, and how can such contamination obscure the distinction between a model's memorization and its true reasoning capabilities?", "answer": "Because syntactic contamination preserves learned syntactic patterns, it can cause models to perform well by memorization rather than reasoning, making it difficult to distinguish true model understanding from recall in syntax-dependent NLP tasks.", "explanation": "Syntactic contamination involves rephrased test data that preserves syntactic patterns from training data, which is particularly problematic for NLP tasks relying on syntactic cues because the model may appear to perform well by recalling learned syntactic structures rather than demonstrating genuine reasoning. This obscures the ability to differentiate whether high performance results from memorization of training patterns or from true generalization and inference, thereby undermining benchmark validity and misleading conclusions about a model\u2019s capabilities.", "question_token_count": 43, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 39, "choices": null}
{"context": "###### Safety\nSafety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content. Datasets such as RealToxicityPrompts\u00a0(Gehman et\u00a0al., 2020) and ToxiGen\u00a0(Hartvigsen et\u00a0al., 2022) assess resilience against producing harmful outputs.\n\nBy providing a controlled environment to measure these aspects, safety benchmarks play a critical role in guiding the development of models that are not only powerful outputs but also responsible and trustworthy for real-world applications.\n\n\n###### Language\nLanguage benchmarks assess the LLMs\u2019 proficiency in specific languages.\n\nGLUE\u00a0(Wang, 2018) and SuperGLUE\u00a0(Wang et\u00a0al., 2019) cover tasks from sentiment analysis to language inference, while CLUE\u00a0(Xu et\u00a0al., 2020) targets Chinese language.\n\nTypo-fixing\u00a0(Suzgun et\u00a0al., 2022) is also widely used.\n\n\n###### Reading Comprehension\nReading comprehension tasks test a model\u2019s ability to extract and infer information from text. Benchmarks like SQuAD\u00a0(Rajpurkar et\u00a0al., 2018), QuAC\u00a0(Choi et\u00a0al., 2018), and BoolQ\u00a0(Clark et\u00a0al., 2019) challenge models to understand passages and draw logical conclusions.\n", "topic": "How safety benchmarks guide the development of responsible and trustworthy LLMs fit for real-world applications, including challenges in measuring harmful output resilience.", "question": "How do safety benchmarks like RealToxicityPrompts and ToxiGen influence the development of large language models to ensure responsible and trustworthy real-world deployment, and what are the fundamental challenges in reliably measuring a model's resilience to generating harmful outputs?", "answer": "By creating controlled environments to evaluate and reduce harmful content generation, safety benchmarks guide ethical model improvements, but measuring resilience is fundamentally challenging due to the context-dependent and subjective nature of toxicity.", "explanation": "Safety benchmarks provide controlled, standardized evaluations that identify and quantify an LLM's propensity to produce harmful or toxic content, guiding researchers to improve models towards ethical alignment and robustness. However, reliably measuring harmful output resilience is challenging because toxicity is often context-dependent, subjective, and subtle, making it difficult to design benchmarks that capture all real-world scenarios and nuances, thus complicating the development of universally safe models.", "question_token_count": 49, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 37, "choices": null}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "The definition and examples of data contamination in LLM benchmarking, with a focus on syntactic contamination and its characteristics.", "question": "How does syntactic contamination in LLM benchmarking challenge the distinction between memorization and reasoning abilities in language models, and why is it considered contamination despite ongoing debates about its classification?", "answer": "Because syntactic contamination rephrases training data with added prefixes, it blurs the line between memorization and reasoning, and is deemed contamination since it can exploit memorized syntactic cues rather than test true generalization.", "explanation": "Syntactic contamination involves rephrasing training data with added prefixes to create test inputs that are superficially different but semantically equivalent, making it difficult to discern whether a model\u2019s correct response is due to memorization of the original data or genuine reasoning. It is considered contamination because some NLP tasks rely primarily on syntactic information, so such rephrasings can unfairly advantage the model by leveraging memorized syntactic patterns rather than demonstrating true generalization.", "question_token_count": 34, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 42, "choices": null}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The criteria proposed for evaluating the effectiveness of dynamic benchmarks and why existing dynamic benchmarks fall short of these criteria.", "question": "What are the key criteria proposed for evaluating the effectiveness of dynamic benchmarks in mitigating data contamination in large language model evaluation, and why do existing dynamic benchmarking approaches fail to fully satisfy these criteria?", "answer": "Effective contamination mitigation, continuous adaptability, transparency, and comprehensive coverage; existing methods are imperfect because they do not fully achieve all these criteria simultaneously.", "explanation": "The criteria for effective dynamic benchmarks focus on robust contamination mitigation, continuous adaptability to new training data, transparency in benchmark updates, and comprehensive coverage to ensure fair evaluation. Existing dynamic benchmarks fall short because they do not fully address all these aspects simultaneously; for example, they may update datasets without ensuring complete removal of contaminated data, lack standardized protocols for transparency, or fail to comprehensively cover evolving model capabilities, leading to imperfect contamination prevention and evaluation reliability.", "question_token_count": 38, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 29, "choices": null}
{"context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n", "topic": "The assumptions and limitations inherent in post-hoc contamination detection methods used in static benchmarks.", "question": "What are the fundamental assumptions underlying post-hoc contamination detection in static LLM benchmarks, and how do these assumptions limit the reliability and transparency of contamination assessment in evaluating model performance?", "answer": "Post-hoc contamination detection assumes indirect inference of contamination without direct training data access, limiting reliability and transparency due to unverifiable assumptions and potential inaccuracies in contamination assessment.", "explanation": "Post-hoc contamination detection assumes that contamination can be inferred after model training by indirect signals without direct access to training data, which leads to high uncertainty and lack of transparency. These assumptions limit reliability because they depend on unverifiable hypotheses and can produce false positives or negatives, undermining confidence in evaluation results and masking true model capabilities.", "question_token_count": 34, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 7, "avg_answer_token_count": 33, "choices": null}
{"context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n", "topic": "Reflect on the broader implications of benchmark data contamination for training large language models and interpreting evaluation results.", "question": "How does the contamination of benchmark data through its inclusion in training sets affect the validity of large language model evaluations, and what role do collision metrics play in mitigating these effects to preserve the integrity of dynamic benchmarking?", "answer": "Contamination inflates evaluation results by exposing models to test data during training, while collision metrics quantify and help control overlap to preserve benchmark novelty and evaluation integrity.", "explanation": "Contamination causes evaluation results to overestimate model capabilities by exposing models to test data during training, undermining benchmark validity; collision metrics quantify overlap between transformed datasets, enabling assessment and reduction of data redundancy and contamination, thereby helping maintain benchmark novelty and trustworthy evaluation outcomes.", "question_token_count": 42, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 7, "question_groundedness_score": 10, "avg_answer_token_count": 32, "choices": null}
{"context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n", "topic": "Challenges related to reliability and reproducibility faced by dynamic benchmarking approaches in LLM contamination detection.", "question": "What are the core factors that undermine the reliability and reproducibility of dynamic benchmarking approaches in detecting data contamination in large language models, and how do these challenges contrast with those faced by static methods?", "answer": "Variability in evolving data, absence of standardized protocols, and difficulty replicating conditions undermine dynamic benchmarking reliability and reproducibility, contrasting with static methods that are consistent but increasingly vulnerable to contamination.", "explanation": "Dynamic benchmarking approaches are undermined by variability in evolving datasets, lack of standardized evaluation protocols, and difficulties in replicating experimental conditions over time, which lead to inconsistent contamination detection results. These challenges contrast with static methods that, while more consistent due to fixed evaluation data, become increasingly vulnerable to contamination as training datasets grow and overlap with static benchmarks, reducing their validity.", "question_token_count": 38, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 37, "choices": null}
{"context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n", "topic": "Strategies for managing encryption keys securely to minimize the risk of data leakage in evaluation environments.", "question": "Considering the vulnerability of encryption methods to key compromise in securing evaluation data, what key management strategies can be employed to minimize the risk of data leakage in private benchmarking environments while balancing computational overhead and operational complexity?", "answer": "Employing secure hardware key storage, strict access controls, regular key rotation, and distributed key management schemes to prevent key compromise while balancing overhead.", "explanation": "The answer is correct because minimizing data leakage risk in encrypted evaluation environments hinges on robust key management practices such as hardware security modules (HSMs) for secure key storage, strict access controls, key rotation policies to limit exposure time, and use of threshold or multi-party key management schemes to avoid single points of failure, all while considering the computational and operational overhead to maintain efficiency and security.", "question_token_count": 41, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 7, "avg_answer_token_count": 29, "choices": null}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "Analyze the problem of data contamination in large language model benchmarking and its implications for the validity of evaluation results.", "question": "How does data contamination from training on publicly available benchmark datasets affect the validity of large language model evaluation results, and what are the inherent limitations of using canary strings to mitigate this issue?", "answer": "Data contamination causes evaluation results to overstate model performance by reflecting memorization rather than generalization, and canary strings mitigate this only if developers are honest and responsive, failing if benchmarking data is intentionally leaked into training.", "explanation": "Data contamination means models have effectively trained on test data, causing evaluation results to reflect memorization rather than true generalization, thus invalidating the benchmark as an independent measure. Canary strings can detect such memorization by embedding unique tokens, but their effectiveness relies on developers' cooperation and cannot prevent intentional leakage, limiting their ability to fully safeguard evaluation integrity.", "question_token_count": 38, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 43, "choices": null}
{"context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n", "topic": "The implications of using static benchmarks for measuring specific competencies like factual knowledge retrieval versus commonsense reasoning.", "question": "How do the inherent characteristics of static benchmarks affect their effectiveness in accurately evaluating competencies like factual knowledge retrieval compared to more nuanced tasks such as commonsense reasoning?", "answer": "Static benchmarks are well-suited for evaluating factual knowledge retrieval due to clear expected outputs but are limited in assessing commonsense reasoning because their fixed nature cannot accommodate the ambiguity and context-sensitivity inherent in such tasks.", "explanation": "Static benchmarks rely on fixed input-output pairs and scoring functions that compare model outputs to expected answers, which suits tasks with clear, objective answers like factual knowledge retrieval. However, for commonsense reasoning, which often involves ambiguity and context-dependent interpretations, static benchmarks may inadequately capture true model ability, limiting their effectiveness in evaluating such nuanced competencies.", "question_token_count": 31, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 6, "avg_answer_token_count": 42, "choices": null}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "The specific objectives and challenges addressed by coding benchmarks such as HumanEval, MBPP, and SWE-Bench in evaluating code generation and debugging capabilities of language models.", "question": "How do coding benchmarks like HumanEval, MBPP, and SWE-Bench differ in their specific objectives and challenges when evaluating language models' code generation and debugging capabilities, and what implications do these differences have for assessing a model\u2019s proficiency in handling progressively complex coding tasks?", "answer": "HumanEval and MBPP test basic code synthesis and debugging, while SWE-Bench addresses more advanced, complex coding challenges requiring deeper reasoning and problem-solving.", "explanation": "HumanEval and MBPP focus on fundamental tasks of code synthesis and debugging, testing a model\u2019s ability to generate correct code snippets and fix errors in relatively straightforward contexts. In contrast, SWE-Bench introduces more advanced challenges that likely require deeper integration of coding skills, multi-step reasoning, and handling complex scenarios. These differences imply a graduated evaluation framework where models must demonstrate not only basic code correctness but also sophisticated problem-solving and adaptive reasoning, thus providing a comprehensive assessment of proficiency in progressively complex coding tasks.", "question_token_count": 54, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 9, "question_groundedness_score": 10, "avg_answer_token_count": 31, "choices": null}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "Examples and sources of exact contamination, including verbatim test examples, benchmark code snippets, and documentation leaks.", "question": "In the context of large language model evaluation, what are the primary examples and sources of exact data contamination, and how might seemingly benign elements like benchmark code snippets or documentation leaks contribute to compromising evaluation validity?", "answer": "Verbatim test examples, benchmark code snippets, and documentation leaks are primary sources of exact contamination because they introduce identical test data into training sets, undermining evaluation validity.", "explanation": "Exact data contamination arises when identical data points are present in both training and test sets. Primary examples include verbatim test examples, code snippets directly copied from benchmark implementations, and documentation leaks that reveal test data. Benchmark code snippets and documentation leaks, although not traditional data sources, can embed test information into training data, leading to contamination that artificially inflates performance metrics and compromises evaluation integrity.", "question_token_count": 41, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 34, "choices": null}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The fundamental challenges posed by data contamination in benchmarking Large Language Models and its impact on the reliability of evaluation results.", "question": "How does data contamination fundamentally undermine the reliability of benchmarking Large Language Models, and what are the inherent challenges in preventing it given the nature of LLM training data collection?", "answer": "It causes inflated, misleading benchmark results by violating training-test separation, and preventing it is difficult due to LLMs' massive, opaque Internet-based training data overlapping with benchmarks.", "explanation": "Data contamination violates the core machine learning principle of separating training and test sets, causing benchmark results to be artificially inflated and misleading; because LLMs train on vast, often opaque Internet-sourced data with unknown overlap with benchmark datasets, it becomes inherently difficult to ensure benchmarks remain uncontaminated, thus compromising the validity and trustworthiness of evaluation outcomes.", "question_token_count": 33, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 35, "choices": null}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The importance of holistic benchmarking approaches for evaluating the performance of large language models as general-purpose task solvers.", "question": "Why is it essential to adopt holistic and dynamic benchmarking approaches for large language models as general-purpose task solvers, and how do these approaches address the limitations posed by static benchmarks in the context of rapidly evolving model capabilities and data contamination risks?", "answer": "Because holistic and dynamic benchmarks comprehensively assess diverse LLM capabilities and adapt to evolving models, preventing static benchmarks from becoming too easy or contaminated, thereby maintaining evaluation relevance and reliability.", "explanation": "Holistic and dynamic benchmarking approaches are essential because they comprehensively evaluate diverse aspects of LLM performance, reflecting their broad task-solving capabilities. They address static benchmarks' limitations by continuously adapting to model improvements, preventing benchmarks from becoming trivially easy or compromised by data contamination. Dynamic benchmarks and contamination detectors ensure evaluation remains challenging, relevant, and reliable despite rapid model evolution and extensive training data exposure.", "question_token_count": 47, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 35, "choices": null}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "The role of competitive programming platforms like Codeforces and datasets like Aider in assessing dynamic problem-solving skills within coding benchmarks.", "question": "How do competitive programming platforms like Codeforces and datasets such as Aider uniquely contribute to evaluating dynamic problem-solving abilities in coding benchmarks, and why are these evaluations critical for advancing AI model capabilities beyond traditional code synthesis and debugging tasks?", "answer": "They provide complex, adaptive challenges that test a model\u2019s ability to solve variable, real-time problems dynamically, essential for assessing advanced coding skills beyond static synthesis and debugging.", "explanation": "Competitive platforms and datasets simulate complex, variable programming challenges that require adaptive thinking, algorithmic creativity, and real-time problem-solving skills, which are not fully tested by static code synthesis or debugging benchmarks. This dynamic assessment is critical because it reflects real-world coding demands and pushes AI models to demonstrate flexible, higher-order reasoning and optimization strategies.", "question_token_count": 46, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 34, "choices": null}
{"context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n", "topic": "Strategies for balancing automation and interpretability in large-scale transformation processes within dynamic benchmarking.", "question": "In the context of dynamic benchmarking for large-scale data transformations, what strategies can be employed to balance the automation benefits of LLM-assisted transformations with the critical need for interpretability and correctness, and how do these strategies mitigate the risks associated with reduced transparency?", "answer": "Employ explainability tools, human-in-the-loop validation, and hybrid rule-based/LLM approaches to maintain traceability and correctness while leveraging automation.", "explanation": "LLM-assisted transformations offer automation and scalability but can reduce interpretability due to their opaque decision-making processes. To balance this, strategies such as integrating explainability tools to provide transparency into model outputs, implementing human-in-the-loop validation to catch errors and ensure correctness, and designing hybrid approaches that combine rule-based and LLM-generated data are employed. These strategies mitigate risks by maintaining traceability, enabling error detection, and preserving interpretability, thus reducing reliance on costly manual verification while ensuring data transformation integrity.", "question_token_count": 50, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 30, "choices": null}
{"context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n", "topic": "The significance of model preference for original versus paraphrased test cases as an indicator of potential data contamination.", "question": "How does a model\u2019s preferential performance on original test cases compared to paraphrased versions serve as a robust indicator of data contamination, and what are the implications of this behavior for the reliability of benchmark evaluations?", "answer": "It indicates memorization of original data due to contamination, compromising benchmark reliability by overstating true generalization.", "explanation": "A model\u2019s better performance on original test cases relative to paraphrased ones suggests it has memorized or been exposed to the original data during training, indicating contamination. This discrepancy reveals that the model is not generalizing but recalling specific data, undermining the benchmark\u2019s ability to accurately assess true generalization and robustness.", "question_token_count": 42, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 21, "choices": null}
{"context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n", "topic": "Critical examination of the limitations of current benchmarking surveys, including coverage gaps due to rapid evolution of LLM methods and tools.", "question": "How does the rapid evolution of large language model benchmarking methods and tools fundamentally limit the comprehensiveness and applicability of current benchmarking surveys, and what are the implications of these limitations for developing reliable and standardized evaluation frameworks?", "answer": "The fast pace of LLM benchmarking evolution causes surveys to miss recent methods, limits reliability and reproducibility of dynamic evaluations, and hinders development of standardized, practical frameworks, reducing comprehensiveness and applicability.", "explanation": "Rapid advancements in LLM benchmarking lead to incomplete coverage in surveys, as new methods and tools may emerge after survey publication. This dynamic evolution challenges the reliability and reproducibility of benchmarks, especially for dynamic approaches, and complicates the establishment of standardized, practical evaluation frameworks. Consequently, surveys may remain preliminary, high-level, and less applicable for practitioners needing detailed, validated methodologies.", "question_token_count": 43, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 40, "choices": null}
{"context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n", "topic": "The mathematical formalization of stability in benchmark complexity and its practical evaluation in experimental trials.", "question": "How does the formalization of stability as the variance of a complexity measurement function across trials inform the practical evaluation of dynamic benchmarks, and why is minimizing this variance critical for distinguishing between performance drops caused by data contamination versus increased task complexity?", "answer": "Stability is measured by the variance of complexity across trials, and minimizing this variance ensures consistent task complexity, allowing performance drops to be attributed to data contamination rather than increased complexity.", "explanation": "Stability is mathematically defined as the variance of complexity measurements (\u03a8(\u00b7)) across different trials; a low variance means the complexity of the transformed datasets is consistent, ensuring that any observed performance drop is more likely due to data contamination rather than changes in task difficulty. Minimizing this variance is therefore essential to isolate the cause of performance changes, enabling reliable interpretation of benchmark results.", "question_token_count": 47, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 36, "choices": null}
{"context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n", "topic": "The methodology behind template-based benchmark generation, including the use of query templates with placeholder variables to create diverse problem instances.", "question": "How does the use of query templates with placeholder variables in template-based benchmark generation contribute to both the diversity of problem instances and the minimization of collision probability, and what are the potential challenges in maintaining the validity and difficulty consistency of these generated problems?", "answer": "By randomly filling placeholder variables in fixed templates, template-based generation creates diverse, unique problems with low collision probability, but ensuring each problem remains valid and consistently difficult requires careful rule design and verification.", "explanation": "Query templates with placeholders allow systematic generation of numerous unique problem instances by randomly filling variables, ensuring a wide variety of inputs and thus diversity. This randomization drastically reduces the chance that two generated problems are identical, minimizing collision probability. However, because the problems are generated by recombining variables within fixed templates, maintaining the logical validity and consistent difficulty of each instance can be challenging, as random combinations might produce trivial, unsolvable, or unintended problem variants, requiring careful rule design and validation.", "question_token_count": 49, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 39, "choices": null}
{"context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n", "topic": "Comparative analysis of interactive evaluation and multi-agent evaluation paradigms, including their complementary strengths and limitations in LLM assessment.", "question": "How do interactive evaluation and multi-agent evaluation paradigms complement each other in assessing large language models, and what are the primary strengths and limitations of each approach in terms of dynamic adaptability, scalability, and quality of benchmark generation?", "answer": "Interactive evaluation offers dynamic, adaptive multi-turn probing with nuanced feedback but limited scalability; multi-agent evaluation provides scalable, diverse, high-quality benchmark generation through specialized agent collaboration but can be complex to coordinate and less interactive.", "explanation": "Interactive evaluation excels in dynamically probing LLM understanding through multi-turn, context-sensitive interactions, enabling nuanced assessment of reasoning and response quality but may be limited by the quality and adaptability of the interviewer LLM. Multi-agent evaluation leverages specialized agents collaborating in a coordinated workflow to automate and scale benchmark creation and evolution, enhancing diversity and quality, yet introduces complexity in coordination and may lack the depth of iterative, human-like interaction. Together, they balance depth of interactive probing with breadth and scalability in benchmark generation.", "question_token_count": 44, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 43, "choices": null}
{"context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n", "topic": "The design and function of contamination detectors in identifying compromised samples within static benchmarks, and how the ITD framework leverages this to rewrite samples without altering difficulty.", "question": "How do contamination detectors function within the ITD framework to identify compromised samples in static benchmarks, and what mechanisms does ITD employ to ensure that the rewriting of these samples preserves their original difficulty levels?", "answer": "Contamination detectors identify contaminated samples by detecting data leakage patterns, after which ITD uses LLM-driven rewriting focused on retaining semantic content and cognitive complexity to preserve original difficulty.", "explanation": "Contamination detectors in ITD analyze static benchmark samples to detect data leakage or in-distribution contamination that could bias model training or evaluation. Once contaminated samples are identified, ITD prompts a large language model to rewrite these samples, carefully preserving the semantic complexity and cognitive challenge so that the difficulty remains consistent with the original, thereby maintaining benchmark integrity while mitigating contamination risks.", "question_token_count": 40, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 8, "avg_answer_token_count": 34, "choices": null}
{"context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n", "topic": "Designing benchmarking frameworks that ensure fairness, accountability, and privacy to avoid disadvantaging particular user groups or research domains.", "question": "How can benchmarking frameworks for large language models be designed to simultaneously mitigate bias inherent in static datasets, protect privacy concerns associated with dynamic data collection, and maintain transparency to prevent misuse, thereby ensuring fairness and accountability without disadvantaging particular user groups or research domains?", "answer": "By combining bias-aware dataset curation and updating, privacy-preserving data collection techniques, and transparent, auditable evaluation processes that enforce fairness and accountability safeguards.", "explanation": "The answer addresses the integration of multiple ethical and technical strategies\u2014such as careful curation and continual updating of benchmark datasets to reduce embedded bias, implementing privacy-preserving data collection methods for dynamic benchmarks, and enforcing transparent, auditable evaluation protocols to prevent manipulation\u2014thus balancing fairness, privacy, and accountability to avoid harm or disadvantage to specific populations.", "question_token_count": 52, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 31, "choices": null}
{"context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n", "topic": "The implications of data contamination on the reliability and trustworthiness of LLM performance assessments.", "question": "How does data contamination undermine the reliability and trustworthiness of LLM performance assessments, and what key design principles should dynamic benchmarks incorporate to effectively mitigate these risks?", "answer": "Data contamination inflates performance by leaking training data into tests, undermining reliability; dynamic benchmarks should use continual updating, standardized criteria, and overlap prevention to maintain trustworthiness.", "explanation": "Data contamination causes overlap between training and test data, leading to inflated performance metrics that do not reflect genuine generalization, thereby compromising reliability and trustworthiness. Dynamic benchmarks must incorporate principles such as continual updating to avoid leakage, standardized evaluation criteria to ensure consistency, and robust mechanisms to detect and prevent overlap, thus preserving the integrity of LLM assessments.", "question_token_count": 31, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 35, "choices": null}
{"context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n", "topic": "The methodology and cognitive considerations behind using LLMs to rewrite benchmark samples while preserving original stylistics and essential knowledge, as exemplified by Auto-Dataset.", "question": "How does the Auto-Dataset methodology leverage large language models to simultaneously preserve the stylistics and essential knowledge of original benchmark samples while generating related questions at varied cognitive levels, and what cognitive theory underpins this approach to ensure meaningful differentiation in question complexity?", "answer": "By prompting LLMs to rewrite samples preserving original style and knowledge for one set and generate cognitively varied related questions for another, guided by Bloom\u2019s taxonomy to ensure differentiated cognitive complexity.", "explanation": "Auto-Dataset prompts LLMs to create two types of samples\u2014one that retains original stylistic and knowledge characteristics and another that produces related questions targeting different cognitive levels. This dual approach is guided by Bloom\u2019s taxonomy, which categorizes cognitive skills into hierarchical levels, enabling the generation of questions that vary meaningfully in complexity while preserving the foundational content and style of the original dataset.", "question_token_count": 50, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 37, "choices": null}
{"context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n", "topic": "The role of continuous collection and updating of benchmarking methods in advancing LLM evaluation practices.", "question": "How does the continuous collection and updating of static and dynamic benchmarking methods in a centralized repository fundamentally contribute to advancing large language model evaluation practices, particularly in the context of mitigating data contamination risks and standardizing dynamic benchmarking criteria?", "answer": "It enables iterative refinement, standardization, and adaptation of benchmarking methods to evolving contamination risks, thereby improving evaluation rigor and reliability.", "explanation": "Continuously collecting and updating benchmarking methods in a centralized repository enables the integration of the latest research innovations, facilitates the identification and closure of gaps such as the lack of standardized dynamic benchmarking criteria, and supports iterative refinement of evaluation protocols. This dynamic process helps in adapting to evolving contamination risks inherent in large training corpora by promoting transparency, reproducibility, and community-driven consensus, thereby fundamentally advancing the rigor and reliability of LLM evaluation.", "question_token_count": 44, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 8, "avg_answer_token_count": 25, "choices": null}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "The integration of algorithmic reasoning and natural language processing in current LLM evaluation frameworks and its significance for future AI development.", "question": "How does the integration of algorithmic reasoning tasks with natural language processing in current LLM evaluation frameworks reflect and influence the trajectory of future AI development toward more versatile and reliable intelligent systems?", "answer": "It reflects and promotes the development of AI systems that unify symbolic algorithmic reasoning with natural language understanding, enabling more versatile, robust, and reliable intelligent models capable of complex problem-solving and generalization.", "explanation": "The integration signifies a shift toward testing LLMs not just on linguistic fluency but on their ability to comprehend, manipulate, and reason over structured data representations translated into natural language, thereby bridging symbolic reasoning and NLP; this approach challenges models to unify problem-solving skills with language understanding, which is essential for developing AI systems capable of complex, multi-modal reasoning and real-world applicability.", "question_token_count": 35, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 4, "question_groundedness_score": 2, "avg_answer_token_count": 38, "choices": null}
{"context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n", "topic": "Strategies to balance adaptability and ethical responsibility when implementing dynamic benchmarks for LLM evaluation.", "question": "What design strategies can be employed to ensure dynamic benchmarks for large language model evaluation maintain adaptability while simultaneously upholding ethical responsibilities such as privacy, fairness, and transparency?", "answer": "Implement strict data governance, fairness-aware data curation, transparency in evaluation criteria, and accountability mechanisms to balance adaptability with ethical responsibilities.", "explanation": "Dynamic benchmarks require continual data updating to remain relevant, but this raises ethical concerns around data privacy, bias perpetuation, and misuse of results. Effective design strategies include implementing strict data governance policies to protect privacy, applying fairness-aware data curation to mitigate bias, ensuring transparency through open reporting of evaluation criteria and results, and incorporating accountability mechanisms that prevent manipulation or unfair advantage. These combined approaches enable dynamic benchmarks to adapt responsibly without compromising ethical standards.", "question_token_count": 33, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 27, "choices": null}
{"context": "###### Safety\nSafety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content. Datasets such as RealToxicityPrompts\u00a0(Gehman et\u00a0al., 2020) and ToxiGen\u00a0(Hartvigsen et\u00a0al., 2022) assess resilience against producing harmful outputs.\n\nBy providing a controlled environment to measure these aspects, safety benchmarks play a critical role in guiding the development of models that are not only powerful outputs but also responsible and trustworthy for real-world applications.\n\n\n###### Language\nLanguage benchmarks assess the LLMs\u2019 proficiency in specific languages.\n\nGLUE\u00a0(Wang, 2018) and SuperGLUE\u00a0(Wang et\u00a0al., 2019) cover tasks from sentiment analysis to language inference, while CLUE\u00a0(Xu et\u00a0al., 2020) targets Chinese language.\n\nTypo-fixing\u00a0(Suzgun et\u00a0al., 2022) is also widely used.\n\n\n###### Reading Comprehension\nReading comprehension tasks test a model\u2019s ability to extract and infer information from text. Benchmarks like SQuAD\u00a0(Rajpurkar et\u00a0al., 2018), QuAC\u00a0(Choi et\u00a0al., 2018), and BoolQ\u00a0(Clark et\u00a0al., 2019) challenge models to understand passages and draw logical conclusions.\n", "topic": "The scope and diversity of language benchmarks like GLUE, SuperGLUE, and CLUE in assessing language proficiency across different languages and linguistic tasks.", "question": "How do the differing focuses of GLUE, SuperGLUE, and CLUE benchmarks collectively contribute to a comprehensive evaluation of language proficiency in large language models across languages and linguistic tasks?", "answer": "GLUE and SuperGLUE evaluate diverse English linguistic tasks, while CLUE targets Chinese, together ensuring multi-task and multilingual proficiency assessment.", "explanation": "GLUE and SuperGLUE assess a broad range of linguistic tasks such as sentiment analysis and language inference, providing a multi-task evaluation of language understanding primarily in English, while CLUE specifically targets the Chinese language, allowing for assessment of language proficiency in a non-English context; together, they complement each other by covering both task diversity and language diversity, enabling more thorough and generalizable evaluation of LLMs\u2019 language capabilities.", "question_token_count": 37, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 7, "question_groundedness_score": 10, "avg_answer_token_count": 29, "choices": null}
{"context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n", "topic": "The structure and role of TreeEval in generating hierarchical question trees to deepen model assessment through subtopic exploration.", "question": "How does TreeEval utilize hierarchical question trees to enhance the depth of model assessment, and what mechanisms enable it to generate follow-up subtopics and questions based on a model\u2019s prior responses?", "answer": "TreeEval uses LLM-generated initial questions and model responses to dynamically create hierarchical follow-up subtopics and questions, forming a multi-turn question tree that deepens assessment by adaptively exploring related topics based on prior answers.", "explanation": "TreeEval begins by generating an initial question on a topic using an LLM, then analyzes the model\u2019s response to dynamically produce follow-up subtopics and corresponding questions, effectively creating a branching hierarchy of inquiries. This adaptive, multi-turn process allows for progressively deeper probing of the model\u2019s understanding by exploring related subtopics informed by prior answers, enabling more nuanced and comprehensive evaluation than static benchmarks.", "question_token_count": 38, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 43, "choices": null}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "Comparative analysis of how different benchmark categories (math, knowledge, technical, open-domain) collectively provide a holistic understanding of a language model\u2019s strengths and limitations.", "question": "How do the distinct categories of benchmarks\u2014math, knowledge, technical, and open-domain\u2014interact to provide a comprehensive evaluation of a language model\u2019s capabilities, and what unique insights does their combined use reveal about the model\u2019s strengths and limitations?", "answer": "They complement each other by collectively assessing reasoning, factual knowledge, domain expertise, and adaptability, revealing nuanced strengths and weaknesses that no single benchmark category could identify alone.", "explanation": "Each benchmark category targets specific competencies: math benchmarks evaluate logical, multi-step problem solving; knowledge benchmarks assess factual recall and domain breadth; technical benchmarks test specialized and extended context understanding; open-domain benchmarks evaluate adaptability and generalization across diverse tasks. Their combined use exposes where a model excels or struggles, revealing nuanced trade-offs between reasoning, knowledge accuracy, domain expertise, and flexibility, thereby offering a holistic diagnostic profile of model performance.", "question_token_count": 50, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 6, "avg_answer_token_count": 33, "choices": null}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "The methodological challenges and potential limitations inherent in current benchmark datasets for coding, instruction-following, and reasoning tasks.", "question": "Considering the diversity of coding, instruction-following, and reasoning benchmarks, what are the primary methodological challenges and inherent limitations these datasets face in effectively evaluating language models\u2019 capabilities, and how might these limitations impact the generalizability and real-world applicability of model performance assessments?", "answer": "Limited task diversity, linguistic and cultural biases, inadequate multi-step reasoning evaluation, and simplified real-world simulations restrict benchmarks\u2019 ability to fully assess model generalization and applicability.", "explanation": "These benchmarks often face challenges such as limited task diversity, linguistic and cultural biases, insufficient modeling of multi-step or compositional reasoning, and constrained simulation of real-world complexities. Such limitations can cause overestimation of model abilities in controlled settings, reducing confidence in their generalization to broader, more dynamic, and nuanced real-world tasks.", "question_token_count": 53, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 7, "avg_answer_token_count": 33, "choices": null}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "The application of randomly generated reasoning graphs in constructing Knights and Knaves puzzles and how this challenges LLM logical deduction capabilities.", "question": "How does the use of randomly generated reasoning graphs in constructing Knights and Knaves puzzles specifically intensify the logical deduction challenges posed to large language models, and what fundamental reasoning capabilities are thereby tested?", "answer": "By encoding unpredictable logical dependencies through random graph structures, these puzzles require LLMs to perform flexible multi-step inference and contradiction resolution, testing their generalized logical deduction and reasoning robustness.", "explanation": "Randomly generated reasoning graphs encode unpredictable and varied logical relationships within Knights and Knaves puzzles, preventing pattern memorization and requiring LLMs to perform robust multi-step inference, contradiction detection, and truth assignment under uncertainty; this intensifies the logical deduction challenge by demanding flexible, generalized reasoning abilities beyond fixed puzzle templates.", "question_token_count": 39, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 6, "avg_answer_token_count": 35, "choices": null}
{"context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n", "topic": "The potential vulnerabilities and limitations of encryption-based protection methods, including key management challenges, computational overhead, and risks arising from compromised encryption keys.", "question": "In the context of protecting evaluation data with encryption, how do the challenges of key management and computational overhead interact to impact the overall security and practicality of these methods, and what are the critical risks associated with compromised encryption keys that can undermine the intended data confidentiality?", "answer": "Key management complexity and computational overhead reduce security and practicality, while compromised keys critically expose encrypted data, undermining confidentiality.", "explanation": "Effective encryption hinges on secure key management; if keys are mishandled or exposed, the encryption\u2019s confidentiality is nullified, allowing unauthorized access. Additionally, encryption introduces computational overhead that can limit scalability and efficiency, potentially discouraging its use or causing performance bottlenecks. Together, these factors affect both the security guarantees and the operational feasibility of encryption-based protection, while compromised keys represent a single point of catastrophic failure that can completely breach data protection.", "question_token_count": 53, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 25, "choices": null}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "Discuss the design and objectives of AntiLeak-Bench in generating queries about newly emerged knowledge post-model cutoff, and how this approach mitigates contamination risks.", "question": "How does AntiLeak-Bench utilize the concept of temporal cutoff to generate evaluation queries on newly emerged knowledge, and in what ways does this methodology effectively mitigate risks of data contamination in large language model benchmarking?", "answer": "By generating queries exclusively about knowledge that emerged after the model\u2019s cutoff date, AntiLeak-Bench ensures no prior training exposure, effectively eliminating data contamination risks in evaluation.", "explanation": "AntiLeak-Bench focuses on creating queries about knowledge that appeared after the model\u2019s knowledge cutoff date, ensuring the model has no prior exposure to this information. This temporal cutoff strategy prevents overlap between training data and evaluation queries, thereby eliminating contamination risks and enabling a more accurate assessment of a model\u2019s ability to generalize to novel information.", "question_token_count": 42, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 35, "choices": null}
{"context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n", "topic": "The broader implications of contamination detection methods on reproducibility, error analysis, and the trustworthiness of model evaluation benchmarks.", "question": "How do current contamination detection methods, including label protection and post-hoc behavioral analyses, collectively impact the reproducibility, error analysis granularity, and overall trustworthiness of model evaluation benchmarks, and what are the systemic trade-offs involved in balancing transparency with protection against data leakage?", "answer": "They limit transparency and independent verification, impeding reproducibility and detailed error analysis, while balancing protection against data leakage with reduced openness, thus affecting trustworthiness.", "explanation": "Label protection restricts transparency and independent verification, forcing reliance on centralized evaluation systems that limit detailed error analysis and reproducibility. Post-hoc detection methods help identify contamination but often rely on imperfect overlap measures or behavioral proxies that can produce false negatives or incomplete assessments. Together, these approaches create a tension between safeguarding benchmarks from data leakage and maintaining openness necessary for reproducibility and nuanced error analysis, ultimately affecting the trustworthiness and scientific rigor of model evaluations.", "question_token_count": 52, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 31, "choices": null}
{"context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n", "topic": "Define the concept of collision in dynamic benchmarking and discuss its significance in maintaining benchmark reliability.", "question": "How does the concept of collision in dynamic benchmarking affect the reliability of benchmarks for evaluating large language models, and why are metrics like Collision Rate and Repeat Trials critical in addressing this issue?", "answer": "Collision measures overlap between transformed benchmark datasets, and Collision Rate and Repeat Trials quantify this overlap to ensure benchmarks remain reliable and novel despite potential training data contamination.", "explanation": "Collision describes the overlap between different transformed versions of a benchmark dataset, which can reduce the benchmark\u2019s ability to generate novel, diverse test cases. This overlap risks data contamination when benchmarks are used in training, thereby compromising their reliability in accurately assessing LLM capabilities. Metrics such as Collision Rate quantify the extent of overlap, while Repeat Trials estimate how many transformation attempts are needed to reproduce existing data variations, together enabling evaluation of a benchmark\u2019s robustness and continued effectiveness despite exposure to training data contamination.", "question_token_count": 37, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 31, "choices": null}
{"context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n", "topic": "Strategies for designing dynamic benchmarks that maintain stable complexity to ensure reliable performance comparisons.", "question": "In designing dynamic benchmarks for large language models, what strategies can be employed to ensure the complexity of transformed datasets remains stable across trials, thereby enabling reliable differentiation between performance drops caused by data contamination versus increased task complexity?", "answer": "Employ generalizable complexity metrics to measure and minimize variance in complexity across trials and design transformations that preserve task complexity stability.", "explanation": "Ensuring stable complexity involves selecting or developing complexity metrics that generalize across domains, monitoring variance in complexity measurements across transformed datasets, and constraining transformations to avoid significant complexity shifts; this stability allows performance drops to be attributed more confidently to data contamination rather than task difficulty changes.", "question_token_count": 43, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 24, "choices": null}
{"context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n", "topic": "The use of embedding-based similarity measures and advanced mapping metrics as robust alternatives for overlap detection in post-hoc contamination analysis.", "question": "How do embedding-based similarity measures and advanced mapping metrics overcome the limitations of exact n-gram matching in post-hoc contamination detection between training and test datasets, and what implications does this have for the reliability of model evaluation?", "answer": "They detect semantic similarity beyond exact matches, reducing false negatives and improving contamination detection accuracy, thereby enhancing evaluation reliability.", "explanation": "Embedding-based similarity measures capture semantic relationships beyond exact token overlap, allowing detection of paraphrased or semantically similar data points missed by strict n-gram matching, which often yields false negatives. Advanced mapping metrics further refine alignment between datasets, improving sensitivity to subtle overlaps. Together, these methods increase the accuracy of contamination detection, thus enhancing the reliability and transparency of model evaluation by ensuring that test data is truly novel relative to training data.", "question_token_count": 43, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 23, "choices": null}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "The impact of recent math competitions like AIME 2024 and CNMO 2024 on advancing the complexity and diversity of math benchmarks for LLM evaluation.", "question": "How do recent math competitions like AIME 2024 and CNMO 2024 enhance the complexity and diversity of math benchmarks, and what implications does their inclusion have for the evaluation of large language models' mathematical reasoning capabilities?", "answer": "They introduce more diverse and intricate multi-step problems that increase benchmark complexity, enabling more rigorous evaluation of LLMs' ability to generalize and reason across varied mathematical challenges.", "explanation": "These competitions introduce a wider variety of problem types and greater intricacy in multi-step reasoning, pushing benchmarks beyond traditional datasets to more robustly test LLMs. Their inclusion challenges models to generalize across diverse and complex tasks, revealing strengths and limitations in mathematical problem-solving that simpler benchmarks might not expose.", "question_token_count": 46, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 7, "question_groundedness_score": 10, "avg_answer_token_count": 33, "choices": null}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The human effort and collaboration involved in creating comprehensive, multi-faceted benchmarks for LLMs.", "question": "Why is extensive human effort and collaboration essential in developing comprehensive, multi-faceted benchmarks for large language models, especially in light of challenges such as rapid model evolution and data contamination?", "answer": "Because creating benchmarks that comprehensively assess diverse LLM capabilities, remain challenging despite rapid model evolution, and mitigate data contamination requires coordinated human expertise to design, update, and monitor dynamic evaluation tasks.", "explanation": "Extensive human effort and collaboration are essential to design benchmarks that cover diverse tasks reflecting real-world use cases, ensure continuous updates to maintain challenge relevance as models improve, and implement contamination detectors to prevent data leakage\u2014all of which require domain expertise and coordinated effort to create reliable, dynamic evaluation frameworks.", "question_token_count": 36, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 38, "choices": null}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "Strategies for interpreting and integrating results from multi-faceted benchmarks to guide future improvements in large language model architectures and training.", "question": "How can the integration of performance insights from multi-step mathematical problem-solving benchmarks and diverse knowledge retrieval benchmarks be strategically leveraged to inform architectural innovations and training methodologies in large language models, considering potential trade-offs between complex reasoning and broad factual accuracy?", "answer": "By systematically analyzing strengths and weaknesses revealed across both benchmark types, developers can identify complementary capabilities and trade-offs, then design hybrid architectures and adaptive training regimes that balance complex reasoning and factual accuracy to optimize overall model performance.", "explanation": "This question challenges the expert to analyze how distinct benchmark results\u2014testing multi-step mathematical reasoning versus broad knowledge retrieval\u2014can be synthesized to guide future LLM designs and training. It demands understanding the different cognitive demands and capabilities these benchmarks assess, the inherent trade-offs between reasoning complexity and knowledge breadth, and how these considerations can drive targeted architectural and training improvements.", "question_token_count": 47, "answer_correctness_score": 9, "explanation_validity_score": 8, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 43, "choices": null}
{"context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n", "topic": "The implications and potential risks of applying dynamic benchmarks with low correctness to LLM evaluation.", "question": "What are the primary risks and broader implications of employing dynamic benchmarks with low correctness scores when evaluating large language models, and how can such deficiencies affect the reliability and trustworthiness of LLM performance assessments?", "answer": "They produce misleading evaluations that compromise reliability, trustworthiness, reproducibility, and can lead to flawed model selection and deployment decisions.", "explanation": "Using dynamic benchmarks with low correctness scores risks producing evaluations that do not accurately reflect the true capabilities of LLMs, leading to misleading conclusions about model quality. This can undermine the reliability of performance comparisons, cause poor model selection, impair reproducibility of results, and erode trust in benchmarking processes, ultimately affecting research directions and deployment decisions.", "question_token_count": 39, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 25, "choices": null}
{"context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n", "topic": "The limitations of exact matching methods in contamination detection and the implications of false negatives on evaluation reliability.", "question": "How do false negatives arising from exact matching methods in dataset contamination detection compromise the reliability of model evaluation metrics, and why does this limitation motivate the use of alternative approaches like embedding-based similarity?", "answer": "False negatives in exact matching allow undetected contamination, inflating performance metrics and reducing evaluation reliability, motivating more robust detection methods like embedding-based similarity.", "explanation": "Exact matching methods fail to detect all overlaps between training and test data, resulting in false negatives where contamination goes unnoticed. This incomplete detection undermines the validity of evaluation metrics by allowing data leakage to persist, which artificially inflates model performance and reduces transparency. Consequently, researchers cannot fully trust benchmark results or perform detailed error analyses. This critical limitation drives the adoption of embedding-based similarity and behavioral analysis methods, which can identify more subtle overlaps and better ensure the integrity and reproducibility of model evaluations.", "question_token_count": 38, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 30, "choices": null}
{"context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n", "topic": "The absence of standardized criteria for evaluating dynamic benchmarks and the importance of establishing such standards for future research.", "question": "How does the absence of standardized evaluation criteria for dynamic benchmarks impede the reliable assessment of large language models, and why is establishing such standards critical for addressing the limitations of static benchmarks and guiding future research in LLM evaluation?", "answer": "It leads to unreliable, inconsistent evaluations and inefficiencies, making standardization essential to ensure fairness, scalability, and to overcome static benchmarks\u2019 limitations in LLM assessment.", "explanation": "Without standardized criteria, dynamic benchmarks lack consistent measures of correctness, scalability, and complexity control, leading to inefficiencies and incomparable results; establishing these standards is crucial to overcome static benchmarks\u2019 contamination and transparency issues, ensuring reliable, fair, and scalable evaluation frameworks that can evolve alongside LLM capabilities.", "question_token_count": 43, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 32, "choices": null}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "The implications of incorporating well-known computational complexity problems into LLM evaluation for understanding model limits and capabilities.", "question": "How does incorporating NP-hard problems like the Traveling Salesman Problem into large language model evaluation frameworks deepen our understanding of LLMs\u2019 reasoning limits and capabilities, particularly in relation to computational complexity and heuristic problem-solving?", "answer": "It reveals LLMs\u2019 ability to heuristically approximate solutions to computationally intractable problems, thereby clarifying their reasoning limits relative to formal complexity boundaries.", "explanation": "Incorporating NP-hard problems such as the Traveling Salesman Problem exposes LLMs to tasks that are computationally intractable in the worst case, enabling evaluation of their ability to approximate or heuristically solve complex problems beyond straightforward logical reasoning. This challenges LLMs to demonstrate reasoning that may not rely on exhaustive search but on pattern recognition or learned heuristics, thereby revealing the boundaries of their problem-solving abilities and highlighting distinctions between algorithmic complexity and learned inference.", "question_token_count": 42, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 30, "choices": null}
{"context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "Critically evaluate the challenges and benefits of applying transformation functions to benchmarking datasets over multiple timestamps in dynamic benchmarking.", "question": "How do transformation functions applied over multiple timestamps in dynamic benchmarking simultaneously address data contamination issues and introduce new challenges in ensuring the fairness and fidelity of LLM evaluations?", "answer": "They mitigate data contamination by continuously altering datasets but create challenges in maintaining consistent, fair, and faithful evaluation standards across evolving datasets.", "explanation": "Transformation functions in dynamic benchmarking modify datasets over time to prevent model training data contamination, thereby enabling more accurate and transparent evaluation. However, these modifications also complicate maintaining a consistent benchmark standard, as evolving data may unintentionally bias results or reduce comparability across timestamps, posing challenges to fairness and fidelity in evaluation.", "question_token_count": 31, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 26, "choices": null}
{"context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n", "topic": "The role of knowledge graphs in conjunction with LLMs to extend examined concepts and generate cognitively diverse questions in the StructEval approach.", "question": "How do knowledge graphs enhance the ability of large language models in the StructEval approach to generate cognitively diverse and conceptually extended questions, and what are the implications of this integration for advancing benchmark complexity?", "answer": "Knowledge graphs supply structured semantic relations that guide LLMs to create extended and cognitively diverse questions, thereby enhancing conceptual coverage and increasing benchmark complexity.", "explanation": "Knowledge graphs provide explicit semantic relationships and structured domain knowledge that guide LLMs to systematically explore and expand on concepts beyond the original benchmark questions, enabling generation of questions at varied cognitive levels and richer conceptual depth, thus advancing the complexity and diversity of benchmarks.", "question_token_count": 41, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 29, "choices": null}
{"context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n", "topic": "Privacy and security challenges introduced by dynamic benchmarks that require continual data collection and updating in AI systems.", "question": "What are the primary privacy and security challenges introduced by dynamic benchmarks in AI systems that require continual data collection and updating, and how do these challenges complicate efforts to ensure ethical benchmarking practices?", "answer": "Continuous data collection in dynamic benchmarks raises risks of data leakage, unauthorized access, and misuse, complicating privacy protections and ethical accountability.", "explanation": "Dynamic benchmarks continuously gather and update data to adapt evaluations, which increases the risk of exposing sensitive information, unauthorized data access, and potential data misuse. This ongoing data influx complicates maintaining user privacy and system security, making it harder to implement consistent protections and transparency, thus challenging the ethical principles of fairness and accountability in AI benchmarking.", "question_token_count": 37, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 27, "choices": null}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "The challenges in distinguishing between an LLM\u2019s memorization of training data and its reasoning capabilities during model evaluation.", "question": "How does syntactic contamination in evaluation benchmarks complicate the distinction between an LLM\u2019s memorization of training data and its reasoning capabilities, and what are the implications of this complication for assessing true model performance?", "answer": "It blurs the line between recall and inference by making syntactic rephrasings appear as novel inputs, causing benchmarks to overestimate true reasoning and thus undermining accurate assessment of model performance.", "explanation": "Syntactic contamination occurs when test inputs are rephrased versions of training data with added prefixes, making it difficult to tell if an LLM\u2019s correct responses result from recalling memorized patterns or from genuine reasoning. This is particularly challenging because some NLP tasks rely heavily on syntactic cues, so models might appear to reason when they are actually matching learned syntax. Consequently, benchmarks containing such contamination can overestimate a model\u2019s abilities, undermining the validity of evaluations, obscuring true generalization, and misleading research and deployment decisions.", "question_token_count": 39, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 39, "choices": null}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "Comparative analysis of static versus dynamic benchmarking methods in the context of continuous LLM training.", "question": "How do dynamic benchmarking methods address the limitations of static benchmarks in the context of continuous training of large language models, particularly regarding data contamination and the evolving capabilities of these models?", "answer": "By continuously updating tasks and datasets to prevent data contamination and maintain challenge as LLMs evolve, dynamic benchmarks ensure valid and robust evaluation despite continuous training.", "explanation": "Dynamic benchmarks mitigate the issues of static benchmarks becoming obsolete or contaminated as LLMs train on more data by continuously updating tasks and datasets to remain challenging and distinct from training data, thereby preserving evaluation validity despite ongoing model improvements.", "question_token_count": 35, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 30, "choices": null}
{"context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n", "topic": "The impact of publicly available rule-generated data on benchmark contamination and the subsequent effects on model training and evaluation.", "question": "How does the availability of publicly generated rule-based datasets specifically exacerbate in-distribution contamination risks in benchmark datasets, and what are the consequent implications for the validity of model training and evaluation results?", "answer": "They increase data leakage by introducing repetitive patterns that appear in both training and test sets, undermining generalization and inflating evaluation metrics.", "explanation": "Publicly available rule-generated datasets increase in-distribution contamination risk because models can encounter identical or near-identical patterns during training and testing, leading to data leakage. This leakage inflates performance metrics by enabling memorization rather than genuine generalization, thereby compromising the validity and reliability of both training and evaluation outcomes.", "question_token_count": 38, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 27, "choices": null}
{"context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n", "topic": "The architecture and workflow of Benchmark Self-Evolving, focusing on how multi-agent frameworks dynamically extend static benchmarks.", "question": "How does the multi-agent architecture of Benchmark Self-Evolving facilitate the dynamic extension of static benchmarks, and what are the key roles played by specialized agents and human-in-the-loop feedback in ensuring scalable, diverse, and high-quality benchmark evolution?", "answer": "By coordinating specialized agents that handle planning, generation, verification, and evaluation tasks, Benchmark Self-Evolving dynamically extends static benchmarks, while human-in-the-loop feedback ensures quality and diversity in the evolving benchmarks.", "explanation": "Benchmark Self-Evolving utilizes a multi-agent framework wherein specialized agents collaboratively plan, generate, verify, and evaluate benchmark tasks, enabling the transformation of static benchmarks into dynamic, continuously evolving ones. Human-in-the-loop feedback integrates quality control by guiding and refining agent outputs, ensuring the benchmarks remain diverse, scalable, and high-quality throughout the evolution process.", "question_token_count": 48, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 41, "choices": null}
{"context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n", "topic": "The significance of data provenance and documentation in addressing contamination and enhancing evaluation integrity for LLMs.", "question": "How does comprehensive data provenance and transparent documentation critically mitigate training-evaluation data contamination in large language models, and why is this transparency essential for maintaining the integrity and fairness of LLM performance benchmarks?", "answer": "They enable detection and exclusion of overlapping evaluation data, ensuring fair, reliable benchmarks by making training data origins transparent and verifiable.", "explanation": "Comprehensive data provenance and transparent documentation allow researchers to trace and verify the origins and contents of training datasets, making it possible to detect and exclude evaluation data overlaps; this transparency is essential because, without it, contamination risks remain hidden, impairing the ability to fairly assess model performance and undermining trust in benchmark results.", "question_token_count": 38, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 26, "choices": null}
{"context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n", "topic": "The conceptual definition of scalability in dynamic benchmarking and its significance in generating reliable benchmark datasets.", "question": "How does the conceptual definition of scalability in dynamic benchmarking integrate the trade-offs between dataset size, transformation cost, and statistical reliability to ensure the generation of reliable benchmark datasets?", "answer": "Scalability measures the amount of data generated per unit cost, balancing larger dataset size to reduce statistical errors against minimizing transformation costs to produce reliable benchmarks efficiently.", "explanation": "Scalability in dynamic benchmarking is defined as the expected ratio of the transformed dataset size to the original dataset size, normalized by the cost of transformation, reflecting how efficiently large datasets can be generated; this balance is crucial because larger datasets reduce statistical errors but must be produced cost-effectively, ensuring benchmarking reliability without prohibitive resource expenditure.", "question_token_count": 34, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 31, "choices": null}
{"context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n", "topic": "Potential strategies and community practices to mitigate contamination and ensure trustworthy evaluation of LLMs despite limited training data transparency.", "question": "Given the inherent opacity of proprietary LLM training data and the scale of pre-training and fine-tuning corpora that increase contamination risks, what integrated strategies and community-driven practices can be employed to rigorously detect, mitigate, and account for training-evaluation data overlap to ensure trustworthy and fair benchmarking of LLMs?", "answer": "Combining retrieval-based contamination detection, designing benchmarks with novel or synthetic data unlikely in training sets, enforcing transparency standards for training data disclosure, and fostering community-driven audit and reporting frameworks.", "explanation": "This answer reflects the multifaceted challenge posed by contamination under opaque training conditions and the need for combining technical detection methods, transparent reporting standards, benchmark design innovations, and collaborative community protocols to uphold evaluation integrity.", "question_token_count": 60, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 36, "choices": null}
{"context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n", "topic": "Comparative analysis of different benchmark rewriting techniques using LLMs in terms of contamination mitigation, sample diversity, and cognitive complexity.", "question": "How do the benchmark rewriting techniques Auto-Dataset, StructEval, ITD, and VarBench differ in their approaches to contamination mitigation, sample diversity enhancement, and cognitive complexity preservation or improvement?", "answer": "ITD focuses on contamination mitigation by detecting and rewriting contaminated samples while preserving difficulty; Auto-Dataset and StructEval enhance cognitive complexity and question diversity through varied question generation and concept expansion; VarBench increases sample diversity by variable replacement but does not explicitly address contamination or cognitive complexity.", "explanation": "Auto-Dataset enhances cognitive complexity by generating related questions at varying cognitive levels while retaining original stylistics but does not explicitly address contamination mitigation. StructEval expands on concepts using knowledge graphs to deepen cognitive complexity and diversify questions but lacks direct contamination control. ITD explicitly targets contamination mitigation by detecting and rewriting contaminated samples while preserving difficulty, focusing on maintaining cognitive complexity and data integrity. VarBench increases sample diversity by identifying and replacing variables in existing samples but does not specifically address contamination or cognitive complexity. Thus, each method prioritizes different aspects, with ITD focused on contamination mitigation, Auto-Dataset and StructEval on cognitive complexity and diversity, and VarBench primarily on diversity.", "question_token_count": 39, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 7, "question_groundedness_score": 10, "avg_answer_token_count": 55, "choices": null}
{"context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n", "topic": "The impact of label protection on transparency, independent verification, and reliance on centralized evaluation systems in machine learning performance assessment.", "question": "How does label protection in machine learning benchmarks fundamentally affect transparency and independent verification, and what are the implications of this effect on researchers' reliance on centralized evaluation systems for performance assessment?", "answer": "Label protection reduces transparency and independent verification by withholding ground-truth labels, causing researchers to depend on centralized evaluation systems that limit detailed error analysis and reproducibility.", "explanation": "Label protection restricts access to ground-truth labels, which prevents researchers from independently verifying results and conducting detailed error analyses. This lack of transparency forces reliance on centralized evaluation systems that provide performance metrics without exposing underlying data or errors, thereby limiting reproducibility and deeper scientific scrutiny.", "question_token_count": 36, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 31, "choices": null}
