{"context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n", "topic": "Discuss the trade-off between the cost of manual verification and the benefits of interpretable transformations in dynamic benchmarking.", "question": "What are the primary trade-offs between the cost of manual verification and the benefits of interpretable transformations in dynamic benchmarking, and how do LLM-assisted transformations impact this balance?", "answer": "The primary trade-off is between the cost of manual verification and the benefits of reduced costs through interpretable transformations, with LLM-assisted transformations requiring additional validation mechanisms to ensure reliability and correctness.", "explanation": "This question requires domain experts to critically evaluate the cost-benefit analysis of interpretable transformations and their implications for dynamic benchmarking, including the potential need for additional validation mechanisms to ensure reliability and correctness.", "question_token_count": 34, "answer_correctness_score": 9, "explanation_validity_score": 8, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 37, "choices": null}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "What is the significance of dynamic benchmarks in addressing the limitations of static benchmarks for LLM evaluation?", "question": "What is the significance of dynamic benchmarks in addressing the limitations of static benchmarks for LLM evaluation?", "answer": "They allow for continuous evaluation and adaptation to the changing performance of LLMs.", "explanation": "Dynamic benchmarks are necessary to address the limitations of static benchmarks, which may become outdated or contaminated as LLMs evolve rapidly.", "question_token_count": 19, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 9, "question_groundedness_score": 9, "avg_answer_token_count": 16, "choices": null}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "What are the potential applications of contamination mitigation techniques in LLM research and development, and how can they impact the field?", "question": "How might contamination mitigation techniques influence the development and deployment of LLMs in real-world applications, and what are the potential consequences of their adoption or neglect?", "answer": "By ensuring accurate assessments of model capabilities, contamination mitigation techniques can facilitate more informed decision-making in model deployment, improve model robustness and generalizability, and ultimately enhance the reliability and trustworthiness of LLMs in real-world applications.", "explanation": "This question requires a deep understanding of the implications of contamination on LLM research and development, as well as the potential applications and consequences of mitigation techniques.", "question_token_count": 31, "answer_correctness_score": 8, "explanation_validity_score": 6, "question_clarity_score": 8, "question_groundedness_score": 7, "avg_answer_token_count": 45, "choices": null}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "What are some potential future directions for research on canary strings and their applications in explainability?", "question": "What potential future directions for research on canary strings and their applications in explainability could build upon their existing limitations, and how might these advancements impact the development of large language models?", "answer": "Investigating the use of canary strings in conjunction with other explainability techniques, such as feature importance and model interpretability, to provide a more comprehensive understanding of model behavior.", "explanation": "This question encourages the domain expert to think critically about the potential future directions for research on canary strings and their applications in explainability, considering the limitations of the current method and the potential implications for the development of large language models.", "question_token_count": 37, "answer_correctness_score": 8, "explanation_validity_score": 7, "question_clarity_score": 7, "question_groundedness_score": 4, "avg_answer_token_count": 36, "choices": null}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "Describe the key differences between table-based and graph-based evaluation methods for LLMs.", "question": "What are the primary differences between Table-Based and Graph-Based evaluation methods for Large Language Models (LLMs)?", "answer": "Table-Based evaluation assesses LLMs using randomly generated SQL tables, whereas Graph-Based evaluation involves evaluating LLMs using randomly generated graphs, such as DAGs and well-known P and NP problems.", "explanation": "The correct answer highlights the fundamental differences in data structures used in these evaluation methods, including SQL tables and randomly generated graphs.", "question_token_count": 22, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 9, "question_groundedness_score": 10, "avg_answer_token_count": 38, "choices": null}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "Discuss the importance of using frameworks such as S3Eval, DyVal, NPHardEval, and Xie et al. (2024) to evaluate LLMs' performance on various tasks.", "question": "What are the primary reasons behind the importance of frameworks like S3Eval, DyVal, NPHardEval, and Xie et al. (2024) in evaluating the performance of Large Language Models?", "answer": "They provide diverse and challenging tasks that push the limits of LLMs' reasoning capabilities, enable the identification of areas for improvement, and facilitate the development of more effective training methods.", "explanation": "These frameworks are crucial in assessing LLMs' reasoning ability, identifying areas for improvement, and developing more effective training methods.", "question_token_count": 42, "answer_correctness_score": 9, "explanation_validity_score": 8, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 36, "choices": null}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "Discuss the potential benefits and drawbacks of using LLMs in various applications, such as natural language processing and computer vision.", "question": "What are the potential benefits and drawbacks of applying Large Language Models to tasks in natural language processing, and how do these trade-offs impact their performance in these domains?", "answer": "The benefits of applying LLMs to tasks in natural language processing include improved accuracy and efficiency, but these models may also introduce potential biases and increased complexity, which can impact their performance in these domains.", "explanation": "This question requires the domain expert to consider the implications of using LLMs in natural language processing and computer vision, analyzing both the benefits (e.g., improved accuracy, efficiency) and drawbacks (e.g., increased complexity, potential bias).", "question_token_count": 33, "answer_correctness_score": 6, "explanation_validity_score": 5, "question_clarity_score": 9, "question_groundedness_score": 2, "avg_answer_token_count": 40, "choices": null}
{"context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n", "topic": "How does the growth of training corpora impact the effectiveness of static benchmarks?", "question": "How does the growth of training corpora affect the probability of contamination in static benchmarks, and what implications does this have for LLM benchmarking?", "answer": "The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121, rendering traditional benchmarks outdated for models trained on web-scale data.", "explanation": "This question aims to probe the domain expert's understanding of the relationship between the growth of training corpora and the effectiveness of static benchmarks, as well as the impact of contamination on LLM benchmarking.", "question_token_count": 28, "answer_correctness_score": 9, "explanation_validity_score": 4, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 38, "choices": null}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "Describe the key differences between HumanEval and MBPP in terms of their testing objectives.", "question": "What are the primary differences between HumanEval and MBPP in terms of their testing objectives, and how do these differences reflect the unique challenges and requirements of each benchmark?", "answer": "HumanEval focuses on code synthesis and debugging, whereas MBPP addresses more advanced challenges, such as code optimization and refactoring.", "explanation": "This question requires the domain expert to demonstrate a deep understanding of the differences between HumanEval and MBPP, as well as their respective testing objectives.", "question_token_count": 34, "answer_correctness_score": 2, "explanation_validity_score": 2, "question_clarity_score": 8, "question_groundedness_score": 5, "avg_answer_token_count": 25, "choices": null}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "How do canary strings help to identify data contamination in LLMs, and what is their limitation?", "question": "How do canary strings help to identify data contamination in LLMs, and what is their limitation, specifically in relation to model trainers' awareness and responsiveness?", "answer": "Canary strings help identify data contamination by detecting memorization of training data, but their effectiveness depends on model trainers being aware of and responsive to these markers, and they are limited when model developers aim to leak benchmarking data to boost scores.", "explanation": "The correct answer highlights the effectiveness of canary strings in detecting memorization of training data, but also notes their limitation in the face of intentional data leakage by model developers.", "question_token_count": 32, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 48, "choices": null}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "How does the rapid pace of LLM development impact the need for robust benchmarking methods, and what are the consequences of failing to address this issue?", "question": "What are the primary consequences of failing to address data contamination in benchmarking Large Language Models (LLMs), and how does this impact the validity of benchmarks and progress in LLM research?", "answer": "Overestimation of model capabilities, undermined benchmark validity, and misleading conclusions about progress in LLM research.", "explanation": "This question requires the domain expert to critically reflect on the potential consequences of failing to address data contamination and understand its impact on the validity of benchmarks and progress in LLM research.", "question_token_count": 36, "answer_correctness_score": 10, "explanation_validity_score": 7, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 21, "choices": null}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "What is the concept of temporal cutoff in Large Language Models (LLMs), and how does it impact the performance of models?", "question": "How does the concept of temporal cutoff in Large Language Models (LLMs) impact their performance, and what are some approaches to mitigate data contamination in model evaluation?", "answer": "Temporal cutoff in LLMs affects their performance by limiting their knowledge to a specific date, and mitigation approaches include using benchmarks like LiveBench, AntiLeak-Bench, AcademicEval, LiveCodeBench, LiveAoPSBench, and Forecastbench.", "explanation": "The correct answer requires a deep understanding of LLMs, their knowledge cutoff dates, and the impact of temporal cutoff on model performance.", "question_token_count": 33, "answer_correctness_score": 8, "explanation_validity_score": 6, "question_clarity_score": 9, "question_groundedness_score": 10, "avg_answer_token_count": 50, "choices": null}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "How can the results of evaluating LLMs' reasoning abilities be used to improve the development of more accurate and reliable models?", "question": "How can the results of evaluating LLMs' reasoning abilities be used to improve the development of more accurate and reliable models?", "answer": "By identifying and addressing limitations in reasoning, optimizing model architectures, and fine-tuning training data to better align with real-world scenarios.", "explanation": "This question requires the test-taker to consider the potential applications and benefits of LLM evaluation and to think critically about how these results can be used to improve model development.", "question_token_count": 25, "answer_correctness_score": 9, "explanation_validity_score": 8, "question_clarity_score": 8, "question_groundedness_score": 4, "avg_answer_token_count": 26, "choices": null}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "What are the potential applications of evaluating LLMs' reasoning abilities in real-world scenarios?", "question": "What are the potential applications of evaluating LLMs' reasoning abilities in real-world scenarios?", "answer": "Evaluating LLMs' reasoning abilities can lead to the development of more accurate and reliable language models, enabling applications in areas such as natural language processing, question-answering systems, and expert systems.", "explanation": "This question encourages the domain expert to think critically about the potential applications and use cases of LLMs in real-world scenarios, such as decision-making, problem-solving, and information processing.", "question_token_count": 18, "answer_correctness_score": 8, "explanation_validity_score": 7, "question_clarity_score": 9, "question_groundedness_score": 4, "avg_answer_token_count": 41, "choices": null}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "How can the use of canary strings contribute to the development of more robust and reliable security models?", "question": "How can the strategic use of canary strings in LLM training data collection contribute to the development of more robust and reliable security models, and what are the implications of using this method?", "answer": "By identifying and filtering out instances of data contamination, canary strings can help ensure that LLMs learn to generalize and do not memorize portions of their training data, thereby enhancing the security of the models.", "explanation": "This question requires the domain expert to think critically about the application of canary strings in LLM security and to consider the implications of using this method. It is a challenging question that requires a deep understanding of the content.", "question_token_count": 37, "answer_correctness_score": 8, "explanation_validity_score": 4, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 41, "choices": null}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "What are the primary focuses of SWE-Bench and Codeforces in evaluating AI models' language understanding abilities?", "question": "What are the primary focuses of SWE-Bench and Codeforces in evaluating AI models' language understanding abilities, and how do they differ from other benchmarks like HumanEval and IFEval?", "answer": "SWE-Bench focuses on more advanced challenges, while Codeforces evaluates dynamic problem-solving and code synthesis, with both benchmarks requiring AI models to comprehend and execute detailed directives.", "explanation": "This question requires the domain expert to identify the primary focuses of SWE-Bench and Codeforces, which involves understanding the nuances of each benchmark and their applications.", "question_token_count": 37, "answer_correctness_score": 8, "explanation_validity_score": 6, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 35, "choices": null}
{"context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n", "topic": "How can static benchmarking be used to evaluate the instruction following abilities of an LLM?", "question": "How can static benchmarking be used to evaluate the instruction following abilities of an LLM, and what specific aspects of instruction following are being assessed through this process?", "answer": "Static benchmarking can be used to evaluate the instruction following abilities of an LLM by assessing its ability to follow instructions, complete tasks, and demonstrate understanding of the task requirements, as indicated by the specific benchmarks and scoring functions used.", "explanation": "This question requires the domain expert to understand how static benchmarking is used to evaluate instruction following abilities of an LLM and what specific aspects of instruction following are being assessed through this process.", "question_token_count": 31, "answer_correctness_score": 8, "explanation_validity_score": 7, "question_clarity_score": 8, "question_groundedness_score": 8, "avg_answer_token_count": 45, "choices": null}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "What are the limitations of traditional static benchmarking methods in LLMs?", "question": "What are the primary limitations of traditional static benchmarking methods in LLMs that make them susceptible to data contamination?", "answer": "Their reliance on static datasets and metrics, lack of adaptability to evolving LLMs, and inability to account for data contamination risks.", "explanation": "The correct answer highlights the limitations of traditional static benchmarking methods, which rely on human-crafted datasets and metrics to evaluate LLMs. These methods are susceptible to data contamination, where benchmark data is inadvertently included in the training phase of LLMs, leading to an inflated and misleading assessment of their performance.", "question_token_count": 22, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 9, "question_groundedness_score": 10, "avg_answer_token_count": 26, "choices": null}
{"context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n", "topic": "What is the significance of the task categories mentioned in Table\u00a02 of the context?", "question": "What is the significance of the task categories mentioned in Table\u00a02 of the context, and how do they contribute to evaluating model performance in various domains?", "answer": "They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, and commonsense reasoning.", "explanation": "The task categories in Table\u00a02 are essential for assessing model performance across different domains, including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension.", "question_token_count": 31, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 7, "question_groundedness_score": 8, "avg_answer_token_count": 31, "choices": null}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "How can the concepts of exact and syntactic contamination be applied in real-world scenarios?", "question": "How can organizations that develop and deploy LLMs ensure that their models are not vulnerable to exact and syntactic contamination, and what strategies can they employ to mitigate its impact on performance?", "answer": "Organizations can employ data preprocessing techniques, such as tokenization and stemming, to reduce the likelihood of exact contamination. To mitigate syntactic contamination, they can use techniques like part-of-speech tagging and dependency parsing to identify and remove contaminated data points.", "explanation": "This question requires the domain expert to think critically about the applications of exact and syntactic contamination in real-world scenarios, considering the types of data that can be contaminated and potential strategies for mitigating its impact.", "question_token_count": 36, "answer_correctness_score": 8, "explanation_validity_score": 7, "question_clarity_score": 7, "question_groundedness_score": 8, "avg_answer_token_count": 49, "choices": null}
{"context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n", "topic": "What are the potential consequences of artificially inflating model performance or selecting biased evaluation criteria in LLM evaluations?", "question": "What are the potential consequences of artificially inflating model performance or selecting biased evaluation criteria in LLM evaluations?", "answer": "Inaccurate model performance assessments, perpetuation of biases, and potential misuse of benchmarking results.", "explanation": "The correct answer requires considering the potential risks and consequences of biased evaluation criteria, including the perpetuation of biases, inaccurate model performance assessments, and the potential for misuse of benchmarking results.", "question_token_count": 21, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 20, "choices": null}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "What are some potential future directions for research on canary strings and their applications in NLP?", "question": "What are some potential future directions for research on canary strings and their applications in NLP, considering the limitations and potential vulnerabilities of this method?", "answer": "Investigating alternative methods for detecting data contamination, developing more sophisticated canary string designs, and exploring the use of canary strings in conjunction with other mitigation techniques.", "explanation": "This question requires the domain expert to think critically about the current state of research on canary strings and identify potential areas for further exploration, considering the limitations and potential vulnerabilities of this method.", "question_token_count": 29, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 33, "choices": null}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "How can the use of canary strings be extended to other types of reliability models, such as Bayesian networks and Monte Carlo methods?", "question": "How can the use of canary strings be extended to other types of reliability models, such as Bayesian networks and Monte Carlo methods, to enhance data contamination detection?", "answer": "By incorporating canary strings into the design of these models, leveraging their unique characteristics to detect data contamination, and implementing strategies to address potential limitations.", "explanation": "The correct answer requires a deep understanding of canary strings and their application in mitigating data contamination, as well as the ability to generalize this concept to other reliability models.", "question_token_count": 33, "answer_correctness_score": 7, "explanation_validity_score": 6, "question_clarity_score": 6, "question_groundedness_score": 4, "avg_answer_token_count": 30, "choices": null}
{"context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n", "topic": "What are the key differences between temporal cutoff and rule-based generation approaches in dynamic benchmarking, and how do these differences impact the evaluation of LLM capabilities?", "question": "What are the key differences between temporal cutoff and rule-based generation approaches in dynamic benchmarking, and how do these differences impact the evaluation of LLM capabilities?", "answer": "Temporal cutoff follows a data collection process similar to static benchmarks, whereas rule-based generation creates novel evaluation data points using predefined rules. The key difference is that temporal cutoff relies on newly released information, whereas rule-based generation uses predefined rules, impacting the evaluation of LLM capabilities by affecting the type and quality of data used for evaluation.", "explanation": "This question requires the domain expert to understand the key differences between temporal cutoff and rule-based generation approaches, including the data collection process, creation of novel evaluation data points, and their impact on the evaluation of LLM capabilities.", "question_token_count": 30, "answer_correctness_score": 9, "explanation_validity_score": 8, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 65, "choices": null}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "How do recent challenges like AIME 2024 and CNMO 2024 refine the evaluation of model performance in math tasks?", "question": "How do recent challenges like AIME 2024 and CNMO 2024 refine the evaluation of model performance in math tasks?", "answer": "They introduce diverse and intricate math tasks to better assess model capacity and provide more accurate evaluations.", "explanation": "This question requires the reader to understand how recent challenges improve the evaluation of model performance in math tasks, which is a crucial aspect of LLM development.", "question_token_count": 27, "answer_correctness_score": 9, "explanation_validity_score": 6, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 19, "choices": null}
{"context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n", "topic": "What is the expected outcome of using dynamic benchmarks in the presence of data contamination, and how can it be mitigated?", "question": "How do the Collision Rate and Repeat Trials metrics help mitigate the effects of data contamination on the robustness of dynamic benchmarks in evaluating LLM capabilities?", "answer": "By quantifying the overlap between transformed versions of the benchmark dataset and the expected number of transformation trials required to fully regenerate an existing transformed dataset, Collision Rate and Repeat Trials help identify potential contamination and guide the development of more robust dynamic benchmarks.", "explanation": "The question requires the domain expert to understand the concept of collision and its implications on dynamic benchmarking, as well as the role of the proposed metrics in assessing the robustness of the benchmark.", "question_token_count": 28, "answer_correctness_score": 9, "explanation_validity_score": 8, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 47, "choices": null}
{"context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n", "topic": "What are some best practices for evaluating diversity in transformed datasets, and how can they be implemented in practice?", "question": "What are some common pitfalls or challenges associated with evaluating diversity in transformed datasets using reference-based metrics like BLEU scores, and how can they be addressed in practice?", "answer": "One common pitfall is over-reliance on BLEU scores, which may not capture nuances in diversity. To address this, it is recommended to use a combination of metrics, such as N-gram metrics, to get a more comprehensive understanding of diversity.", "explanation": "This question requires a deep understanding of the concepts of diversity, reference-based metrics, and their potential limitations in evaluating diversity in transformed datasets.", "question_token_count": 33, "answer_correctness_score": 8, "explanation_validity_score": 4, "question_clarity_score": 9, "question_groundedness_score": 6, "avg_answer_token_count": 52, "choices": null}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "What are some potential implications of data contamination on the ethics and fairness of LLMs, and how can canary strings help to mitigate these implications?", "question": "What are some potential implications of data contamination on the ethics and fairness of LLMs, and how can canary strings help to mitigate these implications?", "answer": "Data contamination can lead to biased and unfair outcomes in LLMs, compromising their ethics, and canary strings can help identify and filter out such instances, thereby mitigating these implications.", "explanation": "The correct answer requires a deep understanding of the implications of data contamination on the ethics and fairness of LLMs and how canary strings can help mitigate these implications.", "question_token_count": 30, "answer_correctness_score": 8, "explanation_validity_score": 6, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 36, "choices": null}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "Provide an example of syntactic contamination in the context of data contamination.", "question": "What is an example of syntactic contamination in the context of data contamination, where a test data point could be found in the training dataset after applying a syntactic transformation that preserves lexical meaning?", "answer": "A test data point with punctuation normalization, such as changing a period to an exclamation mark, that results in the same lexical meaning as a data point in the training dataset.", "explanation": "This question requires the understanding of syntactic contamination and its definition in the context of data contamination.", "question_token_count": 37, "answer_correctness_score": 9, "explanation_validity_score": 6, "question_clarity_score": 9, "question_groundedness_score": 10, "avg_answer_token_count": 36, "choices": null}
{"context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n", "topic": "Evaluate the importance of understanding the underlying mechanisms of LLMs in ensuring transparency and traceability in LLM-assisted transformations.", "question": "What is the significance of understanding the underlying mechanisms of LLMs in ensuring transparency and traceability in LLM-assisted transformations, and how does this impact the need for human involvement in the validation process?", "answer": "Understanding the underlying mechanisms of LLMs is crucial in ensuring transparency and traceability in LLM-assisted transformations, as it allows for the identification of potential biases and errors, and enables the development of more effective validation mechanisms.", "explanation": "The correct answer requires an understanding of the importance of interpretability in dynamic benchmarking and the differences between rule-based and LLM-assisted transformations. It also requires the ability to evaluate the trade-offs between the use of LLMs and the need for human involvement in the validation process.", "question_token_count": 39, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 43, "choices": null}
{"context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n", "topic": "Why is ongoing human involvement essential for maintaining LLM performance, and what are the potential risks associated with relying on automated methods?", "question": "What are the potential risks associated with relying solely on automated methods for maintaining LLM performance, and how can ongoing human involvement mitigate these risks?", "answer": "Ongoing human involvement is essential to mitigate the risks of data contamination and ensure the accuracy and reliability of LLM performance.", "explanation": "The correct answer requires the domain expert to consider the potential risks of data contamination, the importance of verification, and the need for ongoing human effort in maintaining LLM performance.", "question_token_count": 28, "answer_correctness_score": 8, "explanation_validity_score": 8, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 24, "choices": null}
{"context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n", "topic": "How does data contamination occur in live benchmarks, and what are the potential consequences for LLM performance?", "question": "How does the risk of data contamination in live benchmarks affect the accuracy of Large Language Model (LLM) performance evaluations, and what are the potential consequences for the development of reliable LLM metrics?", "answer": "Data contamination can lead to biased or inaccurate LLM performance metrics, undermining the reliability of LLM evaluations and hindering the development of trustworthy LLM metrics.", "explanation": "This question requires the domain expert to analyze the potential consequences of data contamination in live benchmarks and the implications for LLM performance, demonstrating a deep understanding of the limitations of live benchmarks and the importance of verification.", "question_token_count": 39, "answer_correctness_score": 9, "explanation_validity_score": 8, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 30, "choices": null}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "Discuss the importance of diverse benchmarks in evaluating AI models' language understanding abilities comprehensively.", "question": "What are the potential consequences of relying on a single benchmark to evaluate AI models' language understanding abilities, and how do diverse benchmarks mitigate these consequences?", "answer": "A single benchmark may not capture the full spectrum of a model's language understanding abilities, leading to inaccurate or incomplete evaluations. Diverse benchmarks mitigate this consequence by providing a more comprehensive understanding of a model's capabilities.", "explanation": "This question encourages the respondent to think critically about the limitations of a single benchmark and the benefits of using diverse benchmarks in evaluating AI models.", "question_token_count": 30, "answer_correctness_score": 9, "explanation_validity_score": 7, "question_clarity_score": 9, "question_groundedness_score": 6, "avg_answer_token_count": 41, "choices": null}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "How do IFEval and InfoBench datasets differ in their simulation of real-world scenarios requiring clear, step-by-step guidance?", "question": "How do IFEval and InfoBench datasets differ in their simulation of real-world scenarios requiring clear, step-by-step guidance, and what implications does this have for evaluating a model's ability to comprehend and execute detailed directives?", "answer": "IFEval focuses on Chinese instructions, whereas InfoBench simulates real-world scenarios with a broader range of languages, including English.", "explanation": "The correct answer highlights the differences between IFEval and InfoBench datasets in simulating real-world scenarios, which is crucial for evaluating a model's ability to comprehend and execute detailed directives.", "question_token_count": 43, "answer_correctness_score": 7, "explanation_validity_score": 8, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 25, "choices": null}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "What are the implications of using frameworks to evaluate LLMs on tasks such as executing SQL queries and solving P and NP problems?", "question": "What are the potential limitations of relying solely on frameworks like S3Eval, DyVal, and NPHardEval to evaluate the reasoning ability of Large Language Models, and how might these limitations impact their deployment in real-world applications?", "answer": "These frameworks may not capture the full range of real-world scenarios, potentially leading to overconfidence in LLMs' performance and deployment in situations where they are not adequately prepared.", "explanation": "This question requires the test-taker to think critically about the implications of using frameworks to evaluate LLMs, considering both the benefits and potential limitations of these frameworks.", "question_token_count": 47, "answer_correctness_score": 8, "explanation_validity_score": 6, "question_clarity_score": 8, "question_groundedness_score": 8, "avg_answer_token_count": 35, "choices": null}
{"context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n", "topic": "Why is manual verification costly and challenging in dynamic benchmarking, and how can interpretable transformations help alleviate this issue?", "question": "How do interpretable transformations address the challenges of manual verification in dynamic benchmarking, and what are the implications for LLM-assisted transformations?", "answer": "Interpretable transformations reduce the need for manual validation by providing transparency and traceability, which can be particularly important in LLM-assisted transformations that may require additional mechanisms like explainability tools or human-in-the-loop validation.", "explanation": "The question requires the domain expert to understand the challenges of manual verification and the role of interpretable transformations in alleviating these challenges, as well as the nuances of LLM-assisted transformations.", "question_token_count": 26, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 43, "choices": null}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "How can the domain expert identify and address data contamination in their own work or research?", "question": "How can the domain expert develop effective strategies to prevent exact and syntactic data contamination in their LLM training data, ensuring the validity of performance measurements?", "answer": "By implementing data filtering techniques, using diverse training datasets, and ensuring documentation secrecy, among other measures.", "explanation": "This question requires the domain expert to think critically about the implications of data contamination and develop practical strategies to prevent it.", "question_token_count": 29, "answer_correctness_score": 8, "explanation_validity_score": 6, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 21, "choices": null}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "What are the implications of using outdated data in constructing benchmarks, and how does it affect the reliability of the models?", "question": "What are the potential consequences of using outdated data in benchmark construction, and how can it compromise the reliability of Large Language Models?", "answer": "Data contamination, decreased model performance, and inaccurate results.", "explanation": "This question requires the test-taker to understand the implications of temporal cutoff in LLMs and its effects on model reliability, as discussed in the context.", "question_token_count": 26, "answer_correctness_score": 8, "explanation_validity_score": 7, "question_clarity_score": 8, "question_groundedness_score": 8, "avg_answer_token_count": 12, "choices": null}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "How can model developers use canary strings to identify and filter out instances of data contamination in their models?", "question": "How can canary strings be used to identify and filter out instances of data contamination in large language models, and what are the limitations of this approach?", "answer": "Canary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination, but their effectiveness depends on model trainers being aware of and responsive to these markers, and they may not work if a developer aims to leak benchmarking data to boost scores.", "explanation": "This question requires the domain expert to understand the concept of canary strings and their role in mitigating data contamination, as well as their limitations and potential effectiveness.", "question_token_count": 31, "answer_correctness_score": 9, "explanation_validity_score": 7, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 55, "choices": null}
{"context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n", "topic": "How do the massive, diverse datasets used in pre-training phase contribute to the risk of data contamination?", "question": "How do the massive, diverse datasets used in the pre-training phase of LLMs contribute to the risk of data contamination, and what are the implications of this risk for the accuracy and reliability of these models?", "answer": "The massive, diverse datasets used in the pre-training phase of LLMs increase the risk of data contamination by potentially including evaluation data, which can lead to biased or inaccurate model performance. This risk is exacerbated by the scale and complexity of training corpora, making it difficult to entirely exclude evaluation data.", "explanation": "This question requires the domain expert to think critically about the data contamination risks in LLMs and the ways in which pre-training datasets contribute to this risk. It also requires an understanding of the implications of this risk for the accuracy and reliability of these models.", "question_token_count": 42, "answer_correctness_score": 9, "explanation_validity_score": 8, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 61, "choices": null}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "How can the effectiveness of canary strings be evaluated, and what are some potential metrics for measuring their success?", "question": "How can the effectiveness of canary strings be evaluated, and what are some potential metrics for measuring their success?", "answer": "The effectiveness of canary strings can be evaluated using metrics such as the number of times the canary string is detected in the model's output, the percentage of instances where the model correctly identifies the canary string, and the model's performance on tasks that require generalization rather than memorization.", "explanation": "This question requires the domain expert to critically evaluate the effectiveness of canary strings and consider potential metrics for measuring its success. It requires a nuanced understanding of the concept and its limitations.", "question_token_count": 23, "answer_correctness_score": 8, "explanation_validity_score": 8, "question_clarity_score": 8, "question_groundedness_score": 7, "avg_answer_token_count": 57, "choices": null}
{"context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n", "topic": "How do LLM evaluation methods, such as KIEval, generate follow-up questions based on the evaluated model's response?", "question": "How do LLM evaluation methods, such as KIEval, utilize the evaluated model's response to generate follow-up questions, and what are the key factors that influence this process?", "answer": "KIEval generates follow-up questions based on the previous topic and the examined LLM's response, using a combination of natural language processing and machine learning algorithms to identify relevant information and generate targeted questions.", "explanation": "This question requires the domain expert to demonstrate their understanding of the underlying principles of LLM evaluation methods and their ability to critically analyze the complex concepts involved.", "question_token_count": 35, "answer_correctness_score": 8, "explanation_validity_score": 4, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 40, "choices": null}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "How can the use of canary strings contribute to the development of more robust and reliable AI models?", "question": "What role can canary strings play in ensuring the development of more robust and reliable AI models, and how do they contribute to mitigating data contamination risks?", "answer": "Canary strings can serve as a form of data contamination detection, enabling model developers to identify and filter out instances of memorization, ultimately leading to more reliable AI models.", "explanation": "The correct answer highlights the potential benefits of canary strings in detecting data contamination and promoting more robust AI model development.", "question_token_count": 31, "answer_correctness_score": 9, "explanation_validity_score": 8, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 35, "choices": null}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "How can LLM researchers and developers work together to develop more robust benchmarking methods that account for contamination and ensure accurate evaluation of model capabilities?", "question": "How can LLM researchers and developers work together to develop more robust benchmarking methods that account for contamination and ensure accurate evaluation of model capabilities?", "answer": "By implementing data validation and verification techniques, using diverse and representative datasets, and engaging in collaborative development and testing processes.", "explanation": "This question requires the domain expert to consider the potential consequences of contaminated benchmarks and the importance of developing robust benchmarking methods.", "question_token_count": 27, "answer_correctness_score": 9, "explanation_validity_score": 8, "question_clarity_score": 8, "question_groundedness_score": 8, "avg_answer_token_count": 24, "choices": null}
{"context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n", "topic": "What are the key principles for designing benchmarking frameworks that prioritize fairness, accountability, and privacy in AI systems?", "question": "What are the essential criteria for ensuring that benchmarking frameworks for AI systems do not inadvertently perpetuate biases or harm certain user groups?", "answer": "Fairness, accountability, transparency, data protection, and model interpretability.", "explanation": "This question requires the respondent to consider the potential risks of bias and harm associated with AI benchmarking and to identify the key principles for mitigating these risks.", "question_token_count": 26, "answer_correctness_score": 9, "explanation_validity_score": 8, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 16, "choices": null}
{"context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n", "topic": "How do rule-based or manually crafted transformations ensure correctness in dynamic benchmarking, and what are their limitations?", "question": "How do rule-based or manually crafted transformations ensure correctness in dynamic benchmarking, and what are their limitations?", "answer": "Rule-based transformations ensure correctness by relying on predefined rules, but they may lack adaptability and may not account for complex scenarios, requiring additional mechanisms like human-in-the-loop validation.", "explanation": "This question requires the domain expert to demonstrate an understanding of the strengths and weaknesses of rule-based transformations in ensuring correctness in dynamic benchmarking.", "question_token_count": 21, "answer_correctness_score": 8, "explanation_validity_score": 7, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 35, "choices": null}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "What are the implications of contaminated benchmarks on model comparisons, deployment decisions, and policy-making, and how can these be mitigated?", "question": "What are the potential consequences of relying on contaminated benchmarks for model comparisons, deployment decisions, and policy-making in the field of LLM research?", "answer": "Misleading conclusions about progress in LLM research, incorrect model comparisons, and poor deployment decisions.", "explanation": "This question encourages the evaluation of the context's emphasis on the significance of contamination and its implications on real-world applications.", "question_token_count": 28, "answer_correctness_score": 9, "explanation_validity_score": 4, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 19, "choices": null}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "How can the use of canary strings be extended to other types of models, such as transformer models and recurrent neural networks?", "question": "How can the use of canary strings be extended to other types of models, such as transformer models and recurrent neural networks, while addressing their limitations?", "answer": "By incorporating additional unique tokens that are specific to the architecture and characteristics of each model type, and by implementing robust monitoring and filtering mechanisms to detect and prevent data contamination.", "explanation": "The correct answer requires an understanding of the concept of canary strings and their limitations, as well as the ability to generalize this concept to other types of models. The question is designed to encourage critical thinking and deeper understanding of the topic.", "question_token_count": 31, "answer_correctness_score": 8, "explanation_validity_score": 8, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 34, "choices": null}
{"context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n", "topic": "What are the key differences between rule-based and template-based methods for generating test cases, and how do they contribute to LLM development?", "question": "What are the key differences between rule-based and template-based methods for generating test cases, and how do they contribute to LLM development?", "answer": "Rule-based methods rely on predefined rules to generate test cases, resulting in low collision probability, whereas template-based methods use query templates with placeholder variables to generate diverse problem instances.", "explanation": "This question requires the reader to analyze the context and identify the key differences between rule-based and template-based methods for generating test cases, as well as their contributions to LLM development.", "question_token_count": 27, "answer_correctness_score": 9, "explanation_validity_score": 7, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 35, "choices": null}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "How can the effectiveness of canary strings be evaluated in other areas of AI, and what are some potential metrics for measuring their success?", "question": "What are some potential metrics for evaluating the effectiveness of canary strings in detecting data contamination in other areas of AI, and how can model developers ensure their responsiveness to these markers?", "answer": "Metrics for evaluating effectiveness include precision, recall, and F1 score. Model developers can ensure responsiveness by implementing regular audits, providing clear guidelines for model trainers, and incorporating canary strings into the development process.", "explanation": "The correct answer requires a deep understanding of the concept of canary strings and their application in mitigating data contamination. The question demands critical thinking and evaluation of the effectiveness of this approach in other areas of AI.", "question_token_count": 36, "answer_correctness_score": 8, "explanation_validity_score": 8, "question_clarity_score": 6, "question_groundedness_score": 4, "avg_answer_token_count": 42, "choices": null}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "What are the underlying theories that support the development of dynamic benchmarks as a potential solution to the limitations of static benchmarks?", "question": "What are the underlying theories that support the development of dynamic benchmarks as a potential solution to the limitations of static benchmarks?", "answer": "The underlying theories include the concept of adaptive testing, the need for continuous evaluation, and the importance of mitigating data contamination risks.", "explanation": "This question requires the domain expert to understand the theoretical foundations of dynamic benchmarks and how they address the limitations of static benchmarks.", "question_token_count": 24, "answer_correctness_score": 8, "explanation_validity_score": 7, "question_clarity_score": 7, "question_groundedness_score": 6, "avg_answer_token_count": 26, "choices": null}
{"context": "###### Safety\nSafety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content. Datasets such as RealToxicityPrompts\u00a0(Gehman et\u00a0al., 2020) and ToxiGen\u00a0(Hartvigsen et\u00a0al., 2022) assess resilience against producing harmful outputs.\n\nBy providing a controlled environment to measure these aspects, safety benchmarks play a critical role in guiding the development of models that are not only powerful outputs but also responsible and trustworthy for real-world applications.\n\n\n###### Language\nLanguage benchmarks assess the LLMs\u2019 proficiency in specific languages.\n\nGLUE\u00a0(Wang, 2018) and SuperGLUE\u00a0(Wang et\u00a0al., 2019) cover tasks from sentiment analysis to language inference, while CLUE\u00a0(Xu et\u00a0al., 2020) targets Chinese language.\n\nTypo-fixing\u00a0(Suzgun et\u00a0al., 2022) is also widely used.\n\n\n###### Reading Comprehension\nReading comprehension tasks test a model\u2019s ability to extract and infer information from text. Benchmarks like SQuAD\u00a0(Rajpurkar et\u00a0al., 2018), QuAC\u00a0(Choi et\u00a0al., 2018), and BoolQ\u00a0(Clark et\u00a0al., 2019) challenge models to understand passages and draw logical conclusions.\n", "topic": "How does the variety of tasks covered by language benchmarks, such as sentiment analysis and language inference, impact the development of LLMs?", "question": "How do the diverse tasks covered by language benchmarks, such as sentiment analysis and language inference, contribute to the development of LLMs, and what are the potential implications for their real-world applications?", "answer": "The diverse tasks covered by language benchmarks provide a comprehensive evaluation of LLMs' capabilities, enabling developers to identify areas for improvement and fine-tune the models for specific applications, ultimately leading to more robust and reliable models that can be safely deployed in real-world scenarios.", "explanation": "This question encourages the domain expert to critically reflect on the relationship between language benchmarks and LLM development, considering the variety of tasks covered and their potential impact on the models' capabilities.", "question_token_count": 39, "answer_correctness_score": 9, "explanation_validity_score": 8, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 53, "choices": null}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "What are some potential applications of canary strings in other benchmark datasets, and how can they be developed further?", "question": "What are some potential applications of canary strings in other benchmark datasets, and how can they be developed further?", "answer": "Canary strings can be used in various benchmark datasets to identify and filter out instances of data contamination, and they can be developed further by incorporating more sophisticated markers and improving model trainers' awareness of these markers.", "explanation": "This question requires a deep understanding of the concept of canary strings and their potential applications in enhancing the robustness of benchmark datasets.", "question_token_count": 23, "answer_correctness_score": 8, "explanation_validity_score": 7, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 42, "choices": null}
{"context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n", "topic": "What are the key components of the seed dataset used in static benchmarking?", "question": "What are the three key components of the seed dataset in static benchmarking?", "answer": "Input prompts, expected outputs, and scoring function.", "explanation": "The correct answer requires identifying the input prompts, expected outputs, and scoring function as the essential components of the seed dataset.", "question_token_count": 15, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 11, "choices": null}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "Compare the approaches used in PIQA, SIQA, HellaSwag, and WinoGrande in assessing a model's intuitive reasoning skills from multiple perspectives.", "question": "How do PIQA, SIQA, HellaSwag, and WinoGrande differ in their approaches to assessing a model's intuitive reasoning skills from multiple perspectives?", "answer": "PIQA focuses on practical reasoning, SIQA emphasizes social understanding, HellaSwag tests common sense, and WinoGrande requires reasoning with everyday knowledge.", "explanation": "These benchmarks evaluate models in various ways, but PIQA focuses on practical reasoning, SIQA emphasizes social understanding, HellaSwag tests common sense, and WinoGrande requires reasoning with everyday knowledge.", "question_token_count": 34, "answer_correctness_score": 8, "explanation_validity_score": 7, "question_clarity_score": 6, "question_groundedness_score": 6, "avg_answer_token_count": 33, "choices": null}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "What are some potential challenges and limitations associated with the use of canary strings in other areas of security, and how can they be addressed?", "question": "What are some potential challenges and limitations associated with the use of canary strings in other areas of security, and how can they be addressed?", "answer": "Canary strings may be vulnerable to exploitation by developers aiming to leak benchmarking data, and their effectiveness depends on model trainers being aware of and responsive to these markers. Addressing these limitations requires implementing robust security measures and ensuring that model trainers are educated about the potential risks and challenges associated with canary strings.", "explanation": "This question requires the domain expert to critically analyze the limitations of canary strings and explore potential challenges and solutions in other areas of security, demonstrating a deep understanding of the concept and its broader implications.", "question_token_count": 29, "answer_correctness_score": 8, "explanation_validity_score": 7, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 60, "choices": null}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "How do syntactic transformations relate to data contamination, and what are the implications for LLM research and development?", "question": "How do the implications of considering syntactic transformations as a form of contamination in LLM research relate to the development of more robust benchmarks?", "answer": "It highlights the need for a more nuanced approach to identifying and preventing contamination in benchmarking LLMs, ensuring that evaluations accurately reflect a model's true capabilities.", "explanation": "This question requires a deep understanding of the context, specifically the debate surrounding syntactic transformations and their impact on LLM research and development.", "question_token_count": 26, "answer_correctness_score": 8, "explanation_validity_score": 6, "question_clarity_score": 7, "question_groundedness_score": 10, "avg_answer_token_count": 30, "choices": null}
{"context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n", "topic": "What are some common challenges associated with measuring diversity in transformed datasets, and how can they be addressed?", "question": "What are some potential challenges associated with measuring diversity in transformed datasets, and how can they be addressed?", "answer": "Some potential challenges include the complexity of the mathematical formulas used to calculate diversity, the need for a robust and reliable function to measure diversity, and the potential for bias in the diversity measurement. To address these challenges, one could use more advanced mathematical techniques, such as machine learning algorithms, to improve the accuracy and reliability of the diversity measurement, or use multiple functions to measure diversity and combine the results to reduce bias.", "explanation": "This question requires the domain expert to think critically about the potential challenges associated with measuring diversity in transformed datasets and provide a solution or suggestion for addressing these challenges.", "question_token_count": 21, "answer_correctness_score": 8, "explanation_validity_score": 6, "question_clarity_score": 8, "question_groundedness_score": 6, "avg_answer_token_count": 83, "choices": null}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "Compare and contrast the evaluation methods used in NPHardEval and Xie et al. (2024) for assessing LLMs' reasoning abilities on P and NP problems.", "question": "What are the key differences and similarities between the evaluation methods used in NPHardEval and Xie et al. (2024) for assessing LLMs' reasoning abilities on P and NP problems?", "answer": "NPHardEval uses well-known P and NP problems, such as the Traveling Salesman Problem (TSP), while Xie et al. (2024) uses randomly generated reasoning graphs, including Knights and Knaves puzzles.", "explanation": "This question requires the respondent to analyze the different approaches used in NPHardEval and Xie et al. (2024) and identify the key differences and similarities between them.", "question_token_count": 40, "answer_correctness_score": 7, "explanation_validity_score": 6, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 46, "choices": null}
{"context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "How do static benchmarking schemes fail to provide a transparent yet faithful evaluation of LLMs?", "question": "What are the primary limitations of static benchmarking schemes in evaluating LLMs, making them inadequate for a transparent yet faithful evaluation?", "answer": "Their inability to provide full access to the training dataset, assumptions about model behavior not holding across different models and tasks, and challenges in providing a transparent yet faithful evaluation.", "explanation": "This question encourages critical engagement with the context and invites deeper understanding of the challenges faced by static benchmarking schemes.", "question_token_count": 25, "answer_correctness_score": 8, "explanation_validity_score": 4, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 34, "choices": null}
{"context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n", "topic": "What are the potential consequences of neglecting to evaluate the diversity of transformed datasets, and how can they be mitigated?", "question": "What are the potential consequences of neglecting to evaluate the diversity of transformed datasets, and how can they be mitigated?", "answer": "Neglecting to evaluate diversity can lead to suboptimal performance, low data quality, and biased results. Mitigation strategies include using diversity metrics, such as N-gram metrics or reference-based metrics like BLEU scores, to evaluate and ensure the quality of transformed data.", "explanation": "The correct answer requires understanding the importance of diversity in data transformation and its potential impact on data quality and performance.", "question_token_count": 24, "answer_correctness_score": 8, "explanation_validity_score": 7, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 54, "choices": null}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "How can the performance of LLMs on various tasks be improved using the frameworks mentioned in the context?", "question": "How can the frameworks S3Eval, DyVal, and NPHardEval be used in conjunction with each other to improve the performance of LLMs on a wide range of tasks?", "answer": "By leveraging the strengths of each framework, such as S3Eval's ability to assess LLMs on SQL queries, DyVal's use of DAGs to evaluate reasoning capabilities, and NPHardEval's assessment of LLMs on well-known P and NP problems, the frameworks can be used in conjunction with each other to improve LLM performance on a wide range of tasks.", "explanation": "The question requires the ability to synthesize the information from the three frameworks and understand how they can be combined to improve LLM performance.", "question_token_count": 38, "answer_correctness_score": 8, "explanation_validity_score": 7, "question_clarity_score": 7, "question_groundedness_score": 10, "avg_answer_token_count": 75, "choices": null}
{"context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n", "topic": "How do dynamic transformations affect the complexity of the seed dataset, and what are the implications for LLM performance?", "question": "How can the lack of generalizability of existing complexity metrics impact the stability of dynamic benchmarking methods for Large Language Models?", "answer": "The lack of generalizability of existing complexity metrics can lead to inconsistent results and decreased stability in dynamic benchmarking methods, making it challenging to accurately evaluate LLM performance.", "explanation": "The correct answer is based on the understanding that existing complexity metrics are often domain-specific and do not generalize well across different applications, which can affect the stability of dynamic benchmarking methods.", "question_token_count": 24, "answer_correctness_score": 9, "explanation_validity_score": 8, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 32, "choices": null}
{"context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n", "topic": "What are the challenges of evaluating dynamic benchmarks?", "question": "What is the primary concern with traditional static methods for evaluating dynamic benchmarks in the context of Large Language Model benchmarking?", "answer": "The primary concern is that traditional static methods fall short in preventing data contamination, leading to inefficiencies in evaluation.", "explanation": "This question invites deeper understanding of the challenges associated with evaluating dynamic benchmarks and encourages reflection on the importance of standardized criteria for these benchmarks.", "question_token_count": 23, "answer_correctness_score": 8, "explanation_validity_score": 4, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 23, "choices": null}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "How can the use of canary strings be extended to other types of security models, such as anomaly detection and predictive maintenance?", "question": "How can the use of canary strings be extended to other types of security models, such as anomaly detection and predictive maintenance, to effectively mitigate data contamination?", "answer": "By adapting the canary string approach to incorporate unique tokens that are relevant to the specific security model, and by implementing additional monitoring and filtering mechanisms to detect and prevent data contamination.", "explanation": "This question requires the domain expert to think critically about the applicability of canary strings beyond the context of static benchmarking and consider potential extensions to other types of security models.", "question_token_count": 32, "answer_correctness_score": 8, "explanation_validity_score": 7, "question_clarity_score": 6, "question_groundedness_score": 4, "avg_answer_token_count": 36, "choices": null}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "How can the effectiveness of canary strings be evaluated in other areas of explainability, and what are some potential metrics for measuring their success?", "question": "How can the effectiveness of canary strings be evaluated in other areas of explainability, and what are some potential metrics for measuring their success?", "answer": "The effectiveness of canary strings can be evaluated using metrics such as model performance on out-of-distribution data, robustness to adversarial attacks, and ability to generalize to new tasks. Potential metrics for measuring success include the number of canary strings detected, the percentage of model outputs containing canary strings, and the accuracy of model performance on tasks that require generalization.", "explanation": "This question requires domain experts to critically evaluate the Canary String method and propose potential metrics for measuring its success in other areas of explainability.", "question_token_count": 29, "answer_correctness_score": 8, "explanation_validity_score": 8, "question_clarity_score": 6, "question_groundedness_score": 2, "avg_answer_token_count": 71, "choices": null}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "What are some potential implications of data contamination on the scalability and efficiency of AI models, and how can canary strings help to mitigate these implications?", "question": "What are some potential implications of data contamination on the scalability and efficiency of AI models, and how can canary strings help to mitigate these implications?", "answer": "Data contamination can lead to inefficient model performance, decreased scalability, and reduced generalizability, but canary strings can help identify and filter out contaminated data, thereby mitigating these implications.", "explanation": "This question requires a deep understanding of the topic, including the risks of data contamination and the effectiveness of mitigation strategies.", "question_token_count": 30, "answer_correctness_score": 8, "explanation_validity_score": 6, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 36, "choices": null}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "How can the use of canary strings be integrated into existing LLM development pipelines to ensure that models are trained on high-quality data?", "question": "How can the use of canary strings be effectively integrated into existing LLM development pipelines to ensure that models are trained on high-quality data, considering the limitations of this method and potential developer manipulation?", "answer": "By incorporating canary strings as a proactive measure, employing transparent and accountable model training practices, and regularly auditing model outputs for signs of data contamination.", "explanation": "This question requires the synthesis of high-level general understanding above and beyond the specific context, considering the limitations of canary strings and potential developer manipulation.", "question_token_count": 39, "answer_correctness_score": 8, "explanation_validity_score": 8, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 30, "choices": null}
{"context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n", "topic": "What are the potential consequences of neglecting complexity control in dynamic benchmarks?", "question": "What are the potential consequences of neglecting complexity control in dynamic benchmarks, and how might this impact the reliability and validity of LLM evaluation?", "answer": "Inadequate complexity control in dynamic benchmarks may lead to inefficiencies in evaluation, compromise the reliability and validity of LLM evaluation, and undermine the development of effective benchmarks.", "explanation": "This question encourages the domain expert to consider the implications of complexity control on dynamic benchmarks, reflecting on the potential consequences of neglecting this aspect.", "question_token_count": 27, "answer_correctness_score": 8, "explanation_validity_score": 7, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 34, "choices": null}
{"context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n", "topic": "What are the challenges of accurately measuring complexity in benchmark datasets, and how do existing metrics fall short?", "question": "How do existing complexity metrics, such as DyVal, fall short in accurately measuring complexity in benchmark datasets, and what are the implications of these limitations for dynamic benchmarking?", "answer": "Existing complexity metrics are often domain-specific and do not generalize well across different applications, making it challenging to accurately measure complexity in benchmark datasets.", "explanation": "The answer requires an understanding of the limitations of existing complexity metrics and their implications for dynamic benchmarking.", "question_token_count": 34, "answer_correctness_score": 9, "explanation_validity_score": 7, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 28, "choices": null}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "What are some potential applications of canary strings in other areas of reliability, such as model uncertainty and robustness analysis?", "question": "What are some potential applications of canary strings in other areas of reliability, such as model uncertainty and robustness analysis?", "answer": "Canary strings could be used to detect and prevent data contamination in other areas of reliability, such as model uncertainty and robustness analysis, by serving as markers for model memorization rather than generalization.", "explanation": "The correct answer requires an understanding of the concept of canary strings and their potential uses in mitigating data contamination, as well as an ability to think critically about the implications of this concept for model reliability and robustness analysis.", "question_token_count": 24, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 39, "choices": null}
{"context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n", "topic": "How does the proposed scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) contribute to the assessment of correctness in dynamic benchmarking?", "question": "How does the proposed scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) contribute to the assessment of correctness in dynamic benchmarking?", "answer": "The scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) measures the expected alignment between the transformed dataset's outputs and their corresponding ground truth values, ensuring an objective assessment of correctness.", "explanation": "The question aims to assess the domain expert's understanding of the scoring function's role in evaluating the correctness of dynamic benchmarks.", "question_token_count": 40, "answer_correctness_score": 9, "explanation_validity_score": 7, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 51, "choices": null}
{"context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n", "topic": "Confidential computing and secure multi-party computation, as used in TRUCE (Chandran et al., 2024), can enable private benchmarking and protect sensitive data.", "question": "What are the primary limitations of using confidential computing and secure multi-party computation for private benchmarking, as demonstrated in TRUCE (Chandran et al., 2024)?", "answer": "Dependence on strong key management and vulnerability to compromised encryption or private keys.", "explanation": "The question requires the domain expert to critically evaluate the vulnerabilities of these methods, such as dependence on strong key management and potential exposure to compromised encryption or private keys.", "question_token_count": 35, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 16, "choices": null}
{"context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "What are the primary limitations of post-hot detection methods in accessing the training dataset?", "question": "What are the primary limitations of post-hot detection methods in accessing the training dataset, leading to challenges in detecting overlap between training and evaluation data?", "answer": "Legal and privacy constraints.", "explanation": "This question requires the domain expert to identify the central idea of the context and understand the implications of the limitations of post-hot detection methods.", "question_token_count": 29, "answer_correctness_score": 10, "explanation_validity_score": 7, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 6, "choices": null}
{"context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n", "topic": "What are some of the challenges associated with using static benchmarks for model evaluation?", "question": "What are some potential pitfalls of relying solely on static benchmarks for model evaluation, and how might these limitations affect the development and deployment of language models?", "answer": "Static benchmarks may not capture the nuances of real-world applications, may be biased towards specific tasks or datasets, and may not account for the complexities of human language, leading to potential misalignment between benchmark performance and actual model performance in practical scenarios.", "explanation": "This question requires the domain expert to consider the potential biases and limitations of static benchmarks, as well as their potential impact on model development and deployment.", "question_token_count": 30, "answer_correctness_score": 8, "explanation_validity_score": 7, "question_clarity_score": 9, "question_groundedness_score": 2, "avg_answer_token_count": 48, "choices": null}
{"context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n", "topic": "What is the purpose of using specific functions, such as N-gram metrics or BLEU scores, to measure diversity in transformed datasets?", "question": "What is the purpose of using specific functions, such as N-gram metrics or BLEU scores, to measure diversity in transformed datasets, and how do these functions influence the characteristics of the transformed dataset?", "answer": "To evaluate the variation and differences between the transformed dataset and the seed dataset, and to provide valuable insights into the characteristics of the transformed dataset.", "explanation": "The question requires the domain expert to understand the concept of diversity in transformed datasets and the specific functions used to measure it. The correct answer should demonstrate a deep understanding of the implications of using these functions and their influence on the transformed dataset.", "question_token_count": 40, "answer_correctness_score": 7, "explanation_validity_score": 6, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 29, "choices": null}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "What are some potential applications of canary strings in other areas of AI, such as computer vision and speech recognition?", "question": "How might the concept of canary strings be adapted and applied to detect overfitting in computer vision models, particularly in tasks like image classification and object detection?", "answer": "By incorporating unique, task-specific tokens or markers that are designed to be memorized by models when they overfit, allowing for the detection of overfitting in computer vision models.", "explanation": "The answer requires an understanding of the underlying theory of canary strings and their potential applications in other areas of AI, such as computer vision. The domain expert needs to critically reflect on the concept of canary strings and its implications beyond the context of language models.", "question_token_count": 32, "answer_correctness_score": 8, "explanation_validity_score": 7, "question_clarity_score": 8, "question_groundedness_score": 4, "avg_answer_token_count": 36, "choices": null}
{"context": "###### Safety\nSafety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content. Datasets such as RealToxicityPrompts\u00a0(Gehman et\u00a0al., 2020) and ToxiGen\u00a0(Hartvigsen et\u00a0al., 2022) assess resilience against producing harmful outputs.\n\nBy providing a controlled environment to measure these aspects, safety benchmarks play a critical role in guiding the development of models that are not only powerful outputs but also responsible and trustworthy for real-world applications.\n\n\n###### Language\nLanguage benchmarks assess the LLMs\u2019 proficiency in specific languages.\n\nGLUE\u00a0(Wang, 2018) and SuperGLUE\u00a0(Wang et\u00a0al., 2019) cover tasks from sentiment analysis to language inference, while CLUE\u00a0(Xu et\u00a0al., 2020) targets Chinese language.\n\nTypo-fixing\u00a0(Suzgun et\u00a0al., 2022) is also widely used.\n\n\n###### Reading Comprehension\nReading comprehension tasks test a model\u2019s ability to extract and infer information from text. Benchmarks like SQuAD\u00a0(Rajpurkar et\u00a0al., 2018), QuAC\u00a0(Choi et\u00a0al., 2018), and BoolQ\u00a0(Clark et\u00a0al., 2019) challenge models to understand passages and draw logical conclusions.\n", "topic": "What are the implications of using LLMs in real-world applications without proper evaluation and safety benchmarks?", "question": "What are the potential consequences of using LLMs in real-world applications without proper evaluation and safety benchmarks?", "answer": "The potential consequences may include the development and deployment of models that are not aligned with societal values and norms, leading to harm or negative impact on individuals or communities.", "explanation": "This question requires the domain expert to critically evaluate the implications of using LLMs in real-world applications and to consider the potential risks associated with the lack of safety benchmarks.", "question_token_count": 21, "answer_correctness_score": 9, "explanation_validity_score": 8, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 33, "choices": null}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "What are the underlying theories driving the development of language understanding benchmarks and datasets like those mentioned in the context?", "question": "What are the key theoretical frameworks and assumptions driving the development of language understanding benchmarks and datasets, such as those mentioned in the context?", "answer": "The key theoretical frameworks and assumptions driving the development of language understanding benchmarks and datasets include the need to simulate real-world scenarios, assess a model's ability to apply its knowledge in practical settings, and create more realistic and challenging evaluation scenarios.", "explanation": "The correct answer requires the domain expert to identify the underlying theories driving the development of language understanding benchmarks and datasets, such as the need to simulate real-world scenarios and assess a model's ability to apply its knowledge in practical settings.", "question_token_count": 27, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 46, "choices": null}
{"context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n", "topic": "What are the proposed metrics (Collision Rate and Repeat Trials) for quantifying collision, and how do they help assess the effectiveness of dynamic benchmarks?", "question": "What are the proposed metrics (Collision Rate and Repeat Trials) for quantifying collision in dynamic benchmarking, and how do they help assess the effectiveness of dynamic benchmarks?", "answer": "Collision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating potential contamination among trials, while Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset, providing insight into the benchmark\u2019s ability to produce novel variations.", "explanation": "The correct answer requires an understanding of the proposed metrics and their significance in evaluating the robustness of dynamic benchmarks against data contamination.", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 56, "choices": null}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "What is the significance of contamination in Large Language Model benchmarking, and how does it impact the validity of benchmarks?", "question": "How does data contamination in Large Language Model benchmarking impact the validity of benchmarks, and what are the potential consequences of relying on contaminated benchmarks in model comparisons, deployment decisions, and policy-making?", "answer": "Contaminated benchmarks can lead to overestimation of a model's true capabilities, undermine the validity of benchmarks, and affect model comparisons, deployment decisions, and policy-making.", "explanation": "The correct answer requires a deep understanding of the significance of contamination in LLM benchmarking and its impact on the validity of benchmarks, as well as the potential consequences of relying on contaminated benchmarks.", "question_token_count": 38, "answer_correctness_score": 9, "explanation_validity_score": 8, "question_clarity_score": 9, "question_groundedness_score": 10, "avg_answer_token_count": 34, "choices": null}
{"context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n", "topic": "What is the purpose of the Cost function in the mathematical formulation of scalability?", "question": "What is the primary purpose of incorporating the Cost function in the mathematical formulation of scalability?", "answer": "To enable the calculation of data generation efficiency per unit cost.", "explanation": "The Cost function is used to calculate the proportion of data that can be generated per unit cost, enabling a comprehensive evaluation of scalability that considers both dataset size and associated costs.", "question_token_count": 18, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 13, "choices": null}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "What are some potential implications of data contamination on the explainability and transparency of AI models, and how can canary strings help to mitigate these implications?", "question": "How can canary strings help improve the explainability and transparency of AI models, and what are some potential limitations of this method in preventing data contamination?", "answer": "Canary strings can help improve the explainability and transparency of AI models by identifying instances of data contamination, but their effectiveness depends on model trainers being aware of and responsive to these markers, and they may not work if a developer aims to leak benchmarking data to boost scores.", "explanation": "This question requires the reader to think about the implications of data contamination on AI models and how canary strings can help mitigate these implications.", "question_token_count": 31, "answer_correctness_score": 9, "explanation_validity_score": 8, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 55, "choices": null}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "Compare and contrast the evaluation methods used in S3Eval and DyVal for assessing LLMs' reasoning abilities.", "question": "How do the evaluation methods used in S3Eval and DyVal differ in their approach to assessing LLMs' reasoning abilities, and what are the implications of these differences?", "answer": "S3Eval uses randomly generated SQL tables to evaluate LLMs, while DyVal uses randomly generated directed acyclic graphs (DAGs). The use of DAGs in DyVal allows for more complex and nuanced evaluation of LLMs' reasoning abilities, whereas S3Eval's use of SQL tables may be more limited in its ability to assess LLMs' reasoning under uncertainty.", "explanation": "This question requires the domain expert to compare and contrast the evaluation methods used in S3Eval and DyVal, considering the strengths and weaknesses of each approach and the implications of these differences.", "question_token_count": 35, "answer_correctness_score": 8, "explanation_validity_score": 7, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 74, "choices": null}
{"context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "How does dynamic benchmarking address the limitations of static benchmarking schemes?", "question": "How does dynamic benchmarking address the limitations of static benchmarking schemes, such as restricted access to the training dataset and assumptions about model behavior?", "answer": "Dynamic benchmarking addresses these limitations by modifying the data set during the benchmarking process using a transformation function, ensuring transparency and faithfulness in evaluation.", "explanation": "The correct answer requires a deep understanding of the limitations of static benchmarking schemes and the advantages of dynamic benchmarking, which is a key concept in the provided context.", "question_token_count": 27, "answer_correctness_score": 8, "explanation_validity_score": 6, "question_clarity_score": 9, "question_groundedness_score": 10, "avg_answer_token_count": 28, "choices": null}
{"context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n", "topic": "How does the concept of collision in dynamic benchmarking relate to the potential for data contamination, and what are the implications for evaluating LLM capabilities?", "question": "How does the concept of collision in dynamic benchmarking relate to the potential for data contamination, and what are the implications for evaluating LLM capabilities?", "answer": "Collision in dynamic benchmarking measures the overlap between transformed versions of the benchmark dataset, which can limit the benchmark's ability to generate novel and diverse test cases, thereby affecting the accuracy of LLM evaluations.", "explanation": "The correct answer requires an understanding of the relationship between collision in dynamic benchmarking and data contamination, as well as the implications for evaluating LLM capabilities.", "question_token_count": 28, "answer_correctness_score": 8, "explanation_validity_score": 6, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 39, "choices": null}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "How does exact contamination occur in the context of Large Language Models?", "question": "What is an example of exact contamination in the context of Large Language Models?", "answer": "Verbatim test examples in the training corpora.", "explanation": "Exact contamination occurs when there is an exact duplicate in the benchmark dataset, compromising the validity of performance measurements. In the context of LLMs, this can happen when verbatim test examples appear in the training corpora.", "question_token_count": 16, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 11, "choices": null}
{"context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n", "topic": "What are the implications of reusing data from competitions for LLM development, and how can data contamination be mitigated?", "question": "How can the risk of data contamination be mitigated when using recent information from competitions to evaluate LLMs, given the likelihood of reused problems in future competitions?", "answer": "Verification and validation of evaluation datasets through multiple sources and expert review.", "explanation": "The correct answer is based on the understanding that data contamination can occur when using recent information from competitions, as these problems may be reused in future competitions. The question requires a deep understanding of the implications of reusing data and the potential consequences of data contamination.", "question_token_count": 32, "answer_correctness_score": 9, "explanation_validity_score": 8, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 14, "choices": null}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "What are some potential future directions for research on canary strings and their applications in AI?", "question": "What potential research directions could be explored to improve the effectiveness of canary strings in detecting data contamination in LLMs?", "answer": "Investigating the use of dynamic canary strings, developing more robust detection algorithms, and exploring the impact of canary strings on model performance and generalization.", "explanation": "This question requires the ability to think critically about the limitations of canary strings and the potential avenues for improvement, demonstrating a deep understanding of the concept and its applications.", "question_token_count": 24, "answer_correctness_score": 9, "explanation_validity_score": 8, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 31, "choices": null}
{"context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n", "topic": "Discuss the importance of analyzing model behavior under different conditions (e.g., memorization, partial completions, preference for original over paraphrased test cases). How can this analysis improve post-hoc detection methods?", "question": "What are the potential benefits of analyzing model behavior under different conditions (e.g., memorization, partial completions, preference for original over paraphrased test cases) in improving post-hoc detection methods?", "answer": "It enables more robust post-hoc detection methods by identifying potential contamination and improving the accuracy of evaluation metrics.", "explanation": "This question encourages critical thinking on the importance of model behavior analysis and its potential impact on post-hoc detection. The correct answer requires an understanding of the context and the ability to synthesize high-level general understanding.", "question_token_count": 41, "answer_correctness_score": 8, "explanation_validity_score": 7, "question_clarity_score": 7, "question_groundedness_score": 10, "avg_answer_token_count": 21, "choices": null}
{"context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n", "topic": "What are the potential applications of the standardized criteria for evaluating dynamic benchmarks, and how can they be implemented?", "question": "How can the standardized criteria for evaluating dynamic benchmarks be implemented to ensure effective data contamination risk mitigation in LLMs?", "answer": "By establishing a set of clear and objective evaluation metrics, incorporating diverse benchmarking methods, and continuously monitoring and updating the design principles to adapt to emerging trends and challenges in LLM development.", "explanation": "The answer should demonstrate a comprehensive understanding of the proposed design principles and their potential applications.", "question_token_count": 23, "answer_correctness_score": 8, "explanation_validity_score": 4, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 36, "choices": null}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "What are some potential future directions for research on canary strings and their applications in reliability?", "question": "What are some potential future directions for research on canary strings that could enhance their effectiveness in detecting data contamination in LLMs?", "answer": "Developing more sophisticated canary strings that are less likely to be ignored by model trainers, and exploring the use of canary strings in combination with other methods for detecting data contamination.", "explanation": "This question requires the domain expert to think critically about the limitations of canary strings and potential areas for improvement.", "question_token_count": 26, "answer_correctness_score": 9, "explanation_validity_score": 8, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 36, "choices": null}
{"context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n", "topic": "What is the critical gap identified in the existing dynamic benchmarking methods, and how can it be addressed?", "question": "What is the critical gap identified in the existing dynamic benchmarking methods, and how can it be addressed?", "answer": "The lack of standardized criteria for evaluating dynamic benchmarks can be addressed by proposing a series of optimal design principles for dynamic benchmarking, as suggested in the text.", "explanation": "The correct answer requires a deep understanding of the context, specifically the limitations of existing dynamic benchmarking methods and the need for standardized criteria. The answer should propose a solution to address this gap, demonstrating critical thinking and analysis of the subject matter.", "question_token_count": 21, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 31, "choices": null}
{"context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n", "topic": "Label protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment.", "question": "How does the use of a \"No Derivatives\" license in conjunction with public key encryption address the risk of data leakage in evaluation, as proposed by Jacovi et\u00a0al. (2023)?", "answer": "By preventing automated crawling and reuse of test data.", "explanation": "The correct answer requires an understanding of the limitations of encryption methods and the need for robust protection to prevent data leakage.", "question_token_count": 39, "answer_correctness_score": 9, "explanation_validity_score": 6, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 11, "choices": null}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "What are some proposed methods to enhance static approaches, such as data encryption and post-hoc contamination detection?", "question": "What are some proposed methods to enhance static approaches, such as data encryption and post-hoc contamination detection, and how do they address the limitations of traditional static benchmarking methods in LLM evaluation?", "answer": "Data encryption, post-hoc contamination detection, and regenerating benchmark data are proposed methods to enhance static approaches and address the limitations of traditional static benchmarking methods in LLM evaluation.", "explanation": "This question requires the reader to critically evaluate the proposed methods to enhance static approaches and understand how they address the limitations of traditional static benchmarking methods in LLM evaluation.", "question_token_count": 37, "answer_correctness_score": 9, "explanation_validity_score": 6, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 34, "choices": null}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "How do the assumptions underlying the concept of dynamic benchmarks relate to the challenges associated with static benchmarks?", "question": "How do the assumptions underlying the concept of dynamic benchmarks relate to the challenges associated with static benchmarks, and what implications does this have for the development of adaptable benchmarks for LLMs?", "answer": "Dynamic benchmarks assume that LLMs will continue to evolve and adapt, and therefore, require a more adaptable and flexible assessment framework that can accommodate changing model capabilities and limitations.", "explanation": "This question requires the domain expert to critically analyze the relationship between static and dynamic benchmarks and understand the assumptions underlying dynamic benchmarks.", "question_token_count": 36, "answer_correctness_score": 8, "explanation_validity_score": 7, "question_clarity_score": 8, "question_groundedness_score": 8, "avg_answer_token_count": 34, "choices": null}
{"context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n", "topic": "What are the limitations of LLM-assisted transformations in terms of transparency and traceability, and how can these limitations be addressed?", "question": "What are the limitations of LLM-assisted transformations in terms of transparency and traceability, and how can these limitations be addressed?", "answer": "These limitations can be addressed through the use of explainability tools and human-in-the-loop validation.", "explanation": "The correct answer requires an understanding of the limitations of LLM-assisted transformations and the need for additional mechanisms to ensure reliability and correctness.", "question_token_count": 25, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 20, "choices": null}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "How do dynamic benchmarking methods, such as continuously updating benchmark datasets based on timestamps of LLM training, minimize contamination?", "question": "How do dynamic benchmarking methods, such as continuously updating benchmark datasets based on timestamps of LLM training, minimize contamination?", "answer": "By minimizing the likelihood of contamination and ensuring that benchmark data is not inadvertently included in the training phase of language models.", "explanation": "This question requires the domain expert to critically evaluate the effectiveness of dynamic benchmarking methods in addressing the issue of data contamination.", "question_token_count": 23, "answer_correctness_score": 9, "explanation_validity_score": 6, "question_clarity_score": 9, "question_groundedness_score": 10, "avg_answer_token_count": 24, "choices": null}
{"context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n", "topic": "Robust encryption methods are required to prevent minor text variations from affecting their effectiveness.", "question": "What specific security risks are posed by minor text variations in encryption methods, and how do these risks impact their effectiveness in preventing data leakage?", "answer": "Data contamination risks and model exposure to answers.", "explanation": "This question requires the domain expert to analyze the limitations of encryption methods and their vulnerability to minor text variations, as discussed in the context.", "question_token_count": 28, "answer_correctness_score": 2, "explanation_validity_score": 2, "question_clarity_score": 6, "question_groundedness_score": 6, "avg_answer_token_count": 10, "choices": null}
{"context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n", "topic": "How does the scoring function in static benchmarking evaluate the quality of an LLM's outputs?", "question": "How does the scoring function in static benchmarking evaluate the quality of an LLM's outputs in comparison to the expected outputs?", "answer": "By comparing LLM outputs against expected outputs.", "explanation": "The scoring function is used to evaluate the quality of an LLM's outputs by comparing them against the expected outputs, as described in the static benchmark.", "question_token_count": 24, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 10, "question_groundedness_score": 10, "avg_answer_token_count": 9, "choices": null}
