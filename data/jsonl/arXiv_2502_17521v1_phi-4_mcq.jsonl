{"context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n", "topic": "How does the expert's understanding of the context reflect on their evaluation of the text?", "question": "In the context of understanding the limitations of current static methods, which of the following approaches is likely to improve the efficiency of a dynamic evaluation that adapts to growing data set complexities?", "choices": {"A": "Consistent application of static methodologies", "B": "Adoption of advanced statistical methods", "C": "Incorporating more complex models", "D": "Static models"}, "answer": "B", "explanation": "The correct answer is B, as the context implies the need for innovative methods to address the limitations of existing approaches. The challenge is to adapt the evaluation process to accommodate evolving data complexities.", "answer_correctness_score": 10, "explanation_validity_score": 9, "question_clarity_score": 5, "question_groundedness_score": 9, "question_token_count": 37, "avg_answer_token_count": 6}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "Discuss the application of code synthesis in educational technology and the assessment of domain expertise.", "question": "How do you differentiate between syntactic accuracy and semantic meaningfulness in AI-generated code, and why is this distinction critical for evaluating the efficacy of AI models in educational technology?", "choices": {"A": "Syntactic accuracy refers to the correct use of programming language syntax, whereas semantic meaningfulness ensures that the code performs the intended function logically and effectively; this distinction is crucial as it determines the practical usability of AI-generated code in real-world applications.", "B": "Syntactic accuracy is about following programming language rules, while semantic meaningfulness is about making the code look clean and readable; this distinction is important for ensuring that AI models produce aesthetically pleasing code.", "C": "Syntactic accuracy involves the correct structure of code, whereas semantic meaningfulness pertains to the code's efficiency in execution; this distinction is critical because it impacts the speed at which AI models can generate and debug code.", "D": "Syntactic accuracy is concerned with the length of the code, while semantic meaningfulness focuses on the complexity of the code; this distinction is essential for determining how complex the AI models can make the code."}, "answer": "A", "explanation": "The correct answer, A, addresses both syntactic accuracy and semantic meaningfulness in a way that highlights their importance in ensuring that AI-generated code is not only correct in syntax but also functionally effective, which is crucial for real-world application in educational technology.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 2, "question_token_count": 34, "avg_answer_token_count": 44}
{"context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n", "topic": "[Identify the central ideas and themes of the context, and their implications on the evaluation of domain experts.]", "question": "What is the primary challenge in establishing the validity of benchmarks for LLMs, and how does it influence the reliability and reproducibility of evaluations?", "choices": {"A": "The challenge of ensuring data integrity, which affects the evaluation's accuracy.", "B": "The assumption that static benchmarks will remain relevant as the complexity of models increases.", "C": "The risk that contamination of data could compromise the evaluation's reliability.", "D": "The potential for data contamination to undermine the evaluation's consistency."}, "answer": "D", "explanation": "The correct answer is D) The potential for data contamination to undermine the reliability of the evaluation. This is correct because the text emphasizes the challenges posed by static methods and their implications on data integrity, directly influencing the reliability and reproducibility of the evaluation process.", "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 8, "question_token_count": 28, "avg_answer_token_count": 14}
{"context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n", "topic": "How do implicit assumptions within the content affect the interpretation and evaluation of domain experts?", "question": "Which of the following best describes the significance of the oracle function in the context of ensuring the reliability of the generated dataset?", "choices": {"C": "It ensures that the data integrity is maintained through the generation process.", "D": "It ensures that all generated questions are clear and unambiguous.", "A": "It highlights the importance of oracle functions in maintaining the validity and reliability of the generated datasets.", "B": "It provides a method for evaluating the correctness of the dataset."}, "answer": "A", "explanation": "The correct choice is A, as it addresses the nuanced understanding of the content's depth and the implications of oracle assumptions. The significance of this question lies in understanding the broader implications of the oracle's role in dynamic benchmarking.", "answer_correctness_score": 10, "explanation_validity_score": 8, "question_clarity_score": 8, "question_groundedness_score": 9, "question_token_count": 25, "avg_answer_token_count": 14}
{"context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n", "topic": "Evaluate the role of collision metrics in assessing the reliability of dynamic benchmarking and their influence on the integrity of LLM evaluations.", "question": "How does the interpretation of collision probability influence the reliability of dynamic benchmarking systems in assessing LLM performance?", "choices": {"A": "It determines the transparency and accuracy of benchmark data.", "B": "It provides a framework for developing new methodologies to measure data reliability.", "C": "It impacts the robustness and credibility of system evaluations.", "D": "It enhances the adaptability of algorithms in handling dynamic data transformations."}, "answer": "C", "explanation": "The correct answer is C. The interpretation of collision probability directly influences the robustness and credibility of system evaluations by determining the extent to which dynamic benchmarking can maintain its integrity and reliability when exposed to potential data contamination.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 6, "question_token_count": 20, "avg_answer_token_count": 12}
{"context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n", "topic": "[ The ethical implications of algorithmic transparency in AI systems and the impact of biases in AI ]", "question": "What strategies should be prioritized to ensure that AI evaluation systems can be trusted to be fair and unbiased?", "choices": {"A": "By implementing robust frameworks for transparent assessment metrics.", "B": "By developing evaluation frameworks that can be regularly reviewed to ensure they remain relevant and unbiased.", "C": "Through the lens of ethical guidelines, the evaluation of AI systems should incorporate transparent assessment metrics that provide transparency for end-users.", "D": "By ensuring that the evaluation of the system's performance is consistent with the ethical standards."}, "answer": "A", "explanation": "The correct answer requires an understanding of the nuanced trade-offs between transparency and the risk of bias perpetuation, as well as considering the impact on various stakeholders.", "answer_correctness_score": 8, "explanation_validity_score": 8, "question_clarity_score": 7, "question_groundedness_score": 8, "question_token_count": 22, "avg_answer_token_count": 17}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "Explore the risks associated with using data that is not temporally aligned with the current date in evaluations.", "question": "How does the temporal alignment of data impact the theoretical underpinnings of model evaluations in practical applications, especially when considering the evolution of the field and the incorporation of new data into the existing theoretical framework?", "choices": {"C": "It is crucial for the validation of models in new contexts, ensuring the robustness of model predictions.", "A": "It leads to improved understanding of models' limitations.", "B": "It highlights the need for temporal alignment of data in training and evaluation phases.", "D": "It emphasizes the necessity of temporal data integrity for evaluations and model robustness against future data integration."}, "answer": "B", "explanation": "The correct answer involves synthesizing the understanding that data quality and relevance are critical to the model's performance, highlighting the importance of data integrity and precision in complex models. This question challenges the expert's understanding of the theoretical and practical implications of temporal data integrity on model predictions and evaluations, specifically the challenge of maintaining model performance with temporal data changes. It requires deep knowledge of the theoretical and practical aspects of data science and model evaluations.", "answer_correctness_score": 8, "explanation_validity_score": 6, "question_clarity_score": 6, "question_groundedness_score": 4, "question_token_count": 41, "avg_answer_token_count": 16}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "How have the historical perspectives on the evolution of mathematical concepts influenced contemporary educational practices in mathematics?", "question": "How do the principles underlying quantum mechanics' impact on AI's approach to problem-solving in quantum computing?", "choices": {"A": "They provide a basis for understanding machine learning models in AI.", "B": "They serve as a foundation for quantum computing algorithms.", "C": "They influence the development of quantum algorithms.", "D": "They form the basis for AI problem-solving in complex mathematical models."}, "answer": "D", "explanation": "The correct answer is D. The other options are plausible, but they are subtly incorrect because they do not reflect the nuanced relationship between quantum mechanics and AI development.", "answer_correctness_score": 4, "explanation_validity_score": 3, "question_clarity_score": 9, "question_groundedness_score": 1, "question_token_count": 21, "avg_answer_token_count": 11}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "How can the principles of mathematical reasoning and their applications be integrated into professional practices within mathematics education?", "question": "How can the advanced understanding of educational evaluation principles be applied to improve the competency of professional domain experts in their respective fields?", "choices": {"A": "By refining the educational content to align with the core educational objectives.", "B": "By emphasizing the synthesis of high-level general understanding above and beyond the specific context.", "C": "Through the process of synthesizing information for improved application in relevant professional scenarios.", "D": "By applying the implicit assumptions inherent in the educational content."}, "answer": "A", "explanation": "The correct answer, A, reflects a deep understanding of the complexities involved in evaluating domain experts' competencies, requiring a synthesis of high-level general understanding above and beyond the specific context.", "answer_correctness_score": 7, "explanation_validity_score": 3, "question_clarity_score": 9, "question_groundedness_score": 1, "question_token_count": 25, "avg_answer_token_count": 14}
{"context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n", "topic": "[ Theoretical underpinnings of AI ethics and the role of transparency in AI systems ]", "question": "In the context of AI benchmarking, how can transparency be effectively balanced with privacy and security concerns to ensure fairness and accountability?", "choices": {"A": "By exclusively using static benchmarks to prevent the continuous collection of data.", "B": "By implementing rigorous ethical guidelines that govern data usage and ensure transparency without compromising user privacy.", "C": "By allowing unrestricted access to benchmarking results to foster open research and collaboration.", "D": "By prioritizing the adaptability of dynamic benchmarks over the ethical considerations of data usage."}, "answer": "B", "explanation": "The correct answer involves implementing ethical guidelines that balance transparency with privacy and security, ensuring fairness and accountability without compromising user privacy.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "question_token_count": 25, "avg_answer_token_count": 15}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "What are the implications of incorporating advanced mathematical concepts into modern educational frameworks?", "question": "How do the theoretical foundations and practical applications of computational models influence their ability to solve complex mathematical problems in real-world scenarios?", "choices": {"A": "By enhancing model generalization across different domains.", "B": "By providing a framework for benchmarking and evaluating problem-solving capabilities.", "C": "By focusing on the retrieval of real-world information.", "D": "By improving the efficiency of multi-step problem-solving tasks."}, "answer": "A", "explanation": "The correct answer is A because the context discusses the importance of understanding computational models' capacity to generalize across different types of math problems, which is crucial for solving complex real-world scenarios.", "answer_correctness_score": 8, "explanation_validity_score": 8, "question_clarity_score": 7, "question_groundedness_score": 4, "question_token_count": 25, "avg_answer_token_count": 11}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "Explain how the evolution of mathematical education has impacted current teaching methodologies and their application in modern pedagogy.", "question": "How do theoretical frameworks underpinning multi-step problem-solving methodologies inform the development of educational evaluation tools in contemporary mathematical pedagogy?", "choices": {"A": "They offer a structured approach to assess problem-solving capabilities.", "B": "They provide a basis for integrating computational models with traditional educational paradigms.", "C": "They enable the alignment of theoretical concepts with real-world applications.", "D": "They facilitate the design of datasets to test domain-specific expertise."}, "answer": "C", "explanation": "The correct answer emphasizes the alignment of theoretical concepts with practical applications, reflecting a deep understanding of the educational frameworks' role in developing evaluation tools.", "answer_correctness_score": 8, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 2, "question_token_count": 26, "avg_answer_token_count": 13}
{"context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n", "topic": "What are the central ideas, and nuanced themes and significant relationships within the context.", "question": "How does the integration of hyperlinks impact the coherence and comprehension of content for domain experts, particularly in relation to educational objectives and alignment with content integrity?", "choices": {"A": "The integration of hyperlinks offers a significant impact on the coherence of the content, but may pose challenges in the context of aligning educational goals and content objectives.", "B": "The integration of hyperlinks significantly influences the structure of the information and its implications for the quality and applicability of educational materials.", "C": "The inclusion of hyperlinks provides a substantial impact on the structure and integrity of the content, impacting the quality and coherence of the content.", "D": "The incorporation of hyperlinks necessitates a reevaluation of the implications for domain experts, particularly in terms of understanding and aligning with educational objectives."}, "answer": "D", "explanation": "The answer is D: The incorporation of hyperlinks necessitates a reevaluation of the implications for domain experts, particularly in terms of understanding and aligning with educational objectives. This option best captures the nuanced discussion on the impact of hyperlink integration on educational content and its alignment with educational goals.", "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 1, "question_token_count": 31, "avg_answer_token_count": 28}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "What are the consequences of overlooking data quality and integrity checks on the performance evaluations?", "question": "How does the presence of syntactic transformations in the evaluation data affect the interpretation of model performance in machine learning evaluations?", "choices": {"A": "It ensures that the model's evaluation is consistent across different datasets.", "B": "It allows the model to achieve high accuracy by generalizing well to unseen data.", "C": "It can cause overfitting, leading to overestimated performance on unseen data.", "D": "It can result in misleading performance metrics that do not reflect true capabilities of the model."}, "answer": "D", "explanation": "The correct answer is D. Syntactic transformations in the evaluation data can lead to an overestimated perception of a model's effectiveness. This is because the evaluation metrics may not accurately reflect the model's true capabilities, as they may be inflated due to the memorization of the test data. This type of contamination can give rise to a false perception of the model's performance, making it difficult to determine the actual effectiveness of the model. This can lead to an overestimation of the model's true performance, making it challenging to determine the true performance of the model.", "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 10, "question_token_count": 23, "avg_answer_token_count": 16}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "Theoretical and practical applications of deep learning in natural language processing.", "question": "How does the potential for data leakage impact the reliability of LLMs, and what might be the broader implications for the development of dynamic benchmarking methods in the context of domain-specific applications of language models?", "choices": {"A": "By influencing the generalizability of language models across domains.", "B": "The potential for data leakage is not well-understood.", "C": "The potential for data leakage leading to unreliable performance evaluation.", "D": "The necessity for dynamic benchmarking in LLMs."}, "answer": "D", "explanation": "The correct answer, D, because it encompasses the nuanced relationship between data integrity and the effectiveness of models in LLMs, emphasizing the importance of implementing dynamic evaluation frameworks to ensure reliable performance assessments in domain-specific applications.", "answer_correctness_score": 6, "explanation_validity_score": 7, "question_clarity_score": 8, "question_groundedness_score": 10, "question_token_count": 40, "avg_answer_token_count": 11}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "How have historical changes in the philosophy of mathematics affected the modern approach to teaching mathematics?", "question": "Considering the evolution of mathematical benchmarks, how might the development of these datasets influence the future of pedagogical approaches in mathematics education?", "choices": {"A": "By emphasizing the importance of computational problem-solving in mathematics education.", "B": "By reflecting a shift towards a more holistic and integrated approach to teaching mathematics.", "C": "By encouraging the integration of philosophical underpinnings into the teaching of mathematics.", "D": "By necessitating a re-evaluation of traditional mathematical teaching methods."}, "answer": "B", "explanation": "The correct answer is B. The question is designed to challenge the examinee to consider how the development of these datasets, which encapsulate complex problem-solving tasks, reflect or influence broader pedagogical approaches in mathematics education. The nuanced understanding required to answer this question reflects the potential shift towards a more integrated and holistic approach to teaching mathematics, which is a fundamental aspect of the modern educational philosophy.", "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 4, "question_groundedness_score": 2, "question_token_count": 26, "avg_answer_token_count": 14}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "Historical development and significance of mathematical concepts in education.", "question": "Considering the current limitations of machine learning models in encoding nuanced understanding, as evidenced by their performance on the GSM8K benchmark, which aspect suggests a need for improvement in their capabilities to apply learned concepts in novel situations?", "choices": {"A": "The document suggests that while current models can perform basic reasoning tasks, their performance is suboptimal on more complex problems, indicating a need for enhancements in encoding nuanced understanding in models.", "B": "The provided context indicates that models may struggle with encoding nuanced understanding, particularly in complex reasoning scenarios where the models often fail to encode complex concepts.", "C": "The text highlights that while models can solve simpler problems, they falter in more complex scenarios, showing a limitation in their ability to transfer and apply learned concepts in new situations, which is critical for enhancing their reasoning capabilities.", "D": "The text suggests that while models can learn complex ideas, their capacity to apply these ideas in new contexts is limited, indicating a need for improvements in models' capabilities to encode and understand complex concepts."}, "answer": "B", "explanation": "The correct answer is B, as it highlights the nuanced detail that models' abilities to encode and understand complex concepts is a challenge that needs addressing, which is essential for progress in the field of AI, as identified by the document's focus on the necessity of encoding nuanced understanding in models.", "answer_correctness_score": 8, "explanation_validity_score": 7, "question_clarity_score": 6, "question_groundedness_score": 10, "question_token_count": 44, "avg_answer_token_count": 37}
{"context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n", "topic": "The critical importance of data security and the potential risks associated with data leaks in the context of LLMs.", "question": "In the context of dynamic evaluation methods, how can the introduction of advanced analytical techniques enhance the accuracy and reliability of assessments in domain expertise evaluation?", "choices": {"A": "By providing more flexibility and adaptability in evaluations", "B": "By ensuring higher precision and adaptability in evaluations", "C": "By enabling the systematic identification and resolution of potential data leaks.", "D": "By fostering the development of innovative benchmarks."}, "answer": "C", "explanation": "The correct answer is chosen because it provides a deep insight into how advanced analytical techniques can be systematically integrated to address the inherent challenges posed by the shift to dynamic evaluations, ensuring that the evaluation process is both accurate and reliable. This question and answer pair focus on the critical role of advanced analytical techniques in improving the benchmarking process's precision and reliability, which is an advanced topic requiring deep understanding to evaluate the potential of a data leak in the context of LLMs. The challenge lies in the nuanced understanding of the relationship between data security, the effectiveness of evaluations, and the implications of the shift from static to dynamic methods in the evaluation process.", "answer_correctness_score": 8, "explanation_validity_score": 8, "question_clarity_score": 6, "question_groundedness_score": 2, "question_token_count": 29, "avg_answer_token_count": 10}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "Analyze the potential implications of the central ideas and nuanced themes within the text.", "question": "How might the theoretical framework of the discussed tokens be leveraged to develop more robust, dynamic evaluation metrics for model assessments, and what implications could this have for future research in the context of data contamination in machine learning models?", "choices": {"A": "By enabling the creation of questions that cannot be answered without specific knowledge of the tokens' inherent properties.", "B": "By providing a means to evaluate whether a model has achieved optimal levels of generalization, thereby contributing to the development of more nuanced metrics for assessing model performance.", "C": "By ensuring that models do not memorize training data, the tokens can be utilized to verify that a model has been trained on.", "D": "By isolating the tokens and evaluating how they are used in the real-world applications of the text, we can better understand the implications of tokenization and its impact on the assessment of model memorization."}, "answer": "B", "explanation": "The correct answer is B. The question assesses the domain expert's ability to understand the potential implications of the tokens within the context, requiring a high level of technical knowledge.", "answer_correctness_score": 9, "explanation_validity_score": 7, "question_clarity_score": 4, "question_groundedness_score": 2, "question_token_count": 44, "avg_answer_token_count": 29}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "How do the principles of mathematical reasoning and their historical development inform contemporary educational practices?", "question": "How does the historical development of mathematical reasoning inform the integration of AI-driven methodologies in contemporary educational practices?", "choices": {"A": "By providing a foundational understanding of computational algorithms.", "B": "By offering insights into the pedagogical application of AI in evaluating complex problem-solving skills.", "C": "By illustrating the evolution of mathematical theories that underpin modern computational models.", "D": "By emphasizing the role of historical mathematical principles in shaping current educational technologies."}, "answer": "C", "explanation": "The correct answer requires understanding how historical mathematical reasoning influences the integration of AI in education, particularly through the application of theoretical principles in modern contexts. The focus is on the evolution of mathematical theories and their practical application in AI-driven educational methodologies.", "answer_correctness_score": 8, "explanation_validity_score": 9, "question_clarity_score": 2, "question_groundedness_score": 1, "question_token_count": 21, "avg_answer_token_count": 14}
{"context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n", "topic": "What are the central ideas and nuanced themes within the context of the expert's understanding of the text?", "question": "What are the underlying theoretical implications of integrating static and dynamic benchmarking approaches in the context of LLM evaluation, and how might the synergy between these methods advance the standardization of LLM benchmarking?", "choices": {"D": "It suggests that the combination of static and dynamic methods could lead to a more comprehensive and effective benchmarking strategy.", "A": "It highlights the need for a standardized dynamic approach to the evaluation of LLMs, which ensures the reliability of test results and addresses the scalability issues of traditional static benchmarks.", "B": "The nuanced synergy between static and dynamic methods may provide a more robust framework for evaluating LLMs.", "C": "The combination of static and dynamic benchmarking can lead to misleading results when applied without considering contextual factors."}, "answer": "B", "explanation": "The correct answer (B) reflects the document's discussion on combining static and dynamic methods to address the limitations of each. The synergy between static and dynamic evaluations enhances the reliability and relevance of benchmarking LLMs.", "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 6, "question_token_count": 36, "avg_answer_token_count": 24}
{"context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n", "topic": "How do collision rates influence the effectiveness of dynamic benchmarks in reflecting true LLM capabilities, and what measures can be taken to address this issue?", "question": "How do high collision rates between independently transformed versions of a benchmark dataset impact the credibility of dynamic benchmarking in evaluating LLMs' capabilities?", "choices": {"A": "They indicate a need for more diverse transformations to prevent data contamination.", "B": "They suggest that the benchmark is too sensitive to changes in data transformations.", "C": "They imply that the benchmark's ability to produce novel test cases is compromised.", "D": "They show that the benchmark is effectively preventing data overlap and ensuring reliability."}, "answer": "C", "explanation": "High collision rates suggest that the overlap between transformed datasets is significant, which can compromise the benchmark's ability to produce novel test cases and may lead to data contamination. This impacts the credibility of the dynamic benchmarking process in accurately reflecting LLM capabilities.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "question_token_count": 27, "avg_answer_token_count": 15}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "Analyze the methodological differences and implications of using advanced code generation models like CodeX and CodeX in evaluating coding capabilities.", "question": "Considering the performance data provided for CodeGen 2, CodeGen, and CodeGen, what can be inferred about the impact of architectural differences in code generation models' ability to adapt to new coding challenges?", "choices": {"C": "The effectiveness of these models can be attributed to the underlying architecture of the models.", "D": "While it has been observed in the MBE results, the performance disparity between CodeGen 2.1 and CodeGen 2.1 does not suggest a significant gap in the evolution of language models.", "A": "The differences in model architectures such as CodeGen2 and CodeGen2 have no significant impact on the evolution of coding challenges.", "B": "The slight difference in performance between CodeGen2 and CodeGen2 suggests a potential need for improvement in handling unique challenges."}, "answer": "D", "explanation": "The context describes the performance of models like CodeGen and CodeGen 2.1, highlighting their capabilities in coding tasks. The crucial aspect is that CodeGen2.1 is a significant improvement over CodeGen 2.1, suggesting that the architectural enhancements of CodeGen models, such as those in CodeGen2, are responsible for this improvement. The mention of \"these models\" indicates CodeGen2, which is known for its efficiency in code generation tasks, aligning with the capabilities of CodeGen2.", "answer_correctness_score": 2, "explanation_validity_score": 1, "question_clarity_score": 2, "question_groundedness_score": 1, "question_token_count": 42, "avg_answer_token_count": 27}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "What are the best practices for ensuring data integrity and accuracy in evaluations?", "question": "How do the principles of quantum encryption influence the development of AI models that incorporate cryptographic principles in the context of data integrity within complex networked environments?", "choices": {"C": "Data encryption and AI integration techniques.", "A": "Quantum encryption algorithms are the primary method of ensuring data security in AI-driven applications.", "B": "The influence of cryptographic methods on data security measures in AI-driven cybersecurity.", "D": "The use of quantum encryption methods for ensuring secure data transmission in AI-driven cybersecurity systems."}, "answer": "D", "explanation": "The correct answer is D. It is essential to apply the principles of quantum cryptography to ensure secure data encryption and protect against potential vulnerabilities in traditional methods.", "answer_correctness_score": 8, "explanation_validity_score": 7, "question_clarity_score": 8, "question_groundedness_score": 1, "question_token_count": 30, "avg_answer_token_count": 14}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "The application of knowledge-based assessments in evaluating the effectiveness of educational practices.", "question": "In the context of evaluating models' mathematical reasoning capabilities, how might the use of benchmarks like GSM8K impact the future trajectory of educational assessment frameworks?", "choices": {"A": "It could lead to a more standardized approach to evaluating mathematical reasoning in educational curricula.", "B": "It might influence the emphasis on quantitative skills in educational policy.", "C": "It could enhance the understanding of knowledge gaps in learning assessments.", "D": "It may inspire the development of more comprehensive educational benchmarks."}, "answer": "A", "explanation": "The correct answer is A. The use of benchmarks like GSM8K and MATH further highlights the need for understanding complex mathematical reasoning, which is a critical aspect of evaluating educational content in terms of effectiveness and efficiency.", "answer_correctness_score": 10, "explanation_validity_score": 8, "question_clarity_score": 6, "question_groundedness_score": 4, "question_token_count": 31, "avg_answer_token_count": 14}
{"context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n", "topic": "Evaluate the theoretical underpinnings and practical implications of using \u0398 as a measure of diversity within datasets, ensuring comprehensive coverage of its implications for data science professionals.", "question": "What is the potential impact on data interpretation when using \u0398 (Theta) as a measure of diversity in transformed datasets, particularly in the context of external and internal diversity?", "choices": {"A": "It would increase the complexity of evaluating dataset transformations due to the introduction of multiple diversity metrics.", "B": "It provides a unified measure for both external and internal diversity, simplifying the assessment of dataset transformations.", "C": "It allows for a more granular analysis of dataset variations by differentiating between external and internal diversity measures.", "D": "It reduces the need for other diversity metrics, making \u0398 (Theta) the sole measure for evaluating dataset transformations."}, "answer": "C", "explanation": "The correct answer is C. The context highlights that \u0398 (Theta) measures both external and internal diversity, allowing for a detailed analysis of variations within and between datasets. This differentiation enables a more nuanced understanding of dataset transformations, which is crucial for data scientists.", "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 4, "question_groundedness_score": 9, "question_token_count": 34, "avg_answer_token_count": 20}
{"context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n", "topic": "How does the theoretical framework of the context support the evaluation of the expertise of the subject matter experts?", "question": "How does the integration of \"latent variable models\" in the context of enhancing the interpretability of computational linguistics contribute to the reliability of predictive models in natural language processing (NLP) models within the framework of latent variable models?", "choices": {"A": "By ensuring the reliability of the dataset's integrity through latent variable models.", "B": "It enables the calibration of NLP models to align with the benchmarked ground truth.", "C": "It facilitates the assessment of the data quality in the latent space of the model.", "D": "It allows for the precise prediction of outcomes by modeling the uncertainties in data."}, "answer": "B", "explanation": "The correct answer is B. It enables the calibration of NLP models to align with the benchmarked ground truth. This response reflects the understanding that the integration of latent variable models is essential for aligning predictive models with the ground truth, thereby enhancing their reliability.", "answer_correctness_score": 8, "explanation_validity_score": 8, "question_clarity_score": 2, "question_groundedness_score": 1, "question_token_count": 46, "avg_answer_token_count": 16}
{"context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n", "topic": "Evaluate the ethical considerations and potential challenges associated with the implementation of AI in educational settings.", "question": "Considering the complexities of AI in education, what is the most significant ethical challenge in ensuring AI systems do not exacerbate existing inequalities in access to educational resources?", "choices": {"A": "The potential biases inherent in AI systems.", "B": "The necessity for continuous updates to educational AI models.", "C": "The requirement for human validation to prevent data breaches.", "D": "Ensuring diverse data sets to train unbiased models."}, "answer": "A", "explanation": "The correct answer is A. The question invites domain experts to reflect on the ethical implications and biases that AI systems may perpetuate, particularly in educational settings, where the impact on students' perceptions and the learning environment can be profound. This question challenges the expert's understanding of how AI technologies can be used responsibly and ethically in education. It requires insight into the subtleties of AI application, ethical considerations, and the need for human oversight and the verification of AI-generated content's accuracy and relevance. The complexity of this question lies in the requirement of an in-depth understanding of AI's implications on educational equity, requiring a high level of understanding of both the theoretical underpinnings and practical applications of AI in educational settings.", "answer_correctness_score": 10, "explanation_validity_score": 9, "question_clarity_score": 9, "question_groundedness_score": 1, "question_token_count": 32, "avg_answer_token_count": 10}
{"context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n", "topic": "What are the central ideas, themes, and significant relationships within the context?", "question": "How do the static and dynamic scenarios in AI model evaluation influence the effectiveness of benchmarks in shaping AI model evaluations?", "choices": {"A": "Static scenarios allow for consistent and reliable benchmarks, while dynamic scenarios require adaptable and flexible benchmarks to ensure robust evaluations.", "B": "Static scenarios are outdated and irrelevant, while dynamic scenarios provide the only meaningful benchmarks for AI model evaluations.", "C": "Static scenarios offer simplicity and ease of use, while dynamic scenarios introduce unnecessary complexity and challenges in benchmarking.", "D": "Static scenarios ensure reproducibility, while dynamic scenarios undermine the reliability of benchmarks due to their variability."}, "answer": "A", "explanation": "The correct answer reflects the nuanced understanding of the interplay between static and dynamic scenarios in AI model evaluations, emphasizing the need for adaptable benchmarks to ensure robust evaluations in dynamic environments. The context highlights the complexity of evaluating models in both static and dynamic contexts, underscoring the necessity for benchmarks that can accommodate the evolving nature of AI.", "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 8, "question_token_count": 23, "avg_answer_token_count": 21}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "What are the best practices for data cleansing and management to ensure the integrity of data processing?", "question": "How might the presence of subtle, syntactic transformations in a dataset impact the performance evaluation of a model, and what underlying theories explain these effects?", "choices": {"A": "By creating misleading data, it challenges the model's ability to distinguish between relevant information and noise, leading to incorrect results.", "B": "It allows the model to identify misleading data, which improves the model's capacity to distinguish between relevant and irrelevant data.", "C": "By creating a dataset that is not representative of the real world, it makes it more challenging for the model to evaluate its performance.", "D": "It complicates the validation of the model's performance because it makes it difficult to differentiate between the actual data and the model's learned information."}, "answer": "D", "explanation": "The correct answer D is the correct answer because the text implies that the complex syntactic transformations in the training data can lead to data contamination, which complicates the validation of a model's performance. This includes the challenges in distinguishing between actual data and the model's learned information due to syntactic transformations.", "answer_correctness_score": 10, "explanation_validity_score": 9, "question_clarity_score": 7, "question_groundedness_score": 9, "question_token_count": 29, "avg_answer_token_count": 25}
{"context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n", "topic": "[ The implications of using benchmarks in AI and the ethical considerations in AI systems ]", "question": "Considering the nuanced relationship between data integrity and the transparency of algorithms in model evaluations, how do the principles of transparency and accountability in AI systems' development reflect on the broader implications of AI in ensuring the ethical integrity of AI models?", "choices": {"A": "The principles of transparency in AI systems development are only essential to the extent that they are integrated within the systems to the extent that they are designed into the systems.", "B": "The question of how machine learning models handle uncertainty in AI system development.", "C": "The necessity for AI systems to be transparent in their functioning and the implications for ethical AI in systems.", "D": "The potential impact of data integrity on the effectiveness of AI models in addressing real-world challenges."}, "answer": "A", "explanation": "The principles of transparency in AI systems are crucial to ensuring ethical integrity by maintaining accountability and building trust with stakeholders. This reflects broader implications for AI by emphasizing the need for clear, unbiased, and fair AI models.", "answer_correctness_score": 8, "explanation_validity_score": 9, "question_clarity_score": 7, "question_groundedness_score": 8, "question_token_count": 46, "avg_answer_token_count": 22}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "Explore the central ideas, nuanced themes, and significant relationships within it.", "question": "What are the broader implications of not being able to accurately detect and measure the impact of data contamination in LLMs, and how does it affect the robustness of the models in LLMs?", "choices": {"A": "It leads to an overestimation of the model's capabilities, resulting in less reliable models.", "B": "It results in a significant impact on the robustness of the model training process.", "C": "It becomes difficult to assess the true impact of data contamination in LLMs, making it challenging to identify the causes and implications of data contamination.", "D": "There are no broader implications for the quality of the model."}, "answer": "C", "explanation": "The inability to accurately detect and measure data contamination in LLMs makes it challenging to assess the true impact of such contamination, thereby hindering the identification of its causes and implications. This complexity affects the robustness of the models, as it undermines the integrity of model evaluations and the reliability of the models themselves. Thus, the correct answer highlights the difficulty in assessing the true impact of data contamination, which is crucial for ensuring robust model training and evaluation.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 9, "question_groundedness_score": 6, "question_token_count": 38, "avg_answer_token_count": 19}
{"context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n", "topic": "[ The importance of transparency in AI systems and the ethical use of data in AI technology ]", "question": "How do the ethical considerations of AI models and the theoretical frameworks influence the evaluation of AI systems, and what are the broader societal implications of these models on the ethical use of AI in society?", "choices": {"A": "Theories of AI suggest that ethical guidelines can directly enhance the accuracy of AI models by integrating AI ethics into the evaluation metrics used to assess model performance.", "B": "The models' design and evaluation frameworks can shape societal norms and ethical practices around AI and data usage.", "C": "The interplay between theoretical models of AI and real-world applications requires a profound understanding of the implicit biases in algorithmic designs.", "D": "The evaluation metrics are the sole determinants of the ethical use of AI technology in education."}, "answer": "B", "explanation": "The correct answer is B. It requires the domain expert to critically engage with the content, understanding that while theoretical models can inform practical applications, the nuanced understanding of the relationship between theory and application is essential for making ethical decisions.", "answer_correctness_score": 9, "explanation_validity_score": 8, "question_clarity_score": 7, "question_groundedness_score": 5, "question_token_count": 39, "avg_answer_token_count": 23}
{"context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "Evaluate the effectiveness and limitations of using static datasets for training LLMs, especially in terms of how this approach might lead to data contamination and its consequences.", "question": "In the context of dynamic benchmarking for evaluating LLMs, how does the transformation function \\( T(\\cdot) \\) mitigate the risk of data contamination and enhance the reliability of model assessments?", "choices": {"A": "By ensuring that the evaluation dataset is continuously updated to reflect real-world data dynamics and complexities.", "B": "By maintaining a static dataset that is periodically refreshed to incorporate new data points.", "C": "By using a seed dataset that is expanded incrementally to include new instances over time.", "D": "By applying a fixed transformation to the dataset that does not change over the evaluation period."}, "answer": "A", "explanation": "The correct answer is A: By ensuring that the evaluation dataset is continuously updated to reflect real-world data dynamics and complexities. The transformation function \\( T(\\cdot) \\) is designed to modify the dataset dynamically, thereby avoiding data contamination and ensuring that the model's performance is evaluated in a manner that is representative of real-world scenarios. This continuous update process is crucial for maintaining the relevance and accuracy of the evaluation, addressing the limitations of static datasets.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "question_token_count": 37, "avg_answer_token_count": 17}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "What are the implications of using different code synthesis models for code generation tasks, and how do these models affect the debugging process in real-world scenarios?", "question": "How can a domain expert discern the real-world implications of using transformer-based models versus other architectures in code synthesis and debugging tasks, based on the observed trends in code synthesis and the debugging benchmarks?", "choices": {"A": "Transformers with a shallow and larger number of parameters, like those in the GPT-3 series, show better performance in code synthesis tasks due to their ability to capture context over long sequences, even if they might underperform in debugging tasks.", "B": "Models with a deep and fewer parameters, like Codex, excel in code synthesis due to their efficient training on diverse datasets, even though they might be complex in architecture.", "C": "Models with a deeper architecture, such as those in the PaLM series, excel in code synthesis tasks due to their ability to learn and understand from diverse datasets.", "D": "ecision Criteria:"}, "answer": "B", "explanation": "The correct answer is based on the understanding that models like Codex, built on GPT-3 architecture, are trained to perform tasks effectively due to their ability to synthesize and debug code from complex datasets, as shown in benchmarks.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 4, "question_token_count": 39, "avg_answer_token_count": 30}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "What are the implications of integrating advanced mathematical concepts into modern educational frameworks for future curricula?", "question": "How might the integration of advanced mathematical concepts into AI-driven educational systems impact the future of educational assessments in terms of evaluating mathematical reasoning in students?", "choices": {"D": "Ensures that only students with a strong mathematical background can succeed in future curricula.", "A": "Enables a more nuanced understanding of students' problem-solving skills.", "B": "Facilitates the development of more effective, AI-driven educational tools that adapt to individual learning paces and styles.", "C": "Directs the focus towards a more theoretical understanding of mathematics, limiting its practical applications."}, "answer": "A", "explanation": "The correct answer, A, reflects the content's emphasis on the potential of AI in educational assessments. It encourages a nuanced understanding of how AI can transform educational practices by integrating advanced mathematical concepts, ensuring that questions are challenging for domain experts.", "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 2, "question_token_count": 29, "avg_answer_token_count": 18}
{"context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n", "topic": "[ The implications of algorithmic fairness in AI systems and their societal impacts ]", "question": "How can the limitations of current AI benchmarks, which often fail to capture the full scope of AI systems' capabilities, be addressed to ensure a more comprehensive evaluation that reflects the true nature of these systems?", "choices": {"A": "By maintaining static benchmarks that rely on outdated data sources.", "B": "By implementing dynamic, multi-dimensional benchmarks that adapt to technological advancements.", "C": "By focusing solely on accuracy as the primary metric of evaluation.", "D": "By avoiding the use of any benchmarks to prevent bias."}, "answer": "B", "explanation": "The correct answer is B because the context emphasizes the need for dynamic, multi-dimensional benchmarks that adapt to technological advancements, highlighting the importance of evolving benchmarks to better capture the capabilities of AI systems.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 2, "question_token_count": 41, "avg_answer_token_count": 12}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "Discuss the challenges and benefits of integrating different methodologies for code generation and code synthesis.", "question": "Given the advancements in AI models for code generation, what is the primary challenge in integrating AI-based code generation into software development practices for domain experts?", "choices": {"A": "The necessity for a profound understanding of the limitations and capabilities of AI models in code generation.", "B": "The need for a deep comprehension of the impact of AI on coding practices.", "C": "The emphasis on the challenges in evaluating AI's capabilities in code generation and debugging within the broader scope of software engineering.", "D": "The requirement for specialized knowledge to assess the nuances of AI's role in code generation and the intricacies of debugging code."}, "answer": "C", "explanation": "The correct answer should reflect a deep understanding of the context and implications of AI in software development, highlighting the challenges in evaluating AI's capabilities in code generation and debugging within the broader scope of software engineering.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 2, "question_token_count": 30, "avg_answer_token_count": 20}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "[Topic Text]", "question": "How do symbolic reasoning and computational logic systems synergize to enhance problem-solving capabilities in advanced AI applications, particularly in the context of integrating domain-specific knowledge for solving complex problems?", "choices": {"A": "By employing symbolic reasoning to define the structure of problems and using computational logic to optimize solutions, resulting in improved problem-solving efficiency.", "B": "Through the parallel use of symbolic reasoning and computational logic, each independently contributing to the solution without integration.", "C": "By using computational logic to define problem structures and symbolic reasoning to optimize solutions, which enhances problem-solving efficiency.", "D": "Symbolic reasoning and computational logic are utilized in isolation, each addressing different aspects of problem-solving without synergy."}, "answer": "A", "explanation": "The correct answer is A because it accurately reflects the integration of symbolic reasoning to define problem structures and computational logic to optimize solutions, enhancing problem-solving efficiency in AI applications.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 1, "question_token_count": 35, "avg_answer_token_count": 22}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "Explain the significance of the central ideas presented in the context and their impact on domain experts' competency evaluations.", "question": "How do the methodologies of evaluating the reasoning capabilities of LLMs through structured data types, such as table-based and graph-based evaluations, reflect the models' potential to understand and reason about complex structures?", "choices": {"A": "By generating random SQL queries and DAGs, they measure the models' accuracy in executing structured data queries.", "B": "They assess the models' ability to translate abstract graphs into natural language descriptions and evaluate their reasoning about these descriptions.", "C": "They determine the models' performance on well-known P and NP problems, such as the Traveling Salesman Problem, using synthesized random graphs as inputs.", "D": "They analyze the implicit assumptions and underlying theories about reasoning capabilities, ensuring evaluations accurately reflect the models' reasoning processes."}, "answer": "D", "explanation": "The correct answer reflects an understanding of the broader implications of these methodologies, focusing on the relationship between task complexity and the evaluation of reasoning capabilities, beyond just the performance on specific tasks.", "answer_correctness_score": 8, "explanation_validity_score": 8, "question_clarity_score": 7, "question_groundedness_score": 9, "question_token_count": 40, "avg_answer_token_count": 24}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "Evaluate the potential biases introduced by using outdated data in domain-specific benchmarks.", "question": "How might the use of outdated datasets in the training of machine learning models potentially bias the model's performance metrics, particularly in the detection of subtle shifts in data that could be critical in high-stakes applications?", "choices": {"A": "It could lead to a false sense of security in the model's performance, making it difficult to detect biases and errors that could have a significant impact on the model\u2019s efficacy.", "B": "It can cause the model to overfit the data, leading to a degradation in performance when the model is applied to new, unseen data.", "C": "The model may develop an inherent bias towards the data it was trained on, leading to skewed results that do not accurately represent real-world scenarios, especially if the training data is not reflective of current trends.", "D": "It might reduce the model's ability to generalize across different data sets, causing it to perform poorly in real-world applications where the data distribution may have changed since the model was trained."}, "answer": "C", "explanation": "The correct answer highlights the nuanced understanding that outdated datasets can cause models to develop biases that are not representative of current trends, leading to skewed results and poor generalization in real-world applications. This understanding is crucial for domain experts to ensure that AI models remain accurate and reliable.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 4, "question_token_count": 41, "avg_answer_token_count": 35}
{"context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n", "topic": "[ The relationship between core concepts and significant relationships within the context.]", "question": "How might the implicit assumptions of current evaluation methods impact the future trajectory of AI research and development of more effective models?", "choices": {"A": "The current evaluation methods for LLMs are static and do not reflect the variability in content understanding.", "B": "The evaluation of the dynamic nature of how models are changing, and it is not clear how they will impact the trajectory of AI research and development.", "C": "Understanding the implications of the provided context, the best answer is: The current evaluation methods' static nature may limit the understanding of model adaptability, necessitating more dynamic approaches for future AI research and development.", "D": "The implications of the question generated in the document are discussed in the context of the topic, as well as the limitations of the answer."}, "answer": "C", "explanation": "The correct answer reflects the context's emphasis on the limitations of static evaluation methods and the need for dynamic, multi-faceted evaluation approaches to better capture the capabilities and adaptability of LLMs.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 4, "question_token_count": 24, "avg_answer_token_count": 29}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "How can the principles of code synthesis be applied to improve the accuracy of model-generated code in educational settings?", "question": "In the context of evaluating code synthesis, what are the potential implications of integrating multiple domain-specific models into a single cohesive system architecture, considering the foundational theories of multi-domain learning?", "choices": {"A": "Increased complexity in error handling", "B": "Enhanced code generation and natural language understanding", "C": "Reduced need for external data sources", "D": "Simplified model architecture"}, "answer": "B", "explanation": "The correct answer is B. The integration of domain-specific models in a cohesive architecture that enables the system to learn and adapt its responses is a significant advancement in code generation. This integration of advanced understanding with deep learning principles and techniques such as reinforcement learning, as well as the use of the graph for better understanding of the code generation process, is crucial for effective code synthesis.", "answer_correctness_score": 9, "explanation_validity_score": 8, "question_clarity_score": 7, "question_groundedness_score": 2, "question_token_count": 36, "avg_answer_token_count": 6}
{"context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n", "topic": "Explore the potential consequences of data contamination in dynamic benchmarking and how it affects the assessment of LLM capabilities.", "question": "How does data contamination influence the bias-variance tradeoff in the evaluation of LLMs, and what is its impact on overfitting?", "choices": {"A": "Data contamination primarily increases bias, reducing the model's ability to capture underlying patterns, leading to underfitting.", "B": "Data contamination primarily increases variance, making the model more sensitive to noise and prone to overfitting.", "C": "Data contamination equally affects both bias and variance, leading to balanced model performance.", "D": "Data contamination has no significant impact on the bias-variance tradeoff or overfitting."}, "answer": "B", "explanation": "The correct answer is B. Data contamination increases variance by introducing noise into the dataset, making the model more sensitive to fluctuations and thus more likely to overfit.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 4, "question_token_count": 27, "avg_answer_token_count": 18}
{"context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n", "topic": "What are the central ideas and nuanced themes within the context of the expert's understanding of the text?", "question": "What are the broader implications of adopting a dynamic evaluation standard in LLMs for future research and development of LLMs?", "choices": {"A": "It will significantly reduce computational costs.", "B": "It will enhance the robustness and reliability of LLM performance.", "C": "It will enable a more standardized approach to evaluating LLMs.", "D": "It will create a more adaptable framework for benchmarking."}, "answer": "D", "explanation": "The context suggests that while dynamic approaches show promise, they still face challenges in reliability and reproducibility. Adopting dynamic benchmarks could lead to more adaptable frameworks in handling evolving data complexities.", "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 4, "question_token_count": 24, "avg_answer_token_count": 11}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "Analyze the central ideas and nuanced themes within the context.", "question": "What is the impact of integrating formal methods into the software development process in enhancing the reliability and security of complex systems?", "choices": {"A": "Enhances the ability to predict and mitigate system vulnerabilities", "B": "Provides a systematic approach to software verification, reducing the likelihood of security breaches", "C": "Increases the complexity and cost of software development", "D": "Improves the software development lifecycle by minimizing manual testing efforts"}, "answer": "B", "explanation": "Formal methods provide a systematic and mathematical approach to verifying software systems, ensuring that they meet specified requirements and are free from vulnerabilities, thus reducing security breaches and enhancing reliability.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 1, "question_token_count": 24, "avg_answer_token_count": 12}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "Evaluate the relevance and application of code synthesis in the context of AI and programming education.", "question": "How might the integration of advanced AI in educational technology redefine the pedagogical approach to teaching programming languages, considering both theoretical and practical implications?", "choices": {"A": "By automating the learning process, reducing the need for human instructors.", "B": "By enhancing personalized learning experiences and adapting educational content to individual student needs.", "C": "By focusing solely on theoretical aspects, neglecting practical applications.", "D": "By eliminating the need for traditional programming languages."}, "answer": "B", "explanation": "Advanced AI in educational technology can enhance personalized learning experiences by adapting educational content to individual student needs, aligning with both theoretical and practical implications of integrating AI in teaching programming languages.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 9, "question_groundedness_score": 4, "question_token_count": 28, "avg_answer_token_count": 13}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "How do the principles of cognitive development theories inform current educational practices in mathematics education?", "question": "Given the increasing complexity of datasets used to evaluate models' mathematical capabilities, how might cognitive development theories inform the design of educational interventions that improve student outcomes in mathematics, especially considering the datasets and challenges mentioned (GSM8K, MATH, AIME, CNMO)?", "choices": {"A": "By tailoring educational interventions to the individual cognitive development stages of learners.", "B": "By providing a framework for personalized learning paths that adapt to the proficiency levels of learners.", "C": "By highlighting the importance of foundational skills in early education.", "D": "By emphasizing the role of problem-solving strategies in building mathematical reasoning skills."}, "answer": "D", "explanation": "The correct answer emphasizes the importance of problem-solving strategies in building mathematical reasoning skills. This approach is informed by cognitive development theories, which emphasize the role of complex problem-solving in educational interventions. Such strategies are crucial in informing educational practices, as they help tailor educational interventions to enhance learning outcomes effectively. The answer requires an understanding of how these theories can be practically applied to design effective educational strategies that are responsive to the cognitive development stages of learners, hence the high difficulty level of the question.", "answer_correctness_score": 8, "explanation_validity_score": 8, "question_clarity_score": 6, "question_groundedness_score": 4, "question_token_count": 54, "avg_answer_token_count": 15}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "[Topic Text]", "question": "How do the underlying theories of domain-specific knowledge and their application in evaluating the competency of experts in the provided context influence the practical implications of this text in real-world applications of domain-specific tasks?", "choices": {"A": "They underscore the importance of theoretical knowledge and its direct application in assessing domain-specific tasks.", "B": "They illustrate the significance of practical knowledge without theoretical backing in evaluating domain-specific tasks.", "C": "They highlight the necessity of theoretical assumptions for practical applications in domain-specific tasks.", "D": "They demonstrate the irrelevance of theoretical understanding in the evaluation of domain-specific tasks."}, "answer": "A", "explanation": "The correct answer reflects the context's emphasis on the theoretical underpinnings and their practical application in evaluating domain experts' competency.", "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 4, "question_groundedness_score": 2, "question_token_count": 39, "avg_answer_token_count": 17}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "Evaluate the implications of understanding the nuanced themes and significant relationships within the context.", "question": "Which of the following accurately reflects the deeper implications of using canary strings in mitigating data contamination in training models?", "choices": {"A": "They serve as crucial indicators of model performance in the context of enhancing data integrity and validating the reliability of model evaluations and assessments.", "B": "Canary strings help in identifying data breaches in AI models, ensuring that the models are robust against data leaks.", "C": "Canary strings, by their nature, act as evidence of model contamination when they appear unexpectedly in the output.", "D": "Canary tokens serve as markers for potential model contamination in the outputs of language models, highlighting the need for robust data validation."}, "answer": "A", "explanation": "The correct answer is based on the understanding of the data, ensuring that the question is answerable by a domain expert without explicit reference to the provided context. This question demands an in-depth understanding of the nuances and implications of using these methods in real-world scenarios.", "answer_correctness_score": 8, "explanation_validity_score": 2, "question_clarity_score": 6, "question_groundedness_score": 9, "question_token_count": 23, "avg_answer_token_count": 24}
{"context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n", "topic": "Discuss the role of strategic evaluations in the context of domain expertise.", "question": "How does the evolving nature of data contamination in benchmarking LLMs impact the reliability of dynamic approaches in ensuring data integrity?", "choices": {"A": "It leads to an increased focus on static methods due to their consistency.", "B": "It highlights the need for standardized dynamic evaluation methods to address reliability and reproducibility challenges.", "C": "It reduces the importance of data provenance in understanding LLMs' behaviors.", "D": "It suggests that static methods are sufficient for future research in LLM benchmarking."}, "answer": "B", "explanation": "The context discusses the challenges faced by dynamic approaches in benchmarking LLMs, particularly regarding reliability and reproducibility. It suggests that future research should focus on standardized dynamic evaluation to address these challenges, emphasizing the importance of dynamic approaches in maintaining data integrity.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 6, "question_token_count": 24, "avg_answer_token_count": 15}
{"context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n", "topic": "Evaluate the effectiveness of various educational methodologies in assessing the competency of topic domain experts based on the provided textual information.", "question": "How does the evaluation of domain experts' understanding of LLMs' capabilities and limitations impact the generation of new benchmarks in educational settings?", "choices": {"A": "It enhances the precision and relevance of benchmarks by aligning them with the nuanced understanding of LLMs' strengths and weaknesses.", "B": "It results in the creation of benchmarks that are overly reliant on theoretical knowledge without practical application.", "C": "It leads to the development of benchmarks that focus solely on the technical aspects of LLMs, ignoring their practical implications.", "D": "It undermines the development of new benchmarks by highlighting the limitations of LLMs, leading to a lack of innovation."}, "answer": "A", "explanation": "The correct answer is A because the evaluation of domain experts' understanding of LLMs' capabilities and limitations ensures that the new benchmarks are both precise and relevant, aligning them with the nuanced strengths and weaknesses of LLMs.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 1, "question_token_count": 27, "avg_answer_token_count": 22}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "What are the educational implications of using code synthesis tools to enhance learning in programming classes?", "question": "In what ways can AI-driven systems in education be utilized to forecast and enhance student performance in STEM subjects, considering the theoretical and practical challenges of integrating such technologies?", "choices": {"A": "By creating adaptive learning environments that tailor educational experiences to individual student needs.", "B": "Through the development of AI models that can provide personalized feedback and real-time assessments.", "C": "By employing AI to automate administrative tasks, allowing educators to focus more on teaching.", "D": "By implementing AI to predict and improve student outcomes through the analysis of learning patterns and data."}, "answer": "D", "explanation": "The correct answer focuses on the use of AI to analyze and predict student performance by understanding learning patterns, which is crucial for enhancing educational outcomes in STEM fields. This answer encapsulates the challenges and potential of AI in educational settings.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 1, "question_token_count": 33, "avg_answer_token_count": 17}
{"context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n", "topic": "Discuss the implications of the core ideas, themes, and significant relationships within the context.", "question": "How should the evaluation of LLMs adapt to address the limitations of current benchmarks in accurately reflecting model performance in the face of data contamination?", "choices": {"A": "By enhancing static evaluation methods to include more comprehensive data sets.", "B": "Through the development of dynamic evaluation methods that account for real-time data changes and contamination.", "C": "By solely relying on existing benchmarks and improving data preprocessing techniques.", "D": "By focusing on theoretical model improvements rather than practical evaluation methodologies."}, "answer": "B", "explanation": "Dynamic evaluation methods are suggested as necessary due to their ability to adapt to real-time data changes, which static methods cannot adequately address, especially in the context of data contamination.", "answer_correctness_score": 10, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 10, "question_token_count": 28, "avg_answer_token_count": 14}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "How does the phenomenon of data redundancy and repetition in training datasets affect the outcomes of model performance?", "question": "What are the potential consequences of failing to ensure data integrity during the training phase of an AI model, particularly in the context of data preprocessing and model evaluation?", "choices": {"A": "It can lead to model overfitting and inaccurate predictions.", "B": "It may cause the model to learn incorrect patterns and fail to generalize to new data.", "C": "The model might develop a biased understanding of the data and exhibit signs of data leakage.", "D": "The model might encounter overfitting and data leakage issues."}, "answer": "B", "explanation": "The correct answer is B. It may result in a model that is unable to generalize to new data, potentially leading to model overfitting and overfitting. This would require an expert to understand the relationship between data integrity and model performance, specifically addressing the potential consequences of data overlap and its impact on model accuracy and generalization.", "answer_correctness_score": 9, "explanation_validity_score": 8, "question_clarity_score": 8, "question_groundedness_score": 9, "question_token_count": 32, "avg_answer_token_count": 15}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "How does the concept of data integrity and its effect on model training and evaluation frameworks influence the reliability of results?", "question": "What are the potential implications of syntactic transformations, like punctuation and whitespace adjustments, on the performance of a machine learning model's evaluation metrics, considering the challenges of data integrity in model training?", "choices": {"A": "They could lead to an overestimation of the model's capabilities due to data leakage.", "B": "They may result in an underestimation of the model's capabilities due to data integrity issues.", "C": "They generally do not affect the outcome of the evaluation metrics.", "D": "They can lead to significant improvements in the model's performance metrics."}, "answer": "A", "explanation": "The correct answer should demonstrate the deep understanding of the impact of data processing on model evaluation.", "answer_correctness_score": 10, "explanation_validity_score": 6, "question_clarity_score": 9, "question_groundedness_score": 10, "question_token_count": 37, "avg_answer_token_count": 15}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "Examine the implications of AI-driven evaluation in the development of machine learning models.", "question": "In the context of LLMs evolving into general-purpose solvers, what is the primary implication of utilizing dynamic benchmarks over static benchmarks in assessing model performance?", "choices": {"A": "They reduce the need for human evaluation in model training.", "B": "They mitigate the risk of data contamination in model assessments.", "C": "They improve computational efficiency in processing large datasets.", "D": "They enhance the model's ability to follow instructions without errors."}, "answer": "B", "explanation": "The correct answer is B. Dynamic benchmarks are designed to adapt to evolving LLMs, thereby reducing the risk of data contamination and ensuring the validity of model assessments. This understanding is crucial as static benchmarks may become outdated or too easy for advanced models, leading to inaccurate performance assessments.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 9, "question_token_count": 30, "avg_answer_token_count": 11}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "What is the significance of understanding the implications of data integrity on the model evaluation process?", "question": "In the context of machine learning model evaluation, how does the nuanced understanding of data integrity impact the validity of model assessments in ensuring the model's performance metrics are unbiased and reliable?", "choices": {"A": "It highlights the need for rigorous model evaluation protocols to prevent overfitting and ensure model generalization.", "B": "It is imperative to understand the implications of data quality on the predictive accuracy and reliability of model performance in an unbiased manner.", "C": "It emphasizes the critical need for high data quality standards to ensure the integrity of the evaluation.", "D": "It underscores the importance of unbiased and transparent model assessment procedures in model validation processes."}, "answer": "A", "explanation": "The correct answer (A) is chosen because it requires an in-depth understanding of the context of data integrity and its implications for model evaluation. This question challenges the expert to demonstrate an understanding of how nuanced data details can impact model assessment, ensuring the question is self-contained and does not refer explicitly to the text.", "answer_correctness_score": 6, "explanation_validity_score": 5, "question_clarity_score": 8, "question_groundedness_score": 7, "question_token_count": 35, "avg_answer_token_count": 20}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "How does the evolving complexity of mathematical theories impact the design and implementation of educational frameworks in mathematics?", "question": "How does the integration of algebraic structures into AI models affect the evaluation of their impact on the development of problem-solving algorithms?", "choices": {"A": "They provide a framework for complex problem-solving.", "B": "They enhance the model's ability to generalize across various mathematical problems.", "C": "They ensure the robustness of AI models in handling diverse mathematical challenges.", "D": "They refine the ability to tackle intricate calculations and inferential logic puzzles."}, "answer": "C", "explanation": "The answer is correct because the complexity of the mathematical structures and their application in practical AI problems is crucial for understanding the evaluation of AI's problem-solving capabilities.", "answer_correctness_score": 8, "explanation_validity_score": 6, "question_clarity_score": 2, "question_groundedness_score": 1, "question_token_count": 25, "avg_answer_token_count": 13}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "Evaluate the potential implications of the provided information.", "question": "In the context of evaluating machine learning models, which of the following best describes the implications of data contamination on the integrity of model evaluation processes?", "choices": {"A": "It leads to improved generalization of models by exposing them to diverse datasets.", "B": "It causes models to memorize training data, resulting in poor performance on unseen data.", "C": "It enhances the robustness of models by ensuring they learn from a wide variety of data points.", "D": "It has no significant impact on the evaluation process as models are designed to handle such variations."}, "answer": "B", "explanation": "Data contamination can cause models to memorize training data rather than learning to generalize, which is indicated by the presence of canary strings. This results in poor performance on unseen data, undermining the integrity of the evaluation process.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 6, "question_token_count": 29, "avg_answer_token_count": 17}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "What are the broader implications of applying advanced mathematical concepts in real-world educational settings?", "question": "In what ways might the evolving complexity of mathematical problem-solving datasets, like those seen in recent challenges such as AIME or MATH, impact the future development of educational strategies and pedagogical approaches in mathematical education?", "choices": {"A": "It encourages educators to rethink the integration of complex data into curricula, suggesting a multidisciplinary approach that goes beyond traditional teaching methods.", "B": "The proliferation of advanced math problems in education.", "C": "It offers a framework for understanding the role of intuition in problem-solving.", "D": "It implies a shift towards a more pragmatic educational paradigm."}, "answer": "A", "explanation": "The correct answer is A because the context discusses the increasing complexity of multi-step mathematical problems and the need for educational strategies to evolve in response to new challenges. It reflects a nuanced understanding of the context's implications for educational strategies.", "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 4, "question_token_count": 42, "avg_answer_token_count": 16}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "Investigate the relationship between data currency and a professional's ability to demonstrate understanding.", "question": "In the context of assessing domain expert competency, why is it crucial to ensure the deep understanding of both temporal data's implications and its application in assessing AI model performance?", "choices": {"A": "It ensures the evaluation of AI models in keeping up with the latest advancements in the field.", "B": "[ The answer is correct. ]", "C": "It allows for the assessment of model performance in real-time applications.", "D": "[ It can lead to identifying potential data gaps and the necessity of continuous learning and adaptation in the context of rapid technological evolution.]"}, "answer": "C", "explanation": "The correct answer is C because the underlying assumption is that a domain expert would understand that using only the most recent information allows for an accurate understanding of AI models' performance. This is why it's crucial for the evaluation of AI systems, as it challenges the expert to think about the broader implications and subtleties in the context of implications, such as how to ensure the evaluation process is grounded in the actual use of the system. The correct answer is C, as it tests for an understanding of the implications, going beyond the specific context to the generalization of the importance of using post-knowledge cutoff data in AI evaluations.", "answer_correctness_score": 9, "explanation_validity_score": 8, "question_clarity_score": 6, "question_groundedness_score": 4, "question_token_count": 34, "avg_answer_token_count": 16}
{"context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n", "topic": "How are the central ideas of the context of the text related to the evaluation of domain experts?", "question": "In the context of evaluating domain experts, how does the central idea of dynamic systems' response to contextual changes relate to the implications for competency evaluation in dynamic benchmarking?", "choices": {"A": "The central idea emphasizes the importance of understanding dynamic systems' response to changes in context for evaluating competency.", "B": "The context suggests that dynamic systems' response to changes in context is irrelevant to competency evaluation.", "C": "The text implies that understanding dynamic systems' response to changes in context is only marginally related to competency evaluation.", "D": "The text suggests that dynamic systems' response to changes in context is a secondary consideration in competency evaluation."}, "answer": "A", "explanation": "The correct answer is A because the text discusses the importance of understanding dynamic systems' response to contextual changes in evaluating competency, highlighting its significance in dynamic benchmarking.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 2, "question_token_count": 33, "avg_answer_token_count": 20}
{"context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n", "topic": "[ The implications of using benchmarks in AI and the ethical considerations in AI systems ]", "question": "What are the potential consequences of utilizing static benchmarks in the evaluation of AI systems within the context of AI, given that these benchmarks may not adapt to the dynamic nature of AI evolution and could lead to misrepresentation of AI capabilities?", "choices": {"A": "A static benchmark may not be able to keep up with the AI evolution, which can lead to misrepresentation of AI system's capabilities.", "B": "Benchmarks are crucial for assessing AI capabilities.", "C": "The AI system's performance is crucial for its evaluation.", "D": "There is a need for benchmarks to evaluate the AI system's performance."}, "answer": "A", "explanation": "Option A is correct because it reflects the challenges associated with using static benchmarks that do not adapt to the rapid changes in AI, which can result in a misrepresentation of an AI system's capabilities. The static nature of these benchmarks fails to capture the dynamic advancements in AI, leading to potentially inaccurate assessments.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 2, "question_token_count": 46, "avg_answer_token_count": 15}
{"context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n", "topic": "What are the key components of an effective competency evaluation for a domain expert, and how does this understanding impact the evaluation process?", "question": "In the context of dynamic benchmarking, what is the primary role of the oracle function \ud835\udca2(\u22c5) in ensuring the reliability of datasets, and how does this role impact the evaluation of LLMs when incorrect datasets are generated?", "choices": {"A": "To provide ground truth for scoring functions", "B": "To ensure the ground truth of inputs", "C": "To guarantee the correctness of the generated dataset", "D": "To act as a domain-specific annotator"}, "answer": "C", "explanation": "The primary role of the oracle function in the context of dynamic benchmarking algorithms is to guarantee the correctness of the generated dataset. This role is pivotal in ensuring that the benchmarks maintain their reliability and accuracy when applied to LLMs. The oracle function acts as a critical reference point for verifying the correctness of the dataset, which is essential for ensuring the validity of the evaluation. The correct answer emphasizes the oracle's function as a guarantor of dataset integrity, which is crucial in maintaining the reliability of the benchmark.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "question_token_count": 46, "avg_answer_token_count": 8}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "What is the impact of the emergence of transformer-based models on the evaluation of LLMs?", "question": "What is the implication of employing a single set of metrics for evaluating the diverse capabilities of Large Language Models (LLMs), according to the context?", "choices": {"A": "They ensure comprehensive coverage of various evaluation criteria.", "B": "They simplify the evaluation process by reducing complexity.", "C": "They increase the risk of data leakage due to static datasets.", "D": "They are irrelevant to the evaluation of LLMs."}, "answer": "B", "explanation": "The correct answer is B. The context suggests that a single set of metrics is insufficient for a comprehensive evaluation of LLMs, implying that simplifying the evaluation process with one set of metrics does not capture the diverse capabilities of these models.", "answer_correctness_score": 8, "explanation_validity_score": 8, "question_clarity_score": 6, "question_groundedness_score": 8, "question_token_count": 30, "avg_answer_token_count": 11}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "What is the role of data quality control in ensuring the accuracy of the LLM evaluations and measurements?", "question": "In the context of machine learning, how might syntactic transformations of the input data, which are designed to preserve the original meaning, influence the evaluation of a language model's performance metrics such as perplexity, fluency, and interpretability?", "choices": {"A": "By introducing syntactic variations that maintain the lexical integrity of the original dataset, leading to potential overlaps between the training and test datasets.", "B": "By causing semantic drifts that may not necessarily affect the lexical components but can alter the model's understanding of the meaning.", "C": "Through the inadvertent introduction of additional context that goes beyond the original scope of the training data.", "D": "By establishing a protocol to ensure that the data remains pristine and free from any form of contamination, maintaining the integrity of the training process."}, "answer": "A", "explanation": "The correct answer (A) emphasizes the importance of data integrity and the need to ensure that the training dataset is free from any contamination that could invalidate the performance metrics, which are crucial for the reliable evaluation of language models. This question challenges the respondent to consider the specific ways that syntactic variations can impact the assessment of these models, ensuring a nuanced understanding of the material.", "answer_correctness_score": 9, "explanation_validity_score": 8, "question_clarity_score": 6, "question_groundedness_score": 7, "question_token_count": 47, "avg_answer_token_count": 24}
{"context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n", "topic": "[ The role of transparency in the development and evaluation of AI systems ]", "question": "How does the principle of transparency in AI influence the design and deployment of ethical AI systems, particularly in ensuring accountability and trustworthiness?", "choices": {"A": "Transparency in AI development processes is crucial for building trust in AI systems, as it ensures that stakeholders are engaged and that AI systems are developed with robust, ethical standards in mind.", "B": "Transparency in AI systems is primarily important for building trust and trustworthiness, where stakeholders must be actively engaged in the conversation about the implications of AI development.", "C": "The correct answer.", "D": "None of the above."}, "answer": "C", "explanation": "The correct answer highlights the complex and nuanced understanding of how transparent practices and open communication about AI systems' operations contribute to the public's understanding and acceptance of AI systems, emphasizing the role of transparency in AI development and its ethical implications. The answer stresses the necessity of transparency in AI systems to build trust and credibility among users and researchers.", "answer_correctness_score": 6, "explanation_validity_score": 8, "question_clarity_score": 8, "question_groundedness_score": 6, "question_token_count": 27, "avg_answer_token_count": 19}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "Evaluate the impact of the significant relationships within the text.", "question": "In the context of machine learning model evaluation, how does the use of canary tokens illustrate the inherent trade-offs between enhancing data integrity and maintaining model transparency?", "choices": {"A": "Canary tokens are primarily used to enhance model transparency by providing clear markers for data sources.", "B": "The use of canary tokens allows for the detection of data contamination but may obscure the model's interpretability.", "C": "Canary tokens serve to improve data integrity without affecting model transparency.", "D": "By identifying data leaks, canary tokens ensure both data integrity and complete transparency of model operations."}, "answer": "B", "explanation": "The use of canary tokens, while effective in identifying data contamination, may introduce complexity that can obscure model interpretability, thus highlighting the trade-off between data integrity and transparency.", "answer_correctness_score": 8, "explanation_validity_score": 7, "question_clarity_score": 6, "question_groundedness_score": 4, "question_token_count": 32, "avg_answer_token_count": 18}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "What are the historical developments in mathematics education that have shaped current educational practices?", "question": "How do the computational complexity theories discussed influence the development of educational practices in the context of AI and machine learning?", "choices": {"A": "By providing a framework for evaluating the problem-solving capabilities of AI models in educational settings.", "B": "By setting benchmarks that define the limits of what AI models can achieve in real-world educational applications.", "C": "By emphasizing the importance of computational limits in designing curriculum and educational tools.", "D": "By highlighting the need for interdisciplinary approaches combining computational theory and educational practices."}, "answer": "A", "explanation": "The correct answer requires an understanding of how computational complexity theories are used to evaluate AI models' problem-solving capabilities, setting benchmarks, and influencing the design of educational tools and practices.", "answer_correctness_score": 8, "explanation_validity_score": 7, "question_clarity_score": 1, "question_groundedness_score": 1, "question_token_count": 23, "avg_answer_token_count": 17}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "What are the potential consequences of data leakage on the overall performance and evaluation of models in machine learning pipelines?", "question": "Which of the following best describes the potential impact of data contamination on the evaluation of machine learning models?", "choices": {"A": "It leads to overfitting, resulting in poor generalization.", "B": "It inflates performance metrics by inflating model performance.", "C": "It reduces the variance in the model\u2019s performance.", "D": "It improves the robustness of the model against adversarial examples."}, "answer": "B", "explanation": "The correct answer is B. Data contamination, specifically through overlapping information between training and test datasets, leads to inflated performance metrics. This is because the model may appear to perform better than it truly does, due to overfitting on the evaluation set. This phenomenon skews performance metrics, making it difficult to assess the true performance of the model under real-world conditions.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "question_token_count": 21, "avg_answer_token_count": 11}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "Discuss the central ideas, nuanced themes, and significant relationships within the context.", "question": "Given the implications of using canary tokens for data integrity in LLMs, how would you anticipate the impact of this method on model robustness in the context of evaluating the effectiveness of NLP models' performance benchmarks.", "choices": {"A": "Enhancing the reliability of the models.", "B": "Ensuring data integrity and detecting data contamination.", "C": "Increasing the training data size for better generalization.", "D": "Minimizing the model's exposure to adversarial examples."}, "answer": "B", "explanation": "Canary strings are primarily used for data integrity and contamination detection, which ensures the reliability of the models by maintaining data quality. Thus, the correct answer is B.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 2, "question_groundedness_score": 6, "question_token_count": 42, "avg_answer_token_count": 9}
{"context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n", "topic": "The ethical considerations and potential risks of data leaks in the field of LLMs, with a focus on data privacy and data security.", "question": "Considering the vulnerabilities of RSA encryption in the face of quantum computing advancements, which of the following is the most critical implication for data security protocols?", "choices": {"A": "The necessity of developing quantum-resistant algorithms to ensure future-proof encryption methods.", "B": "The potential for quantum computing to enhance RSA encryption through increased computational power.", "C": "The immediate replacement of RSA encryption with elliptic curve cryptography.", "D": "The development of a new quantum-based encryption method that solely relies on classical computing principles."}, "answer": "A", "explanation": "The context discusses the risk that quantum computing poses to traditional encryption methods like RSA, emphasizing the need for new quantum-resistant encryption techniques. This makes the necessity of developing quantum-resistant algorithms the most critical implication for data security protocols.", "answer_correctness_score": 10, "explanation_validity_score": 9, "question_clarity_score": 4, "question_groundedness_score": 1, "question_token_count": 29, "avg_answer_token_count": 15}
{"context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n", "topic": "What are the central ideas and nuanced themes within the context of the expert's understanding of the text?", "question": "In the context of evolving large language models (LLMs), what is a significant implication of using static benchmarking methods as opposed to dynamic approaches?", "choices": {"A": "Static methods provide consistent evaluations but may fail to reflect the true performance of models as they evolve.", "B": "Static methods are easier to implement and ensure reproducibility across different studies.", "C": "Dynamic approaches are more vulnerable to data contamination, leading to unreliable evaluations.", "D": "Dynamic approaches are less adaptable to the rapid changes in model architectures and training data."}, "answer": "A", "explanation": "Static benchmarking methods offer consistency but may not effectively measure the evolving performance of models, as they do not adapt to changes in training data and model architectures. In contrast, dynamic approaches, though challenging, aim to provide more accurate assessments by reflecting the current state of models.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "question_token_count": 29, "avg_answer_token_count": 16}
{"context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n", "topic": "Critically analyze the importance of static benchmarks in the context of machine learning model evaluation and their impact on model reliability.", "question": "What are the implications of relying on static benchmarks for evaluating machine learning models in dynamic real-world applications, and how might this affect the perceived reliability and adaptability of these models?", "choices": {"A": "Static benchmarks provide a consistent measure of model performance but may fail to capture the dynamic nature of real-world scenarios, potentially leading to an overestimation of model reliability.", "B": "Static benchmarks are sufficient for evaluating all aspects of model performance, ensuring a comprehensive understanding of a model's capabilities.", "C": "Dynamic real-world applications are adequately represented by static benchmarks, ensuring that models are robust and adaptable to changing conditions.", "D": "Static benchmarks eliminate the need for adaptive evaluation methods by providing a fixed and unchanging measure of model performance."}, "answer": "A", "explanation": "Static benchmarks offer a consistent way to measure performance but may not reflect the dynamic nature of real-world applications, potentially misrepresenting a model's reliability and adaptability. This highlights the importance of adaptive evaluation methods in capturing the true capabilities of models in changing environments.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 2, "question_token_count": 35, "avg_answer_token_count": 25}
{"context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n", "topic": "What are the central ideas and nuanced themes within the context.", "question": "In the context of the continuous evolution of brain-computer interfaces, how do the established principles of neural plasticity and white matter activity correlations suggest potential advancements in the design of advanced prosthetics and robotic systems?", "choices": {"A": "They indicate a possible shift towards static, non-adaptive systems that prioritize reliability over adaptability.", "B": "They suggest the integration of dynamic, context-sensitive evaluation methods that could enhance the adaptability and functionality of prosthetics and robotic systems.", "C": "They imply a focus on reducing the complexity of neural interfaces to make them more accessible to users with varying levels of technical expertise.", "D": "They emphasize the need for standardized, static evaluation methods to ensure consistent results across different applications."}, "answer": "B", "explanation": "The principles of neural plasticity and the correlation between neural activity and white matter activity suggest that dynamic, context-sensitive evaluation methods could be integrated into the design of advanced prosthetics and robotic systems, enhancing their adaptability and functionality in response to evolving data environments.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 9, "question_groundedness_score": 1, "question_token_count": 41, "avg_answer_token_count": 22}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "What are the strengths and weaknesses of various metrics in language modeling?", "question": "In the context of designing an evaluation framework for language models that mitigates the risk of data contamination, how would you address the potential for bias in dynamic benchmarks, given the constraints of computational efficiency and the necessity for generalizability?", "choices": {"A": "By proposing a hybrid approach that integrates the principles of static benchmarks with continuous updates, thereby enhancing the reliability and validity of the evaluations.", "B": "By emphasizing the importance of data encryption to secure benchmarking processes and ensure the integrity of model evaluations.", "C": "By focusing on the application of dynamic data generation techniques to maintain relevance.", "D": "Through the development of advanced machine learning algorithms that can self-regulate and adapt to new data inputs."}, "answer": "A", "explanation": "The correct answer is A. This option correctly identifies the hybrid approach's ability to balance the need for dynamic updates with the principles of static benchmarks, which enhances both the reliability and validity of evaluations. This approach is effective in maintaining the integrity of the benchmarks, ensuring that the evaluation of language models remains accurate and unbiased. The explanation reflects a deep understanding of the implications of integrating dynamic updates with static benchmark principles, aiming to enhance evaluation accuracy.", "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 8, "question_token_count": 46, "avg_answer_token_count": 20}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "Discuss the potential implications of the provided information.", "question": "What is the broader implication of using canary strings in identifying data contamination in model training?", "choices": {"A": "Canary strings help in boosting model performance by providing additional data points.", "B": "Canary strings serve as markers to identify data memorization, indicating potential data contamination.", "C": "Canary strings improve model training efficiency by reducing the need for large datasets.", "D": "Canary strings are used to enhance the accuracy of static benchmarking by adding unique identifiers."}, "answer": "B", "explanation": "The correct answer is B. Canary strings are used as markers to identify data memorization, indicating potential data contamination, which reflects a broader implication for maintaining data integrity in model training.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "question_token_count": 19, "avg_answer_token_count": 16}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "Discuss the differentiation between syntactic and semantic contamination and its implications for evaluating large language models.", "question": "In the context of evaluating the robustness of large language models, what is a nuanced implication of failing to detect and address potential data contamination in the training set?", "choices": {"A": "It could lead to an overestimation of the model's ability to generalize beyond memorization of training data.", "B": "It could result in the model having an inflated sense of understanding and reasoning.", "C": "It might cause the model to appear to have a better understanding of the training data.", "D": "It may lead to the incorrect assumption that the model can accurately predict unseen data."}, "answer": "A", "explanation": "The correct answer is A. The explanation is that failing to address data contamination in the training set leads to an overestimation of the model's generalization capabilities, particularly in the context of assessing the robustness of the model's understanding of novel situations. This question encourages deep engagement with the content, reflecting on the potential for model evaluation to be influenced by data contamination. It requires a sophisticated understanding of the implications of data quality on the validity of model evaluations. The question is designed to challenge even the smartest domain experts, as it requires a nuanced understanding of the potential impact of data quality on the robustness assessment of the model's performance. This level of complexity is ensured by the intricate nature of the implications and the requirement for domain experts to consider the broader ramifications of data quality on model evaluation, beyond the specifics provided in the context.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "question_token_count": 32, "avg_answer_token_count": 18}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "How do the underlying theories of code synthesis and debugging inform the assessment of a domain expert's ability to identify and fix code issues?", "question": "In the context of neural network models, what role does the integration of quantum principles play in enhancing the accuracy of predictions for real-world applications?", "choices": {"A": "By refining the model\u2019s parameters to incorporate a higher level of detail into the code synthesis tasks.", "B": "Through the optimization of existing algorithms to improve the performance of the model.", "C": "By enabling the model to leverage domain-specific knowledge to enhance the accuracy of predictions in real-world applications.", "D": "By refining the debugging processes and evaluation criteria to incorporate theoretical physics principles into the model's performance."}, "answer": "A", "explanation": "The correct answer (A) emphasizes the integration of advanced debugging algorithms and traditional machine learning models to potentially enhance the efficacy of AI models in predicting robustness. The correct answer is A, as it captures the essence of refining evaluation criteria and incorporating domain-specific knowledge to improve AI models' performance.", "answer_correctness_score": 4, "explanation_validity_score": 3, "question_clarity_score": 9, "question_groundedness_score": 1, "question_token_count": 29, "avg_answer_token_count": 18}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "Evaluate the role of dynamic benchmarks in enhancing the evaluation of LLMs in context understanding tasks.", "question": "How can dynamic benchmarks address the challenges posed by the rapid evolution of AI models in maintaining an accurate and relevant performance assessment?", "choices": {"A": "By providing static metrics that remain unchanged over time, ensuring consistency in evaluation.", "B": "By evolving alongside the models, offering adaptable and relevant metrics that reflect the latest capabilities.", "C": "By focusing solely on historical data to predict future performance trends.", "D": "By limiting the scope of evaluation to a narrow range of tasks, ensuring depth over breadth."}, "answer": "B", "explanation": "Dynamic benchmarks are designed to evolve with AI models, ensuring that performance evaluations remain relevant and accurate as models advance. Static benchmarks may become outdated, while dynamic ones adapt to reflect the latest capabilities and complexities of evolving AI systems.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 6, "question_token_count": 25, "avg_answer_token_count": 16}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "How do the principles of data management and the importance of maintaining data quality standards affect the reliability of model evaluations?", "question": "How can the subtleties of data contamination impact the reliability of benchmark results in machine learning model evaluations?", "choices": {"A": "It leads to biased evaluations due to improper training data.", "B": "It reveals that the model's ability to generalize is compromised by overlapping datasets.", "C": "It identifies that data integrity checks are crucial for ensuring data quality during the training phase.", "D": "It underscores the potential for compromised data to impact model evaluations."}, "answer": "C", "explanation": "The correct answer (C) highlights the importance of data integrity checks, which are essential to maintain data quality and ensure reliable model evaluations.", "answer_correctness_score": 7, "explanation_validity_score": 6, "question_clarity_score": 8, "question_groundedness_score": 9, "question_token_count": 22, "avg_answer_token_count": 14}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "Examine the significance of evaluation in LLMs.", "question": "In the context of evaluating large language models (LLMs), what is a potential implication of relying solely on static benchmarks for assessing model performance?", "choices": {"A": "Dynamic benchmarks are less reliable than static benchmarks.", "B": "Static benchmarks offer a consistent baseline for measuring LLM performance.", "C": "Static benchmarks may fail to account for the evolving capabilities and applications of LLMs.", "D": "They ensure consistent standards across various models."}, "answer": "C", "explanation": "Static benchmarks might become outdated quickly as LLMs evolve, failing to capture the latest advancements and real-world applicability.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "question_token_count": 29, "avg_answer_token_count": 12}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "Evaluate how well professionals understand the central themes.", "question": "In the context of evaluating the effectiveness of canary tokens in the detection of data contamination, which of the following best describes the primary implications of the text?", "choices": {"A": "The effectiveness of canary tokens is contingent upon model developers being aware and responsive to these markers.", "B": "The use of canary strings is a foolproof method for all types of data contamination detection.", "C": "The context implies that canary strings are effective only when the data is clean.", "D": "The text suggests that the method is ineffective in cases where the developer aims to leak data."}, "answer": "A", "explanation": "The correct answer (A) is derived from the context which emphasizes that the effectiveness of canary strings depends on the model trainers' awareness and response to these markers. This implies that if the developers are not responsive, the canary strings will not effectively mitigate data contamination. The other options are incorrect as they either provide absolute statements or misinterpret the context's implications.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 8, "question_token_count": 32, "avg_answer_token_count": 19}
{"context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n", "topic": "What are the implicit assumptions in the text?", "question": "Which of the following best encapsulates the role of machine learning in dynamically assessing the efficacy of models?", "choices": {"A": "A methodological approach for simulating the dynamic behavior of complex systems", "B": "A framework for systematically evaluating the performance of various models", "C": "An idealized method for assessing the capabilities of models in a controlled environment", "D": "A computational method for evaluating model performance"}, "answer": "B", "explanation": "The correct answer is B. It highlights the significance of understanding and applying machine learning in AI to assess the performance of models and systems. The correct answer is B. It reflects a high level of complexity, requiring a deep understanding of the subject matter, as it involves the development of a computational method for the assessment of a model or system. The incorrect answers are plausible yet subtly incorrect because they each suggest a different aspect of evaluation and the role of machine learning in this context.", "answer_correctness_score": 9, "explanation_validity_score": 8, "question_clarity_score": 2, "question_groundedness_score": 1, "question_token_count": 21, "avg_answer_token_count": 11}
{"context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n", "topic": "What are the implicit assumptions and subtle details in the context, and how do they affect the competency evaluation of domain experts?", "question": "In the context of computational theories and applications, how does the implicit assumption about the relationship between the central concept and the underlying theory affect the validity and generalizability of the dynamic benchmarking algorithm?", "choices": {"D": "The practical application of the theoretical understanding in the context of domain experts' evaluations.", "C": "Theoretical understanding of the relationship between the central idea and the practical implementation.", "A": "Theoretical implications of the underlying relationship between the core concept and its practical applications.", "B": "The practical application of the underlying theory in the context of domain experts' understanding."}, "answer": "B", "explanation": "The chosen answer provides insight into the theoretical underpinning, highlights the critical engagement with the material, and the correct application of the information. The correct answer elucidates the critical understanding of the content and implications for domain experts' competency evaluations.", "answer_correctness_score": 6, "explanation_validity_score": 5, "question_clarity_score": 8, "question_groundedness_score": 6, "question_token_count": 38, "avg_answer_token_count": 16}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "Investigate the challenges of developing a holistic understanding of LLMs for complex systems.", "question": "In the rapidly advancing fields of artificial intelligence, how does one ensure the adaptability and relevance of evaluation metrics for assessing domain expertise, considering the rapid pace of technological and methodological advancements in the field?", "choices": {"A": "By incorporating static benchmarks that evolve with the technology's progress and the changing landscape of domain knowledge.", "B": "Utilize dynamic benchmarks that can adjust to the rapid technological changes and their impact on domain-specific knowledge.", "C": "By focusing on the dynamic interplay between the consistency of evaluation metrics and the evolving nature of expertise.", "D": "Relying on traditional, unchanging metrics that do not account for the dynamic nature of technological and methodological advancements."}, "answer": "A", "explanation": "The correct answer involves understanding the importance of dynamic benchmarks and the dynamic nature of benchmarks that can evolve with the rapid advancements in both technology and domain-specific knowledge. This ensures the comprehensive assessment of expertise in fast-evolving fields.", "answer_correctness_score": 4, "explanation_validity_score": 8, "question_clarity_score": 6, "question_groundedness_score": 4, "question_token_count": 39, "avg_answer_token_count": 21}
{"context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n", "topic": "What are the implications of the ideas and concepts presented in the context?", "question": "How do the nuanced considerations of data validity, reliability, and the impact of bias in data collection influence the development of advanced algorithms for assessing the correctness of datasets used in benchmarking models?", "choices": {"A": "The nuanced understanding of the relationship between data and its implications in benchmarking processes can lead to more informed and accurate evaluations of the methodological approaches.", "B": "The nuanced understanding of the methodologies used in benchmarking datasets can lead to more accurate evaluations of the effectiveness of dynamic testing.", "C": "The understanding of the interplay between data collection processes and the implications for the analysis of the accuracy of results will be discussed, highlighting the importance of the data used in the creation of benchmarks.", "D": "The context of the data used in the process of evaluation is essential for understanding the implications of the evaluation of the benchmark and its impact on the development of the new algorithm."}, "answer": "D", "explanation": "The context explicitly states that the core of the evaluation of the benchmark is the use of data, specifically the understanding of the impact of the use of data in the process of evaluation. The correct answer is therefore D, as it directly reflects the core focus on the significance of data in the evaluation process.", "answer_correctness_score": 7, "explanation_validity_score": 8, "question_clarity_score": 6, "question_groundedness_score": 4, "question_token_count": 37, "avg_answer_token_count": 31}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "Discuss the challenges of developing meaningful and effective evaluation metrics for LLMs.", "question": "How do you differentiate between the theoretical constructs and practical applications of advanced AI evaluation metrics in the context of their implications for educational impact?", "choices": {"A": "The integration of diverse metrics.", "B": "The refinement of evaluation strategies.", "C": "The critical importance of multi-dimensional evaluation.", "D": "The nuanced interplay of metrics for effective assessment."}, "answer": "C", "explanation": "The correct answer highlights the importance of synthesizing high-level general understanding with specific focus on the evaluation of educational content.", "answer_correctness_score": 8, "explanation_validity_score": 6, "question_clarity_score": 6, "question_groundedness_score": 4, "question_token_count": 27, "avg_answer_token_count": 8}
{"context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n", "topic": "Discuss the implications of relying solely on centralized evaluation metrics in the context of educational content, and evaluate the importance of independent verification of data integrity.", "question": "What are the potential risks of an over-reliance on centralized evaluation systems in educational content, and how can these risks impact the integrity and reproducibility of educational assessments?", "choices": {"A": "Centralized systems inherently provide the most unbiased and accurate performance metrics, ensuring the integrity of educational assessments.", "B": "Over-reliance on centralized systems can lead to a lack of transparency and independent verification, potentially compromising the integrity and reproducibility of educational assessments.", "C": "Centralized systems eliminate the need for independent verification, thus enhancing the accuracy and integrity of educational assessments.", "D": "The risks associated with centralized systems are negligible, as they are designed to be inherently transparent and reproducible."}, "answer": "B", "explanation": "Centralized evaluation systems, while providing a standardized approach, can obscure the transparency and independent verification necessary to ensure the integrity and reproducibility of educational assessments. This over-reliance can result in a lack of critical examination and potential biases in the evaluation process.", "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 8, "question_token_count": 34, "avg_answer_token_count": 23}
{"context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n", "topic": "How do the concepts discussed in the context impact the evaluation of a domain expert's understanding of the content?", "question": "How does the integration of oracle-based functions influence the reliability of dynamic benchmarking algorithms in assessing large language models?", "choices": {"A": "It ensures the objective evaluation of the dataset's alignment with the ground truth.", "B": "It guarantees the dataset's applicability to a variety of LLMs.", "C": "It provides a theoretical framework for dataset transformation.", "D": "It ensures the dataset's relevance to current LLMs."}, "answer": "A", "explanation": "The correct answer is A. The oracle function \\(\\mathcal{G}(\\cdot)\\) is central to the correctness criterion, as it provides the ground truth for comparison, ensuring objective evaluation. The oracle ensures that the dataset maintains its integrity by aligning with the ground truth, which is crucial for accurate LLM benchmarking. Higher correctness scores indicate that the benchmark maintains integrity, aligning with the ground truth.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 6, "question_token_count": 22, "avg_answer_token_count": 13}
{"context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n", "topic": "Examine the theoretical foundations and implications of using \u0398 as a measure of diversity within datasets, ensuring a profound understanding of its role and impact on professional data science practices.", "question": "What is the potential impact of using \u0398 as a measure of diversity on the interpretability and reliability of data transformations in professional data science practices?", "choices": {"A": "It ensures uniformity across all dataset transformations, eliminating variability.", "B": "It allows for the assessment of transformation quality by quantifying variability, aiding in the identification of overfitting or underfitting.", "C": "It simplifies the transformation process by reducing the need for multiple metrics.", "D": "It primarily focuses on increasing the size of the dataset rather than its diversity."}, "answer": "B", "explanation": "The correct answer is B. \u0398 measures diversity by quantifying variability between datasets, which helps assess the quality of transformations and identify issues like overfitting or underfitting, thus impacting interpretability and reliability.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "question_token_count": 29, "avg_answer_token_count": 16}
{"context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n", "topic": "What are the central ideas and nuanced themes within the context of the expert's understanding of the text?", "question": "What is the primary challenge in developing standardized dynamic evaluation methods for LLMs as implied by the context?", "choices": {"A": "Ensuring the reliability and reproducibility of dynamic approaches.", "B": "The rapid evolution of LLM development and benchmarking techniques.", "C": "Addressing all potential challenges and innovations in the field.", "D": "Developing in-depth implementation guidelines for practitioners."}, "answer": "A", "explanation": "The context highlights that while dynamic approaches show promise, they face challenges in reliability and reproducibility, which is the primary challenge in developing standardized dynamic evaluation methods.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "question_token_count": 21, "avg_answer_token_count": 10}
{"context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n", "topic": "The potential impact of data leaks and the importance of data integrity in the context of LLMs.", "question": "Considering the shift from static to dynamic benchmarking for LLMs, what is a critical factor for ensuring the reliability and success of LLMs in industrial applications?", "choices": {"A": "Enhanced data encryption protocols", "B": "Dynamic benchmarking standards", "C": "Robust data integrity mechanisms", "D": "The potential of integrating AI in educational curriculums"}, "answer": "C", "explanation": "The context emphasizes the importance of LLMs in dynamic benchmarking and the need for robust data integrity mechanisms to ensure the reliable deployment of AI models in industrial applications, as they help maintain data quality and prevent contamination, which is crucial for the success of LLMs.", "answer_correctness_score": 8, "explanation_validity_score": 8, "question_clarity_score": 6, "question_groundedness_score": 4, "question_token_count": 31, "avg_answer_token_count": 6}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "Compare and contrast the different methodologies and their impact on the development of evaluation tasks for code generation and debugging tasks.", "question": "What is the impact of integrating nuanced themes and significant relationships in the context of evaluating domain expert knowledge about the competency of topic domain experts based on the provided textual information?", "choices": {"A": "Understanding the relationship between the implicit assumptions and underlying theories in the context of the text.", "B": "The synthesis of high-level general understanding above and beyond the specific context.", "C": "The detailed analysis of the text segment to analyze, understand, and generate questions about.", "D": "The pedagogical value of the content and comprehension of the subject matter."}, "answer": "A", "explanation": "The correct answer is A, as it encapsulates the essence of the topic by requiring a deep understanding of the core concepts. This is why the answer is correct: Understanding the context is crucial for evaluating the competency of a topic domain expert. The answer to the question is A, as it is a highly challenging question that requires the synthesis of high-level general understanding above and beyond the specific context.", "answer_correctness_score": 8, "explanation_validity_score": 6, "question_clarity_score": 2, "question_groundedness_score": 1, "question_token_count": 34, "avg_answer_token_count": 16}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "How do the historical developments in mathematical theories influence current teaching methods in mathematics education?", "question": "How does the historical evolution of mathematical theories influence the development and application of contemporary AI models in educational contexts, and what are the implications for modern educational methodologies?", "choices": {"A": "By providing foundational principles that guide algorithmic development, leading to improved instructional strategies.", "B": "Through the introduction of symbolic representation, which has minimal impact on AI development.", "C": "By directly applying ancient theories without modification in modern contexts.", "D": "By serving as a historical reference with no practical implications for current AI methodologies."}, "answer": "A", "explanation": "The correct answer emphasizes the foundational role of mathematical theories in guiding the development of algorithms and their application in educational strategies, reflecting the nuanced understanding of their evolution and impact on AI.", "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 1, "question_token_count": 32, "avg_answer_token_count": 15}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "Describe the methodologies used in evaluating topic domain experts' understanding of the provided textual information, and discuss the implications of the findings.", "question": "What are the potential implications of using random graph-based evaluations on the overall understanding and competency evaluation of domain experts in large language models?", "choices": {"A": "They provide a precise and consistent measure of reasoning ability across different domains.", "B": "They introduce variability that can obscure the true reasoning capabilities of the models.", "C": "They ensure a thorough understanding of specific domain knowledge without context.", "D": "They highlight the limitations of deterministic approaches in evaluating reasoning abilities."}, "answer": "B", "explanation": "Random graph-based evaluations introduce variability, which can obscure the true reasoning capabilities of the models, making it difficult to measure their reasoning ability accurately across different contexts.", "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 4, "question_groundedness_score": 2, "question_token_count": 27, "avg_answer_token_count": 14}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "Describe the nuances in the evaluation of competency evaluation in domain experts, considering the implications of the underlying theories discussed.", "question": "Considering the methodologies described, which of the following statements is accurate regarding the significance of the model's capabilities in applying the given data analysis to predict the value of the root node?", "choices": {"A": "It demonstrates how the model's capabilities in handling structured data can be evaluated, emphasizing the model's ability to process and synthesize information effectively.", "B": "It illustrates the importance of evaluating the model's ability to accurately determine the position of the root node in the graph, as the model is assessed on its capacity to make this prediction.", "C": "It shows how various models can be leveraged to assess the model's ability to predict the outcomes of different scenarios, highlighting the significance of using such datasets for evaluation.", "D": "It indicates the importance of graph theory in evaluating the reasoning ability of LLMs, demonstrating the model's capacity to utilize graph-based methodologies in its decision-making processes."}, "answer": "D", "explanation": "The correct answer is D, as the context underscores the importance of graph theory and graph-based methodologies in evaluating the reasoning ability of LLMs. It highlights how the model's capacity to use such frameworks is crucial in its decision-making processes.", "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "question_token_count": 35, "avg_answer_token_count": 31}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "Explain the implications of incorrect data labeling on the validity of performance assessments.", "question": "In the context of training machine learning models, what is the likely impact of improperly labeled data on the model's ability to generalize from the training data to the evaluation data?", "choices": {"A": "It can lead to overfitting, where the model performs well on training data but poorly on unseen data.", "B": "It can cause underfitting, where the model fails to learn the data patterns effectively.", "C": "It can result in poor generalization, where the model cannot apply learned patterns to new data.", "D": "It might result in data leakage, where the model accesses future data, compromising data privacy."}, "answer": "C", "explanation": "The correct answer is C. The correct response focuses on understanding how labeling inaccuracies affect model performance by highlighting the influence on model training, particularly how subtle distinctions in data can significantly impact model performance. This question is challenging as it requires the domain expert to have a deep understanding of data integrity issues and the impact of data quality on model performance, and how inaccuracies in data labeling can impact the performance of machine learning models.", "answer_correctness_score": 10, "explanation_validity_score": 8, "question_clarity_score": 9, "question_groundedness_score": 1, "question_token_count": 34, "avg_answer_token_count": 19}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "What are the implications of poor data quality on the validity of performance metrics in domain-specific evaluations?", "question": "In the context of machine learning model evaluation, how might the presence of subtle syntactic variations in the data, despite appearing to maintain lexical integrity, actually lead to a misunderstanding of a model's performance generalization to new, unseen scenarios?", "choices": {"A": "It can create a misleading sense of model robustness due to semantic overlap in data used for training.", "B": "It could lead to an overestimation of model performance in real-world applications.", "C": "It can result in a false sense of security regarding the model's ability to handle novel inputs.", "D": "It might lead to an underestimation of the model's capabilities in adapting to novel scenarios."}, "answer": "B", "explanation": "The correct answer is B: It challenges domain experts to think about the interplay between the appearance of data quality and its implications for real-world model performance. This question requires a deep understanding of the limitations of the evaluation context, how the knowledge of domain experts, and the understanding of the text's implications, as well as the application of a high level of general understanding.", "answer_correctness_score": 9, "explanation_validity_score": 6, "question_clarity_score": 8, "question_groundedness_score": 9, "question_token_count": 45, "avg_answer_token_count": 18}
