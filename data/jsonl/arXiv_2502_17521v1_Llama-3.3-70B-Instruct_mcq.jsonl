{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The significance of coding tasks in evaluating a model's capability to generate and understand programming code.", "question": "What is the primary advantage of using dynamic benchmarks over static benchmarks in evaluating the performance of Large Language Models, particularly in the context of coding tasks?", "choices": {"A": "Dynamic benchmarks are more comprehensive and cover a wider range of tasks.", "B": "Dynamic benchmarks can adapt to the evolving capabilities of LLMs and mitigate data contamination issues.", "C": "Dynamic benchmarks are more efficient and require less computational resources.", "D": "Dynamic benchmarks are more suitable for instruction-following tasks than coding tasks."}, "answer": "B", "explanation": "The correct answer, B, highlights the primary advantage of dynamic benchmarks, which is their ability to adapt to the evolving capabilities of LLMs and mitigate data contamination issues. This is particularly important in the context of coding tasks, where static benchmarks may become too easy for stronger LLMs or introduce contamination risks.", "question_token_count": 30, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 8, "avg_answer_token_count": 14}
{"context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n", "topic": "The importance of fair and reliable benchmarks in evaluating LLMs and mitigating the effects of data contamination.", "question": "What is a critical consequence of the proprietary nature of LLM training data on the evaluation of their performance?", "choices": {"A": "Increased risk of overfitting to specific tasks", "B": "Impeded ability to verify and mitigate potential overlaps between training and evaluation data", "C": "Reduced need for human-annotated datasets in fine-tuning phases", "D": "Improved ability to detect and exclude contaminated evaluation data"}, "answer": "B", "explanation": "The correct answer, B, is a direct consequence of the proprietary nature of LLM training data, as it impedes the community's ability to verify and mitigate potential overlaps between training and evaluation data. This, in turn, exacerbates the risk of data contamination and makes it challenging to accurately assess LLM performance.", "question_token_count": 21, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 12}
{"context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n", "topic": "The challenges in balancing benchmark transparency with the need to prevent data contamination, and how dynamic benchmarking strategies mitigate these challenges.", "question": "What metric is most indicative of a dynamic benchmark's ability to prevent data contamination by generating novel variations, and how does it relate to the benchmark's overall effectiveness in evaluating LLM capabilities?", "choices": {"A": "Collision Rate, as it directly measures overlap between transformations.", "B": "Repeat Trials, because it quantifies the potential for novel test case generation.", "C": "Training Data Size, since larger datasets inherently reduce contamination risk.", "D": "Model Complexity, as more complex models are less susceptible to contamination."}, "answer": "B", "explanation": "The correct answer, Repeat Trials, is the metric that quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset. This metric provides insight into the benchmark's ability to produce novel variations, which is crucial for preventing data contamination and ensuring the benchmark remains effective in evaluating LLM capabilities. Collision Rate, while important, measures the overlap between two transformations, indicating potential contamination but not directly the ability to generate novel variations.", "question_token_count": 37, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 13}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "The C-SimpleQA benchmark evaluates the factuality ability of language models to answer short questions in Chinese, and it requires a model to demonstrate a high level of proficiency in language understanding and generation.", "question": "What primary aspect of language models does the C-SimpleQA benchmark evaluate?", "choices": {"A": "Coding proficiency", "B": "Instruction following", "C": "Factuality ability", "D": "Reasoning skills"}, "answer": "C", "explanation": "The C-SimpleQA benchmark is specifically designed to evaluate the factuality ability of language models to answer short questions in Chinese, as mentioned in the context.", "question_token_count": 16, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 10, "avg_answer_token_count": 3}
{"context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The potential biases and limitations of dynamic benchmarking, including the impact of dataset quality and transformation functions on the evaluation results, and the strategies for mitigating these biases and limitations.", "question": "What is the primary challenge in ensuring the quality of dynamic benchmarking datasets, and how can the choice of transformation function mitigate or exacerbate this challenge?", "choices": {"A": "The primary challenge is ensuring dataset diversity, and a poorly chosen transformation function can exacerbate this issue by introducing bias.", "B": "The primary challenge is maintaining dataset size, and a well-chosen transformation function can mitigate this issue by generating new samples.", "C": "The primary challenge is controlling for dataset noise, and a poorly chosen transformation function can mitigate this issue by introducing randomness.", "D": "The primary challenge is evaluating dataset relevance, and a well-chosen transformation function can exacerbate this issue by adapting too closely to the model."}, "answer": "A", "explanation": "The correct answer requires an understanding of the trade-offs involved in dynamic benchmarking and the potential impact of transformation functions on dataset quality. A poorly chosen transformation function can introduce bias, which can exacerbate the challenge of ensuring dataset diversity. This requires the ability to analyze the implications of different transformation functions and their potential effects on dataset quality.", "question_token_count": 30, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 25}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "The application of temporal cutoff in constructing reliable benchmarks for LLM evaluation, highlighting the importance of using data collected after the model's knowledge cutoff date.", "question": "What primary advantage does the temporal cutoff approach offer in constructing benchmarks for LLM evaluation, particularly in relation to data collected after the model's knowledge cutoff date?", "choices": {"A": "Enhanced model training efficiency", "B": "Mitigation of data contamination", "C": "Increased model complexity", "D": "Improved human-machine interaction"}, "answer": "B", "explanation": "The temporal cutoff approach is primarily advantageous because it helps mitigate data contamination by using data that is new and unknown before the model's knowledge cutoff date. This ensures that the model is evaluated on its ability to generalize and learn from new information, rather than memorizing existing data.", "question_token_count": 30, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 5}
{"context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n", "topic": "The significance of benchmark rewriting in enhancing the diversity and quality of training datasets for Large Language Models.", "question": "What is the primary mechanism by which ITD mitigates the risk of in-distribution contamination in static benchmarks during the rewriting process?", "choices": {"A": "By using knowledge graphs to develop extended questions", "B": "By prompting an LLM to rewrite contaminated samples while preserving their difficulty levels", "C": "By identifying and replacing variables in samples from existing benchmarks", "D": "By generating new samples that retain the stylistics and essential knowledge of the original"}, "answer": "B", "explanation": "ITD utilizes a contamination detector to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them, preserving their difficulty levels. This approach directly addresses the risk of in-distribution contamination by targeting and rewriting the contaminated samples.", "question_token_count": 26, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 12}
{"context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n", "topic": "The relationship between static benchmarking and other methods of model evaluation, such as dynamic benchmarking, and how they complement each other in assessing model capabilities.", "question": "What is the primary advantage of combining static and dynamic benchmarking methods for evaluating the capabilities of Large Language Models, and how do these approaches complement each other in assessing model performance?", "choices": {"A": "Static benchmarking provides a baseline for model performance, while dynamic benchmarking assesses the model's ability to adapt to new data and tasks.", "B": "Static benchmarking evaluates model performance on a fixed set of tasks, while dynamic benchmarking evaluates the model's ability to generalize to real-world scenarios.", "C": "Static benchmarking measures model performance at a particular point in time, while dynamic benchmarking continuously updates the benchmarking tasks to reflect changing real-world conditions.", "D": "Static benchmarking focuses on model accuracy, while dynamic benchmarking focuses on model robustness and adaptability."}, "answer": "A", "explanation": "The correct answer, A, highlights the complementary nature of static and dynamic benchmarking. Static benchmarking provides a baseline for model performance, while dynamic benchmarking assesses the model's ability to adapt to new data and tasks, offering a more comprehensive understanding of model capabilities.", "question_token_count": 35, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 2, "avg_answer_token_count": 24}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "Proposing novel benchmarks or datasets that could address gaps in current evaluations of LLM math problem-solving and knowledge retrieval abilities.", "question": "What novel dataset or benchmark could be developed to specifically evaluate an LLM's ability to apply mathematical reasoning to solve real-world, interdisciplinary problems that involve both technical knowledge and nuanced understanding of social or environmental contexts?", "choices": {"A": "A dataset focusing on basic arithmetic operations in isolation", "B": "A benchmark evaluating LLMs on solving complex, interdisciplinary problems that require both mathematical and social science knowledge", "C": "A dataset limited to trivia questions on historical mathematical discoveries", "D": "A benchmark assessing only the ability to recall mathematical formulas without application"}, "answer": "B", "explanation": "The correct answer, B, proposes a benchmark that addresses a gap in current evaluations by focusing on interdisciplinary problems that require both technical mathematical knowledge and an understanding of social or environmental contexts. This would provide a more comprehensive assessment of an LLM's ability to apply mathematical reasoning in real-world scenarios.", "question_token_count": 41, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 6, "avg_answer_token_count": 13}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "The WinoGrande benchmark is designed to assess a model's ability to reason about nuanced and complex scenarios, and it requires a deep understanding of language and cognition.", "question": "What key aspect of language comprehension does the WinoGrande benchmark aim to assess, and how does it require models to integrate background knowledge with logical reasoning?", "choices": {"A": "Ability to generate code and debug programs", "B": "Capacity for intuitive reasoning and application of everyday knowledge", "C": "Skill in comprehending and executing detailed directives", "D": "Ability to evaluate the factuality of short questions in multiple languages"}, "answer": "B", "explanation": "The WinoGrande benchmark is designed to assess a model's ability to reason about nuanced and complex scenarios, requiring a deep understanding of language and cognition. This involves integrating background knowledge with logical reasoning to arrive at plausible answers, which is a key aspect of intuitive reasoning and application of everyday knowledge.", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 10}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "The ethical implications of intentionally leaking benchmarking data to boost LLM scores and the need for regulatory measures to prevent such practices.", "question": "What regulatory measure could most effectively prevent developers from intentionally leaking benchmarking data to boost LLM scores, considering the limitations of technical mitigation methods like canary strings?", "choices": {"A": "Implementing strict penalties for data leakage", "B": "Establishing an independent auditing body for LLM development", "C": "Developing more sophisticated canary string algorithms", "D": "Promoting ethical guidelines without enforcement mechanisms"}, "answer": "B", "explanation": "The correct answer requires an understanding of the ethical implications of data leakage and the limitations of technical solutions like canary strings. It also demands recognition of the need for a regulatory framework that goes beyond technical fixes to ensure compliance and ethical behavior among developers.", "question_token_count": 31, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 2, "avg_answer_token_count": 8}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "Consequences of data contamination on the validity of performance measurements in LLM evaluation.", "question": "What type of data contamination occurs when a test data point is modified through syntactic transformations, such as punctuation normalization or synonym substitution, and is found to exist in the training dataset, potentially compromising the validity of LLM performance measurements?", "choices": {"A": "Exact Contamination", "B": "Syntactic Contamination", "C": "Semantic Contamination", "D": "Lexical Contamination"}, "answer": "B", "explanation": "Syntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, preserving lexical meaning. This type of contamination is distinct from exact contamination, where there are exact duplicates, and is not directly related to semantic or lexical contamination, which are not defined in the provided context.", "question_token_count": 45, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 10, "avg_answer_token_count": 4}
{"context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n", "topic": "Comparing and contrasting different encryption methods, such as public key encryption and secure multi-party computation, in the context of model evaluation.", "question": "What is the primary advantage of using secure multi-party computation over public key encryption for model evaluation, considering the trade-offs between confidentiality, computational overhead, and key management complexity?", "choices": {"A": "Enhanced model performance due to access to more training data", "B": "Improved protection against data leakage through secure data sharing protocols", "C": "Reduced computational overhead compared to public key encryption methods", "D": "Simplified key management due to the elimination of private keys"}, "answer": "B", "explanation": "Secure multi-party computation allows for private benchmarking by enabling the computation on private data without revealing the data itself, which is a significant advantage over public key encryption in terms of protecting against data leakage. This method ensures that test data and model parameters remain confidential during the evaluation process.", "question_token_count": 35, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 4, "avg_answer_token_count": 11}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The concept and benefits of dynamic benchmarks in mitigating the issues associated with static benchmarks for LLMs.", "question": "What primary advantage do dynamic benchmarks offer over static benchmarks in evaluating the performance of rapidly evolving Large Language Models?", "choices": {"A": "They are more cost-effective to develop and maintain.", "B": "They can adapt to changes in model architecture and training data, reducing data contamination risks.", "C": "They provide a more detailed analysis of model performance on specific tasks.", "D": "They are less prone to overfitting and can generalize better to new tasks."}, "answer": "B", "explanation": "Dynamic benchmarks are proposed as a solution to the limitations of static benchmarks, which can become too easy for stronger LLMs or introduce data contamination issues. The primary advantage of dynamic benchmarks is their ability to adapt to changes in model architecture and training data, thereby reducing the risk of data contamination and providing a more accurate evaluation of model performance.", "question_token_count": 22, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 6, "avg_answer_token_count": 14}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "Designing and implementing effective canary strings that can reliably detect memorization without interfering with the model's ability to learn and generalize.", "question": "What is a potential risk of using canary strings to detect memorization in LLMs, and how can it be mitigated to ensure the model's ability to learn and generalize is not compromised?", "choices": {"A": "The risk is that canary strings may be too obvious, allowing models to learn to avoid them, and it can be mitigated by using more subtle and context-dependent markers.", "B": "The risk is that canary strings may interfere with the model's ability to learn and generalize, and it can be mitigated by using a combination of canary strings and other evaluation methods.", "C": "The risk is that model developers may not be aware of or responsive to canary strings, and it can be mitigated by providing clear guidelines and incentives for their use.", "D": "The risk is that canary strings may not be effective against intentional data leakage, and it can be mitigated by using additional security measures, such as data encryption and access controls."}, "answer": "B", "explanation": "The correct answer, B, highlights a potential risk of using canary strings, which is that they may interfere with the model's ability to learn and generalize. This risk can be mitigated by using a combination of canary strings and other evaluation methods, such as testing the model on unseen data or using alternative metrics to evaluate its performance.", "question_token_count": 39, "answer_correctness_score": 8, "explanation_validity_score": 8, "question_clarity_score": 6, "question_groundedness_score": 5, "avg_answer_token_count": 36}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "The role of random generation in table-based and graph-based evaluations of LLMs.", "question": "What primary advantage does the use of random generation in graph-based evaluations, such as DyVal and NPHardEval, offer over traditional fixed-input methods for assessing LLM reasoning capabilities?", "choices": {"A": "Increased computational efficiency", "B": "Enhanced ability to generalize across diverse problem domains", "C": "Improved scalability for real-world applications", "D": "Simplified interpretation of evaluation results"}, "answer": "B", "explanation": "The use of random generation in graph-based evaluations allows for the creation of a wide variety of graphs with different structures and complexities, which can help assess an LLM's ability to generalize its reasoning across diverse problem domains. This is a key advantage because real-world problems often involve complex, variable structures that fixed-input methods may not adequately capture.", "question_token_count": 37, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 6}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The current state of research on dynamic benchmarking methods for LLMs and the need for systematic surveys and comprehensive discussions on the topic.", "question": "What is the primary limitation of static benchmarking methods for LLMs that dynamic benchmarking approaches aim to address, and how do dynamic benchmarking methods mitigate this limitation?", "choices": {"A": "Data contamination due to training data overlap, which dynamic benchmarking mitigates through continuous updates and regeneration of benchmark datasets.", "B": "Model overfitting, which dynamic benchmarking mitigates through regularization techniques.", "C": "Lack of diversity in benchmark datasets, which dynamic benchmarking mitigates through data augmentation.", "D": "Inability to evaluate model generalizability, which dynamic benchmarking mitigates through multi-task learning."}, "answer": "A", "explanation": "The correct answer is A) Data contamination due to training data overlap, which dynamic benchmarking mitigates through continuous updates and regeneration of benchmark datasets. This is because static benchmarking methods are prone to data contamination, where the model is trained on data that is also used for evaluation, leading to inflated performance metrics. Dynamic benchmarking approaches address this limitation by continuously updating and regenerating benchmark datasets to minimize contamination.", "question_token_count": 31, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 18}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "The significance of using recent data in evaluating LLMs, as demonstrated by the various benchmarks mentioned, to ensure contamination-free assessments.", "question": "What is the primary advantage of using benchmarks like LiveBench and AntiLeak-Bench in evaluating LLMs, in terms of mitigating data contamination?", "choices": {"A": "They allow for the use of outdated knowledge to assess model performance.", "B": "They enable the evaluation of models on a fixed, static dataset.", "C": "They utilize data collected after the model's knowledge cutoff date to prevent contamination.", "D": "They focus on assessing model performance on a specific, narrow domain."}, "answer": "C", "explanation": "The correct answer, C, highlights the primary advantage of using benchmarks like LiveBench and AntiLeak-Bench, which is to utilize data collected after the model's knowledge cutoff date to prevent contamination. This approach helps to ensure that the evaluation is contamination-free and assesses the model's ability to generalize to new, unseen information.", "question_token_count": 30, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 14}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "Comparing the effectiveness of canary strings with other proposed methods for addressing data contamination in LLMs.", "question": "What is a significant drawback of relying solely on canary strings to detect data contamination in LLMs, particularly in scenarios where model developers intentionally attempt to manipulate benchmark scores?", "choices": {"A": "The method is too resource-intensive and requires significant computational power.", "B": "It is ineffective against intentional data leakage by model developers seeking to boost benchmark scores.", "C": "The approach is too complex and requires extensive expertise in natural language processing.", "D": "It can only detect contamination in specific types of LLMs and not in others."}, "answer": "B", "explanation": "The correct answer, B, highlights a crucial limitation of canary strings mentioned in the context: their ineffectiveness if a developer aims to leak benchmarking data to boost scores intentionally. This scenario underscores the importance of considering the motivations and actions of model developers when evaluating methods for mitigating data contamination.", "question_token_count": 34, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 15}
{"context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The problem formulation for dynamic benchmarking, including the definition of a dynamic benchmark as a tuple containing a static benchmark dataset and a transformation function.", "question": "What is the primary purpose of the transformation function in dynamic benchmarking, and how does it modify the static benchmark dataset to generate the dynamic dataset for evaluating Large Language Models?", "choices": {"A": "To increase the size of the dataset by adding new samples.", "B": "To modify the dataset during benchmarking to avoid possible data contamination and ensure a transparent evaluation.", "C": "To reduce the complexity of the evaluation process by simplifying the dataset.", "D": "To integrate multiple static datasets into a single dynamic dataset."}, "answer": "B", "explanation": "The transformation function in dynamic benchmarking is crucial as it modifies the static benchmark dataset to generate the dynamic dataset. This process is essential for avoiding data contamination and ensuring that the evaluation of Large Language Models is transparent and faithful. The correct answer reflects this understanding, highlighting the function's role in modifying the dataset to prevent contamination and ensure a reliable evaluation.", "question_token_count": 34, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 14}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "The construction of Knights and Knaves puzzles with random reasoning graphs by Xie et al. for evaluating LLMs.", "question": "What is the primary advantage of using random reasoning graphs in the construction of Knights and Knaves puzzles for evaluating LLMs, as proposed by Xie et al.?", "choices": {"A": "Improved scalability of puzzle generation", "B": "Enhanced ability to model real-world reasoning scenarios", "C": "Increased difficulty in distinguishing between Knights and Knaves", "D": "Reduced reliance on domain-specific knowledge"}, "answer": "B", "explanation": "The correct answer, \"Enhanced ability to model real-world reasoning scenarios\", reflects the idea that random reasoning graphs can be used to generate puzzles that more closely resemble real-world reasoning tasks, making the evaluation of LLMs more effective. The other options are incorrect because they do not accurately capture the primary advantage of using random reasoning graphs in the construction of Knights and Knaves puzzles.", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 4, "avg_answer_token_count": 8}
{"context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n", "topic": "The potential strategies for mitigating data contamination in LLM training, including data curation, filtering, and anonymization techniques.", "question": "What technique involves modifying personal identifiable information in training datasets to prevent re-identification while preserving data utility for LLM training, thereby mitigating contamination risks?", "choices": {"A": "Data augmentation", "B": "Data anonymization", "C": "Data filtering", "D": "Data retrieval"}, "answer": "B", "explanation": "Data anonymization is a technique used to protect personal identifiable information by modifying or removing direct identifiers, making it difficult to re-identify individuals. This method is crucial in mitigating data contamination risks in LLM training by ensuring that the training data does not compromise the privacy of individuals while still maintaining its utility for model training.", "question_token_count": 29, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 1, "avg_answer_token_count": 3}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "The DyVal framework and its use of randomly generated directed acyclic graphs (DAGs) to assess the reasoning capabilities of LLMs.", "question": "What fundamental challenge does the use of randomly generated DAGs in DyVal pose to the development of more advanced LLMs, in terms of scaling up to more complex reasoning tasks?", "choices": {"A": "Insufficient training data for very large graphs", "B": "Inability to generalize beyond specific graph structures", "C": "Difficulty in capturing long-range dependencies within the graph", "D": "Limitations in current architectures to efficiently process graph-based inputs"}, "answer": "D", "explanation": "The use of DAGs in DyVal highlights the importance of structured reasoning in LLMs. However, as graphs increase in complexity, current architectures may struggle with efficiently processing these structures, limiting their ability to scale to more complex reasoning tasks. This challenge is fundamental because it touches on the core design and capabilities of LLMs in handling graph-based inputs, which are essential for many real-world applications.", "question_token_count": 36, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 7, "question_groundedness_score": 8, "avg_answer_token_count": 10}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "Evaluating the importance of open-domain evaluations provided by AlpacaEval and ArenaHard in assessing LLM knowledge and capabilities.", "question": "What is the primary advantage of using open-domain evaluations like AlpacaEval and ArenaHard in assessing LLM knowledge and capabilities, compared to traditional benchmarks?", "choices": {"A": "They provide a more comprehensive assessment of a model's mathematical abilities.", "B": "They offer a more nuanced evaluation of a model's knowledge in specific domains.", "C": "They enable a more realistic assessment of a model's capabilities in real-world scenarios.", "D": "They allow for a more efficient evaluation of a model's performance in multiple tasks."}, "answer": "C", "explanation": "The correct answer, C, highlights the primary advantage of using open-domain evaluations like AlpacaEval and ArenaHard, which is to provide a more realistic assessment of a model's capabilities in real-world scenarios. This is because these evaluations go beyond traditional benchmarks by providing a more comprehensive and challenging assessment of a model's abilities.", "question_token_count": 30, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 8, "avg_answer_token_count": 15}
{"context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n", "topic": "Assessing the potential risks and consequences of encryption compromise or private key exposure in secure evaluation methods.", "question": "What is the primary consequence of encryption compromise or private key exposure in secure evaluation methods that utilize confidential computing and secure multi-party computation?", "choices": {"A": "Model performance improvement", "B": "Increased computational overhead", "C": "Data confidentiality breach", "D": "Enhanced evaluation integrity"}, "answer": "C", "explanation": "The primary consequence of encryption compromise or private key exposure in secure evaluation methods is a data confidentiality breach. This is because encryption methods are used to secure evaluation data, and compromise or exposure of the private key would allow unauthorized access to the data.", "question_token_count": 27, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 4}
{"context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n", "topic": "A comparative analysis of static and dynamic benchmarking methods for LLMs, including their limitations and potential applications.", "question": "What fundamental challenge in evaluating dynamic benchmarks for LLMs necessitates the development of standardized criteria, and how might this impact the mitigation of data contamination risks?", "choices": {"A": "The lack of a unified framework for dynamic benchmarking hinders the comparison of model performances, thereby exacerbating data contamination risks.", "B": "The absence of standardized criteria for dynamic benchmarks stems from the dynamic nature of internet-derived training corpora, which inherently resists static evaluation methods.", "C": "The primary challenge lies in balancing the trade-off between model performance and data privacy, which standardized criteria could address by prioritizing privacy-preserving benchmarks.", "D": "The need for standardized criteria arises from the discrepancy between the static benchmarks used in model training and the dynamic environments in which LLMs are deployed, necessitating benchmarks that can adapt to real-world scenarios."}, "answer": "A", "explanation": "The correct answer, A, identifies the core issue with dynamic benchmarks as the lack of a unified framework, which complicates the comparison of model performances and, by extension, the assessment of data contamination risks. This answer demonstrates an understanding of the challenges in benchmarking LLMs and the importance of standardized criteria in mitigating data contamination.", "question_token_count": 31, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 30}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "The significance of addressing data contamination in LLM benchmarking, including its consequences on the validity of benchmarks, model comparisons, deployment decisions, and policy-making, and the need for robust approaches to identify and prevent contamination.", "question": "What is the primary consequence of failing to address data contamination in LLM benchmarking on the validity of model comparisons and deployment decisions?", "choices": {"A": "It leads to underestimation of model capabilities.", "B": "It results in overestimation of model capabilities and undermines the reliability of benchmarks.", "C": "It has no significant impact on model comparisons and deployment decisions.", "D": "It solely affects the syntactic transformations in NLP applications."}, "answer": "B", "explanation": "The correct answer, B, reflects the central idea of the context that contaminated benchmarks can lead to misleading conclusions about model capabilities, influencing model comparisons, deployment decisions, and policy-making. This is because contamination can cause models to be tested on data they have already seen, thereby overestimating their true capabilities.", "question_token_count": 25, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 12}
{"context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The applications of dynamic benchmarking in real-world scenarios, including the evaluation of LLMs in different domains and tasks, and the challenges and opportunities associated with these applications.", "question": "What is the primary advantage of using dynamic benchmarking over static benchmarking schemes for evaluating Large Language Models, and how does the transformation function T(\u22c5) contribute to this advantage?", "choices": {"A": "Dynamic benchmarking provides a more comprehensive evaluation by modifying the dataset during evaluation, and T(\u22c5) helps to avoid data contamination by transforming the dataset.", "B": "Dynamic benchmarking is more efficient than static benchmarking, and T(\u22c5) reduces the computational complexity of the evaluation process.", "C": "Dynamic benchmarking provides a more transparent evaluation by modifying the dataset during evaluation, and T(\u22c5) ensures that the evaluation is faithful to the model's performance.", "D": "Dynamic benchmarking is more suitable for evaluating LLMs in specific domains, and T(\u22c5) helps to adapt the benchmark to the domain-specific requirements."}, "answer": "A", "explanation": "The correct answer is A, as dynamic benchmarking provides a more comprehensive evaluation by modifying the dataset during evaluation, and the transformation function T(\u22c5) helps to avoid data contamination by transforming the dataset. This requires a deep understanding of the concept of dynamic benchmarking and its advantages over static benchmarking schemes.", "question_token_count": 35, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 28}
{"context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n", "topic": "The role of proprietary training data in exacerbating data contamination and impeding accurate assessment of LLM performance.", "question": "What is the primary consequence of proprietary training data on the evaluation of Large Language Models?", "choices": {"A": "Improved model generalizability", "B": "Enhanced data privacy", "C": "Impeded accurate assessment of model performance", "D": "Increased risk of model overfitting"}, "answer": "C", "explanation": "The correct answer, \"Impeded accurate assessment of model performance\", is supported by the context, which states that proprietary training data \"complicating the accurate assessment of their true performance\". The other options are incorrect because they do not accurately reflect the primary consequence of proprietary training data on LLM evaluation.", "question_token_count": 18, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 6}
{"context": "###### Safety\nSafety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content. Datasets such as RealToxicityPrompts\u00a0(Gehman et\u00a0al., 2020) and ToxiGen\u00a0(Hartvigsen et\u00a0al., 2022) assess resilience against producing harmful outputs.\n\nBy providing a controlled environment to measure these aspects, safety benchmarks play a critical role in guiding the development of models that are not only powerful outputs but also responsible and trustworthy for real-world applications.\n\n\n###### Language\nLanguage benchmarks assess the LLMs\u2019 proficiency in specific languages.\n\nGLUE\u00a0(Wang, 2018) and SuperGLUE\u00a0(Wang et\u00a0al., 2019) cover tasks from sentiment analysis to language inference, while CLUE\u00a0(Xu et\u00a0al., 2020) targets Chinese language.\n\nTypo-fixing\u00a0(Suzgun et\u00a0al., 2022) is also widely used.\n\n\n###### Reading Comprehension\nReading comprehension tasks test a model\u2019s ability to extract and infer information from text. Benchmarks like SQuAD\u00a0(Rajpurkar et\u00a0al., 2018), QuAC\u00a0(Choi et\u00a0al., 2018), and BoolQ\u00a0(Clark et\u00a0al., 2019) challenge models to understand passages and draw logical conclusions.\n", "topic": "Evaluating the importance of safety benchmarks like RealToxicityPrompts and ToxiGen in ensuring the generation of non-toxic content by LLMs.", "question": "What is the primary benefit of using datasets like RealToxicityPrompts and ToxiGen in evaluating the safety of LLMs?", "choices": {"A": "They provide a comprehensive assessment of language proficiency.", "B": "They offer a controlled environment to measure resilience against producing harmful outputs.", "C": "They are primarily used for reading comprehension tasks.", "D": "They are designed to test the ability of LLMs to generate creative content."}, "answer": "B", "explanation": "The correct answer, B, highlights the importance of safety benchmarks in evaluating the ability of LLMs to generate non-toxic content. The other options are incorrect because they either focus on language proficiency (A), reading comprehension tasks (C), or creative content generation (D), which are not the primary benefits of using datasets like RealToxicityPrompts and ToxiGen.", "question_token_count": 28, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 12}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The importance of benchmarking Large Language Models (LLMs) in evaluating their performance and capabilities.", "question": "What is the primary challenge faced by static benchmarks in evaluating the performance of rapidly evolving Large Language Models, and how do dynamic benchmarks propose to address this issue?", "choices": {"A": "Static benchmarks become outdated due to LLMs' rapid evolution, and dynamic benchmarks propose to address this by continuously updating the evaluation tasks and datasets.", "B": "Static benchmarks are prone to data contamination, and dynamic benchmarks propose to address this by using contamination detectors and rotating evaluation tasks.", "C": "Static benchmarks are insufficient for assessing LLMs' instruction-following capabilities, and dynamic benchmarks propose to address this by incorporating more diverse and complex instruction-following tasks.", "D": "Static benchmarks are ineffective in evaluating LLMs' coding capabilities, and dynamic benchmarks propose to address this by using more advanced coding tasks and assessment metrics."}, "answer": "A", "explanation": "The correct answer, A, accurately identifies the primary challenge faced by static benchmarks as their inability to keep pace with the rapid evolution of LLMs. Dynamic benchmarks propose to address this issue by continuously updating the evaluation tasks and datasets, ensuring that the benchmarks remain relevant and effective in assessing LLM performance.", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 29}
{"context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n", "topic": "The relationship between complexity, performance, and stability in dynamic benchmarks, and its implications for reliable evaluation of LLMs.", "question": "What is the primary factor that determines the stability of a dynamic benchmarking method, according to the formulation provided?", "choices": {"A": "Variance in performance across different LLMs", "B": "Variance in complexity across different trials", "C": "Increase in task complexity without data contamination", "D": "Domain-specific complexity metrics"}, "answer": "B", "explanation": "The correct answer is based on the formulation provided in the context, which states that stability can be formulated as the variance in complexity across different trials, indicating that high variance suggests the dynamic benchmarking method is not stable.", "question_token_count": 22, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 8, "avg_answer_token_count": 7}
{"context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n", "topic": "The ethical considerations and potential consequences of data contamination in LLMs, and how dynamic benchmarking can address these concerns.", "question": "What primary ethical consideration does dynamic benchmarking address in mitigating data contamination risks in large language models?", "choices": {"A": "Privacy concerns through data anonymization", "B": "Bias reduction by ensuring diverse training data", "C": "Security vulnerabilities by updating benchmarking criteria", "D": "Accountability through transparent model performance evaluation"}, "answer": "D", "explanation": "Dynamic benchmarking primarily addresses the ethical consideration of accountability by ensuring that large language models are continuously evaluated against updated and relevant criteria, thereby providing a transparent assessment of their performance and potential risks associated with data contamination.", "question_token_count": 19, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 7}
{"context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n", "topic": "The importance of considering the rapidly evolving nature of LLM development and benchmarking techniques when designing benchmarking methods.", "question": "What is the primary challenge in developing effective benchmarking methods for LLMs, given the rapid evolution of the field?", "choices": {"A": "Ensuring the scalability of benchmarking methods to large training datasets", "B": "Developing methods that can detect and mitigate data contamination", "C": "Creating benchmarking methods that are consistent across different LLM models", "D": "Balancing the need for standardized evaluation with the need for adaptability to emerging challenges and innovations"}, "answer": "D", "explanation": "The primary challenge in developing effective benchmarking methods for LLMs is balancing the need for standardized evaluation with the need for adaptability to emerging challenges and innovations, as the field is rapidly evolving.", "question_token_count": 23, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 12}
{"context": "###### Safety\nSafety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content. Datasets such as RealToxicityPrompts\u00a0(Gehman et\u00a0al., 2020) and ToxiGen\u00a0(Hartvigsen et\u00a0al., 2022) assess resilience against producing harmful outputs.\n\nBy providing a controlled environment to measure these aspects, safety benchmarks play a critical role in guiding the development of models that are not only powerful outputs but also responsible and trustworthy for real-world applications.\n\n\n###### Language\nLanguage benchmarks assess the LLMs\u2019 proficiency in specific languages.\n\nGLUE\u00a0(Wang, 2018) and SuperGLUE\u00a0(Wang et\u00a0al., 2019) cover tasks from sentiment analysis to language inference, while CLUE\u00a0(Xu et\u00a0al., 2020) targets Chinese language.\n\nTypo-fixing\u00a0(Suzgun et\u00a0al., 2022) is also widely used.\n\n\n###### Reading Comprehension\nReading comprehension tasks test a model\u2019s ability to extract and infer information from text. Benchmarks like SQuAD\u00a0(Rajpurkar et\u00a0al., 2018), QuAC\u00a0(Choi et\u00a0al., 2018), and BoolQ\u00a0(Clark et\u00a0al., 2019) challenge models to understand passages and draw logical conclusions.\n", "topic": "Comparing and contrasting the strengths and weaknesses of different language benchmarks for LLMs.", "question": "What is the primary distinction between the objectives of safety benchmarks and language benchmarks in assessing LLMs?", "choices": {"A": "Safety benchmarks focus on language proficiency, while language benchmarks focus on reading comprehension.", "B": "Safety benchmarks evaluate a model's ability to generate non-toxic content, while language benchmarks assess language proficiency and reading comprehension.", "C": "Safety benchmarks assess reading comprehension, while language benchmarks evaluate a model's ability to generate non-toxic content.", "D": "Safety benchmarks and language benchmarks have identical objectives, with the only difference being the datasets used."}, "answer": "B", "explanation": "The correct answer, B, highlights the primary distinction between safety benchmarks, which focus on evaluating a model's ability to generate non-toxic content, and language benchmarks, which assess language proficiency and reading comprehension. This requires a deep understanding of the context and the ability to analyze complex information.", "question_token_count": 20, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 20}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "The importance of temporal cutoff in evaluating Large Language Models to prevent data contamination and ensure reliable assessments.", "question": "What is the primary advantage of using a temporal cutoff when constructing datasets to evaluate Large Language Models?", "choices": {"A": "To increase the size of the dataset", "B": "To reduce the computational resources required for evaluation", "C": "To mitigate data contamination and ensure reliable assessments", "D": "To improve the model's performance on out-of-domain tasks"}, "answer": "C", "explanation": "The correct answer, C, highlights the primary advantage of using a temporal cutoff, which is to prevent data contamination and ensure reliable assessments. This is because the temporal cutoff ensures that the data used to evaluate LLMs is not contaminated with information that the models may have been trained on.", "question_token_count": 20, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 10, "avg_answer_token_count": 9}
{"context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n", "topic": "The role of transparency in benchmarking and its potential risk of data contamination, and how dynamic benchmarking addresses this challenge.", "question": "What primary challenge does dynamic benchmarking aim to address by introducing the concept of collision, and how does this relate to the transparency of benchmarking algorithms?", "choices": {"A": "Ensuring the diversity of training data for LLMs", "B": "Preventing the overlap between different transformations of the benchmark dataset to maintain reliable evaluation", "C": "Enhancing the computational efficiency of benchmarking processes", "D": "Promoting the public availability of benchmarking results"}, "answer": "B", "explanation": "The correct answer, B, highlights the core challenge dynamic benchmarking addresses through the concept of collision: preventing the overlap between different transformations of the benchmark dataset. This is crucial for maintaining the reliability of the benchmarking process, especially when the benchmarking algorithm is publicly available, thereby risking data contamination.", "question_token_count": 29, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 11}
{"context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n", "topic": "Evaluating the trade-offs between the security benefits of encryption and label protection methods and their potential drawbacks, including computational overhead and vulnerability to compromise.", "question": "What is the primary trade-off that developers must consider when deciding between encryption and label protection methods for securing evaluation data?", "choices": {"A": "The trade-off between model performance and evaluation speed", "B": "The trade-off between security benefits and computational overhead", "C": "The trade-off between data contamination risk and model interpretability", "D": "The trade-off between evaluation integrity and model generalizability"}, "answer": "B", "explanation": "The correct answer, \"The trade-off between security benefits and computational overhead\", reflects the central challenge in evaluating the trade-offs between encryption and label protection methods. While encryption provides strong security guarantees, it introduces extra computational overhead, making it vulnerable to compromise if the encryption is broken or the private key is exposed. In contrast, label protection methods may be effective in preventing data contamination, but they may not provide the same level of security as encryption methods.", "question_token_count": 24, "answer_correctness_score": 10, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 10}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The challenges associated with static benchmarks for LLMs, including data contamination and the potential for benchmarks to become too easy for stronger models.", "question": "What is the primary consequence of using static benchmarks to evaluate the performance of rapidly evolving LLMs, and how can this issue be mitigated?", "choices": {"A": "The primary consequence is that static benchmarks become too easy for stronger models, and this issue can be mitigated by using dynamic benchmarks.", "B": "The primary consequence is that static benchmarks introduce data contamination issues, and this issue can be mitigated by using contamination detectors.", "C": "The primary consequence is that static benchmarks fail to evaluate the instruction-following capabilities of LLMs, and this issue can be mitigated by using coding tasks.", "D": "The primary consequence is that static benchmarks overestimate the performance of LLMs, and this issue can be mitigated by using human evaluation."}, "answer": "A", "explanation": "The correct answer is A because static benchmarks can become too easy for stronger models as they evolve rapidly, and using dynamic benchmarks can help mitigate this issue by providing a more challenging and adaptive evaluation framework.", "question_token_count": 29, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 28}
{"context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n", "topic": "Understanding the importance of robust encryption in defeating advanced decontamination methods and its implications for secure benchmarking.", "question": "What critical factor determines the effectiveness of encryption methods in preventing data leakage during the evaluation of machine learning models, despite the presence of advanced decontamination techniques?", "choices": {"A": "Complexity of the encryption algorithm", "B": "Strength of key management and privacy protocols", "C": "Volume of the data being encrypted", "D": "Type of machine learning model being evaluated"}, "answer": "B", "explanation": "The correct answer, \"Strength of key management and privacy protocols,\" is based on the understanding that while encryption methods are effective against data leakage, their success heavily depends on how well the keys are managed and privacy is maintained. If the encryption is compromised or the private key is exposed, the protection is lost. This requires a deep understanding of the relationship between encryption, key management, and the security of benchmarking processes.", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 7}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "The SWE-Bench benchmark addresses advanced challenges in code synthesis and debugging, and it requires a model to demonstrate a high level of proficiency in programming and problem-solving.", "question": "What critical skill does the SWE-Bench benchmark primarily aim to assess in models through its advanced code synthesis and debugging challenges?", "choices": {"A": "Proficiency in natural language understanding", "B": "Ability to recognize and apply programming patterns", "C": "Capacity for logical reasoning and problem-solving in complex coding scenarios", "D": "Knowledge of specific programming languages and frameworks"}, "answer": "C", "explanation": "The SWE-Bench benchmark is designed to evaluate a model's ability to handle complex coding tasks, which requires more than just understanding code or recognizing patterns. It demands the application of logical reasoning and problem-solving skills to debug and synthesize code effectively.", "question_token_count": 25, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 2, "avg_answer_token_count": 8}
{"context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n", "topic": "The role of Auto-Dataset in generating new samples that retain the stylistics and essential knowledge of the original samples, and its potential applications.", "question": "What significant advantage does Auto-Dataset offer in generating new samples for training models, in terms of retaining the original sample's characteristics?", "choices": {"A": "Improved model interpretability", "B": "Enhanced sample diversity without contamination risk", "C": "Retention of stylistics and essential knowledge", "D": "Increased model training speed"}, "answer": "C", "explanation": "Auto-Dataset is specifically designed to generate new samples that retain the stylistics and essential knowledge of the original samples, which is crucial for training models effectively without losing the core information of the original data.", "question_token_count": 27, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 6}
{"context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n", "topic": "The evaluation of LLMs using a combination of interactive and multi-agent approaches, considering the synergies and potential outcomes of integrating these methods in a comprehensive assessment framework.", "question": "What potential advantages could arise from combining the dynamic benchmark creation capabilities of multi-agent frameworks with the nuanced response evaluation of interactive methods, and how might this integration impact the overall assessment of LLMs?", "choices": {"A": "Enhanced scalability and accuracy through automated benchmark creation and human-in-the-loop feedback.", "B": "Improved response evaluation and dynamic benchmark adaptation through integrated interactive and multi-agent approaches.", "C": "Reduced evaluation complexity and increased efficiency by replacing human evaluators with LLMs.", "D": "Complete automation of LLM evaluation, eliminating the need for human feedback and oversight."}, "answer": "B", "explanation": "The correct answer, B, recognizes the potential advantages of combining dynamic benchmark creation with nuanced response evaluation, leading to improved assessment of LLMs. This integration could enable more accurate and informative evaluations by leveraging the strengths of both approaches.", "question_token_count": 39, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 16}
{"context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n", "topic": "The broader societal impact of AI benchmarks, including potential disadvantages to certain user groups or research domains, and how benchmarking frameworks can be designed to minimize harm.", "question": "What critical consideration should be prioritized when designing dynamic AI benchmarks to mitigate potential harm to certain user groups or research domains?", "choices": {"A": "Data Volume", "B": "Model Complexity", "C": "Privacy and Security", "D": "Computational Efficiency"}, "answer": "C", "explanation": "The correct answer, Privacy and Security, is critical because dynamic benchmarks involve the continual collection and updating of data, which raises significant privacy and security concerns. Prioritizing these aspects can help mitigate potential harm by ensuring that user data is protected and that the benchmarking process does not compromise sensitive information.", "question_token_count": 25, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 3}
{"context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n", "topic": "The application of external diversity measurement in evaluating the variation between the transformed dataset and the seed dataset.", "question": "What is the primary purpose of using a function like \u0398\u2062(\u22c5) in measuring external diversity between a transformed dataset and a seed dataset?", "choices": {"A": "To evaluate the internal consistency of the transformed dataset", "B": "To quantify the variation between the transformed dataset and the seed dataset", "C": "To assess the quality of the seed dataset", "D": "To compare the performance of different transformation algorithms"}, "answer": "B", "explanation": "The correct answer, B, reflects the primary purpose of using a function like \u0398\u2062(\u22c5) in measuring external diversity, which is to quantify the variation between the transformed dataset and the seed dataset. This requires a deep understanding of the concept of external diversity and its significance in evaluating dataset transformation.", "question_token_count": 29, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 10}
{"context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n", "topic": "The comparison of different diversity measurement functions, such as \u0398\u2062(\u22c5), and their strengths and weaknesses in various dataset transformation scenarios.", "question": "What is a key consideration when selecting a diversity measurement function, such as \u0398\u2062(\u22c5), for evaluating the diversity of a transformed dataset in a scenario where the seed dataset has a complex structure?", "choices": {"A": "The function's ability to capture internal diversity", "B": "The function's sensitivity to dataset size", "C": "The function's robustness to noise in the seed dataset", "D": "The function's computational efficiency"}, "answer": "A", "explanation": "When selecting a diversity measurement function, such as \u0398\u2062(\u22c5), it is essential to consider the function's ability to capture internal diversity, as this aspect is crucial in evaluating the diversity of a transformed dataset, especially in scenarios where the seed dataset has a complex structure. Internal diversity measures the differences between two transformation trials, which is vital in assessing the variability of the transformed dataset.", "question_token_count": 40, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 7}
{"context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n", "topic": "The challenges of detecting data contamination using exact matching techniques, and the benefits of employing more robust methods such as embedding-based similarity and improved mapping metrics.", "question": "What is the primary limitation of using exact matching techniques for post-hoc detection of data contamination in machine learning models?", "choices": {"A": "They are computationally expensive and require large amounts of memory.", "B": "They are prone to false positives and may incorrectly identify clean data as contaminated.", "C": "They often lead to false negatives, failing to detect contaminated data due to their strict matching criteria.", "D": "They are not suitable for detecting contamination in large datasets and require significant preprocessing."}, "answer": "C", "explanation": "The correct answer, C, highlights the primary limitation of exact matching techniques, which is their tendency to produce false negatives. This is because exact matching requires an exact match between the training and test data, which may not always be possible due to variations in formatting, spelling, or other factors. The other options are incorrect because they do not accurately describe the primary limitation of exact matching techniques.", "question_token_count": 23, "answer_correctness_score": 10, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 16}
{"context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n", "topic": "The role of ethical guidelines in ensuring fair and responsible data usage and model transparency in LLM evaluations, and the challenges of developing and implementing such guidelines.", "question": "What is a critical challenge in developing ethical guidelines for LLM evaluations that balance the need for model transparency with the potential risks of data misuse and contamination?", "choices": {"A": "Ensuring the guidelines are universally applicable across all AI systems.", "B": "Managing the tension between model interpretability and the protection of sensitive data.", "C": "Developing benchmarks that are static and unchanging to prevent bias.", "D": "Ignoring the ethical implications to prioritize model performance."}, "answer": "B", "explanation": "The correct answer requires an understanding of the ethical complexities involved in LLM evaluations, particularly the trade-off between model transparency (which is crucial for trust and accountability) and the risks associated with data misuse and contamination. Option B directly addresses this challenge by acknowledging the need to balance model interpretability (a facet of transparency) with data protection, which is a key ethical consideration.", "question_token_count": 30, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 12}
{"context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n", "topic": "The importance of scalability in dynamic benchmarking methods for generating large-scale benchmark datasets while minimizing associated costs.", "question": "What primary factor does the scalability of a dynamic benchmarking method aim to optimize in the generation of large-scale benchmark datasets?", "choices": {"A": "Dataset size regardless of cost", "B": "Cost-effectiveness per unit of data generated", "C": "Transformation speed of the dataset", "D": "Manual effort required for data transformation"}, "answer": "B", "explanation": "The scalability of a dynamic benchmarking method is about generating large-scale benchmark datasets while minimizing associated costs, essentially optimizing the cost-effectiveness per unit of data generated. This reflects the method's ability to balance dataset size with the costs incurred during the transformation process.", "question_token_count": 24, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 6}
{"context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n", "topic": "The relationship between diversity and the quality of the transformed dataset, including the impact of diversity on downstream tasks and applications.", "question": "What potential downstream task would most significantly benefit from a dataset transformation process that prioritizes internal diversity measured by BLEU scores?", "choices": {"A": "Image recognition", "B": "Language translation", "C": "Speech synthesis", "D": "Text summarization"}, "answer": "B", "explanation": "Language translation tasks often require a diverse set of sentences or phrases to learn the nuances of language, including grammar, syntax, and context. Internal diversity, as measured by BLEU scores (which assess the quality of machine translation), would directly impact the model's ability to generate fluent and accurate translations. Thus, a dataset transformation prioritizing this aspect would most benefit language translation tasks.", "question_token_count": 25, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 3}
{"context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n", "topic": "The impact of dynamic benchmarking correctness on the overall evaluation and development of Large Language Models, including potential biases and areas for improvement.", "question": "What is the primary consequence of a dynamic benchmarking algorithm failing to guarantee the correctness of its generated dataset on the evaluation and development of Large Language Models?", "choices": {"A": "It leads to overfitting of the LLMs to the benchmark.", "B": "It results in a false sense of reliability, potentially misleading the evaluation of LLMs.", "C": "It causes a significant reduction in the computational resources required for LLM training.", "D": "It ensures that the LLMs are more generalizable across different tasks and datasets."}, "answer": "B", "explanation": "The correctness of a dynamic benchmark is crucial for the reliable evaluation of Large Language Models. If a dynamic benchmarking algorithm cannot guarantee the correctness of its generated dataset, it may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations. This is because the evaluation of LLMs depends on the accuracy and reliability of the benchmarking process. Incorrect or misleading evaluations can hinder the development of LLMs by providing inaccurate feedback on their performance.", "question_token_count": 30, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 16}
{"context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The concept of dynamic benchmarking and its significance in evaluating LLMs, including the role of transformation functions in modifying the dataset during benchmarking.", "question": "What is the primary purpose of using transformation functions in dynamic benchmarking, and how do they contribute to a more accurate evaluation of LLMs?", "choices": {"A": "To increase the size of the dataset and provide more training data for LLMs.", "B": "To modify the dataset during benchmarking and avoid possible data contamination, providing a more accurate evaluation of LLMs.", "C": "To evaluate the performance of LLMs on a specific task or dataset.", "D": "To compare the performance of different LLMs on a static benchmark dataset."}, "answer": "B", "explanation": "The correct answer, B, highlights the primary purpose of using transformation functions in dynamic benchmarking, which is to modify the dataset during benchmarking and avoid possible data contamination. This provides a more accurate evaluation of LLMs, as it helps to prevent overfitting and ensures that the model is evaluated on a diverse range of data.", "question_token_count": 28, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 17}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The relationship between LLM benchmarking and the development of more advanced and general-purpose task solvers.", "question": "What approach could dynamic LLM benchmarks incorporate to balance the need for continuous challenge and relevance with the risk of data contamination, considering the rapid evolution of LLM capabilities?", "choices": {"A": "Implementing periodic human evaluation and feedback loops", "B": "Utilizing solely automated contamination detection and benchmark generation", "C": "Incorporating multimodal tasks that require both textual and visual understanding", "D": "Focusing exclusively on edge cases and rare scenarios in benchmark design"}, "answer": "A", "explanation": "The correct approach involves incorporating human evaluation and feedback to ensure that benchmarks remain relevant and challenging while mitigating contamination risks through active monitoring and adaptation. This method allows for a balanced and dynamic assessment of LLM capabilities.", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 11}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "The method of LiveCodeBench in continuously collecting new human-written coding problems from online coding competition platforms like LeetCode.", "question": "What primary advantage does LiveCodeBench's continuous collection of new human-written coding problems offer in evaluating language models?", "choices": {"A": "Reduced risk of overfitting to existing datasets", "B": "Improved ability to handle outdated coding paradigms", "C": "Enhanced capacity to mitigate data contamination and ensure reliability", "D": "Increased focus on theoretical coding concepts"}, "answer": "C", "explanation": "LiveCodeBench's method of continuously collecting new human-written coding problems helps to mitigate data contamination by ensuring that the evaluation dataset is not contaminated with information that the model has already seen. This approach enables a more reliable evaluation of language models.", "question_token_count": 23, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 8}
{"context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n", "topic": "The role of post-hoc detection in mitigating data contamination, including techniques such as n-gram matching, embedding-based similarity, and analyzing model behavior under different conditions.", "question": "What technique is used to detect data contamination by comparing model performance across benchmarks, as proposed by Dekoninck et al.?", "choices": {"A": "Embedding-based similarity", "B": "N-gram matching", "C": "CONSTAT", "D": "Masked input analysis"}, "answer": "C", "explanation": "CONSTAT is a technique proposed by Dekoninck et al. to detect contamination by comparing model performance across benchmarks. This technique is mentioned in the context as a method for analyzing model behavior under different conditions.", "question_token_count": 25, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 4}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "The CommonsenseQA benchmark evaluates a model's ability to apply everyday knowledge and intuitive reasoning skills to answer questions that require common sense and world knowledge.", "question": "What characteristic of the CommonsenseQA benchmark distinguishes it from other language model evaluation challenges?", "choices": {"A": "Emphasis on code synthesis", "B": "Requirement for logical reasoning and background knowledge integration", "C": "Focus on instruction following and execution", "D": "Use of dynamic problem-solving datasets"}, "answer": "B", "explanation": "The correct answer is B, as the CommonsenseQA benchmark is specifically designed to assess a model's ability to integrate background knowledge with logical reasoning to arrive at plausible answers. This distinguishes it from other challenges that focus on coding, instruction following, or dynamic problem-solving.", "question_token_count": 18, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 1, "question_groundedness_score": 8, "avg_answer_token_count": 7}
{"context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n", "topic": "The comparison between different generation methods, including rule-based and template-based approaches, and their potential trade-offs.", "question": "What is a key advantage of rule-based generation methods over template-based approaches in terms of collision probability?", "choices": {"A": "Higher collision probability", "B": "Lower collision probability", "C": "Equivalent collision probability", "D": "Unknown collision probability"}, "answer": "B", "explanation": "The correct answer is based on the information provided in the context, which states that rule-based generation methods feature an extremely low collision probability.", "question_token_count": 21, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 4}
{"context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n", "topic": "The interpretation of the scalability equation as the proportion of data that can be generated per unit cost in dynamic benchmarking.", "question": "What does the scalability equation fundamentally represent in the context of dynamic benchmarking, in terms of the trade-off between data generation and cost?", "choices": {"A": "The ratio of transformation costs to dataset sizes", "B": "The proportion of data that can be generated per unit cost", "C": "The maximum dataset size achievable with a given budget", "D": "The average time spent on transformation per dataset"}, "answer": "B", "explanation": "The correct answer, B, reflects the interpretation of the scalability equation as the proportion of data that can be generated per unit cost, which is a critical aspect of evaluating the scalability of dynamic benchmarking methods.", "question_token_count": 27, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 10}
{"context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n", "topic": "The importance of standardized evaluation tools like static benchmarks in comparing and improving model performance across different tasks and domains.", "question": "What is the primary advantage of using static benchmarks to evaluate the performance of large language models across different tasks and domains?", "choices": {"A": "They provide a dynamic and adaptive assessment of model performance.", "B": "They enable the comparison of model performance across different tasks and domains using a standardized evaluation framework.", "C": "They focus solely on evaluating model performance in a specific task or domain.", "D": "They prioritize model performance in terms of computational efficiency over accuracy."}, "answer": "B", "explanation": "The correct answer, B, highlights the key benefit of static benchmarks in providing a standardized evaluation framework for comparing model performance across different tasks and domains. This allows for a more comprehensive understanding of model strengths and weaknesses and facilitates the development of more robust and generalizable models.", "question_token_count": 24, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 4, "avg_answer_token_count": 14}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "The NPHardEval method and its evaluation of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP), using random graphs.", "question": "What fundamental limitation of Large Language Models does the NPHardEval method, using random graphs for the Traveling Salesman Problem, most directly test?", "choices": {"A": "Ability to generalize across different graph sizes", "B": "Capacity for exact solutions in polynomial time", "C": "Scalability in solving NP-hard problems", "D": "Robustness against adversarial graph structures"}, "answer": "C", "explanation": "The NPHardEval method tests LLMs on NP-hard problems like TSP, which are known for their computational intractability. The use of random graphs scales the problem size, directly challenging the LLM's ability to solve NP-hard problems efficiently, which is a fundamental limitation due to the nature of these problems requiring more than polynomial time for exact solutions as the size of the input increases.", "question_token_count": 30, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 7}
{"context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n", "topic": "The limitations of label protection in terms of transparency and independent verification, and how these limitations can impede detailed error analysis and reproducibility.", "question": "What is a potential consequence of relying on centralized evaluation systems for performance metrics in machine learning, given the limitations of label protection?", "choices": {"A": "Improved model generalizability", "B": "Enhanced data privacy", "C": "Reduced ability to detect model errors", "D": "Increased model interpretability"}, "answer": "C", "explanation": "The correct answer, C: Reduced ability to detect model errors, is a consequence of relying on centralized evaluation systems for performance metrics. This is because the limitations of label protection can impede detailed error analysis and reproducibility, making it more challenging to detect and address model errors.", "question_token_count": 26, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 5}
{"context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n", "topic": "The importance of verification in live benchmarks for evaluating Large Language Models, and the potential consequences of overlooking this step.", "question": "What is the primary consequence of neglecting verification in live benchmarks for LLM evaluation, and how can it compromise the results?", "choices": {"A": "Increased efficiency in the evaluation process", "B": "Improved accuracy in the evaluation results", "C": "Data contamination and potential reuse of problems", "D": "Reduced human effort in the collection process"}, "answer": "C", "explanation": "The correct answer, C, is supported by the context, which highlights the importance of verification in live benchmarks and the potential consequences of overlooking this step, including data contamination and reuse of problems. The other options are incorrect because neglecting verification would not lead to increased efficiency, improved accuracy, or reduced human effort.", "question_token_count": 24, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 4, "avg_answer_token_count": 7}
{"context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n", "topic": "The importance of standardized criteria for evaluating dynamic benchmarks and the need for future research in this area.", "question": "What primary challenge does the development of dynamic benchmarks for evaluating large language models (LLMs) face, according to the need for future research in this area?", "choices": {"A": "Lack of transparency in static benchmarking methods", "B": "Insufficient training data for LLMs", "C": "Absence of standardized criteria for evaluation", "D": "Inability to scale dynamic benchmarks for web-scale data"}, "answer": "C", "explanation": "The correct answer, \"Absence of standardized criteria for evaluation,\" reflects the primary challenge identified in the context regarding dynamic benchmarks. This challenge is highlighted as a key insight and a future direction for research, emphasizing the need for standardized criteria to effectively evaluate dynamic benchmarks.", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 9}
{"context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n", "topic": "The role of dynamic benchmarking methods, such as LatestEval, DARG, and C2LEVA, in addressing the limitations of traditional static benchmarks and evaluating LLMs.", "question": "What is the primary advantage of using dynamic benchmarking methods like LatestEval, DARG, and C2LEVA over traditional static benchmarks in evaluating Large Language Models?", "choices": {"A": "They are more scalable and efficient in evaluation.", "B": "They can effectively address the issue of data contamination and provide more reliable evaluations.", "C": "They are less transparent and have lower assumptions about contaminated models.", "D": "They are more suitable for models trained on small-scale data."}, "answer": "B", "explanation": "The correct answer, B, highlights the primary advantage of dynamic benchmarking methods, which is their ability to address the issue of data contamination and provide more reliable evaluations. This is a key insight from the context, which discusses the limitations of traditional static benchmarks and the role of dynamic benchmarks in overcoming these limitations.", "question_token_count": 33, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 13}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "The approach of AntiLeak-Bench in generating queries about newly emerged knowledge unknown before the model's knowledge cutoff date to eliminate potential data contamination.", "question": "What is the primary advantage of AntiLeak-Bench's approach in generating queries about newly emerged knowledge unknown before the model's knowledge cutoff date?", "choices": {"A": "Reducing the computational cost of evaluating LLMs", "B": "Eliminating potential data contamination and ensuring a fair evaluation of the model's ability to generalize to new information", "C": "Increasing the size of the training dataset", "D": "Improving the model's performance on out-of-domain tasks"}, "answer": "B", "explanation": "AntiLeak-Bench's approach is designed to eliminate potential data contamination by generating queries about newly emerged knowledge unknown before the model's knowledge cutoff date. This ensures that the model is evaluated on information it could not have learned during its training, providing a fair assessment of its ability to generalize to new information.", "question_token_count": 29, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 12}
{"context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n", "topic": "A comparison of StructEval and ITD in terms of their approaches to expanding on examined concepts and detecting contaminated samples, respectively.", "question": "What is the primary difference in how StructEval and ITD approach the issue of contaminated samples in static benchmarks, in terms of their methodology for expansion or rewriting of samples?", "choices": {"A": "StructEval uses knowledge graphs for contamination detection, while ITD employs LLMs for sample expansion.", "B": "StructEval expands on examined concepts using LLMs and knowledge graphs, whereas ITD detects and rewrites contaminated samples using a contamination detector and LLMs.", "C": "StructEval rewrites samples based on cognitive levels, and ITD generates new samples by replacing variables.", "D": "StructEval identifies contaminated samples through pre-defined rules, and ITD uses LLMs to preserve sample difficulty levels."}, "answer": "B", "explanation": "The correct answer highlights the distinct methodologies of StructEval and ITD. StructEval focuses on expanding examined concepts using LLMs and knowledge graphs, while ITD is centered on detecting contaminated samples with a contamination detector and then rewriting them with LLMs to preserve difficulty levels.", "question_token_count": 35, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 23}
{"context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n", "topic": "The limitations of the proposed criteria for dynamic benchmarking, including the need for further refinement and validation in real-world applications.", "question": "What critical factor must be prioritized in the refinement and validation of dynamic benchmarking criteria for Large Language Models to ensure their reliability and reproducibility in real-world applications?", "choices": {"A": "Increased dataset size", "B": "Standardization of evaluation metrics", "C": "Adaptability to evolving model architectures", "D": "Enhanced computational resources"}, "answer": "B", "explanation": "The correct answer, \"Standardization of evaluation metrics,\" is crucial because it directly addresses the challenge of ensuring reliability and reproducibility in dynamic benchmarking approaches. Without standardized metrics, comparisons across different models and benchmarking methods become problematic, hindering the advancement of LLMs. The other options, while potentially beneficial, do not directly tackle the core issue of reliability and reproducibility in dynamic benchmarking.", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 2, "question_groundedness_score": 5, "avg_answer_token_count": 5}
{"context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n", "topic": "The ethical implications of dynamic benchmarks, including privacy and security concerns, and how these can be addressed in the design of benchmarking frameworks.", "question": "What critical design consideration must dynamic benchmarking frameworks prioritize to mitigate privacy and security risks while ensuring fairness and transparency in AI evaluations?", "choices": {"A": "Regular model updates without user consent", "B": "Continuous data collection with anonymization protocols", "C": "Transparency in data sources and evaluation metrics", "D": "External audits for bias detection and mitigation"}, "answer": "C", "explanation": "The correct answer, \"Transparency in data sources and evaluation metrics,\" is crucial because it directly addresses the ethical concerns related to dynamic benchmarks. Transparency ensures that the data used for benchmarking is not biased, and the evaluation metrics are fair and relevant, thereby mitigating privacy and security risks. This approach also promotes accountability, as stakeholders can review and critique the benchmarking process.", "question_token_count": 25, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 7}
{"context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n", "topic": "A discussion on the importance of preserving difficulty levels when rewriting contaminated samples, as achieved by ITD, and its impact on the overall quality of the training dataset.", "question": "What critical aspect of sample rewriting does ITD preserve, which is essential for maintaining the quality of the training dataset, and how does this differentiation impact the efficacy of the resulting dataset compared to methods that do not preserve this aspect?", "choices": {"A": "Contextual relevance", "B": "Difficulty levels", "C": "Stylistic consistency", "D": "Knowledge breadth"}, "answer": "B", "explanation": "ITD preserves the difficulty levels of the original samples when rewriting them, which is crucial for maintaining the quality and challenge of the training dataset. This differentiation is essential because it ensures that the rewritten samples continue to test the cognitive abilities they were originally designed to assess, thereby contributing to a more robust and effective training dataset.", "question_token_count": 46, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 3}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "Common cases of data contamination, such as verbatim test examples in training corpora, code snippets from benchmark implementations, or documentation leaks.", "question": "What type of data contamination occurs when a test data point can be transformed into a training data point through syntactic transformations, such as punctuation normalization or synonym substitution, without altering its lexical meaning?", "choices": {"A": "Exact contamination", "B": "Syntactic contamination", "C": "Semantic contamination", "D": "Lexical contamination"}, "answer": "B", "explanation": "Syntactic contamination occurs when a test data point can be found in the training dataset after applying syntactic transformations, such as punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing, while preserving lexical meaning.", "question_token_count": 38, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 3}
{"context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n", "topic": "The potential applications and limitations of using knowledge graphs in conjunction with Large Language Models for developing extended questions, as seen in StructEval.", "question": "What is the primary mechanism by which StructEval utilizes Large Language Models and knowledge graphs to generate extended questions, and what is the key benefit of this approach in terms of expanding on examined concepts from the original benchmark?", "choices": {"A": "StructEval uses LLMs to generate new questions based on pre-defined rules, and the key benefit is increased sample diversity.", "B": "StructEval employs knowledge graphs to identify related concepts and LLMs to generate questions at different cognitive levels, allowing for a more comprehensive assessment of understanding.", "C": "StructEval utilizes a contamination detector to identify contaminated samples and then prompts an LLM to rewrite them, preserving their difficulty levels.", "D": "StructEval prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples with retained stylistics and essential knowledge."}, "answer": "B", "explanation": "The correct answer requires an understanding of the specific approach used by StructEval, which involves using knowledge graphs to expand on examined concepts and LLMs to generate extended questions. This approach allows for a more comprehensive assessment of understanding, as it presents related questions at different cognitive levels.", "question_token_count": 43, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 27}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "Assessing the coverage and effectiveness of multi-domain tasks in MMLU, BBH, and AGI Eval for evaluating LLM internal knowledge.", "question": "What is the primary advantage of using multi-domain tasks like MMLU, BBH, and AGI Eval to evaluate LLM internal knowledge, and how do they complement other knowledge benchmarks like NaturalQuestions and TriviaQA?", "choices": {"A": "They focus exclusively on real-world information retrieval.", "B": "They provide a more comprehensive assessment of LLM internal knowledge by covering a broader range of topics and tasks.", "C": "They are designed specifically for technical and long-context challenges.", "D": "They are limited to evaluating math problem-solving abilities."}, "answer": "B", "explanation": "The correct answer, B, highlights the primary advantage of using multi-domain tasks like MMLU, BBH, and AGI Eval, which is to provide a more comprehensive assessment of LLM internal knowledge. This is because these benchmarks cover a broader range of topics and tasks, complementing other knowledge benchmarks like NaturalQuestions and TriviaQA that focus on retrieving real-world information.", "question_token_count": 44, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 2, "question_groundedness_score": 6, "avg_answer_token_count": 13}
{"context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n", "topic": "The potential benefits and challenges of using dynamic benchmarking methods in LLM development, including their potential to improve model reliability and reproducibility.", "question": "What is the primary challenge that dynamic benchmarking methods face in LLM development, which hinders their potential to improve model reliability and reproducibility?", "choices": {"A": "Vulnerability to contamination", "B": "Lack of standardized evaluation protocols", "C": "Insufficient training data", "D": "High computational complexity"}, "answer": "B", "explanation": "The correct answer, \"Lack of standardized evaluation protocols,\" is supported by the context, which states that dynamic approaches face challenges in reliability and reproducibility. The other options are incorrect because vulnerability to contamination is a limitation of static methods, insufficient training data is not mentioned as a challenge specific to dynamic benchmarking, and high computational complexity is not discussed in the context as a primary challenge.", "question_token_count": 27, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 6, "avg_answer_token_count": 5}
{"context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n", "topic": "The impact of data contamination on the reliability of LLM evaluations and the need for methods to prevent or detect contamination.", "question": "What is the primary factor that contributes to the decreased effectiveness of static benchmarks in evaluating LLMs as the size of the training corpus increases?", "choices": {"A": "Increased model complexity", "B": "Growing size of the test dataset", "C": "Higher probability of data contamination", "D": "Improved model performance on existing benchmarks"}, "answer": "C", "explanation": "The correct answer is based on the context, which states that the probability of contamination increases with the size of the training corpus, rendering traditional benchmarks outdated for models trained on web-scale data.", "question_token_count": 28, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 6}
{"context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n", "topic": "The role of the function \u0398\u2062(\u22c5) in measuring diversity between two datasets and its possible implementations, such as N-gram metrics or reference-based metrics like BLEU scores.", "question": "What property of the BLEU score makes it more suitable for measuring external diversity between a transformed dataset and a seed dataset, as opposed to internal diversity between two transformation trials?", "choices": {"A": "Sensitivity to word order", "B": "Robustness to outliers", "C": "Ability to capture semantic similarities", "D": "Insensitivity to dataset size"}, "answer": "A", "explanation": "The BLEU score is a reference-based metric that measures the similarity between two datasets by comparing their n-gram sequences. Its property of being sensitive to word order makes it more suitable for measuring external diversity, where the goal is to compare the transformed dataset to a seed dataset with a specific word order. In contrast, internal diversity between two transformation trials may not require such sensitivity to word order.", "question_token_count": 35, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 5}
{"context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n", "topic": "The challenges and limitations of using static benchmarks for evaluating model performance, including potential biases in the seed dataset and the scoring function.", "question": "What is a primary concern when using a static benchmark with a seed dataset and scoring function to evaluate the performance of large language models, and how might this concern impact the validity of the evaluation results?", "choices": {"A": "The concern is that the model may overfit to the training data, and this could lead to inaccurate results.", "B": "The concern is that the seed dataset may contain biases that are reflected in the scoring function, potentially favoring certain models or outputs over others.", "C": "The concern is that the scoring function may not accurately capture the nuances of human language understanding, leading to misleading results.", "D": "The concern is that the model may not be able to generalize well to new, unseen data, and this could limit the usefulness of the evaluation."}, "answer": "B", "explanation": "The correct answer, B, highlights the potential for biases in the seed dataset and scoring function, which can impact the validity of the evaluation results. This concern is critical because it can lead to unfair or misleading evaluations, where certain models are unfairly advantaged or disadvantaged due to biases in the benchmark.", "question_token_count": 40, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 7, "avg_answer_token_count": 26}
{"context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n", "topic": "The role of transformation space in scalability, including the relationship between the size of the transformed dataset and the original dataset.", "question": "What property of the transformation space directly influences the scalability of dynamic benchmarking methods, as measured by the ratio of transformed dataset size to original dataset size per unit cost?", "choices": {"A": "Dataset dimensionality", "B": "Transformation complexity", "C": "Cost function linearity", "D": "Data distribution entropy"}, "answer": "B", "explanation": "The correct answer, transformation complexity, is implied by the context as it discusses the transformation space and its impact on scalability through the formula provided. The transformation complexity directly affects how much data can be generated per unit cost, influencing scalability. The other options, while related to data and transformations, do not directly address the property of the transformation space that influences scalability as described.", "question_token_count": 33, "answer_correctness_score": 10, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 3}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The limitations of static benchmarks in keeping pace with the rapid evolution of LLMs and their continuous training on available data.", "question": "What is a potential consequence of static benchmarks becoming too easy for stronger LLMs, and how can dynamic benchmarks mitigate this issue?", "choices": {"A": "Static benchmarks may lead to overestimation of model performance, and dynamic benchmarks can mitigate this by continuously updating the evaluation tasks.", "B": "Static benchmarks may result in underestimation of model performance, and dynamic benchmarks can mitigate this by reducing the frequency of evaluation tasks.", "C": "Static benchmarks may cause data contamination, and dynamic benchmarks can mitigate this by using a fixed dataset for evaluation.", "D": "Static benchmarks may lead to model overfitting, and dynamic benchmarks can mitigate this by increasing the model's training data."}, "answer": "A", "explanation": "The correct answer is A, as static benchmarks may become too easy for stronger LLMs, leading to overestimation of model performance. Dynamic benchmarks can mitigate this issue by continuously updating the evaluation tasks to keep pace with the evolving capabilities of LLMs.", "question_token_count": 26, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 9, "avg_answer_token_count": 24}
{"context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n", "topic": "The potential applications and implications of post-hoc detection methods in real-world scenarios, including the potential impact on model reliability, data integrity, and decision-making processes.", "question": "What is the primary advantage of using embedding-based similarity over exact matching in post-hoc detection methods for identifying data contamination?", "choices": {"A": "Improved robustness to false positives", "B": "Enhanced ability to detect subtle overlaps between training and test data", "C": "Increased computational efficiency", "D": "Simplified interpretation of results"}, "answer": "B", "explanation": "The correct answer, B, highlights the key benefit of using embedding-based similarity, which is its ability to detect subtle overlaps between training and test data. This is a critical aspect of post-hoc detection, as exact matching often leads to false negatives. The other options are incorrect because while embedding-based similarity may have some secondary benefits, such as improved robustness to false positives (A), it is not primarily used for this purpose. Similarly, options C and D are incorrect because embedding-based similarity may not necessarily be more computationally efficient or provide simpler interpretation of results.", "question_token_count": 24, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 7}
{"context": "### 1 Introduction\nThe field of natural language processing (NLP) has advanced rapidly in recent years, fueled by breakthroughs in Large Language Models (LLMs) such as GPT-4, Claude3, and DeepSeek\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024); Wan et\u00a0al. (2023). Trained on vast amounts of Internet-sourced data, these models have demonstrated remarkable capabilities across various applications, including code generation, text summarization, and mathematical reasoning\u00a0Codeforces (2025); Hu et\u00a0al. (2024).\n\nTo develop and enhance LLMs, beyond advancements in model architectures and training algorithms, a crucial area of research focuses on effectively evaluating their intelligence. Traditionally, LLM evaluation has relied on static benchmarking, which involves using carefully curated human-crafted datasets and assessing model performance with appropriate metrics\u00a0Wang (2018); Achiam et\u00a0al. (2023); Gunasekar et\u00a0al. (2023).\n\nFor example, in 2021, OpenAI introduced the HumanEval\u00a0Chen et\u00a0al. (2021) dataset to benchmark the code generation capabilities of LLMs; HumanEval has since become a widely used standard for this purpose.\n\nHowever, because these static benchmarks are released on the Internet for transparent evaluation, and LLMs gather as much data as possible from the Internet for training, potential data contamination is unavoidable\u00a0Magar and Schwartz (2022); Deng et\u00a0al. (2024b); Li et\u00a0al. (2024d); Sainz et\u00a0al. (2024); Balloccu et\u00a0al. (2024). Data contamination occurs when benchmark data is inadvertently included in the training phase of language models, leading to an inflated and misleading assessment of their performance. While this issue has been recognized for some time\u2014stemming from the fundamental machine learning principle of separating training and test sets\u2014it has become even more critical with the advent of LLMs. These models often scrape vast amounts of publicly available data from the Internet, significantly increasing the likelihood of contamination\u00a0Achiam et\u00a0al. (2023); Liu et\u00a0al. (2024). Furthermore, due to privacy and commercial concerns, tracing the exact training data for these models is challenging, if not impossible, complicating efforts to detect and mitigate potential contamination.\n\nFigure 1: The progress of benchmarking LLM\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nTo mitigate the risk of data contamination in LLM benchmarking, researchers have proposed various methods to enhance static approaches, such as data encryption\u00a0Jacovi et\u00a0al. (2023) and post-hoc contamination detection Shi et\u00a0al. (2024). However, due to the inherent limitations of static methods, researchers have introduced new dynamic benchmarking schema (Fig.\u00a01 illustrates the evolution from static to dynamic benchmarking).\n\nOne of the dynamic benchmarking methods involves continuously updating benchmark datasets based on the timestamps of LLM training to minimize contamination\u00a0White et\u00a0al. (2024); Jain et\u00a0al. (2024). Another method focuses on regenerating benchmark data to reconstruct original benchmarks\u00a0Chen et\u00a0al. (2024); Zhou et\u00a0al. (2025); Mirzadeh et\u00a0al. (2025), thereby reducing the likelihood of contamination.\n\nDespite the numerous approaches proposed for transparent LLM benchmarking, no systematic survey has summarized the rapid progress of dynamic benchmarking methods. Existing literature reviews primarily focus on post-hoc detection of data contamination and do not address emerging dynamic benchmarking strategies\u00a0Deng et\u00a0al. (2024a); Ravaut et\u00a0al. (2024). Moreover, no existing work discusses criteria for evaluating dynamic benchmarks themselves. Additionally, these reviews often lack a comprehensive discussion of the strengths and limitations of each method.\n\nTo bridge this gap, we first conduct a systematic survey of benchmarking methods for LLMs designed to mitigate the risk of data contamination, covering both static and dynamic benchmarks. We summarize state-of-the-art methods and provide an in-depth discussion of their strengths and limitations. Furthermore, we are the first to propose a set of criteria for evaluating dynamic benchmarks. Our study reveals that existing dynamic benchmarks do not fully satisfy these proposed criteria, implying the imperfection of current design. We hope that our criteria will provide valuable insights for the future design and standardization of dynamic benchmarking methods.\n\nFigure 2:  Taxonomy of research on benchmarking LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n\nThe paper is structured as illustrated in Fig.\u00a02. We begin with an overview of data contamination (\u00a72), establishing the necessary background. Next, we provide a comprehensive review of static benchmarking methods and examine how existing research has improved them to mitigate potential data contamination (\u00a73). We then outline key criteria for an effective dynamic benchmark and introduce existing dynamic benchmarking approaches (\u00a74). Finally, in \u00a75, we reflect on insights from prior work, summarize current challenges, and explore future research directions.\n\nTable 1: Examples of Data Contamination in LLMs\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The limitations of traditional static benchmarking approaches in evaluating LLMs and the potential risks of data contamination.", "question": "What are the primary criteria for assessing the efficacy of dynamic benchmarking methods in reducing data contamination risks in Large Language Models?", "choices": {"A": "Model accuracy, training time, and dataset size", "B": "Data encryption, post-hoc contamination detection, and benchmark updating frequency", "C": "Evaluation metrics, contamination risk assessment, and benchmark adaptability", "D": "Model architecture, training algorithm, and hyperparameter tuning"}, "answer": "C", "explanation": "The correct answer, C: Evaluation metrics, contamination risk assessment, and benchmark adaptability, reflects the key criteria for evaluating dynamic benchmarking methods, as discussed in the context. These criteria are essential for assessing the effectiveness of dynamic benchmarking approaches in mitigating data contamination risks in LLMs.", "question_token_count": 24, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 7, "avg_answer_token_count": 11}
{"context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n", "topic": "The theoretical foundations underlying the use of multi-agent systems in LLM evaluation, including the concept of agent-based methods and their application in dynamic benchmark creation.", "question": "What is the primary advantage of using a multi-agent framework, as seen in BENCHAGENTS, in automated benchmark creation for LLM evaluation?", "choices": {"A": "Improved model accuracy", "B": "Enhanced human-in-the-loop feedback", "C": "Scalable and diverse benchmark generation", "D": "Reduced computational complexity"}, "answer": "C", "explanation": "The correct answer, C: Scalable and diverse benchmark generation, is supported by the context, which highlights the potential of multi-agent frameworks in creating scalable, diverse, and high-quality benchmarks. The other options are incorrect because they do not accurately reflect the primary advantage of using a multi-agent framework in automated benchmark creation.", "question_token_count": 29, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 5}
{"context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n", "topic": "The potential applications and benefits of dynamic benchmarking methods in evaluating LLMs, including improved reliability and validity.", "question": "What primary challenge do static benchmarking methods face that dynamic benchmarking aims to address, particularly in the context of evaluating Large Language Models trained on web-scale data?", "choices": {"A": "Lack of transparency in label protection", "B": "Inability to balance correctness with scalability", "C": "Increasing probability of data contamination as training corpora grow", "D": "Neglect of complexity control in evaluation"}, "answer": "C", "explanation": "Static benchmarking methods become less effective as training corpora grow due to the increasing probability of data contamination, which compromises evaluation reliability. Dynamic benchmarking methods are proposed as a solution to address this limitation.", "question_token_count": 31, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 7, "avg_answer_token_count": 8}
{"context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n", "topic": "The scalability and diversity of benchmarks generated through multi-agent collaborations, as compared to traditional static benchmarks, and the implications for LLM evaluation and development.", "question": "What significant advantage do multi-agent collaborations offer in generating benchmarks for LLM evaluation, as compared to traditional static benchmarks, in terms of enhancing model development?", "choices": {"A": "Improved model interpretability", "B": "Enhanced benchmark diversity and scalability", "C": "Reduced need for human feedback", "D": "Increased dependence on static benchmarks"}, "answer": "B", "explanation": "The correct answer, \"Enhanced benchmark diversity and scalability,\" reflects the core benefit of using multi-agent collaborations for LLM evaluation, as discussed in the context. This approach allows for the dynamic generation of high-quality, diverse benchmarks, which can more comprehensively assess LLM capabilities and foster more effective model development.", "question_token_count": 30, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 8, "avg_answer_token_count": 5}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "The C-Eval benchmark focuses on Chinese instructions, and it evaluates a model's ability to comprehend and execute detailed directives in a specific linguistic and cultural context.", "question": "What challenge does the C-Eval benchmark pose to language models in terms of instruction following, and how does its focus on Chinese instructions impact the evaluation of a model's ability to comprehend and execute detailed directives?", "choices": {"A": "It poses a challenge in terms of syntax and semantics, and its focus on Chinese instructions helps to evaluate a model's ability to handle linguistic and cultural nuances.", "B": "It poses a challenge in terms of common sense and world knowledge, and its focus on Chinese instructions is irrelevant to the evaluation of a model's ability to comprehend and execute detailed directives.", "C": "It poses a challenge in terms of code generation and debugging, and its focus on Chinese instructions helps to evaluate a model's ability to handle programming languages.", "D": "It poses a challenge in terms of machine translation and language generation, and its focus on Chinese instructions helps to evaluate a model's ability to handle language pairs."}, "answer": "A", "explanation": "The correct answer, A, requires an understanding of the significance of cultural and linguistic context in instruction following and how benchmarks like C-Eval contribute to the development of more sophisticated language models. The incorrect answers, B, C, and D, are plausible but incorrect, and require a subtle understanding of the differences between various benchmarks and their focuses.", "question_token_count": 41, "answer_correctness_score": 10, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 32}
{"context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n", "topic": "The role of data contamination in LLM benchmarking, including its potential impact on model performance and reliability.", "question": "What is the primary challenge that dynamic benchmarking methods for LLMs face in the context of data contamination, which hinders their ability to reliably assess model performance?", "choices": {"A": "Vulnerability to overfitting due to small dataset sizes", "B": "Difficulty in ensuring reproducibility across different contamination scenarios", "C": "Inability to account for the evolving nature of training datasets", "D": "Lack of standardized criteria for evaluating contamination effects"}, "answer": "B", "explanation": "Dynamic benchmarking methods face challenges in reliability and reproducibility, particularly in ensuring that the results can be consistently reproduced across different scenarios, including various types and levels of data contamination. This challenge is crucial because it directly affects the trustworthiness of the performance assessments of LLMs.", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 10}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "Evaluating the effectiveness of GSM8K and MATH datasets in assessing a model's ability to solve complex, multi-step math problems.", "question": "What is the primary advantage of using a combination of GSM8K and MATH datasets to evaluate a model's ability to solve complex math problems, compared to using a single dataset?", "choices": {"A": "Increased dataset size and diversity", "B": "Improved evaluation of a model's ability to generalize across different math problem types", "C": "Enhanced assessment of a model's reasoning and problem-solving skills", "D": "Reduced risk of overfitting to a specific dataset or problem type"}, "answer": "B", "explanation": "The correct answer is B) Improved evaluation of a model's ability to generalize across different math problem types. Using a combination of GSM8K and MATH datasets allows for a more comprehensive evaluation of a model's ability to solve complex math problems, as these datasets cover a wide range of math topics and problem types. This helps to ensure that the model is not overfitting to a specific dataset or problem type, but rather has a more general understanding of mathematical concepts.", "question_token_count": 35, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 4, "avg_answer_token_count": 11}
{"context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n", "topic": "The trade-offs between model performance and data contamination in LLM development and deployment.", "question": "What is a primary consequence of the proprietary nature of LLM training data on the assessment of model performance?", "choices": {"A": "Improved model generalizability", "B": "Enhanced transparency in model development", "C": "Increased risk of overestimation of model capabilities", "D": "Reduced need for human-annotated datasets"}, "answer": "C", "explanation": "The proprietary nature of LLM training data complicates the accurate assessment of model performance, as it impedes the community's ability to verify and mitigate potential overlaps between training and evaluation data. This can lead to an overestimation of model capabilities, as the true performance of the model may be inflated due to contamination.", "question_token_count": 21, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 7}
{"context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n", "topic": "The relationship between data contamination and model performance, including the ways in which contamination can affect model accuracy, fairness, and robustness, and the implications of this relationship for model development and deployment.", "question": "What is the primary challenge in post-hoc detection of data contamination, and how can embedding-based similarity techniques help mitigate this challenge?", "choices": {"A": "The primary challenge is the difficulty in identifying exact matches, and embedding-based similarity can help by allowing for more flexible and robust detection of overlaps.", "B": "The primary challenge is the lack of transparency in label protection, and embedding-based similarity can help by providing an alternative to centralized evaluation systems.", "C": "The primary challenge is the computational cost of post-hoc detection, and embedding-based similarity can help by reducing the number of comparisons required.", "D": "The primary challenge is the scarcity of training data, and embedding-based similarity can help by generating additional training examples."}, "answer": "A", "explanation": "The correct answer, A, requires an understanding of the limitations of exact matching in post-hoc detection and the potential benefits of embedding-based similarity techniques in mitigating these limitations. The incorrect options, B, C, and D, are plausible but incorrect, requiring the test-taker to carefully consider the context and the relationship between data contamination and model performance.", "question_token_count": 26, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 8, "avg_answer_token_count": 26}
{"context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n", "topic": "The challenges and limitations of measuring diversity in dataset transformation and potential solutions to overcome these challenges.", "question": "What is a crucial consideration when choosing a function \u0398\u2062(\u22c5) to measure diversity between datasets in dataset transformation, and how might this impact the trade-off between external and internal diversity?", "choices": {"A": "The function should prioritize simplicity and computational efficiency.", "B": "The function should be sensitive to subtle differences in dataset structure and content.", "C": "The function should exclusively rely on reference-based metrics like BLEU scores.", "D": "The function should focus solely on maximizing external diversity."}, "answer": "B", "explanation": "The correct answer requires an understanding of the complexities involved in measuring diversity and the importance of selecting a function that can accurately capture subtle differences in dataset structure and content. This consideration is crucial because it directly impacts the trade-off between external and internal diversity, which is essential for maintaining a balanced and diverse transformed dataset.", "question_token_count": 38, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 12}
{"context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n", "topic": "The potential benefits and challenges of using dynamic benchmarks, as opposed to static ones, in the evaluation of LLMs, considering the context of rapid advancements in AI technology.", "question": "What is a key challenge in using multi-agent frameworks for dynamic benchmark creation in LLM evaluation, and how might it impact the scalability and diversity of the resulting benchmarks?", "choices": {"A": "Ensuring the consistency of agent specializations", "B": "Mitigating the risk of cascading errors across agent interactions", "C": "Balancing the trade-off between benchmark quality and computational resources", "D": "Addressing the potential for emergent biases in agent-generated benchmarks"}, "answer": "D", "explanation": "The correct answer, D, highlights a crucial challenge in using multi-agent frameworks for dynamic benchmark creation. As agents interact and generate benchmarks, there is a risk of emergent biases arising from the complex interactions between agents, which can impact the quality and diversity of the resulting benchmarks. This requires careful consideration and mitigation strategies to ensure the benchmarks are fair, reliable, and effective in evaluating LLMs.", "question_token_count": 33, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 5, "avg_answer_token_count": 11}
{"context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n", "topic": "The risks of evaluation data overlap in LLM pre-training and fine-tuning phases and their impact on model performance.", "question": "What is a primary consequence of the opacity of proprietary training data in LLMs regarding the assessment of their true performance?", "choices": {"A": "Increased model interpretability", "B": "Improved data privacy", "C": "Impeded verification and mitigation of potential data overlaps", "D": "Enhanced model generalizability"}, "answer": "C", "explanation": "The opacity of proprietary training data complicates the accurate assessment of LLM performance by impeding the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data, which can lead to contaminated models and overestimated performance.", "question_token_count": 24, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 6}
{"context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n", "topic": "The trade-offs between dataset size and cost in dynamic benchmarking, including strategies for optimizing scalability while controlling costs.", "question": "What strategy can be employed to optimize the scalability of dynamic benchmarking methods while controlling costs, considering the proportion of data generated per unit cost?", "choices": {"A": "Prioritizing dataset size over cost considerations", "B": "Implementing automated transformation processes to reduce manual effort", "C": "Selecting transformation functions that minimize cost per unit of generated data", "D": "Focusing solely on reducing the size of the original dataset"}, "answer": "C", "explanation": "The correct answer, \"Selecting transformation functions that minimize cost per unit of generated data,\" reflects a strategy that directly addresses the trade-off between dataset size and cost. By choosing transformation functions that are cost-efficient, dynamic benchmarking methods can generate larger datasets without disproportionately increasing costs. This approach aligns with the equation provided, which interprets scalability as the proportion of data generated per unit cost.", "question_token_count": 28, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 5, "avg_answer_token_count": 10}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "The importance of assessing LLMs on varying task difficulties, such as controlling the number of nodes and edges in DAGs.", "question": "What critical consideration in constructing DAGs for LLM evaluation, such as in DyVal, allows for the controlled assessment of reasoning capabilities across varying task complexities?", "choices": {"A": "The type of nodes used", "B": "The number of edges and nodes", "C": "The direction of edges", "D": "The presence of cycles"}, "answer": "B", "explanation": "The correct answer, \"The number of edges and nodes,\" reflects the context's emphasis on controlling task difficulty through the variation of DAG structures. This is a critical aspect of evaluating LLMs, as it allows for a nuanced assessment of their reasoning capabilities under different conditions.", "question_token_count": 31, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 5}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "The ARC benchmark requires the integration of background knowledge with logical reasoning to arrive at plausible answers, and it pushes models to demonstrate a deep understanding of academic concepts and principles.", "question": "What characteristic of the ARC benchmark distinguishes it from other language model evaluation benchmarks?", "choices": {"A": "Emphasis on code synthesis", "B": "Focus on instruction following", "C": "Integration of background knowledge with logical reasoning", "D": "Evaluation of factuality in short questions"}, "answer": "C", "explanation": "The ARC benchmark is unique in its requirement for the integration of background knowledge with logical reasoning to arrive at plausible answers. This characteristic distinguishes it from other benchmarks that focus on code synthesis, instruction following, or factuality in short questions.", "question_token_count": 16, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 2, "avg_answer_token_count": 6}
{"context": "###### Safety\nSafety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content. Datasets such as RealToxicityPrompts\u00a0(Gehman et\u00a0al., 2020) and ToxiGen\u00a0(Hartvigsen et\u00a0al., 2022) assess resilience against producing harmful outputs.\n\nBy providing a controlled environment to measure these aspects, safety benchmarks play a critical role in guiding the development of models that are not only powerful outputs but also responsible and trustworthy for real-world applications.\n\n\n###### Language\nLanguage benchmarks assess the LLMs\u2019 proficiency in specific languages.\n\nGLUE\u00a0(Wang, 2018) and SuperGLUE\u00a0(Wang et\u00a0al., 2019) cover tasks from sentiment analysis to language inference, while CLUE\u00a0(Xu et\u00a0al., 2020) targets Chinese language.\n\nTypo-fixing\u00a0(Suzgun et\u00a0al., 2022) is also widely used.\n\n\n###### Reading Comprehension\nReading comprehension tasks test a model\u2019s ability to extract and infer information from text. Benchmarks like SQuAD\u00a0(Rajpurkar et\u00a0al., 2018), QuAC\u00a0(Choi et\u00a0al., 2018), and BoolQ\u00a0(Clark et\u00a0al., 2019) challenge models to understand passages and draw logical conclusions.\n", "topic": "Analyzing the impact of typo-fixing benchmarks on the linguistic accuracy of LLMs.", "question": "What is the primary benefit of incorporating typo-fixing benchmarks in the evaluation of LLMs' linguistic accuracy?", "choices": {"A": "Improved sentiment analysis", "B": "Enhanced language inference", "C": "Increased robustness to out-of-vocabulary words", "D": "Better handling of grammatical errors and improved overall language coherence"}, "answer": "D", "explanation": "The correct answer is D, as typo-fixing benchmarks directly assess the model's ability to correct errors and improve the coherence of the generated text. This is a critical aspect of linguistic accuracy, as it reflects the model's ability to generate text that is not only grammatically correct but also readable and understandable.", "question_token_count": 22, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 4, "avg_answer_token_count": 7}
{"context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n", "topic": "The application of multi-agent systems in benchmark creation, including the process of planning, generation, verification, and evaluation, as exemplified by BENCHAGENTS.", "question": "What key benefit does the multi-agent framework, as utilized in BENCHAGENTS, offer to the benchmark creation process, in terms of the resulting benchmarks' characteristics?", "choices": {"A": "Improved Consistency", "B": "Enhanced Scalability and Diversity", "C": "Reduced Human Intervention", "D": "Increased Error Rate"}, "answer": "B", "explanation": "The correct answer, Enhanced Scalability and Diversity, reflects the potential of multi-agent systems to generate benchmarks that are not only scalable but also diverse and of high quality. This is a key benefit highlighted in the context, showcasing the advantage of using coordinated LLM agents in the benchmark creation process.", "question_token_count": 34, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 4}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "The IFEval dataset is designed to simulate real-world scenarios that require clear, step-by-step guidance, and it evaluates a model's ability to comprehend and execute detailed directives.", "question": "What is the primary purpose of the IFEval dataset in evaluating language models?", "choices": {"A": "To assess a model's ability to generate and debug code", "B": "To evaluate a model's ability to comprehend and execute detailed directives", "C": "To test a model's intuitive reasoning skills from multiple perspectives", "D": "To measure a model's ability to integrate background knowledge with logical reasoning"}, "answer": "B", "explanation": "The IFEval dataset is designed to simulate real-world scenarios that require clear, step-by-step guidance, and it evaluates a model's ability to comprehend and execute detailed directives. This is the primary purpose of the IFEval dataset, making option B the correct answer.", "question_token_count": 16, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 9, "question_groundedness_score": 10, "avg_answer_token_count": 11}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "Evaluation benchmark data protection methods to prevent data contamination and ensure reliable performance measurements.", "question": "What method can be employed to prevent syntactic contamination in evaluation benchmark data, ensuring that test data points cannot be transformed into training data points through syntactic transformations?", "choices": {"A": "Data anonymization", "B": "Syntactic normalization of training data", "C": "Lexical substitution in test data", "D": "Application of differential privacy techniques"}, "answer": "B", "explanation": "Syntactic normalization of training data can help prevent syntactic contamination by reducing the variability in the training data, making it more difficult for test data points to be transformed into training data points through syntactic transformations. This approach focuses on the training data itself, rather than the test data, and aims to minimize the overlap between the two datasets.", "question_token_count": 31, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 1, "question_groundedness_score": 2, "avg_answer_token_count": 6}
{"context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n", "topic": "Discussing the ethical implications of using encryption and label protection methods in model evaluation, including issues of data privacy and access control.", "question": "What is the primary risk to model evaluation integrity if the private key used for encrypting test data is exposed?", "choices": {"A": "Model overfitting to training data", "B": "Unauthorized access to test data, potentially leading to data contamination", "C": "Increased computational overhead due to re-encryption needs", "D": "Loss of model interpretability due to encrypted data"}, "answer": "B", "explanation": "The primary risk to model evaluation integrity if the private key is exposed is the potential for unauthorized access to the test data. This could lead to data contamination, where models are trained on or exposed to the test data, thereby compromising the evaluation process. This requires understanding the implications of encryption compromise on data privacy and access control.", "question_token_count": 23, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 10, "avg_answer_token_count": 9}
{"context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n", "topic": "Challenges in measuring complexity and the need for generalizable metrics across different applications.", "question": "What is a key limitation of existing complexity metrics that hinders their effectiveness in evaluating the stability of dynamic benchmarking methods across different applications?", "choices": {"A": "They are too sensitive to minor changes in task complexity.", "B": "They are often domain-specific and do not generalize well across different applications.", "C": "They are too simplistic and fail to account for the nuances of real-world tasks.", "D": "They are too resource-intensive and impractical for large-scale deployment."}, "answer": "B", "explanation": "The correct answer, B, highlights the key limitation of existing complexity metrics, which is their lack of generalizability across different applications. This limitation is explicitly mentioned in the context as a challenge in measuring complexity. The other options, while plausible, do not accurately capture the primary limitation of existing complexity metrics.", "question_token_count": 27, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 14}
