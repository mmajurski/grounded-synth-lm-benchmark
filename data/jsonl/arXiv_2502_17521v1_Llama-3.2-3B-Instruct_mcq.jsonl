{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "Addressing the issue of contamination is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM's true ability to handle novel and unseen data, especially given the rapid pace of LLM development.", "question": "Can syntactic transformations, such as prefixing test data with a string, be considered true data contamination in benchmarking LLMs, and why or why not?", "choices": {"D": "The impact of syntactic transformations on LLMs' generalization is unclear and requires further research to determine their true nature.", "A": "Yes, syntactic transformations can be considered contamination because they rely on memorized information and not the LLM's reasoning capability during inference.", "C": "It depends on the specific context and application, as some NLP tasks may rely more heavily on syntactic information than others.", "B": "No, syntactic transformations do not constitute contamination because they are a natural part of language processing."}, "answer": "A", "explanation": "This question requires the domain expert to think critically about the implications of syntactic transformations on LLMs' ability to generalize and make decisions.", "question_token_count": 31, "answer_correctness_score": 8, "explanation_validity_score": 4, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 23}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "Temporal Cutoff in Benchmarking LLMs", "question": "What is the primary advantage of using a temporal cutoff date when constructing benchmarks for Large Language Models (LLMs)?", "choices": {"C": "The primary advantage of using a temporal cutoff date is that it allows for the exclusion of data that is no longer relevant to the model's knowledge cutoff date, thereby improving the reliability of the benchmark.", "D": "Temporal cutoff dates enable benchmarks to focus on the most recent and relevant data, reducing the risk of contamination and improving the overall quality of the benchmark.", "A": "Using a temporal cutoff date helps to mitigate data contamination by ensuring that the model is only trained on data that is relevant to its knowledge cutoff date.", "B": "By using a temporal cutoff date, benchmarks can avoid incorporating outdated information that may not be relevant to the model's current knowledge state."}, "answer": "C", "explanation": "The correct answer is C, as it accurately reflects the primary advantage of using a temporal cutoff date in benchmarking LLMs. The other options, while related to the concept, do not capture the primary benefit.", "question_token_count": 23, "answer_correctness_score": 9, "explanation_validity_score": 8, "question_clarity_score": 9, "question_groundedness_score": 10, "avg_answer_token_count": 31}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "Instruction Following and Everyday Knowledge", "question": "How do benchmarks that evaluate everyday knowledge, such as PIQA, SIQA, and HellaSwag, impact the generalizability of language models to real-world scenarios, and what implications do these findings have for the development of more effective language models?", "choices": {"C": "This answer is correct because it highlights the significance of integrating background knowledge with logical reasoning to arrive at plausible answers, demonstrating a deeper understanding of the relationship between everyday knowledge and language model performance.", "D": "This answer is incorrect because it downplays the importance of everyday knowledge in evaluating language models, neglecting its impact on the development of more effective models.", "A": "This answer is incorrect because it fails to consider the broader implications of everyday knowledge on language model performance.", "B": "This answer is partially correct, as it acknowledges the importance of everyday knowledge but overlooks its impact on the evaluation of language models."}, "answer": "C", "explanation": "The correct answer (C) recognizes the importance of integrating background knowledge with logical reasoning to arrive at plausible answers, demonstrating a deeper understanding of the relationship between everyday knowledge and language model performance. The other options fail to consider the broader implications of everyday knowledge on language model performance, making them incorrect.", "question_token_count": 51, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 4, "avg_answer_token_count": 29}
{"context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n", "topic": "How do N-gram metrics and reference-based metrics, such as BLEU scores, measure diversity in datasets?", "question": "What is the primary distinction between N-gram metrics and reference-based metrics, such as BLEU scores, when measuring diversity in datasets?", "choices": {"C": "Reference-based metrics, such as BLEU scores, are more suitable for evaluating external diversity due to their ability to quantify variation between datasets.", "A": "BLEU scores provide a more comprehensive understanding of external diversity by considering the entire dataset.", "B": "N-gram metrics offer a more precise measurement of internal diversity by analyzing the differences between individual datasets.", "D": "N-gram metrics are more effective in measuring internal diversity because they focus on the differences between individual datasets."}, "answer": "B", "explanation": "The question requires a nuanced understanding of diversity metrics and their applications. The correct answer, B, highlights the importance of N-gram metrics in measuring internal diversity by analyzing the differences between individual datasets. This distinction is crucial for effective evaluation purposes.", "question_token_count": 27, "answer_correctness_score": 10, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 4, "avg_answer_token_count": 21}
{"context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n", "topic": "Analysis of Rule-Based Generation Approaches", "question": "What type of dynamic benchmarks can be used to evaluate LLM capabilities, and how do they differ from static benchmarks?", "choices": {"C": "LLM-based generation benchmarks leverage the strong generative capabilities of LLMs to create evaluation data points.", "A": "Temporal cutoff benchmarks rely on newly released information to create evaluation data points.", "B": "Rule-based generation benchmarks use predefined rules to generate novel evaluation data points.", "D": "Hybrid benchmarks combine elements of the above approaches."}, "answer": "C", "explanation": "The question requires a deep understanding of dynamic benchmarks and their differences from static benchmarks. The correct answer can be inferred by analyzing the context, which emphasizes the importance of interpretability and the various approaches to dynamic benchmarking.", "question_token_count": 23, "answer_correctness_score": 8, "explanation_validity_score": 7, "question_clarity_score": 6, "question_groundedness_score": 8, "avg_answer_token_count": 15}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "Opportunities for Innovation in LLM Benchmarking", "question": "What are some potential risks associated with using static benchmarks for Large Language Models (LLMs), and how can dynamic benchmarks mitigate these issues?", "choices": {"D": "Insufficient evaluation of model performance", "A": "Overfitting to training data", "B": "Data contamination risks", "C": "Difficulty in adapting to changing model architectures"}, "answer": "B", "explanation": "The correct answer is B) Data contamination risks, as the context mentions the development of contamination detectors to quantify contamination risks. The other options are incorrect because overfitting to training data (A) is a risk associated with LLMs, but not specifically related to benchmarking; difficulty in adapting to changing model architectures (C) is a challenge for LLMs, but not directly related to benchmarking; and insufficient evaluation of model performance (D) is a general issue, but not the specific risk mentioned in the context.", "question_token_count": 28, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 9, "question_groundedness_score": 1, "avg_answer_token_count": 6}
{"context": "###### Limitations\nThe collection process typically requires significant human effort\u00a0(White et\u00a0al., 2024; Jain et\u00a0al., 2024), and continuous updates demand ongoing human involvement. Despite the popularity of temporal cutoffs, using recent information from competitions to evaluate LLMs can still lead to data contamination, as these problems are likely to be reused in future competitions\u00a0(Wu et\u00a0al., 2024). Verification is often overlooked in these live benchmarks\u00a0(White et\u00a0al., 2024).\n\n\n##### 4.3.2 Rule-Based Generation\nThis method synthesizes new test cases based on predefined rules, featuring an extremely low collision probability\u00a0(Zhu et\u00a0al., 2024a).\n\n\n###### Template-Based\nGSM-Symbolic\u00a0(Mirzadeh et\u00a0al., 2025) creates dynamic math benchmarks by using query templates with placeholder variables, which are randomly filled to generate diverse problem instances.\n\nMathador-LM(Kurtic et\u00a0al., 2024) generates evaluation queries by adhering to the rules of Mathador games(Puma et\u00a0al., 2023) and varying input numbers.\n\nMMLU-CF\u00a0(Zhao et\u00a0al., 2024) follows the template of multiple-choice questions and generates novel samples by shuffling answer choices and randomly replacing incorrect options with \"None of the other choices.\"\n", "topic": "How do methods like GSM-Symbolic and Mathador-LM generate dynamic math benchmarks for LLMs?", "question": "What are the key differences between the query template-based approaches of GSM-Symbolic and Mathador-LM in generating dynamic math benchmarks for LLMs?", "choices": {"C": "GSM-Symbolic generates novel samples by shuffling answer choices, whereas Mathador-LM varies input numbers.", "A": "GSM-Symbolic uses query templates with placeholder variables, while Mathador-LM generates evaluation queries based on the rules of Mathador games.", "B": "GSM-Symbolic follows a rule-based approach, whereas Mathador-LM uses a template-based method.", "D": "GSM-Symbolic and Mathador-LM both use multiple-choice questions as a template."}, "answer": "A", "explanation": "The question is designed to probe the understanding of the context and the differences between the query template-based approaches of GSM-Symbolic and Mathador-LM. The correct answer requires a deep understanding of the context and the ability to analyze the information provided.", "question_token_count": 31, "answer_correctness_score": 10, "explanation_validity_score": 8, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 24}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "Mitigating Data Contamination in LLM Evaluation", "question": "What is the primary advantage of using data collected after the LLM's knowledge cutoff date in constructing dataset to evaluate the model while mitigating data contamination?", "choices": {"C": "Reducing the computational resources required for data collection and processing.", "D": "Increasing the model's ability to generalize to new, unseen knowledge domains.", "A": "Eliminating outdated knowledge that may be irrelevant to the model's current capabilities.", "B": "Preventing the introduction of new, potentially contaminated data that may skew the evaluation results."}, "answer": "B", "explanation": "The correct answer, B, is supported by the context, which highlights the importance of using data collected after the LLM's knowledge cutoff date to eliminate potential data contamination. This approach ensures that the evaluation results are more accurate and reliable.", "question_token_count": 29, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 15}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "Impact of Knowledge Cutoff Date on LLM Benchmarking", "question": "What is the primary advantage of using a knowledge cutoff date when constructing benchmarking datasets for Large Language Models (LLMs)?", "choices": {"A": "Eliminates potential data contamination", "B": "Reduces the accuracy of benchmarking results", "C": "Increases the complexity of model evaluation", "D": "Decreases the representativeness of the dataset"}, "answer": "A", "explanation": "The primary advantage of using a knowledge cutoff date is that it eliminates potential data contamination, which can affect the accuracy of benchmarking results. By using data collected after the cutoff date, benchmarkers can ensure that the dataset is representative of the model's performance and not skewed by recent information.", "question_token_count": 24, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 9, "question_groundedness_score": 10, "avg_answer_token_count": 8}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "The potential applications of contamination in LLM research involve the development of robust approaches to identify and prevent contamination, ensuring the accuracy and reliability of evaluations.", "question": "How can the development of robust approaches to identify and prevent syntactic contamination in LLMs ensure the accuracy and reliability of evaluations, and what implications does this have for benchmarking LLMs?", "choices": {"B": "The development of robust approaches to identify and prevent syntactic contamination is crucial for ensuring the accuracy and reliability of evaluations, but it also requires careful consideration of the potential biases and limitations of the methods used.", "A": "This approach can be achieved through the use of techniques such as data augmentation and adversarial training, which can help to identify and prevent syntactic contamination.", "C": "Contamination can be mitigated through the use of domain-adaptive training methods, which can help to adapt the model to the specific task and domain, reducing the risk of contamination.", "D": "The development of robust approaches to identify and prevent syntactic contamination is a critical step in ensuring the accuracy and reliability of evaluations, but it also requires a deeper understanding of the underlying mechanisms and limitations of LLMs."}, "answer": "A", "explanation": "The correct answer is A, as this approach can be achieved through the use of techniques such as data augmentation and adversarial training, which can help to identify and prevent syntactic contamination. The other options are incorrect because they either provide a more general statement (B), focus on domain-adaptive training methods (C), or emphasize the importance of understanding the underlying mechanisms and limitations of LLMs (D).", "question_token_count": 36, "answer_correctness_score": 8, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 8, "avg_answer_token_count": 37}
{"context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n", "topic": "Assessing LLM-Based Generation Approaches", "question": "What is a key mechanism required to ensure the correctness of LLM-assisted transformations, particularly in dynamic benchmarks, and how does it differ from rule-based transformations?", "choices": {"A": "Explainability tools", "B": "Human-in-the-loop validation", "C": "Temporal cutoff approach", "D": "Hybrid approach"}, "answer": "A", "explanation": "This question requires the test-taker to demonstrate an understanding of the importance of interpretability in LLM-assisted transformations, specifically in dynamic benchmarks. The correct answer, explainability tools, is a key mechanism to ensure correctness, whereas rule-based transformations are inherently interpretable. The incorrect options, human-in-the-loop validation, temporal cutoff approach, and hybrid approach, are related to dynamic benchmarks but do not directly address the need for interpretability in LLM-assisted transformations.", "question_token_count": 31, "answer_correctness_score": 10, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 4}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "Evolution of Instruction-Following Tasks and Coding Tasks", "question": "As LLMs continue to evolve, what is a primary challenge faced by traditional benchmarks, and how can dynamic benchmarks mitigate these issues?", "choices": {"C": "Insufficient attention to the model's training data", "D": "Overemphasis on static evaluation metrics", "A": "Inadequate capture of the model's full capabilities", "B": "Increased risk of data contamination"}, "answer": "B", "explanation": "The correct answer is B, as dynamic benchmarks are necessary to address the rapid evolution of LLMs and potential data contamination issues. This question requires the reader to understand the limitations of traditional benchmarks and the importance of adapting to the model's evolving capabilities.", "question_token_count": 27, "answer_correctness_score": 10, "explanation_validity_score": 8, "question_clarity_score": 6, "question_groundedness_score": 4, "avg_answer_token_count": 8}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "Developing robust approaches to identify and prevent contamination is essential for ensuring the validity and reliability of evaluations in LLM research, especially given the rapid pace of LLM development.", "question": "What are some potential challenges and limitations in developing a robust approach to identifying and preventing syntactic contamination in LLM research, and how might these challenges impact the validity and reliability of benchmarks?", "choices": {"C": "The difficulty lies in developing a universal framework for identifying contamination, as different LLMs may respond differently to the same type of contamination.", "B": "One significant limitation is the potential for contamination to occur through subtle changes in wording or phrasing, making it difficult to detect and prevent.", "A": "The challenge lies in distinguishing between LLM's ability to recall memorized information and its reasoning capability during inference.", "D": "One potential challenge is the risk of over-reliance on syntactic information, which may lead to biased or inaccurate results."}, "answer": "A", "explanation": "The correct answer is A, as it highlights the challenge of distinguishing between LLM's ability to recall memorized information and its reasoning capability during inference. This is a critical issue in LLM research, as it can impact the validity and reliability of benchmarks. The other options, while related to contamination, do not directly address the challenge of distinguishing between LLM's ability to recall memorized information and its reasoning capability during inference.", "question_token_count": 36, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 6, "avg_answer_token_count": 25}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "How do canary strings differ from other methods proposed for mitigating data contamination in static benchmark datasets?", "question": "How do canary strings differ from other methods proposed for mitigating data contamination in static benchmark datasets, particularly in terms of their effectiveness and limitations?", "choices": {"C": "Other methods, such as data masking, are more effective in mitigating data contamination than canary strings.", "D": "Canary strings are not a feasible solution for mitigating data contamination in static benchmark datasets.", "A": "Canary strings are effective in identifying data contamination but may not work if a developer aims to leak benchmarking data.", "B": "Canary strings are not effective in mitigating data contamination and may even exacerbate the issue."}, "answer": "A", "explanation": "The question aims to probe the domain expert's understanding of canary strings and their limitations. It requires critical thinking and analysis of the content to distinguish between effective and ineffective methods for mitigating data contamination.", "question_token_count": 29, "answer_correctness_score": 10, "explanation_validity_score": 7, "question_clarity_score": 6, "question_groundedness_score": 4, "avg_answer_token_count": 20}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "Comprehension and Generation in Language Understanding", "question": "How do benchmarks like PIQA, SIQA, and HellaSwag assess a model's ability to apply everyday knowledge and integrate background knowledge with logical reasoning?", "choices": {"C": "By focusing on the integration of background knowledge with logical reasoning to arrive at plausible answers.", "D": "By testing model's ability to generate and debug code.", "A": "By testing model's ability to generate plausible answers in a variety of scenarios.", "B": "By simulating real-world scenarios and evaluating model's ability to follow detailed directives."}, "answer": "C", "explanation": "The correct answer is C, as these benchmarks specifically focus on evaluating a model's ability to integrate background knowledge with logical reasoning to arrive at plausible answers. The other options are incorrect because they describe different types of benchmarks or tasks.", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 14}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "The underlying theories of contamination in LLMs involve the development of robust approaches to identify and prevent contamination, ensuring the accuracy and reliability of evaluations.", "question": "Can contamination in LLMs be considered a form of overfitting, and if so, how can we develop robust approaches to prevent it?", "choices": {"C": "It's unclear whether contamination is a form of overfitting, as the relationship between the two concepts is complex and requires further investigation.", "D": "Contamination can only be prevented through the use of more extensive training datasets, which would help the model generalize better.", "A": "Yes, contamination can be viewed as a form of overfitting, as it involves the model's familiarity with the training data influencing its performance on unseen data.", "B": "No, contamination is distinct from overfitting, as it primarily concerns the model's reliance on memorized information rather than its ability to generalize."}, "answer": "A", "explanation": "The correct answer (A) acknowledges that contamination can be viewed as a form of overfitting, as it involves the model's familiarity with the training data influencing its performance on unseen data. This answer demonstrates an understanding of the theoretical underpinnings of contamination and its implications for LLM evaluations.", "question_token_count": 28, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 4, "avg_answer_token_count": 27}
{"context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n", "topic": "Challenges of Direct Overlap Detection", "question": "What are some of the challenges and limitations associated with post-hoc detection methods for evaluating the performance of deep learning models, particularly in terms of detecting overlap and contamination?", "choices": {"A": "The primary challenge of post-hoc detection methods is the high false negative rate, which can lead to inaccurate evaluation of model performance.", "B": "The main limitation of post-hoc detection methods is the reliance on centralized evaluation systems, which can impede detailed error analysis and reproducibility.", "C": "The primary challenge of post-hoc detection methods is the need for more robust techniques, such as embedding-based similarity and improved mapping metrics, to accurately detect overlap and contamination.", "D": "The main limitation of post-hoc detection methods is the lack of transparency and independent verification, which can lead to biased evaluation of model performance."}, "answer": "C", "explanation": "The correct answer is C. The text highlights the need for more robust techniques, such as embedding-based similarity and improved mapping metrics, to accurately detect overlap and contamination. This is because exact matching often leads to false negatives, and more robust methods are necessary to accurately evaluate model performance. The other options are incorrect because they do not accurately capture the challenges and limitations associated with post-hoc detection methods.", "question_token_count": 33, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 28}
{"context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n", "topic": "What are the key considerations for designing a fair and transparent benchmarking framework for large language models?", "question": "What are the key considerations for designing a fair and transparent benchmarking framework for large language models, particularly in light of potential biases, contamination, and privacy concerns?", "choices": {"C": "Implementing a combination of both static and dynamic benchmarks, requiring careful consideration of evaluation criteria and potential misuse of benchmarking results.", "D": "Relying on benchmarking frameworks that are not transparent about their methodology or data sources, potentially leading to biased or contaminated results.", "A": "Focusing solely on static benchmarks, which may perpetuate biases if not carefully constructed.", "B": "Prioritizing dynamic benchmarks, which may raise privacy and security concerns regarding data collection and updating."}, "answer": "C", "explanation": "The correct answer, C, requires a deep understanding of the context and the ability to identify key considerations for designing a fair and transparent benchmarking framework. The other options are incorrect because they do not fully address the potential biases, contamination, and privacy concerns associated with benchmarking frameworks.", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 9, "question_groundedness_score": 8, "avg_answer_token_count": 22}
{"context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n", "topic": "Categorization of Dynamic Benchmarks", "question": "What are the four types of dynamic benchmarks based on their construction process, and what are the key differences between them?", "choices": {"C": "Temporal cutoff, rule-based generation, and LLM-based generation are the four types of dynamic benchmarks, with the latter being the most reliable.", "D": "Temporal cutoff, rule-based generation, and LLM-based generation are the four types of dynamic benchmarks, with hybrid approaches being the least reliable.", "A": "Temporal cutoff and rule-based generation are the only two types of dynamic benchmarks that can be categorized as either rule-based or LLM-based.", "B": "Temporal cutoff and rule-based generation are the two types of dynamic benchmarks that can be categorized as either LLM-based or hybrid approaches."}, "answer": "D", "explanation": "This question requires the test-taker to understand the categorization of dynamic benchmarks into four types based on their construction process. The correct answer (D) requires the test-taker to recall the four types (temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches) and their key differences. The incorrect answers (A, B, and C) are plausible but incorrect, requiring the test-taker to critically evaluate the options and choose the correct answer.", "question_token_count": 24, "answer_correctness_score": 7, "explanation_validity_score": 6, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 27}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "Strategies for Detection and Prevention", "question": "What is the primary concern with having exact duplicates in both the training and test datasets, according to the formal definition of data contamination?", "choices": {"D": "Syntactic transformations can preserve lexical meaning while contaminating the dataset.", "C": "Documentation leaks are a common source of exact contamination.", "A": "Syntactic contamination is a more significant concern than exact contamination.", "B": "Exact contamination can lead to biased performance measurements."}, "answer": "B", "explanation": "The correct answer requires an understanding of the formal definition of data contamination and the distinction between exact and syntactic contamination. The question aims to test the test-taker's ability to identify the primary concern with having exact duplicates in both the training and test datasets.", "question_token_count": 27, "answer_correctness_score": 10, "explanation_validity_score": 7, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 12}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "Developing robust approaches to identify and prevent contamination is essential for ensuring the validity and reliability of evaluations in LLM research, especially given the rapid pace of LLM development.", "question": "How can a robust approach to identifying and preventing data contamination in benchmarking LLMs ensure the validity and reliability of evaluations, especially in the face of rapid LLM development?", "choices": {"A": "Implementing a hybrid approach that combines manual curation with automated methods to detect contamination.", "B": "Developing a framework that prioritizes syntactic information for decision-making in NLP applications.", "C": "Focusing on the need for a robust approach to identifying and preventing contamination, given the rapid pace of LLM development.", "D": "Emphasizing the importance of distinguishing between an LLM's ability to recall memorized information and its reasoning capability during inference."}, "answer": "C", "explanation": "This question requires the synthesis of high-level general understanding above and beyond the specific context, as it demands an understanding of the concept of data contamination in benchmarking LLMs and the need for a robust approach to mitigating its effects.", "question_token_count": 33, "answer_correctness_score": 7, "explanation_validity_score": 6, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 20}
{"context": "###### Interactive Evaluation\nIn this category, inspired by the human interview process, LLMs are evaluated through multi-round interactions with an LLM \u00a0(Li et\u00a0al., 2023b).\n\nLLM-as-an-Interviewer\u00a0(Kim et\u00a0al., 2024) employs an interviewer LLM that first paraphrases queries from existing static benchmarks and then conducts a multi-turn evaluation by posing follow-up questions or providing feedback on the examined LLM\u2019s responses.\n\nTreeEval\u00a0(Li et\u00a0al., 2024b) begins by generating an initial question on a given topic using an LLM. Based on the previous topic and the examined LLM\u2019s response, it then generates follow-up subtopics and corresponding questions to further assess the model.\n\nKIEval\u00a0(Yu et\u00a0al., 2024) generates follow-up questions based on the evaluated model\u2019s response to an initial question from a static benchmark.\n\n\n###### Multi-Agent Evaluation\nInspired by the recent success of multi-agents systems\u00a0(Guo et\u00a0al., 2024), multi-agent collaborations are used to construct dynamic benchmarks.\n\nBenchmark Self-Evolving\u00a0(Wang et\u00a0al., 2024a) employs a multi-agent framework to dynamically extend existing static benchmarks, showcasing the potential of agent-based methods.\n\nGiven a task description, BENCHAGENTS\u00a0(Butt et\u00a0al., 2024) leverages a multi-agent framework for automated benchmark creation.\n\nIt splits the process into planning, generation, verification, and evaluation\u2014each handled by a specialized LLM agent.\n\nThis coordinated approach, with human-in-the-loop feedback, yields scalable, diverse, and high-quality benchmarks.\n", "topic": "Human-in-the-Loop Evaluation for Assessing AI Model Performance", "question": "What are the primary benefits and limitations of using human-in-the-loop evaluation in assessing AI model performance, and how do these factors impact the development of high-quality benchmarks?", "choices": {"D": "Human-in-the-loop evaluation is only suitable for specific domains and tasks, and its benefits are limited to these areas.", "A": "Human-in-the-loop evaluation provides more accurate and nuanced assessments of AI model performance but requires significant human expertise and resources.", "C": "Human-in-the-loop evaluation can be performed efficiently and cost-effectively using automated methods, but it may lack nuance and context-dependent insights.", "B": "Human-in-the-loop evaluation can be time-consuming and expensive, but it offers more realistic and context-dependent evaluations of AI models."}, "answer": "A", "explanation": "The correct answer (A) highlights the primary benefits of human-in-the-loop evaluation, including more accurate and nuanced assessments of AI model performance. However, it also notes the limitations of this approach, such as requiring significant human expertise and resources.", "question_token_count": 34, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 6, "avg_answer_token_count": 25}
{"context": "###### Safety\nSafety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content. Datasets such as RealToxicityPrompts\u00a0(Gehman et\u00a0al., 2020) and ToxiGen\u00a0(Hartvigsen et\u00a0al., 2022) assess resilience against producing harmful outputs.\n\nBy providing a controlled environment to measure these aspects, safety benchmarks play a critical role in guiding the development of models that are not only powerful outputs but also responsible and trustworthy for real-world applications.\n\n\n###### Language\nLanguage benchmarks assess the LLMs\u2019 proficiency in specific languages.\n\nGLUE\u00a0(Wang, 2018) and SuperGLUE\u00a0(Wang et\u00a0al., 2019) cover tasks from sentiment analysis to language inference, while CLUE\u00a0(Xu et\u00a0al., 2020) targets Chinese language.\n\nTypo-fixing\u00a0(Suzgun et\u00a0al., 2022) is also widely used.\n\n\n###### Reading Comprehension\nReading comprehension tasks test a model\u2019s ability to extract and infer information from text. Benchmarks like SQuAD\u00a0(Rajpurkar et\u00a0al., 2018), QuAC\u00a0(Choi et\u00a0al., 2018), and BoolQ\u00a0(Clark et\u00a0al., 2019) challenge models to understand passages and draw logical conclusions.\n", "topic": "What are the implications of using benchmarks like CLUE and BoolQ for evaluating LLMs in Chinese and general language understanding?", "question": "How do benchmarks like CLUE and BoolQ influence the evaluation of LLMs in Chinese language understanding, and what are the potential limitations of relying solely on these benchmarks?", "choices": {"A": "The use of benchmarks like CLUE and BoolQ provides a controlled environment to assess the robustness and proficiency of LLMs in Chinese language understanding, but it may not fully capture the complexities of real-world language use.", "B": "CLUE and BoolQ benchmarks are limited in their ability to evaluate the contextual understanding of LLMs, as they primarily focus on task-specific language understanding.", "C": "The reliance on benchmarks like CLUE and BoolQ may lead to an overemphasis on task-specific language understanding, potentially overlooking the importance of common sense and real-world knowledge.", "D": "CLUE and BoolQ benchmarks are insufficient for evaluating the ability of LLMs to generate coherent and contextually relevant text in Chinese."}, "answer": "A", "explanation": "The correct answer is A, as the use of benchmarks like CLUE and BoolQ provides a controlled environment to assess the robustness and proficiency of LLMs in Chinese language understanding. However, relying solely on these benchmarks may have limitations, such as neglecting the complexities of real-world language use.", "question_token_count": 34, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 6, "avg_answer_token_count": 34}
{"context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n", "topic": "Scalability Equation", "question": "What is the primary implication of the equation Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) = \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 / \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 regarding the scalability of dynamic benchmarking methods?", "choices": {"C": "The equation indicates that the scalability of a transformation process is determined by the size of the original dataset.", "A": "The equation suggests that the cost of transformation is directly proportional to the size of the original dataset.", "B": "The equation implies that the size of the transformed dataset is directly proportional to the cost of transformation.", "D": "The equation suggests that the transformation process should be able to handle large datasets efficiently in order to minimize costs."}, "answer": "A", "explanation": "The correct answer is A, as the equation suggests that the cost of transformation is directly proportional to the size of the original dataset. This implies that as the size of the dataset increases, the cost of transformation also increases.", "question_token_count": 120, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 20}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "Addressing the issue of contamination is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM's true ability to handle novel and unseen data, especially given the rapid pace of LLM development.", "question": "What is the primary concern about contamination in benchmarking Large Language Models (LLMs), and why is it essential to address this issue?", "choices": {"C": "Difficulty in distinguishing between memorized information and reasoning capabilities during inference.", "A": "Overestimation of a model's true capabilities due to exposure to training data.", "B": "Undermining the validity of benchmarks and leading to misleading conclusions about progress in LLM research.", "D": "Limited generalization to real-world scenarios due to contamination."}, "answer": "B", "explanation": "B (Undermining the validity of benchmarks and leading to misleading conclusions about progress in LLM research.)", "question_token_count": 27, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 14}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "Evaluating the Effectiveness of Math Benchmark Datasets in Assessing Model Performance", "question": "Can math benchmark datasets like GSM8K and MATH be used to evaluate a model's ability to generalize to new math problems, or do they only provide a snapshot of the model's performance on a specific set of problems?", "choices": {"C": "The relationship between math benchmark datasets and model generalization is complex, and more research is needed to fully understand the implications of using these datasets.", "A": "The use of math benchmark datasets is limited to assessing a model's performance on specific math problems and may not generalize to new, unseen problems.", "B": "Math benchmark datasets can be used to evaluate a model's ability to generalize to new math problems, as they provide a diverse set of problems that test the model's ability to apply mathematical concepts.", "D": "Math benchmark datasets are not suitable for evaluating model performance, as they are often biased towards specific math problems and do not provide a comprehensive view of the model's abilities."}, "answer": "B", "explanation": "Answer B is correct because math benchmark datasets like GSM8K and MATH provide a diverse set of problems that test the model's ability to apply mathematical concepts and generalize to new problems. While there may be limitations to using these datasets, they can be used to evaluate a model's performance on a specific set of problems and provide insights into its ability to generalize.", "question_token_count": 43, "answer_correctness_score": 8, "explanation_validity_score": 8, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 31}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "Understanding the nature and significance of contamination in LLMs is essential for developing robust approaches to identify and prevent contamination, ensuring the accuracy and reliability of evaluations.", "question": "Can syntactic contamination in LLMs be considered a true data contamination, or does it represent a LLM's ability to recall memorized information?", "choices": {"B": "No, syntactic contamination does not constitute true data contamination, as it is a reflection of the LLM's ability to recall memorized information.", "C": "It depends on the specific application, as some NLP applications rely primarily on syntactic information for decision-making.", "A": "Yes, syntactic contamination can be considered a true data contamination, as it involves rephrasing the training data with the addition of a prefix string.", "D": "Syntactic contamination is a minor issue that can be ignored, as it does not significantly impact the overall performance of LLMs."}, "answer": "B", "explanation": "The correct answer is B, as syntactic contamination does not constitute true data contamination. The authors consider such transformations as contamination, but it is challenging to distinguish between the LLM's ability to recall memorized information and its reasoning capability during inference.", "question_token_count": 28, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 26}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "LLM Benchmarking Challenges and Opportunities", "question": "What are the primary challenges and limitations of using static benchmarks for LLMs, and how can dynamic benchmarks address these issues?", "choices": {"C": "Static benchmarks introduce data contamination issues and do not account for LLM evolution.", "D": "Dynamic benchmarks are only suitable for small-scale evaluations.", "A": "Static benchmarks become too easy for stronger LLMs over time.", "B": "Dynamic benchmarks can only be used for a limited number of tasks."}, "answer": "C", "explanation": "The correct answer, C, is supported by the context, which mentions that static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data, introducing data contamination issues. The other options are incorrect because they do not accurately reflect the challenges and limitations of static benchmarks and the benefits of dynamic benchmarks.", "question_token_count": 25, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 10, "avg_answer_token_count": 13}
{"context": "###### Limitations\nThe pre-defined rules may limit sample diversity, and publicly available rule-generated data may increase the risk of in-distribution contamination during training\u00a0(Tu et\u00a0al., 2024).\n\n\n###### Benchmark Rewriting\nIn this category, LLMs are employed to rewrite samples from existing static benchmarks, which may be contaminated.\n\nAuto-Dataset\u00a0(Ying et\u00a0al., 2024) prompts LLMs to generate two types of new samples: one that retains the stylistics and essential knowledge of the original, and another that presents related questions at different cognitive levels\u00a0(Bloom et\u00a0al., 1956).\n\nStructEval\u00a0(Cao et\u00a0al., 2024) expands on examined concepts from the original benchmark by using LLMs and knowledge graphs to develop a series of extended questions.\n\nITD\u00a0(Zhu et\u00a0al., 2024c) utilizes a contamination detector\u00a0(Shi et\u00a0al., 2024) to identify contaminated samples in static benchmarks and then prompts an LLM to rewrite them while preserving their difficulty levels.\n\nVarBench\u00a0(Qian et\u00a0al., 2024) prompts LLMs to identify and replace variables in samples from existing benchmarks, generating new samples.\n", "topic": "Methods for Generating Diversified Samples (Auto-Dataset, StructEval, ITD, VarBench)", "question": "Which method is most likely to address the issue of in-distribution contamination in static benchmarks during training?", "choices": {"A": "Auto-Dataset", "B": "StructEval", "C": "ITD", "D": "VarBench"}, "answer": "C", "explanation": "The correct answer is ITD, as it utilizes a contamination detector to identify contaminated samples and then prompts an LLM to rewrite them while preserving their difficulty levels.", "question_token_count": 20, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 3}
{"context": "### Abstract\nData contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking.\n\nIn this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap\u2014the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks.\n\nThis survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts.\n\nWe maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link\u00a0111GitHub Link: Static-to-Dynamic LLM Benchmarking.\n", "topic": "The role of LLMs in data contamination and the need for continuous research and development in this area.", "question": "What are the primary limitations of existing static benchmarking methods for LLMs, and how do they differ from dynamic benchmarking methods in reducing data contamination risks?", "choices": {"B": "Dynamic benchmarks are more effective in reducing data contamination risks, but they are also more prone to errors due to the complexity of real-world data.", "A": "Static benchmarks are limited in their ability to capture the nuances of real-world data and are often biased towards certain datasets.", "C": "The limitations of static benchmarks are largely due to the lack of standardized criteria for evaluating them, whereas dynamic benchmarks are not yet widely adopted.", "D": "Static benchmarks are more suitable for small-scale LLMs, whereas dynamic benchmarks are more effective for large-scale LLMs."}, "answer": "C", "explanation": "The correct answer is C, as the limitations of static benchmarks are largely due to the lack of standardized criteria for evaluating them, whereas dynamic benchmarks are not yet widely adopted. This question requires a deep understanding of LLMs, data contamination, and benchmarking methods.", "question_token_count": 30, "answer_correctness_score": 4, "explanation_validity_score": 2, "question_clarity_score": 6, "question_groundedness_score": 4, "avg_answer_token_count": 26}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "How does the DyVal framework construct DAGs and transform them into natural language descriptions, and what are the implications for evaluating LLMs' reasoning capabilities?", "question": "How does the DyVal framework construct DAGs and transform them into natural language descriptions, and what are the implications for evaluating LLMs' reasoning capabilities?", "choices": {"C": "The DyVal framework uses a graph-based approach to construct DAGs and transforms them into natural language descriptions using a graph-to-text model.", "A": "The DyVal framework constructs DAGs using a rule-based process and transforms them into natural language descriptions using a similar process.", "B": "The DyVal framework uses a machine learning approach to construct DAGs and transforms them into natural language descriptions using a semantic role labeling approach.", "D": "The DyVal framework uses a logic-based approach to construct DAGs and transforms them into natural language descriptions using a formal proof-based approach."}, "answer": "A", "explanation": "The correct answer is A, as the DyVal framework constructs DAGs using a rule-based process and transforms them into natural language descriptions using a similar process. The other options are incorrect, as they describe different approaches that are not supported by the context.", "question_token_count": 31, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 27}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The Future of LLM Benchmarking", "question": "What are some potential risks associated with using static benchmarks for Large Language Models (LLMs), and how can dynamic benchmarks mitigate these issues?", "choices": {"C": "Difficulty in adapting to changing model architecture", "A": "Overfitting to training data", "B": "Data contamination issues", "D": "Inability to evaluate model performance on unseen data"}, "answer": "B", "explanation": "The correct answer is B: Data contamination issues. The context explicitly mentions that unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues. Dynamic benchmarks, on the other hand, can adapt to the evolving nature of LLMs and mitigate these issues.", "question_token_count": 28, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 9, "avg_answer_token_count": 7}
{"context": "##### 4.3.1 Temporal Cutoff\nSince LLMs typically have a knowledge cutoff date, using data collected after this cutoff to construct dataset can help evaluate the model while mitigating data contamination.\n\nThis approach has been widely adopted to construct reliable benchmarks that prevent contamination.\n\nLiveBench\u00a0(White et\u00a0al., 2024) collects questions based on the latest information source, e.g., math competitions from the past 12 months, with new questions added and updated every few months.\n\nAntiLeak-Bench\u00a0(Wu et\u00a0al., 2024) generates queries about newly emerged knowledge that was unknown before the model\u2019s knowledge cutoff date to eliminate potential data contamination.\n\nAcademicEval\u00a0(Zhang et\u00a0al., 2024a) designs academic writing tasks on latest arXiv papers.\n\nLiveCodeBench\u00a0(Jain et\u00a0al., 2024) continuously collects new human-written coding problems from online coding competition platforms like LeetCode.\n\nLiveAoPSBench\u00a0(Mahdavi et\u00a0al., 2025) collects live math problems from the Art of Problem Solving forum.\n\nForecastbench\u00a0(Karger et\u00a0al., 2024) updates new forecasting questions on a daily basis from different data sources, e.g., prediction markets.\n", "topic": "Potential Applications of These Benchmarks in Improving LLM Performance", "question": "Can the use of temporal cutoffs and live datasets effectively mitigate data contamination and improve the reliability of benchmarks for evaluating LLM performance?", "choices": {"A": "Yes, but only if the temporal cutoff is set to a specific date, e.g., the model's knowledge cutoff date.", "B": "No, as data contamination can still occur even with temporal cutoffs and live datasets.", "C": "It depends on the specific dataset and the task being evaluated.", "D": "The use of temporal cutoffs and live datasets is not a viable solution for evaluating LLM performance."}, "answer": "A", "explanation": "This answer requires the test-taker to understand the concept of temporal cutoffs and live datasets in the context of benchmarking LLMs. The correct answer, A, acknowledges the potential benefits of using temporal cutoffs and live datasets in mitigating data contamination. The incorrect answers, B, C, and D, do not accurately reflect the potential benefits of these approaches.", "question_token_count": 26, "answer_correctness_score": 9, "explanation_validity_score": 8, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 19}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "The impact of contamination on benchmarking LLMs can lead to misleading conclusions about progress in LLM research, requiring careful consideration to ensure the accuracy and reliability of evaluations.", "question": "What are the primary concerns and challenges associated with contamination in benchmarking LLMs, and how can they be mitigated to ensure the accuracy and reliability of evaluations?", "choices": {"C": "Contamination can undermine the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability.", "D": "The risk of contamination is highest in NLP applications that rely primarily on syntactic information for decision-making.", "A": "Distinguishing between memorized information and reasoning capability during inference is crucial in identifying contamination.", "B": "The primary concern is overestimating a model's true capabilities, leading to misleading conclusions about progress in LLM research."}, "answer": "B", "explanation": "The correct answer requires an understanding of the context's nuances and implications. Option B is the most accurate, as it highlights the primary concern of overestimating a model's true capabilities, which can lead to misleading conclusions about progress in LLM research.", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 6, "avg_answer_token_count": 21}
{"context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n", "topic": "Label Protection in Benchmarking", "question": "How can robust encryption methods ensure the confidentiality of test data and model parameters, while also addressing the limitation of strong key management and computational overheads?", "choices": {"A": "By using public-key cryptography and a \"No Derivatives\" license, which block automated crawling and reuse, but may introduce computational overheads.", "B": "By leveraging confidential computing and secure multi-party computation, which enable private benchmarking, but may compromise encryption if the private key is exposed.", "C": "By keeping test labels hidden from public access, which maintains evaluation integrity, but may allow models to learn or memorize them during training.", "D": "By using a combination of public-key cryptography and secure multi-party computation, which balances security and computational efficiency."}, "answer": "D", "explanation": "The correct answer is D. A combination of public-key cryptography and secure multi-party computation can ensure the confidentiality of test data and model parameters while addressing the limitation of strong key management and computational overheads. This approach balances security and computational efficiency, making it a robust solution for private benchmarking.", "question_token_count": 30, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 26}
{"context": "###### Safety\nSafety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content. Datasets such as RealToxicityPrompts\u00a0(Gehman et\u00a0al., 2020) and ToxiGen\u00a0(Hartvigsen et\u00a0al., 2022) assess resilience against producing harmful outputs.\n\nBy providing a controlled environment to measure these aspects, safety benchmarks play a critical role in guiding the development of models that are not only powerful outputs but also responsible and trustworthy for real-world applications.\n\n\n###### Language\nLanguage benchmarks assess the LLMs\u2019 proficiency in specific languages.\n\nGLUE\u00a0(Wang, 2018) and SuperGLUE\u00a0(Wang et\u00a0al., 2019) cover tasks from sentiment analysis to language inference, while CLUE\u00a0(Xu et\u00a0al., 2020) targets Chinese language.\n\nTypo-fixing\u00a0(Suzgun et\u00a0al., 2022) is also widely used.\n\n\n###### Reading Comprehension\nReading comprehension tasks test a model\u2019s ability to extract and infer information from text. Benchmarks like SQuAD\u00a0(Rajpurkar et\u00a0al., 2018), QuAC\u00a0(Choi et\u00a0al., 2018), and BoolQ\u00a0(Clark et\u00a0al., 2019) challenge models to understand passages and draw logical conclusions.\n", "topic": "What are some essential safety benchmarks for evaluating the robustness of LLMs in generating non-toxic and ethically aligned content?", "question": "What are some essential safety benchmarks for evaluating the robustness of LLMs in generating non-toxic and ethically aligned content?", "choices": {"C": "CLUE and typo-fixing tasks", "D": "SQuAD and QuAC", "A": "RealToxicityPrompts and ToxiGen", "B": "GLUE and SuperGLUE"}, "answer": "A", "explanation": "The correct answer requires an understanding of the importance of safety benchmarks in evaluating LLMs' robustness and their role in generating ethically aligned content. RealToxicityPrompts and ToxiGen are examples of benchmarks that assess the model's ability to produce non-toxic outputs, while GLUE, SuperGLUE, CLUE, and typo-fixing tasks evaluate the model's proficiency in specific languages and reading comprehension tasks.", "question_token_count": 25, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 9, "question_groundedness_score": 10, "avg_answer_token_count": 8}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "Developing robust approaches to identify and prevent contamination is essential for ensuring the validity and reliability of evaluations in LLM research, especially given the rapid pace of LLM development.", "question": "What are the potential consequences of syntactic contamination in benchmarking LLMs, and how can robust approaches to identifying and preventing contamination ensure the validity and reliability of evaluations?", "choices": {"C": "Contamination in benchmarking LLMs can be mitigated by developing models that are specifically designed to handle novel and unseen data, thereby reducing the impact of syntactic transformations.", "D": "The significance of contamination in benchmarking LLMs lies in its potential to influence model comparisons, deployment decisions, and policy-making, highlighting the need for robust approaches to identifying and preventing contamination.", "A": "Syntactic contamination can lead to overestimation of a model's true capabilities, undermining the validity of benchmarks and making it difficult to assess generalization, robustness, and real-world applicability.", "B": "Syntactic contamination can be addressed through the use of preprocessing techniques, such as removing prefixes or suffixes, to ensure that the test data is not contaminated with memorized information."}, "answer": "D", "explanation": "The correct answer is D, as contamination in benchmarking LLMs can have significant consequences for model comparisons, deployment decisions, and policy-making, emphasizing the need for robust approaches to identifying and preventing contamination.", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 6, "avg_answer_token_count": 36}
{"context": "###### Coding\nCoding benchmarks measure a model\u2019s ability to generate and debug code. HumanEval\u00a0(Chen et\u00a0al., 2021) and MBPP\u00a0(Austin et\u00a0al., 2021) test code synthesis and debugging, whereas SWE-Bench\u00a0(Jimenez et\u00a0al., 2024; Yang et\u00a0al., 2025) addresses more advanced challenges. Competitive platforms like Codeforces\u00a0(Codeforces, 2025) and datasets such as Aider\u00a0(Aider, 2025) further probe dynamic problem solving.\n\n\n###### Instruction Following\nInstruction benchmarks evaluate a model\u2019s ability to comprehend and execute detailed directives. Datasets like IFEval\u00a0(Zhou et\u00a0al., 2023) and InfoBench\u00a0(Qin et\u00a0al., 2024) simulate real-world scenarios requiring clear, step-by-step guidance, with C-Eval\u00a0(Huang et\u00a0al., 2024) focusing on Chinese instructions.\n\n\n###### Reasoning\nUnderstanding and applying everyday knowledge is a key aspect of language comprehension. Benchmarks such as PIQA\u00a0(Bisk et\u00a0al., 2020), SIQA\u00a0(Sap et\u00a0al., 2019), HellaSwag\u00a0(Zellers et\u00a0al., 2019), and WinoGrande\u00a0(Sakaguchi et\u00a0al., 2021) are designed to assess a model\u2019s intuitive reasoning skills from multiple perspectives. In addition, academic challenge sets like ARC\u00a0(Clark et\u00a0al., 2018), OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), and CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) push models further by requiring the integration of background knowledge with logical reasoning to arrive at plausible answers. C-SimpleQA\u00a0(He et\u00a0al., 2024) evaluates the factuality ability of language models to answer short questions in Chinese.\n", "topic": "Assessing Cognitive Architectures for Human-like Reasoning", "question": "Can you design a benchmark dataset that combines elements of code generation, instruction following, and everyday knowledge understanding to evaluate a model's ability to reason and apply background knowledge in a dynamic problem-solving context?", "choices": {"A": "Implementing a dataset that incorporates both explicit and implicit instructions, along with a set of everyday scenarios, could help assess a model's capacity to integrate context-specific knowledge with logical reasoning.", "B": "A benchmark that focuses solely on code generation and debugging might not fully capture a model's ability to apply everyday knowledge and reasoning in complex scenarios.", "C": "Utilizing a dataset that consists of a mix of factual and narrative questions, along with a set of real-world problem-solving challenges, could provide a more comprehensive evaluation of a model's reasoning abilities.", "D": "Developing a benchmark that relies solely on explicit instructions and lacks real-world context might not adequately assess a model's capacity to apply everyday knowledge and reasoning in dynamic problem-solving situations."}, "answer": "C", "explanation": "The correct answer (C) requires the integration of background knowledge with logical reasoning, which is a key aspect of human-like reasoning. The other options, while potentially useful, do not fully capture the complexity of the task.", "question_token_count": 39, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 2, "question_groundedness_score": 5, "avg_answer_token_count": 34}
{"context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n", "topic": "Formulating a Stability Metric for Dynamic Benchmarking", "question": "How can the proposed stability metric in Equation (1) be used to quantify the impact of complexity on performance in dynamic benchmarking of LLMs?", "choices": {"C": "By comparing the complexity of the transformed dataset to the complexity of the original dataset, highlighting changes in complexity over time.", "A": "By directly measuring the variance in complexity across trials, indicating the degree of instability in the benchmarking method.", "B": "By applying the complexity measurement function to the seed dataset, providing a generalizable metric for evaluating the complexity of the benchmark.", "D": "By using the stability metric to identify potential data contamination in the benchmark dataset, and adjusting for its impact on performance."}, "answer": "A", "explanation": "The correct answer is A, as the stability metric is defined as the variance in complexity across different trials, indicating the degree of instability in the benchmarking method. The other options, while related to complexity and stability, are not directly addressing the question of how the proposed stability metric can be used to quantify the impact of complexity on performance.", "question_token_count": 29, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 4, "avg_answer_token_count": 23}
{"context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n", "topic": "Scoring Function", "question": "What is the primary evaluation criterion for assessing the quality of dynamic benchmarking algorithms, and how is it quantified using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 )?", "choices": {"A": "The correctness of the generated dataset", "B": "The alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values", "C": "The domain-specific annotator used to generate the ground truth", "D": "The expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values"}, "answer": "D", "explanation": "The primary evaluation criterion is the alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values. However, the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) is not explicitly stated in the context. Answer A is incorrect because it only mentions the correctness of the generated dataset without considering the scoring function. Answer C is also incorrect because the domain-specific annotator is not the primary evaluation criterion. Answer D is the correct answer as it accurately describes the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values.", "question_token_count": 48, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 12}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "Developing robust approaches to identify and prevent contamination is essential for ensuring the validity and reliability of evaluations in LLM research, especially given the rapid pace of LLM development.", "question": "Can syntactic transformations, such as rephrasing test data with the addition of a prefix string, be considered true data contamination in LLM research, and why or why not?", "choices": {"A": "Yes, syntactic transformations can be considered true data contamination because they can be used to test an LLM's ability to recall memorized information, rather than its reasoning capability during inference.", "B": "No, syntactic transformations cannot be considered true data contamination because they do not provide novel and unseen data, which is essential for evaluating an LLM's robustness and generalization.", "C": "It depends on the specific context and application, as syntactic transformations may be considered contamination in some cases but not others.", "D": "The distinction between syntactic transformations and true data contamination is unclear, and more research is needed to develop a comprehensive understanding of this issue."}, "answer": "A", "explanation": "The correct answer is A, syntactic transformations can be considered true data contamination because they can be used to test an LLM's ability to recall memorized information, rather than its reasoning capability during inference. This is because syntactic transformations may be used to rephrase test data with the addition of a prefix string, which can be seen as a form of memorization rather than true reasoning.", "question_token_count": 36, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 30}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "The debate about whether syntactic transformations constitute true data contamination in LLMs highlights the need for a robust approach to identify and prevent contamination, ensuring the accuracy and reliability of evaluations.", "question": "Can syntactic transformations be considered true data contamination in LLMs, and what are the implications of such contamination for evaluations?", "choices": {"C": "Syntactic transformations may be a form of data contamination, but their impact on evaluations is unclear and requires further research.", "D": "Syntactic transformations are not relevant to the accuracy and reliability of evaluations in LLMs.", "B": "No, syntactic transformations are not data contamination, as they are a natural consequence of LLMs' ability to recall memorized information.", "A": "Yes, syntactic transformations can be considered true data contamination, as they may lead to an LLM's ability to recall memorized information being misattributed to its reasoning capability during inference."}, "answer": "A", "explanation": "The correct answer is A, as syntactic transformations can lead to an LLM's ability to recall memorized information being misattributed to its reasoning capability during inference. This can result in misleading conclusions about a model's capabilities and undermine the validity of benchmarks.", "question_token_count": 24, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 9, "question_groundedness_score": 10, "avg_answer_token_count": 27}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "Understanding the difference between LLMs' ability to recall memorized information and their reasoning capability during inference is essential for developing robust approaches to identify and prevent contamination.", "question": "Can syntactic transformations be considered as data contamination, and how can we distinguish between an LLM's ability to recall memorized information and its reasoning capability during inference in the context of benchmarking LLMs?", "choices": {"A": "Yes, syntactic transformations can be considered as contamination, as they can be used to test an LLM's ability to recall memorized information.", "B": "No, syntactic transformations do not constitute contamination, as they are a natural part of language processing and do not necessarily imply that the LLM has seen the data before.", "C": "It depends on the context in which the syntactic transformation is used, as it may be considered contamination in some cases but not in others.", "D": "The distinction between contamination and not contamination is not relevant, as both can be addressed through more robust benchmarking methods."}, "answer": "C", "explanation": "The correct answer is C, as it acknowledges the complexity of the issue and the need for context-dependent evaluation. The other options oversimplify the problem or provide incorrect answers.", "question_token_count": 39, "answer_correctness_score": 9, "explanation_validity_score": 8, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 28}
{"context": "###### Safety\nSafety benchmarks are essential for evaluating the robustness of LLM\u2019s ability to generate non-toxic and ethically aligned content. Datasets such as RealToxicityPrompts\u00a0(Gehman et\u00a0al., 2020) and ToxiGen\u00a0(Hartvigsen et\u00a0al., 2022) assess resilience against producing harmful outputs.\n\nBy providing a controlled environment to measure these aspects, safety benchmarks play a critical role in guiding the development of models that are not only powerful outputs but also responsible and trustworthy for real-world applications.\n\n\n###### Language\nLanguage benchmarks assess the LLMs\u2019 proficiency in specific languages.\n\nGLUE\u00a0(Wang, 2018) and SuperGLUE\u00a0(Wang et\u00a0al., 2019) cover tasks from sentiment analysis to language inference, while CLUE\u00a0(Xu et\u00a0al., 2020) targets Chinese language.\n\nTypo-fixing\u00a0(Suzgun et\u00a0al., 2022) is also widely used.\n\n\n###### Reading Comprehension\nReading comprehension tasks test a model\u2019s ability to extract and infer information from text. Benchmarks like SQuAD\u00a0(Rajpurkar et\u00a0al., 2018), QuAC\u00a0(Choi et\u00a0al., 2018), and BoolQ\u00a0(Clark et\u00a0al., 2019) challenge models to understand passages and draw logical conclusions.\n", "topic": "What are some key differences between language benchmarks like GLUE and SuperGLUE, and reading comprehension tasks like SQuAD and QuAC?", "question": "What are the key differences between the tasks and evaluation metrics of SuperGLUE and SQuAD, and how do these differences reflect the unique characteristics of each benchmark?", "choices": {"A": "SuperGLUE focuses on tasks like question-answering and text classification, whereas SQuAD emphasizes reading comprehension and information extraction.", "B": "SQuAD is a more general reading comprehension benchmark, whereas SuperGLUE is a specialized benchmark that covers a range of tasks.", "C": "The evaluation metrics for SuperGLUE and SQuAD differ, with SuperGLUE using metrics like accuracy and F1-score, and SQuAD using metrics like ROUGE and METEOR.", "D": "The tasks and evaluation metrics of SuperGLUE and SQuAD are identical, reflecting the similarity between these two benchmarks."}, "answer": "A", "explanation": "The correct answer (A) highlights the key differences between SuperGLUE and SQuAD, with SuperGLUE focusing on tasks like question-answering and text classification, and SQuAD emphasizing reading comprehension and information extraction. The incorrect answers (B, C, and D) do not accurately reflect the differences between these benchmarks.", "question_token_count": 34, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 9, "question_groundedness_score": 6, "avg_answer_token_count": 30}
{"context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n", "topic": "Proportion of Data Generated per Unit Cost", "question": "What is the primary implication of the equation Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) on the scalability of a dynamic benchmark, in terms of the proportion of data generated per unit cost?", "choices": {"D": "The equation shows that the scalability of a dynamic benchmark is independent of the proportion of data generated per unit cost.", "A": "The equation suggests that the scalability of a dynamic benchmark is directly proportional to the size of the original dataset.", "B": "The equation implies that the cost of generating a larger dataset is directly related to the size of the transformed dataset.", "C": "The equation indicates that the proportion of data generated per unit cost is inversely related to the scalability of the dynamic benchmark."}, "answer": "B", "explanation": "The correct answer requires an understanding of the relationship between the size of the transformed dataset and the size of the original dataset, as well as the implications of the equation on the scalability of the dynamic benchmark. The equation suggests that the scalability of a dynamic benchmark is directly related to the size of the transformed dataset, which means that a larger dataset can be generated per unit cost.", "question_token_count": 48, "answer_correctness_score": 7, "explanation_validity_score": 6, "question_clarity_score": 5, "question_groundedness_score": 9, "avg_answer_token_count": 23}
{"context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n", "topic": "How can the use of different metrics, such as N-gram metrics and BLEU scores, affect the evaluation of diversity in datasets?", "question": "How do the different characteristics of N-gram metrics and BLEU scores impact the evaluation of external and internal diversity in datasets, and what are the implications of this for diversity metrics in general?", "choices": {"C": "The choice of metric between N-gram metrics and BLEU scores depends on the specific characteristics of the dataset, with N-gram metrics being more suitable for datasets with complex structures and BLEU scores being more suitable for datasets with simpler structures.", "A": "The use of N-gram metrics tends to emphasize internal diversity, while BLEU scores focus on external diversity, highlighting the need for a nuanced understanding of these metrics in evaluating dataset diversity.", "B": "Both N-gram metrics and BLEU scores are equally effective in evaluating external diversity, but N-gram metrics are more suitable for assessing internal diversity, suggesting a more balanced approach to diversity evaluation.", "D": "The evaluation of diversity in datasets using N-gram metrics and BLEU scores is inherently subjective, with different researchers potentially arriving at different conclusions regarding the relative merits of each metric."}, "answer": "A", "explanation": "The question requires a deep understanding of the characteristics of N-gram metrics and BLEU scores, as well as their implications for evaluating diversity in datasets. The correct answer highlights the importance of nuanced understanding and careful consideration of the characteristics of each metric. The incorrect answers (B, C, and D) oversimplify the relationship between the metrics and dataset diversity, failing to account for the complexities of each metric.", "question_token_count": 38, "answer_correctness_score": 8, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 4, "avg_answer_token_count": 39}
{"context": "##### 4.2.3 Collision\nOne of the main motivations for dynamic benchmarking is to address the challenge of balancing transparent benchmarking with the risk of data contamination. Since the benchmarking algorithm is publicly available, an important concern arises: If these benchmarks are used to train LLM, can they still reliably reflect the true capabilities of LLMs?\n\nTo evaluate the robustness of a dynamic benchmark against this challenge, we introduce the concept of collision in dynamic benchmarking. Collision refers to the extent to which different transformations of the benchmark dataset produce overlapping data, potentially limiting the benchmark\u2019s ability to generate novel and diverse test cases. To quantify this, we propose the following metrics:\n\nCollision Rate measures the percentage of overlap between two independently transformed versions of the benchmark dataset, indicating how much poential contamination among two trials. Repeat Trials quantifies the expected number of transformation trials required to fully regenerate an existing transformed dataset Ti\u2062(\ud835\udc9f)subscript\ud835\udc47\ud835\udc56\ud835\udc9fT\\_{i}(\\mathcal{D})italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ), providing insight into the benchmark\u2019s ability to produce novel variations.\n\nThese metrics help assess whether a dynamic benchmark remains effective in evaluating LLM capabilities, even when exposed to potential training data contamination.\n", "topic": "Defining Collision in Dynamic Benchmarking: Understanding the Implications for Evaluating LLM Capabilities", "question": "How does the collision rate impact the effectiveness of dynamic benchmarks in evaluating LLM capabilities?", "choices": {"C": "Collision rates have no significant impact on benchmark effectiveness, as the focus should be on the number of repeat trials rather than overlap between datasets.", "A": "High collision rates can lead to reduced benchmark effectiveness due to increased overlap between transformed datasets, potentially resulting in less diverse and novel test cases.", "B": "Low collision rates are necessary for robust dynamic benchmarking, as they indicate minimal overlap between transformed datasets, ensuring more diverse and novel test cases.", "D": "High collision rates can lead to increased benchmark effectiveness, as they ensure more overlap between transformed datasets, resulting in more diverse and novel test cases."}, "answer": "A", "explanation": "The correct answer (A) reflects the understanding that high collision rates can lead to reduced benchmark effectiveness due to increased overlap between transformed datasets, potentially resulting in less diverse and novel test cases. This is in line with the proposed metrics for quantifying collision, which highlight the importance of assessing the extent of overlap between transformed datasets.", "question_token_count": 17, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 28}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "Future Applications of LLM Benchmarking", "question": "What are some potential risks associated with using static benchmarks for Large Language Models (LLMs), and how might these risks impact the performance and reliability of LLMs over time?", "choices": {"C": "Insufficient evaluation of model performance on diverse tasks", "D": "Increased risk of model bias towards certain populations or demographics", "A": "Overfitting to specific training data", "B": "Introduction of data contamination issues"}, "answer": "B", "explanation": "The correct answer is B: Introduction of data contamination issues. The context mentions that static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data, which can lead to data contamination issues. This is a critical problem that requires attention from researchers and developers.", "question_token_count": 35, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 8}
{"context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n", "topic": "Hybrid Approaches to Dynamic Benchmarking", "question": "How might the limitations of rule-based and LLM-based generation approaches in dynamic benchmarking be addressed through the incorporation of hybrid approaches, and what are the potential implications for the evaluation of LLM capabilities?", "choices": {"A": "The incorporation of hybrid approaches could provide a more comprehensive understanding of LLM capabilities by combining the strengths of rule-based and LLM-based generation methods.", "B": "Hybrid approaches may not address the limitations of rule-based and LLM-based generation approaches, as they may still rely on the same underlying assumptions and biases.", "C": "The potential implications of hybrid approaches on the evaluation of LLM capabilities are unclear, and further research is needed to fully understand their impact.", "D": "Hybrid approaches may exacerbate the limitations of rule-based and LLM-based generation approaches, leading to a decrease in the overall accuracy of LLM evaluations."}, "answer": "A", "explanation": "This question requires the domain expert to think critically about the limitations of different approaches to dynamic benchmarking and how they might be addressed through hybrid approaches. The correct answer (A) acknowledges the potential benefits of hybrid approaches in providing a more comprehensive understanding of LLM capabilities. The incorrect answers (B, C, and D) represent potential pitfalls or misconceptions about the implications of hybrid approaches, requiring the domain expert to carefully consider the nuances of the subject matter.", "question_token_count": 38, "answer_correctness_score": 10, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 4, "avg_answer_token_count": 28}
{"context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n", "topic": "Challenges and Future Directions in LLM Benchmarking", "question": "What are the primary limitations of traditional static benchmarks in preventing data contamination in LLM benchmarking, and how do these limitations impact the evaluation of models trained on web-scale data?", "choices": {"A": "The primary limitation is the lack of transparency in label protection, leading to high assumptions about contaminated models.", "B": "The primary limitation is the inability to address complexity control, resulting in inefficiencies in evaluation.", "C": "The primary limitation is the lack of scalability in dynamic benchmarks, making it difficult to balance correctness with scalability.", "D": "The primary limitation is the lack of standardized criteria for evaluating dynamic benchmarks, leading to a lack of transparency in evaluation."}, "answer": "B", "explanation": "The correct answer is B. The primary limitation of traditional static benchmarks is the inability to address complexity control, resulting in inefficiencies in evaluation. This limitation is a key challenge in preventing data contamination in LLM benchmarking.", "question_token_count": 34, "answer_correctness_score": 6, "explanation_validity_score": 4, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 21}
{"context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n", "topic": "What are the implications of high external diversity and internal diversity in dataset transformation?", "question": "What would be the primary consequence of having high internal diversity and low external diversity in a dataset transformation process?", "choices": {"C": "Low accuracy in model performance, but high generalizability", "D": "High accuracy in model performance, but low generalizability", "A": "Low accuracy in model performance", "B": "High accuracy in model performance, but reduced generalizability"}, "answer": "B", "explanation": "High internal diversity and low external diversity can lead to high accuracy in model performance but reduced generalizability, as the model may not be able to generalize well to new, unseen data due to the low external diversity. This is because the model is not learning to recognize diverse patterns in the data.", "question_token_count": 22, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 9}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "The debate about whether syntactic transformations constitute true data contamination in LLMs highlights the need for a robust approach to identify and prevent contamination.", "question": "Can syntactic transformations in LLMs be considered true data contamination, given that some NLP applications rely primarily on syntactic information for decision-making?", "choices": {"C": "It depends on the specific context and application, as syntactic transformations can be beneficial or detrimental depending on the task.", "D": "Syntactic transformations may be a form of data contamination, but it is difficult to distinguish between an LLM's ability to recall memorized information and its reasoning capability during inference.", "A": "Yes, syntactic transformations can be considered true data contamination because they can lead to biased or inaccurate results.", "B": "No, syntactic transformations are not true data contamination because they are a natural consequence of language development and learning."}, "answer": "D", "explanation": "The correct answer is D, as it acknowledges the complexity of the issue and the difficulty in distinguishing between an LLM's ability to recall memorized information and its reasoning capability during inference. This answer requires a deep understanding of the content and demonstrates understanding and competency.", "question_token_count": 27, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 25}
{"context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n", "topic": "How can static benchmarks be adapted to prevent biases and ensure fairness in model evaluation?", "question": "How can static benchmarks be adapted to prevent biases and ensure fairness in model evaluation?", "choices": {"A": "Utilizing diverse and representative data sources to reduce bias in benchmarking datasets.", "B": "Implementing regularization techniques to penalize biased models and encourage fairness.", "C": "Developing dynamic benchmarks that adapt to changing data sources and user needs.", "D": "Employing human evaluators to provide subjective feedback and ensure fairness."}, "answer": "C", "explanation": "The correct answer, C, is the most appropriate choice because dynamic benchmarks that adapt to changing data sources and user needs can effectively address the need for fairness in model evaluation. Static benchmarks, if not carefully constructed, can perpetuate biases, and dynamic benchmarks can help mitigate this issue. The other options, while related to fairness and bias, are not the most effective ways to adapt static benchmarks.", "question_token_count": 17, "answer_correctness_score": 6, "explanation_validity_score": 7, "question_clarity_score": 8, "question_groundedness_score": 8, "avg_answer_token_count": 14}
{"context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n", "topic": "Standardizing Evaluation Criteria for Dynamic Benchmarks", "question": "What are the primary challenges and limitations in developing and evaluating dynamic benchmarks for LLMs, and how can standardized criteria help address these issues?", "choices": {"A": "The lack of transparency and high assumptions about contaminated models in traditional static benchmarking methods pose significant challenges.", "B": "The introduction of dynamic benchmarks has introduced new issues, such as balancing correctness with scalability, and neglecting complexity control, resulting in inefficiencies in evaluation.", "C": "The primary challenge in developing and evaluating dynamic benchmarks is the lack of standardized criteria, which hinders the development of more effective benchmarks.", "D": "The lack of transparency and high assumptions about contaminated models in traditional static benchmarking methods have limited the development of dynamic benchmarks, and the current evaluation methods are not sufficient to address these issues."}, "answer": "C", "explanation": "The correct answer is C. The primary challenge in developing and evaluating dynamic benchmarks is the lack of standardized criteria, which hinders the development of more effective benchmarks. This is evident from the study, which highlights the need for standardized criteria to guide the development of more effective benchmarks. The other options, while related to the topic, are not the primary challenge.", "question_token_count": 28, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 28}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "The underlying theories of contamination in LLMs involve the development of robust approaches to identify and prevent contamination, ensuring the accuracy and reliability of evaluations.", "question": "Can syntactic transformations be considered a form of true data contamination in LLMs, and if so, how can we effectively distinguish between memorized information and reasoning capability during inference?", "choices": {"A": "No, syntactic transformations do not constitute true data contamination.", "B": "Yes, syntactic transformations are a form of true data contamination, and it is challenging to distinguish between memorized information and reasoning capability during inference.", "C": "It depends on the context, but syntactic transformations can be considered a form of data contamination in certain situations.", "D": "There is ongoing debate about whether syntactic transformations constitute true data contamination, and more research is needed to determine the correct answer."}, "answer": "B", "explanation": "This question is designed to probe the domain expert's understanding of syntactic contamination in LLMs and their ability to reason about the challenges in distinguishing between memorized information and reasoning capability during inference. The correct answer requires the synthesis of high-level general understanding above and beyond the specific context.", "question_token_count": 35, "answer_correctness_score": 9, "explanation_validity_score": 7, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 22}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "Addressing the issue of contamination is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM's true ability to handle novel and unseen data, especially given the rapid pace of LLM development.", "question": "Can syntactic contamination, as defined by the addition of a prefix string to the training data, be considered a true form of data contamination in benchmarking LLMs, and what are the implications of this perspective on model evaluations?", "choices": {"A": "Yes, syntactic contamination is a true form of data contamination, as it challenges an LLM's ability to reason and generalize.", "B": "No, syntactic contamination is not a true form of data contamination, as it is merely a rephrasing of existing data and does not require novel reasoning or generalization.", "C": "It depends on the specific application and the type of information being processed, as syntactic contamination may be more significant in certain domains than others.", "D": "Syntactic contamination is a form of semantic contamination, as it alters the meaning and context of the original data, rather than merely rephrasing it."}, "answer": "D", "explanation": "The correct answer is D, as syntactic contamination is a form of semantic contamination, altering the meaning and context of the original data. The other options are incorrect, as they either overstate or understate the significance of syntactic contamination.", "question_token_count": 44, "answer_correctness_score": 2, "explanation_validity_score": 1, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 30}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "The significance of contamination in LLMs requires careful consideration to ensure the accuracy and reliability of evaluations, especially given the rapid pace of LLM development.", "question": "Can syntactic transformations in LLM benchmarking be considered true data contamination, and if so, what implications does this have for model evaluations and comparisons?", "choices": {"C": "It depends on the specific NLP application, as syntactic information may be more or less important in different contexts.", "D": "The distinction between syntactic contamination and true data contamination is irrelevant to model evaluations, as both are forms of data poisoning.", "A": "Yes, syntactic transformations should be considered true data contamination, as they can be misinterpreted as novel data.", "B": "No, syntactic transformations do not constitute true data contamination, as they are a natural consequence of the training data."}, "answer": "C", "explanation": "The correct answer, C, acknowledges the complexity of the issue and the varying importance of syntactic information across different NLP applications. It recognizes that syntactic transformations can be a challenge in distinguishing between memorized information and reasoning capability during inference.", "question_token_count": 28, "answer_correctness_score": 8, "explanation_validity_score": 8, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 23}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "The potential applications of contamination in LLM research involve the development of robust approaches to identify and prevent contamination, ensuring the accuracy and reliability of evaluations.", "question": "Can contamination in LLM evaluations be effectively addressed through the development of syntactic transformation-based detection methods, and what are the potential implications of such approaches on model generalization and real-world applicability?", "choices": {"C": "Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making.", "D": "The development of robust approaches to contamination identification and prevention is crucial for ensuring the accuracy and reliability of evaluations.", "A": "This approach may lead to overestimation of a model's true capabilities by inadvertently testing it on contaminated data.", "B": "Syntactic transformation-based detection methods may not be effective in distinguishing between an LLM's ability to recall memorized information and its reasoning capability during inference."}, "answer": "D", "explanation": "The correct answer (D) emphasizes the importance of developing robust approaches to contamination identification and prevention, which is a critical aspect of ensuring the accuracy and reliability of evaluations. The other options (A, B, and C) are incorrect because they either overestimate the effectiveness of syntactic transformation-based detection methods (A), fail to distinguish between an LLM's ability to recall memorized information and its reasoning capability during inference (B), or downplay the significance of contamination on model comparisons, deployment decisions, and policy-making (C).", "question_token_count": 36, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 7, "question_groundedness_score": 2, "avg_answer_token_count": 25}
{"context": "###### Math\nMath benchmarks evaluate a model\u2019s ability to solve multi-step math problems.\n\nDatasets such as GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021) require models to work through complex problems. Recent challenges like AIME 2024\u00a0(of\u00a0America, 2024) and CNMO 2024\u00a0(Society, 2024) further test a model\u2019s capacity to tackle diverse and intricate math tasks.\n\n\n###### Knowledge\nKnowledge benchmarks evaluate LLM internal knowledge. NaturalQuestions\u00a0(Kwiatkowski et\u00a0al., 2019) and TriviaQA\u00a0(Joshi et\u00a0al., 2017) focus on retrieving real-world information, while multi-domain tasks are covered by MMLU\u00a0(Hendrycks et\u00a0al., 2020), BBH\u00a0(Suzgun et\u00a0al., 2022), and AGI Eval\u00a0(Zhong et\u00a0al., 2023). Recent extensions like MMLU-Redux\u00a0(Gema et\u00a0al., 2024) and MMLU-Pro\u00a0(Wang et\u00a0al., 2024b) refine these assessments further.\n\nAdditionally, ControlBench\u00a0(Darioush et\u00a0al., 2024), FRAMES\u00a0(Krishna et\u00a0al., 2024), and GPQA Diamond\u00a0(Rein et\u00a0al., 2023) target technical and long-context challenges, with open-domain evaluations provided by AlpacaEval\u00a0(Li et\u00a0al., 2023c) and ArenaHard\u00a0(Li et\u00a0al., 2024a).\n", "topic": "Understanding the Key Differences Between Knowledge Benchmark Datasets and Tasks", "question": "What are the primary differences between knowledge benchmarks and math benchmarks in evaluating the performance of large language models?", "choices": {"D": "Math benchmarks are used to assess LLM's ability to reason abstractly, whereas knowledge benchmarks evaluate factual knowledge.", "C": "Knowledge benchmarks are limited to multiple-choice questions, whereas math benchmarks involve open-ended problems.", "A": "Both focus on complex math tasks.", "B": "Math benchmarks focus on real-world math applications, while knowledge benchmarks evaluate LLM internal knowledge."}, "answer": "B", "explanation": "The correct answer is B, as math benchmarks focus on real-world math applications, while knowledge benchmarks evaluate LLM internal knowledge, such as natural language understanding, common sense, and reasoning. ControlBench, FRAMES, and GPQA Diamond target technical and long-context challenges, further emphasizing the distinction.", "question_token_count": 21, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 16}
{"context": "### Ethical Considerations\nOur work is rooted in the goal of enhancing the transparency and fairness of LLM evaluations, which can help mitigate the risks of bias and contamination in AI systems. However, ethical concerns arise when considering the use of both static and dynamic benchmarks. Static benchmarks, if not carefully constructed, can inadvertently perpetuate biases, especially if they rely on outdated or biased data sources. Dynamic benchmarks, while offering a more adaptive approach, raise privacy and security concerns regarding the continual collection and updating of data. Moreover, transparency and the potential for misuse of benchmarking results, such as artificially inflating model performance or selecting biased evaluation criteria, must be carefully managed. It is essential that benchmarking frameworks are designed with fairness, accountability, and privacy in mind, ensuring they do not inadvertently harm or disadvantage certain user groups or research domains. Lastly, we encourage further exploration of ethical guidelines surrounding data usage, model transparency, and the broader societal impact of AI benchmarks.\n", "topic": "How can the broader societal impact of AI benchmarks be assessed, and what implications does this have for model development and deployment?", "question": "How can the broader societal impact of AI benchmarks be assessed, and what implications does this have for model development and deployment?", "choices": {"C": "The societal impact of AI benchmarks is largely irrelevant to model development and deployment, as the primary focus should be on technical performance and efficiency.", "A": "Focusing solely on technical performance metrics may lead to overlooking the potential risks and biases in AI benchmarks, ultimately hindering the development of more transparent and fair AI systems.", "D": "Assessing the broader societal impact of AI benchmarks requires a multidisciplinary approach, involving experts from ethics, sociology, and computer science to ensure a comprehensive understanding of the implications.", "B": "Implementing dynamic benchmarks can help mitigate the risks of bias and contamination, but careful consideration of ethical guidelines is essential to ensure fairness and accountability."}, "answer": "B", "explanation": "The correct answer (B) is supported by the context, which highlights the importance of dynamic benchmarks and the need for careful consideration of ethical guidelines. The other options are incorrect because they either overlook the potential risks and biases (A and C) or downplay the significance of the societal impact (C).", "question_token_count": 25, "answer_correctness_score": 4, "explanation_validity_score": 3, "question_clarity_score": 8, "question_groundedness_score": 8, "avg_answer_token_count": 31}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "The impact of contamination on benchmarking LLMs can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making.", "question": "What is the primary concern with using syntactic contamination in benchmarking LLMs, and how can it impact model comparisons and deployment decisions?", "choices": {"C": "Syntactic contamination is not a significant concern in benchmarking LLMs, as it does not affect model performance.", "D": "Syntactic contamination can only occur in NLP applications that rely primarily on syntactic information for decision-making.", "A": "Syntactic contamination can lead to overestimation of a model's true capabilities by testing it on data it has already seen, undermining the validity of benchmarks.", "B": "Syntactic contamination can only occur when the test data is derived from the training data by rephrasing it with the addition of a prefix string."}, "answer": "A", "explanation": "The correct answer requires an understanding of the concept of syntactic contamination and its potential impact on benchmarking LLMs. It also requires critical thinking about the implications of syntactic contamination on model comparisons and deployment decisions.", "question_token_count": 26, "answer_correctness_score": 10, "explanation_validity_score": 9, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 27}
{"context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n", "topic": "Evaluating Large Language Models: Limitations and Challenges", "question": "What is a primary limitation of traditional static benchmarking methods for evaluating Large Language Models (LLMs), and how does it impact the reliability of model evaluation?", "choices": {"A": "Static benchmarks become less effective as training corpora grow, making contamination issues more prevalent.", "B": "Dynamic benchmarks are more scalable, but they neglect complexity control, resulting in inefficiencies in evaluation.", "C": "Traditional methods face challenges due to a lack of transparency and high assumptions about contaminated models, but they have been largely replaced by dynamic benchmarks.", "D": "The use of web-scale data complicates contamination issues, but traditional static methods are still widely used and effective."}, "answer": "A", "explanation": "The correct answer is A, as the context highlights that static benchmarks become less effective as training corpora grow, leading to an increased probability of contamination issues. This is a significant limitation of traditional static benchmarking methods, impacting the reliability of model evaluation.", "question_token_count": 31, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 22}
{"context": "###### Limitations\nLabel protection limits transparency and independent verification, and it forces researchers to rely on centralized evaluation systems for performance metrics, which can impede detailed error analysis and reproducibility.\n\n\n##### 3.3.4 Post-hoc Detection\nPost-hoc detection mitigates data contamination by identifying overlaps between Dt\u2062r\u2062a\u2062i\u2062nsubscript\ud835\udc37\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5bD\\_{train}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_r italic\\_a italic\\_i italic\\_n end\\_POSTSUBSCRIPT\n\nand Dt\u2062e\u2062s\u2062tsubscript\ud835\udc37\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61D\\_{test}italic\\_D start\\_POSTSUBSCRIPT italic\\_t italic\\_e italic\\_s italic\\_t end\\_POSTSUBSCRIPT. This is typically done through n-gram matching at various levels, such as tokens\u00a0Touvron et\u00a0al. (2023) or words\u00a0Radford et\u00a0al. (2019); Brown et\u00a0al. (2020); Chowdhery et\u00a0al. (2023). However, exact matching often leads to false negatives, prompting the use of more robust techniques like embedding-based similarity\u00a0Riddell et\u00a0al. (2024); Lee et\u00a0al. (2023); Gunasekar et\u00a0al. (2023) and improved mapping metrics\u00a0Li et\u00a0al. (2024d); Xu et\u00a0al. (2024).\n\nBeyond direct overlap detection, post-hoc methods also analyze model behavior under different conditions, such as memorization through masked inputs\u00a0Ranaldi et\u00a0al. (2024); Chang et\u00a0al. (2023), partial completions\u00a0Anil et\u00a0al. (2023); Golchin and Surdeanu (2024), or preference for original over paraphrased test cases\u00a0Duarte et\u00a0al. (2024); Golchin and Surdeanu (2023); Zong et\u00a0al. (2024). For instance, Dekoninck et\u00a0al.\u00a0(2024) propose CONSTAT, which detects contamination by comparing model performance across benchmarks.\n", "topic": "Analyzing Model Behavior Under Different Conditions", "question": "How can post-hoc detection methods effectively analyze model behavior under different conditions, such as memorization and partial completions, to identify contamination and improve model reliability?", "choices": {"C": "Post-hoc detection methods should focus on analyzing model performance across benchmarks to identify contamination.", "D": "Model contamination can be effectively detected by comparing model performance on original and paraphrased test cases.", "A": "Traditional overlap detection techniques are often insufficient for detecting model contamination, as they may lead to false negatives.", "B": "Improved mapping metrics and embedding-based similarity analysis can provide a more robust approach to detecting model contamination."}, "answer": "B", "explanation": "This question requires the domain expert to understand the limitations of traditional overlap detection techniques and the need for more nuanced approaches to analyzing model behavior under different conditions. The correct answer, B, highlights the importance of improved mapping metrics and embedding-based similarity analysis in detecting model contamination.", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 19}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "Data Contamination in Machine Learning", "question": "Can you provide an example of syntactic contamination in a machine learning benchmark dataset, and how it can be identified?", "choices": {"C": "Exact duplicates of test data points in the training dataset", "A": "Verbatim test examples appearing in training corpora", "B": "Data points that are identical after syntactic transformation", "D": "Code snippets from benchmark implementations"}, "answer": "B", "explanation": "The correct answer is B, as syntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such as punctuation normalization or synonym substitution, while preserving lexical meaning. This requires a deep understanding of the concept of syntactic contamination and its distinction from exact contamination.", "question_token_count": 23, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 4, "question_groundedness_score": 9, "avg_answer_token_count": 9}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "Understanding the nature and significance of contamination in LLMs is essential for developing robust approaches to identify and prevent contamination, ensuring the accuracy and reliability of evaluations.", "question": "Can syntactic transformations be considered true data contamination in benchmarking LLMs, and what are the implications of this assumption for NLP applications that rely primarily on syntactic information for decision-making?", "choices": {"A": "Yes, syntactic transformations should be considered true data contamination, as they can lead to overestimation of a model's true capabilities by inadvertently testing it on data it has already seen.", "B": "No, syntactic transformations should not be considered true data contamination, as they can be distinguished from an LLM's ability to recall memorized information and its reasoning capability during inference.", "C": "It depends on the specific application and the type of syntactic transformation being used, as some transformations may be more problematic than others.", "D": "The assumption that syntactic transformations constitute true data contamination is not relevant to the evaluation of LLMs, as it does not affect the model's ability to handle novel and unseen data."}, "answer": "A", "explanation": "The correct answer is A, as syntactic transformations can indeed be considered true data contamination, as they can lead to overestimation of a model's true capabilities. This is relevant to NLP applications that rely primarily on syntactic information for decision-making, as it highlights the potential impact of contamination on these applications. The incorrect answers (B, C, and D) do not accurately capture the implications of syntactic transformations for LLMs.", "question_token_count": 35, "answer_correctness_score": 10, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 32}
{"context": "### 3 Static Benchmarking\nIn this section, we summarize a collection of static benchmarks that have been widely used to evaluate various aspects of model performance. These benchmarks cover a broad range of tasks including math, language, coding, reasoning, knowledge, safety, instruction following, and reading comprehension. They serve as standardized evaluation tools to measure model abilities in areas such as arithmetic problem-solving, natural language understanding, program synthesis, commonsense reasoning, factual knowledge retrieval, toxicity detection, and more. Table\u00a02 provides an overview of these benchmarks along with the corresponding task categories and key references.\n\n\n#### 3.1 Problem Formulation\nA static benchmark is given by\n\n\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae(.))\\mathcal{D}=(\\mathcal{X},\\mathcal{Y},\\mathcal{S}(.))caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( . ) ),\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the seed dataset, consisting of input prompts \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic\\_X, expected outputs \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y, and a scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ) that evaluates the quality of an LLM\u2019s outputs by comparing them against \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic\\_Y.\n", "topic": "Evaluation Metrics for LLM Outputs", "question": "What is the primary function of a scoring function in evaluating the quality of a large language model's (LLM) outputs, and how does it contribute to the overall assessment of model performance?", "choices": {"D": "To measure the computational resources required for the model's generation", "C": "To identify the biases in the model's training data", "B": "To evaluate the quality of the LLM's outputs by comparing them against the expected outputs", "A": "To determine the semantic meaning of the input prompts"}, "answer": "B", "explanation": "The correct answer (B) highlights the primary function of a scoring function in evaluating the quality of an LLM's outputs. The scoring function compares the LLM's outputs against the expected outputs, providing a standardized evaluation tool for assessing model performance. The other options are incorrect because they do not accurately describe the role of a scoring function in evaluating LLM performance.", "question_token_count": 38, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 12}
{"context": "###### Table-Based\nS3Eval\u00a0(Lei et\u00a0al., 2024) evaluates the reasoning ability of LLMs by assessing their accuracy in executing random SQL queries on randomly generated SQL tables.\n\n\n###### Graph-Based\nIn this category, LLMs are evaluated with randomly generated graphs.\n\nFor instance, DyVal\u00a0(Zhu et\u00a0al., 2024a) assesses the reasoning capabilities of LLMs using randomly generated directed acyclic graphs (DAGs).\n\nThe framework first constructs DAGs with varying numbers of nodes and edges to control task difficulty.\n\nThese DAGs are then transformed into natural language descriptions through rule-based conversion. Finally, the LLM is evaluated by querying it for the value of the root node.\n\nSimilarly, NPHardEval\u00a0(Fan et\u00a0al., 2024) evaluates the reasoning ability of LLMs on well-known P and NP problems, such as the Traveling Salesman Problem (TSP).\n\nRandom graphs of varying sizes are synthesized as inputs for TSP to assess the LLM\u2019s performance.\n\nXie et\u00a0al. (2024) automatically constructs Knights and Knaves puzzles with random reasoning graph.\n", "topic": "What are the differences between graph-based and table-based evaluations, and how do these differences impact the evaluation of LLMs' reasoning abilities?", "question": "What are the key differences in the design and implementation of graph-based and table-based evaluations for LLMs' reasoning abilities, and how do these differences impact the evaluation of LLMs' ability to reason abstractly?", "choices": {"D": "The main advantage of graph-based evaluations is that they provide a more comprehensive representation of complex relationships between entities, whereas table-based evaluations are limited to simple relational data.", "A": "The primary difference between graph-based and table-based evaluations is the use of directed acyclic graphs (DAGs) versus randomly generated SQL tables, respectively.", "B": "Graph-based evaluations are more challenging to implement than table-based evaluations due to the need for rule-based conversion of DAGs into natural language descriptions.", "C": "The choice of evaluation method (graph-based vs. table-based) depends on the specific task and the type of reasoning required, with graph-based evaluations being more suitable for abstract reasoning tasks."}, "answer": "A", "explanation": "The correct answer (A) highlights the key difference between graph-based and table-based evaluations, which is the use of DAGs versus SQL tables. This difference has a significant impact on the evaluation of LLMs' reasoning abilities, as DAGs provide a more comprehensive representation of complex relationships between entities. The incorrect answers (B, C, and D) are plausible but incorrect, and require careful consideration of the context to eliminate them.", "question_token_count": 43, "answer_correctness_score": 10, "explanation_validity_score": 8, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 32}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The Role of Dynamic Benchmarks in Addressing Data Contamination Issues", "question": "How can dynamic benchmarks, in conjunction with contamination detectors, effectively address data contamination issues in rapidly evolving Large Language Models?", "choices": {"C": "Dynamic benchmarks can be designed to adapt to changing model performance, but may require significant human effort to maintain their accuracy.", "D": "Contamination detectors and dynamic benchmarks can be combined to provide a comprehensive understanding of data contamination risks and opportunities for improvement.", "A": "Static benchmarks can be easily adapted to changing model performance, but may not account for the nuances of data contamination.", "B": "Contamination detectors can provide early warnings of data contamination, but may not be sufficient to mitigate its effects on model performance."}, "answer": "D", "explanation": "The correct answer (D) highlights the potential of combining contamination detectors and dynamic benchmarks to effectively address data contamination issues. This combination can provide a comprehensive understanding of data contamination risks and opportunities for improvement, making it a more robust approach than relying on static benchmarks or contamination detectors alone.", "question_token_count": 24, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 8, "avg_answer_token_count": 24}
{"context": "### 6 Conclusion\nThis survey reviews the literature on data contamination in LLM benchmarking, analyzing both static and dynamic approaches. We find that static methods, though consistent, become more vulnerable to contamination as training datasets grow. While dynamic approaches show promise, they face challenges in reliability and reproducibility. Future research should focus on standardized dynamic evaluation, and practical mitigation tools.\n\n\n### Limitations\nWhile this survey provides a comprehensive overview of static and dynamic benchmarking methods for LLMs, there are several limitations to consider. First, due to the rapidly evolving nature of LLM development and benchmarking techniques, some recent methods or tools may not have been fully covered. As benchmarking practices are still emerging, the methods discussed may not yet account for all potential challenges or innovations in the field. Additionally, our proposed criteria for dynamic benchmarking are a first step and may need further refinement and validation in real-world applications. Lastly, this survey focuses primarily on high-level concepts and may not delve into all the fine-grained technical details of specific methods, which may limit its applicability to practitioners seeking in-depth implementation guidelines.\n", "topic": "The need for standardized dynamic evaluation and practical mitigation tools in LLM benchmarking.", "question": "What is a crucial aspect of developing practical mitigation tools for LLM benchmarking, considering the limitations of both static and dynamic approaches?", "choices": {"C": "Enhanced model interpretability", "A": "Standardized evaluation frameworks are essential for developing practical mitigation tools for LLM benchmarking, as they provide a consistent and reliable way to assess the performance of LLMs and identify areas for improvement.", "B": "Improved data quality control", "D": "Standardized evaluation frameworks"}, "answer": "D", "explanation": "The correct answer, standardized evaluation frameworks, is crucial for developing practical mitigation tools for LLM benchmarking. This is because standardized frameworks provide a consistent and reliable way to assess the performance of LLMs, allowing for the identification of areas for improvement and the development of effective mitigation tools.", "question_token_count": 25, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 8, "avg_answer_token_count": 13}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "The impact of contamination on benchmarking LLMs can lead to misleading conclusions about progress in LLM research, requiring careful consideration to ensure the accuracy and reliability of evaluations.", "question": "Can syntactic transformations be considered a form of contamination in benchmarking LLMs, and if so, how can they be effectively mitigated?", "choices": {"C": "It depends on the context, as syntactic transformations may or may not be considered contamination depending on the specific benchmarking scenario.", "D": "Syntactic transformations are a form of contamination, but they can be mitigated by using more robust evaluation metrics.", "A": "Yes, syntactic transformations can be considered contamination, as they can lead to overestimation of a model's true capabilities.", "B": "No, syntactic transformations are not a form of contamination, as they do not necessarily imply that the model has seen the data before."}, "answer": "A", "explanation": "This question requires the domain expert to consider the nuances of data contamination and the potential consequences of inaccurate or misleading conclusions. The correct answer, A, requires the domain expert to understand that syntactic transformations can be considered contamination if they lead to overestimation of a model's true capabilities. The incorrect answers, B, C, and D, require the domain expert to demonstrate a lack of understanding of the concept of contamination and its implications.", "question_token_count": 27, "answer_correctness_score": 10, "explanation_validity_score": 8, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 24}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "Definition and Implications", "question": "What is the primary difference between exact contamination and syntactic contamination in the context of data contamination, and how do these types of contamination impact the validity of performance measurements?", "choices": {"C": "The primary difference between exact and syntactic contamination is the level of transformation required to induce contamination, with exact contamination requiring a complete duplication and syntactic contamination requiring only a syntactic transformation.", "A": "Exact contamination occurs when there is an exact duplicate in the benchmark dataset, while syntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation.", "B": "Syntactic contamination is a more severe form of contamination, as it can occur even when the data points are not exact duplicates.", "D": "Data contamination is a form of bias that can arise from the overlap between training and test datasets, and it can have significant implications for the validity of performance measurements."}, "answer": "A", "explanation": "The correct answer is A, as exact contamination refers to the presence of an exact duplicate in both the training and test datasets, while syntactic contamination refers to the presence of a test data point in the training dataset after a syntactic transformation. This distinction is crucial in understanding the impact of data contamination on performance measurements.", "question_token_count": 33, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 32}
{"context": "#### 3.3 Methods for Mitigation\nDue to the nature of LLM training data collection and the public availability of these static benchmark datasets, there is a risk that LLMs may inadvertently encounter and use them, leading to data contamination. To address this issue, several methods have been proposed to enhance static benchmarking and mitigate the impact of data contamination.\n\n\n##### 3.3.1 Canary String\nCanary strings are deliberately crafted, unique tokens embedded within a dataset to serve as markers for data contamination. When a model\u2019s output unexpectedly includes these tokens, it strongly indicates that the model has memorized portions of its training data rather than learning to generalize. For instance, the BIG-Bench dataset incorporates these strings so that model developers can identify and filter out such instances Jacovi et\u00a0al. (2023).\n\n\n###### Limitations\nThe effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.\n\nIf a developer aims to leak benchmarking data to boost scores, this method will not work.\n", "topic": "What are the limitations of relying on canary strings for mitigating data contamination, and how can developers address these limitations?", "question": "What are the limitations of relying on canary strings for mitigating data contamination, and how can developers address these limitations?", "choices": {"C": "Canary strings can only detect data contamination, not prevent it.", "B": "The effectiveness of canary strings depends on model trainers being aware of and responsive to these markers.", "A": "If a developer aims to leak benchmarking data to boost scores, this method will not work.", "D": "Data contamination can occur even with the presence of canary strings."}, "answer": "B", "explanation": "The correct answer is B because it highlights the limitation that the effectiveness of canary strings depends on model trainers being aware of and responsive to these markers. This is a critical consideration for developers who want to ensure that their models are not contaminated with benchmarking data. The other options are incorrect because they do not accurately capture the limitations of canary strings.", "question_token_count": 24, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 16}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "The debate about whether syntactic transformations constitute true data contamination in LLMs highlights the need for a robust approach to identify and prevent contamination, ensuring the accuracy and reliability of evaluations.", "question": "Can syntactic transformations, such as adding a prefix string to rephrase test data, be considered a form of true data contamination in LLMs, and what implications does this have for the validity of benchmarking evaluations?", "choices": {"A": "Yes, as syntactic transformations can blur the distinction between memorized information and reasoning capability during inference.", "B": "No, as syntactic transformations are a natural consequence of LLM training data and do not constitute true data contamination.", "C": "It depends on the specific application and context, as syntactic transformations may have different implications for different NLP tasks.", "D": "Perhaps, as syntactic transformations may have some impact on benchmark validity, but further research is needed to fully understand their implications."}, "answer": "A", "explanation": "The correct answer is A, as syntactic transformations can indeed blur the distinction between memorized information and reasoning capability during inference, making them a form of true data contamination in LLMs.", "question_token_count": 42, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 22}
{"context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n", "topic": "Evaluation of Temporal Cutoff Dynamic Benchmarks", "question": "What can be a significant challenge in ensuring the interpretability of dynamic benchmarks for LLM evaluation, and how might this impact the reliability and correctness of the evaluation results?", "choices": {"C": "The complexity of dynamic benchmarks can make it difficult to identify patterns or anomalies, hindering the ability to interpret the results accurately.", "A": "Lack of transparency in LLM models can lead to difficulties in understanding the generated data, making it challenging to ensure interpretability.", "B": "Insufficient human-in-the-loop validation can lead to errors in the evaluation process, as automated tools may not be able to detect all biases or flaws.", "D": "The use of rule-based generation approaches can lead to inconsistencies in the evaluation data, making it challenging to establish a common understanding of the results."}, "answer": "A", "explanation": "The correct answer (A) highlights the significance of transparency in LLM models as a challenge in ensuring interpretability. This is because LLMs are inherently complex, and without transparent explanations, it is difficult to understand how the generated data was created. This challenge is critical in dynamic benchmarking, where interpretability is crucial for ensuring the reliability and correctness of the evaluation results.", "question_token_count": 33, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 28}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "The significance of contamination in LLMs is significant, especially given the rapid pace of LLM development, and requires careful consideration to ensure the accuracy and reliability of evaluations.", "question": "Can syntactic transformations, such as the addition of a prefix string, be considered a form of true data contamination in LLM evaluations, and what implications does this have for the development of robust benchmarks?", "choices": {"A": "Yes, syntactic transformations can be considered true data contamination, as they may test an LLM's ability to recall memorized information instead of its reasoning capability during inference.", "B": "No, syntactic transformations are not a form of data contamination, as they are a natural part of language processing and do not compromise the validity of LLM evaluations.", "C": "It depends on the specific application and context, as syntactic transformations may be beneficial in certain scenarios but detrimental in others.", "D": "Syntactic transformations are a form of semantic contamination, which can have a more significant impact on LLM evaluations than syntactic contamination."}, "answer": "A", "explanation": "The correct answer is A, as syntactic transformations can indeed be considered a form of true data contamination, as they may test an LLM's ability to recall memorized information instead of its reasoning capability during inference. This has significant implications for the development of robust benchmarks, as it highlights the need for a more nuanced approach to evaluating LLMs.", "question_token_count": 39, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 28}
{"context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n", "topic": "Implications of Dynamic Benchmarks for LLM Evaluation", "question": "What is a critical aspect of dynamic benchmarking for LLM evaluation that ensures correctness and reliability?", "choices": {"A": "Rule-based generation", "B": "Temporal cutoff", "C": "LLM-based generation", "D": "Human-in-the-loop validation"}, "answer": "D", "explanation": "Dynamic benchmarking requires interpretable transformations to ensure correctness and reliability, which can be achieved through rule-based or LLM-assisted transformations, or by employing additional mechanisms like explainability tools or human-in-the-loop validation.", "question_token_count": 18, "answer_correctness_score": 10, "explanation_validity_score": 9, "question_clarity_score": 1, "question_groundedness_score": 8, "avg_answer_token_count": 4}
{"context": "##### 4.2.4 Stable of Complexity\nDynamic benchmarks must also account for complexity to help users determine whether a performance drop in an LLM on the transformed dataset is due to potential data contamination or an increase in task complexity. If a dynamic transformation increases the complexity of the seed dataset, a performance drop is expected, even without data contamination. However, accurately measuring the complexity of a benchmark dataset remains a challenging task. Existing work has proposed various complexity metrics, but these are often domain-specific and do not generalize well across different applications. For example, DyVal\u00a0(Zhu et\u00a0al., 2024a) proposes applying graph complexity to evaluate the complexity of reasoning problems.\n\nFormally, given a complexity measurement function \u03a8\u2062(\u22c5)\u03a8\u22c5\\Psi(\\cdot)roman\\_\u03a8 ( \u22c5 ), the stability can be formulated as:\n\nThis equation can be interpreted as the variance in complexity across different trials, where high variance indicates that the dynamic benchmarking method is not stable.\n", "topic": "Accounting for Complexity in Evaluating LLM Performance", "question": "Can the proposed complexity metric (DyVal) effectively capture the stability of dynamic benchmarking methods across different applications and datasets?", "choices": {"A": "Yes, as the metric is domain-specific and can be applied to various reasoning problems.", "B": "No, as the metric is limited to graph complexity and does not account for other factors that may affect stability.", "C": "It depends on the specific use case, as the metric's effectiveness can vary depending on the dataset and application.", "D": "The metric is not necessary, as stability can be measured using traditional metrics such as variance and standard deviation."}, "answer": "C", "explanation": "The correct answer is C, as the DyVal metric is domain-specific and may not generalize well across different applications and datasets. The question requires the domain expert to think critically about the limitations and potential applications of the metric.", "question_token_count": 24, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 7, "question_groundedness_score": 6, "avg_answer_token_count": 21}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "The impact of contamination on benchmarking LLMs can lead to misleading conclusions about progress in LLM research, requiring careful consideration to ensure the accuracy and reliability of evaluations.", "question": "Can syntactic contamination in benchmarking LLMs be considered a significant threat to the validity of benchmarks, and if so, how can it be mitigated?", "choices": {"A": "Yes, syntactic contamination can lead to misleading conclusions about progress in LLM research, and it can be mitigated by using techniques such as data preprocessing and model validation.", "B": "No, syntactic contamination is not a significant threat to the validity of benchmarks, and it can be ignored by simply re-running the benchmarking process.", "C": "Maybe, syntactic contamination may be a threat to the validity of benchmarks, but its impact can be mitigated by using more advanced techniques such as transfer learning and ensemble methods.", "D": "It depends on the specific benchmarking framework and the type of contamination, but syntactic contamination can be mitigated by using a combination of techniques such as data preprocessing and model validation."}, "answer": "A", "explanation": "The correct answer is A, as syntactic contamination can indeed lead to misleading conclusions about progress in LLM research. The authors of the text acknowledge the potential for syntactic contamination and emphasize the need for a robust approach to identifying and preventing contamination. The correct answer also highlights the importance of using techniques such as data preprocessing and model validation to mitigate the impact of syntactic contamination.", "question_token_count": 30, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 33}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "Understanding and mitigating data contamination in benchmarking LLMs is crucial for ensuring the accuracy and reliability of evaluations.", "question": "What are some potential challenges in distinguishing between an LLM's ability to recall memorized information and its reasoning capability during inference, particularly in the context of syntactic contamination?", "choices": {"D": "The challenge is inherent in the nature of LLMs and cannot be fully addressed through any means.", "B": "The challenge is largely mitigated through the use of large amounts of high-quality training data and extensive model fine-tuning.", "A": "This challenge is primarily addressed through the use of carefully designed evaluation metrics and testing protocols.", "C": "The challenge can be resolved through the implementation of more sophisticated algorithms and machine learning techniques."}, "answer": "B", "explanation": "This question is designed to encourage the test-taker to think critically about the challenges associated with distinguishing between an LLM's ability to recall memorized information and its reasoning capability during inference. The correct answer, B, requires an understanding of the importance of high-quality training data and model fine-tuning in mitigating this challenge. The incorrect answers, A, C, and D, represent common pitfalls or oversimplifications that might be encountered in evaluating this challenge.", "question_token_count": 33, "answer_correctness_score": 6, "explanation_validity_score": 5, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 20}
{"context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n", "topic": "Minimizing Costs in Dynamic Benchmarking", "question": "What is the primary objective of quantifying the scalability of dynamic benchmarking methods in minimizing costs, and how does it relate to the proportion of data generated per unit cost?", "choices": {"B": "Minimizing the size of the transformed dataset to reduce costs.", "A": "Maximizing the transformed dataset size while minimizing costs.", "C": "Optimizing the cost function to achieve the best balance between scalability and cost.", "D": "Increasing the original dataset size to reduce costs."}, "answer": "A", "explanation": "The primary objective is to maximize the transformed dataset size while minimizing costs, as indicated by the equation Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) , which represents the proportion of data that can be generated per unit cost. This approach enables the generation of large-scale benchmark datasets while minimizing associated costs.", "question_token_count": 33, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 9, "avg_answer_token_count": 12}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "The debate about whether syntactic transformations constitute true data contamination in LLMs highlights the need for a robust approach to identify and prevent contamination, ensuring the accuracy and reliability of evaluations.", "question": "Can syntactic transformations be considered as true data contamination in LLMs, and what implications does this have for the evaluation of their capabilities?", "choices": {"A": "Yes, syntactic transformations can be considered as true data contamination in LLMs, as they can lead to overestimation of a model's true capabilities by inadvertently testing it on data it has already seen.", "B": "No, syntactic transformations cannot be considered as true data contamination in LLMs, as they do not affect the model's ability to recall memorized information.", "C": "It depends on the specific application and the type of syntactic transformation, as some transformations may be more detrimental to evaluation accuracy than others.", "D": "Syntactic transformations are not a significant concern for LLM evaluation, as the primary focus should be on the model's ability to reason and generalize."}, "answer": "A", "explanation": "The correct answer is A, as syntactic transformations can indeed be considered as true data contamination in LLMs, given that some NLP applications rely primarily on syntactic information for decision-making. This has significant implications for the evaluation of LLMs, as contaminated benchmarks can lead to misleading conclusions about progress in LLM research.", "question_token_count": 27, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 31}
{"context": "###### Limitations\nPost-hot detection methods face several limitations. Full access to the training dataset is often restricted due to legal and privacy constraints, making overlap detection challenging. Additionally, assumptions about model behavior, such as higher memorization or lower perplexity for contaminated instances, may not hold across different models and tasks.\n\n\n### 4 Dynamic Benchmarking\nDue to the inherent limitations of static benchmarking schemes, they face challenges in providing a transparent yet faithful evaluation of LLMs. To address this, dynamic benchmarking has been proposed.\n\n\n#### 4.1 Problem Formulation\nA dynamic benchmark is defined as\n\n\u212cdynamic=(\ud835\udc9f,T\u2062(\u22c5)),\ud835\udc9f=(\ud835\udcb3,\ud835\udcb4,\ud835\udcae\u2062(\u22c5))formulae-sequencesubscript\u212cdynamic\ud835\udc9f\ud835\udc47\u22c5\ud835\udc9f\ud835\udcb3\ud835\udcb4\ud835\udcae\u22c5\\mathcal{B}\\_{\\text{dynamic}}=(\\mathcal{D},T(\\cdot)),\\quad\\mathcal{D}=(\\mathcal%\n\n{X},\\mathcal{Y},\\mathcal{S}(\\cdot))caligraphic\\_B start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = ( caligraphic\\_D , italic\\_T ( \u22c5 ) ) , caligraphic\\_D = ( caligraphic\\_X , caligraphic\\_Y , caligraphic\\_S ( \u22c5 ) )\n\nwhere \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D represents the static benchmark dataset.\n\nThe transformation function T\u2062(\u22c5)\ud835\udc47\u22c5T(\\cdot)italic\\_T ( \u22c5 ) modifies the data set during the benchmarking to avoid possible data contamination.\n\nThe dynamic dataset for the evaluation of an LLM can then be expressed as\n\n\ud835\udc9ft=Tt\u2062(\ud835\udc9f),\u2200t\u2208{1,\u2026,N}formulae-sequencesubscript\ud835\udc9f\ud835\udc61subscript\ud835\udc47\ud835\udc61\ud835\udc9ffor-all\ud835\udc611\u2026\ud835\udc41\\mathcal{D}\\_{t}=T\\_{t}(\\mathcal{D}),\\quad\\forall t\\in\\{1,\\dots,N\\}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT = italic\\_T start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT ( caligraphic\\_D ) , \u2200 italic\\_t \u2208 { 1 , \u2026 , italic\\_N }\n\nwhere \ud835\udc9ftsubscript\ud835\udc9f\ud835\udc61\\mathcal{D}\\_{t}caligraphic\\_D start\\_POSTSUBSCRIPT italic\\_t end\\_POSTSUBSCRIPT represents the evaluation data set at the timestamp t\ud835\udc61titalic\\_t, and N\ud835\udc41Nitalic\\_N is the total timestamp number, which could be finite or infinite. If the seed dataset \ud835\udc9f\ud835\udc9f\\mathcal{D}caligraphic\\_D is empty, the dynamic benchmarking dataset will be created from scratch.\n\nTable 3: Existing dynamic benchmarks and their quality on our summarized criteria. \u25cf\u25cf\\CIRCLE\u25cf represents support, \u25d0\u25d0\\LEFTcircle\u25d0 represents partial support, and \u25cb\u25cb\\Circle\u25cb represents no support\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "What are the limitations of static benchmarking schemes in evaluating large language models, and how can dynamic benchmarking address these limitations?", "question": "What is the primary limitation of full access to the training dataset in post-hot detection methods, and how can dynamic benchmarking schemes address this limitation?", "choices": {"A": "The primary limitation of full access to the training dataset in post-hot detection methods is that it restricts overlap detection, making it challenging to identify contaminated instances.", "B": "The primary limitation of full access to the training dataset in post-hot detection methods is that it restricts the ability to modify the data during benchmarking, leading to a static evaluation.", "C": "The primary limitation of full access to the training dataset in post-hot detection methods is that it restricts the ability to create a dynamic benchmark dataset from scratch, forcing the use of existing datasets.", "D": "The primary limitation of full access to the training dataset in post-hot detection methods is that it restricts the ability to evaluate models on new, unseen data, leading to a lack of generalizability."}, "answer": "A", "explanation": "The correct answer is A, as the primary limitation of full access to the training dataset in post-hot detection methods is indeed that it restricts overlap detection, making it challenging to identify contaminated instances. This is a critical limitation in post-hot detection methods, as it can lead to false negatives and poor performance.", "question_token_count": 29, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 35}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "The potential applications of contamination in LLM research involve the development of robust approaches to identify and prevent contamination, ensuring the accuracy and reliability of evaluations.", "question": "What is a key challenge in distinguishing between an LLM's ability to recall memorized information and its reasoning capability during inference, and how does this relate to the significance of contamination in benchmarking LLMs?", "choices": {"C": "The impact of contamination on benchmarks can be mitigated by developing robust approaches to identify and prevent contamination.", "A": "The lack of a clear distinction between memorization and reasoning capabilities can lead to overestimation of a model's true capabilities.", "B": "The use of syntactic transformations can be misleading in evaluating an LLM's reasoning ability.", "D": "The significance of contamination in benchmarking LLMs is a topic of ongoing debate among researchers."}, "answer": "B", "explanation": "The correct answer is B, as the question aims to probe the domain expert's understanding of the challenge in distinguishing between memorization and reasoning capabilities, and how this relates to the significance of contamination in benchmarking LLMs.", "question_token_count": 40, "answer_correctness_score": 2, "explanation_validity_score": 1, "question_clarity_score": 8, "question_groundedness_score": 4, "avg_answer_token_count": 20}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "Data Contamination Issues and Contamination Detectors", "question": "What are some potential risks associated with using static benchmarks for LLMs, and how can contamination detectors help mitigate these issues?", "choices": {"C": "Reducing the need for human evaluation and increasing model training efficiency.", "D": "Increasing the risk of overestimating model performance.", "A": "Overfitting to the training data.", "B": "Introducing data contamination issues and making benchmarks too easy for stronger LLMs."}, "answer": "B", "explanation": "The correct answer is B, as static benchmarks may become too easy for stronger LLMs or introduce data contamination issues, which can lead to inaccurate evaluations of model performance. Contamination detectors can help mitigate these issues by quantifying contamination risks and providing a more accurate assessment of model performance.", "question_token_count": 25, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 12}
{"context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n", "topic": "Quantitative Measure of Scalability", "question": "What is the expected proportion of data that can be generated per unit cost, as represented by the equation \u2225T_i(D)\u2225 / \u2225D\u2225?", "choices": {"C": "The proportion of data that can be generated per unit manual effort.", "A": "The proportion of data that can be generated per unit time.", "B": "The proportion of data that can be generated per unit monetary cost.", "D": "The proportion of data that can be generated per unit transformation cost."}, "answer": "B", "explanation": "The correct answer is B, as the equation represents the proportion of data that can be generated per unit monetary cost. This is because the cost associated with the transformation process is measured using the function Cost(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) , which includes monetary cost.", "question_token_count": 32, "answer_correctness_score": 9, "explanation_validity_score": 8, "question_clarity_score": 5, "question_groundedness_score": 10, "avg_answer_token_count": 13}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "Addressing the issue of contamination is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM's true ability to handle novel and", "question": "What is the primary concern regarding contamination in benchmarking Large Language Models (LLMs), and how might it affect the validity of evaluations?", "choices": {"C": "The potential impact of contaminated benchmarks on deployment decisions, policy-making, and model comparisons.", "D": "The significance of syntactic transformations in NLP applications.", "A": "The risk of overestimating a model's true capabilities by inadvertently testing it on data it has already seen.", "B": "The challenge in distinguishing between memorized information and reasoning capability during inference."}, "answer": "C", "explanation": "The correct answer is C, as the primary concern regarding contamination in benchmarking LLMs is its potential impact on deployment decisions, policy-making, and model comparisons. This concern arises because contaminated benchmarks may lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making.", "question_token_count": 27, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 16}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "Exact vs. Syntactic Contamination", "question": "What is the primary distinction between exact and syntactic contamination in the context of Large Language Models (LLMs) training and testing datasets?", "choices": {"D": "Syntactic contamination occurs when a test data point could be found in the training dataset after a semantic transformation, while exact contamination occurs when there are exact duplicates in the benchmark dataset.", "C": "Exact contamination occurs when there are exact duplicates in the benchmark dataset, while syntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation.", "A": "Exact contamination occurs when there are exact duplicates in the benchmark dataset, while syntactic contamination occurs when a test data point could be found in the training dataset after a semantic transformation.", "B": "Syntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, while exact contamination occurs when there are exact duplicates in the benchmark dataset."}, "answer": "C", "explanation": "The correct answer is C) Exact contamination occurs when there are exact duplicates in the benchmark dataset, while syntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation. This is because exact contamination refers to the presence of identical data points in both the training and testing datasets, while syntactic contamination refers to the presence of data points that are similar but not identical due to syntactic transformations.", "question_token_count": 27, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 36}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "Importance of Dynamic Benchmarks in Assessing Model Performance", "question": "Can dynamic benchmarks effectively mitigate the risks of contamination and provide a holistic view of LLM performance as they evolve?", "choices": {"C": "It depends on the specific application and use case, as dynamic benchmarks may not be effective in all scenarios.", "A": "Yes, dynamic benchmarks can provide a more accurate assessment of LLM performance by adapting to the changing model landscape.", "B": "No, static benchmarks are sufficient for assessing LLM performance, and dynamic benchmarks are not necessary.", "D": "The impact of dynamic benchmarks on model performance assessment is still unclear, and more research is needed to determine their effectiveness."}, "answer": "A", "explanation": "The correct answer is A, as dynamic benchmarks can provide a more accurate assessment of LLM performance by adapting to the changing model landscape. This is in line with the discussion in the context about the limitations of static benchmarks and the need for dynamic benchmarks to mitigate contamination risks.", "question_token_count": 22, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 9, "question_groundedness_score": 9, "avg_answer_token_count": 21}
{"context": "##### 4.2.6 Interpretability\nDynamic benchmarking generates large volumes of transformed data, making manual verification costly and challenging. To ensure correctness, the transformation process must be interpretable. Interpretable transformations reduce the need for extensive manual validation, lowering costs. Rule-based or manually crafted transformations are inherently interpretable, while LLM-assisted transformations depend on the model\u2019s transparency and traceability. In such cases, additional mechanisms like explainability tools, or human-in-the-loop validation may be needed to ensure reliability and correctness.\n\n\n#### 4.3 Existing Work\nBuilding on the task formats of static benchmarks, dynamic benchmarks have been introduced to assess LLM capabilities while minimizing data contamination and ensuring fairness.\n\nTable\u00a02 summarizes recent dynamic benchmarks for LLM evaluation.\n\nDynamic benchmarks can be categorized into four types based on their construction process: temporal cutoff, rule-based generation, LLM-based generation, and hybrid approaches.\n\nTemporal cutoff follows a data collection process similar to static benchmarks, with the key difference being that data is gathered from newly released information.\n\nRule-based and LLM-based generation approaches create novel evaluation data points using predefined rules or by leveraging the strong generative capabilities of LLMs.\n\nFinally, hybrid approaches combine the idea of these different approaches.\n", "topic": "Defining the Interpretable Transformation Process in Dynamic Benchmarking", "question": "What are some potential mechanisms that can be employed to ensure the interpretability of the transformation process in dynamic benchmarking, and how do these mechanisms differ from those used in static benchmarking?", "choices": {"A": "Human-in-the-loop validation", "B": "Explainability tools", "C": "Rule-based or manually crafted transformations", "D": "LLM-assisted transformations with enhanced transparency and traceability"}, "answer": "B", "explanation": "The question aims to probe the domain expert's understanding of the importance of interpretability in dynamic benchmarking and the various mechanisms that can be employed to achieve it. The correct answer, \"Explainability tools\" and \"Human-in-the-loop validation\", highlights the potential mechanisms that can be used to ensure interpretability, while \"Rule-based or manually crafted transformations\" and \"LLM-assisted transformations with enhanced transparency and traceability\" are incorrect as they are not directly related to ensuring interpretability.", "question_token_count": 36, "answer_correctness_score": 7, "explanation_validity_score": 8, "question_clarity_score": 6, "question_groundedness_score": 4, "avg_answer_token_count": 7}
{"context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n", "topic": "Scalability in Dynamic Benchmarking Methods", "question": "What is the primary implication of the equation representing scalability in dynamic benchmarking methods?", "choices": {"C": "A", "D": "B", "A": "C", "B": "B"}, "answer": "C", "explanation": "The correct answer is C, as the equation represents the proportion of data that can be generated per unit cost. This is evident from the context, which states that the scalability of a dynamic benchmark is quantified as the expectation over the entire transformation space.", "question_token_count": 16, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 2}
{"context": "#### 2.2 Contamination from LLM Training\nUnlike traditional models with clear separations between training and evaluation data, LLMs are pre-trained on massive, diverse datasets\u2014often scraped from the web (e.g., FineWeb\u00a0Penedo et\u00a0al. (2024))\u2014which increases the risk of evaluation data overlap. In the post-training phase, models are further fine-tuned on large human-annotated\u00a0Mukherjee et\u00a0al. (2023); Kim et\u00a0al. (2023) or synthetic datasets\u00a0Ding et\u00a0al. (2023); Teknium (2023); Wang et\u00a0al. (2023) that may resemble evaluation tasks, further compounding contamination risks. Although retrieval-based detection methods\u00a0Team et\u00a0al. (2024); Achiam et\u00a0al. (2023) exist, the sheer scale and complexity of training corpora make it difficult to entirely exclude evaluation data. Additionally, many LLMs keep their training data proprietary\u00a0Dubey et\u00a0al. (2024); Yang et\u00a0al. (2024), complicating the accurate assessment of their true performance and highlighting the need for fair and reliable benchmarks. This opacity further exacerbates data contamination, as it impedes the community\u2019s ability to verify and mitigate potential overlaps between training and evaluation data.\n", "topic": "Strategies for Mitigating Data Overlap and Ensuring Evaluation Integrity", "question": "Can the evaluation of large language models (LLMs) be considered fair and reliable when their training data is proprietary, and the risk of data overlap between training and evaluation sets is high?", "choices": {"C": "It depends on the specific use case and the measures taken to mitigate contamination risks.", "D": "The evaluation integrity of LLMs is already compromised due to the opacity of their training data.", "A": "Yes, as the model's performance can be evaluated based on the fine-tuned human-annotated datasets.", "B": "No, as the proprietary training data and potential overlap between training and evaluation data cannot be accurately assessed."}, "answer": "D", "explanation": "The correct answer is D, as the evaluation integrity of LLMs is already compromised due to the opacity of their training data, making it challenging to accurately assess their true performance and mitigate potential overlaps between training and evaluation data.", "question_token_count": 38, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 7, "question_groundedness_score": 10, "avg_answer_token_count": 19}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "The significance of contamination in LLMs requires careful consideration to ensure the accuracy and reliability of evaluations, especially given the rapid pace of LLM development.", "question": "Can syntactic transformations, such as prefixing the training data with a specific string, be considered a form of contamination in LLMs, and how can this be mitigated in benchmarking evaluations?", "choices": {"A": "No, syntactic transformations do not constitute true data contamination, as they can be seen as an extension of the model's ability to recall memorized information.", "B": "Yes, syntactic transformations can be considered contamination, as they may inadvertently test the model on data it has already seen, undermining the validity of benchmarks.", "C": "It depends on the context and the specific application, as syntactic transformations may be beneficial in certain cases, but detrimental in others.", "D": "Syntactic transformations are not a concern in LLMs, as they do not affect the model's ability to generalize to novel data."}, "answer": "B", "explanation": "The correct answer is B, as syntactic transformations can be considered contamination if they inadvertently test the model on data it has already seen, undermining the validity of benchmarks. This requires a nuanced understanding of the topic and the ability to critically evaluate the implications of syntactic transformations on LLMs.", "question_token_count": 38, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 8, "question_groundedness_score": 10, "avg_answer_token_count": 28}
{"context": "##### 4.2.2 Scalability\nThe next evaluation criterion is scalability, which measures the ability of dynamic benchmarking methods to generate large-scale benchmark datasets. A smaller dataset can introduce more statistical errors during the benchmarking process. Therefore, an optimal dynamic benchmark should generate a larger dataset while minimizing associated costs. The scalability of a dynamic benchmark is quantified as:\n\nThis represents the expectation over the entire transformation space, where \u2225Ti\u2062(\ud835\udc9f)\u2225delimited-\u2225\u2225subscript\ud835\udc47\ud835\udc56\ud835\udc9f\\lVert T\\_{i}(\\mathcal{D})\\rVert\u2225 italic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( caligraphic\\_D ) \u2225 is the size of the transformed dataset, and \u2225\ud835\udc9f\u2225delimited-\u2225\u2225\ud835\udc9f\\lVert\\mathcal{D}\\rVert\u2225 caligraphic\\_D \u2225 is the size of the original dataset. The function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) measures the cost associated with the transformation process, which could include monetary cost, time spent, or manual effort according to the detailed scenarios.\n\nThis equation could be interpreted as the proportion of data that can be generated per unit cost.\n", "topic": "Efficiency Evaluation of Dynamic Benchmarking Methods", "question": "How does the function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) impact the scalability of dynamic benchmarking methods, and what implications does this have for the transformation process?", "choices": {"C": "How does the relationship between the size of the original and transformed datasets affect the overall scalability of the dynamic benchmark?", "A": "If the function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) is directly proportional to the size of the transformed dataset, what would be the optimal strategy for generating large-scale benchmark datasets?", "B": "Can the scalability of dynamic benchmarking methods be improved by incorporating more efficient algorithms for optimizing the transformation process?", "D": "What are the potential consequences of ignoring the cost associated with the transformation process when evaluating the scalability of dynamic benchmarking methods?"}, "answer": "A", "explanation": "The correct answer is A, as the function Cost\u2062(\u22c5)Cost\u22c5\\textsf{Cost}(\\cdot)Cost ( \u22c5 ) directly impacts the scalability of dynamic benchmarking methods by determining the optimal strategy for generating large-scale benchmark datasets.", "question_token_count": 43, "answer_correctness_score": 8, "explanation_validity_score": 7, "question_clarity_score": 2, "question_groundedness_score": 9, "avg_answer_token_count": 28}
{"context": "##### 3.3.2 Encryption\nEncryption methods secure evaluation data by making it inaccessible to unauthorized parties, preventing its accidental inclusion in training sets. Jacovi et\u00a0al. (2023) propose encrypting test data with a public key and a \u201cNo Derivatives\u201d license to block automated crawling and reuse. Yang et\u00a0al. (2023) show that even advanced decontamination methods can be defeated by minor text variations, emphasizing the need for robust encryption. Similarly, TRUCE (Chandran et\u00a0al., 2024) leverages confidential computing and secure multi-party computation to enable private benchmarking, ensuring that test data and model parameters remain confidential.\n\n\n###### Limitation\nWhile these methods effectively protect against data leakage, they depend on strong key management, they introduce extra computational overheads. These methods are vulnerable if encryption is compromised or private key is exposed.\n\n\n##### 3.3.3 Label Protection\nLabel protection involves keeping the true answers of a test set hidden from public access so that only an authorized evaluator can use them during model assessment. This approach is common in benchmarks such as GLUE Wang (2018), SuperGLUE Wang et\u00a0al. (2019), and OpenAI\u2019s HumanEval Chen et\u00a0al. (2021), etc., where the test labels are withheld to prevent models from learning or memorizing them during training. The key advantage of this method is its ability to maintain evaluation integrity by preventing model exposure to answers, thereby mitigating data contamination risks.\n", "topic": "Limitations of Encryption Methods", "question": "What is a major limitation of encryption methods used to protect evaluation data, and how can it be mitigated?", "choices": {"A": "Robust key management is essential to prevent data leakage, but it increases computational overheads.", "B": "The use of strong encryption can prevent data exposure, but it may not be sufficient to protect against minor text variations.", "C": "Confidential computing and secure multi-party computation can enable private benchmarking, but they rely on public key and \"No Derivatives\" license.", "D": "Label protection is a common approach to maintaining evaluation integrity, but it may not be sufficient to prevent model exposure to answers."}, "answer": "A", "explanation": "The correct answer is A, as robust key management is essential to prevent data leakage, but it increases computational overheads. This limitation is a major concern, as it can compromise the effectiveness of encryption methods. The other options are incorrect, as they either focus on the benefits of encryption methods (B) or the benefits of alternative approaches (C and D).", "question_token_count": 23, "answer_correctness_score": 10, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 23}
{"context": "###### Examples of each contamination\nWe provide contamination examples in Table\u00a01. In the case of syntactic contamination, the test data is derived from the training data by rephrasing it with the addition of a prefix string. There is ongoing debate about whether such syntactic transformations constitute true data contamination, as it is challenging to distinguish between an LLM\u2019s ability to recall memorized information and its reasoning capability during inference. In this work, we consider such syntactic transformations as contamination, given that some NLP applications rely primarily on syntactic information for decision-making.\n\n\n###### Significance of contamination\nUnderstanding and mitigating potential data contamination in benchmarking LLMs is significant, especially given the rapid pace of LLM development. Without a robust approach to identifying and preventing contamination, evaluations may overestimate a model\u2019s true capabilities by inadvertently testing it on data it has already seen. This undermines the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. Contaminated benchmarks can lead to misleading conclusions about progress in LLM research, influencing model comparisons, deployment decisions, and policy-making. Addressing this issue is crucial for ensuring that benchmarks provide an accurate and reliable measure of an LLM\u2019s true ability to handle novel and unseen data.\n", "topic": "The potential applications of contamination in LLM research involve the development of robust approaches to identify and prevent contamination, ensuring the accuracy and reliability of evaluations.", "question": "What are the primary implications of contaminating LLM benchmarking data, and how can robust approaches to identify and prevent contamination be developed to ensure the accuracy and reliability of evaluations?", "choices": {"C": "Influencing model comparisons, deployment decisions, and policy-making with inaccurate or unreliable measures of an LLM's true ability to handle novel and unseen data.", "D": "Potentially leading to the development of more robust and accurate LLMs through the identification and prevention of contamination.", "A": "Overestimating a model's true capabilities by inadvertently testing it on data it has already seen, leading to misleading conclusions about progress in LLM research.", "B": "Undermining the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability."}, "answer": "B", "explanation": "The correct answer is B, as contaminating LLM benchmarking data can lead to undermining the validity of benchmarks, making it difficult to assess generalization, robustness, and real-world applicability. This undermines the reliability of benchmarks, making it challenging to assess a model's true capabilities.", "question_token_count": 34, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 7, "avg_answer_token_count": 26}
{"context": "#### 2.3 LLM Benchmarking\nAs LLMs evolve into general-purpose task solvers, it is crucial to develop benchmarks that provide a holistic view of their performance.\n\nTo this end, significant human effort has been dedicated to building comprehensive benchmarks that assess various aspects of model performance. For example, instruction-following tasks evaluate a model\u2019s ability to interpret and execute commands\u00a0(Zhou et\u00a0al., 2023; Qin et\u00a0al., 2024; Huang et\u00a0al., 2024), while coding tasks assess its capability to generate and understand programming code\u00a0(Chen et\u00a0al., 2021; Austin et\u00a0al., 2021; Jimenez et\u00a0al., 2024; Codeforces, 2025; Aider, 2025).\n\nDespite their usefulness, static benchmarks face challenges as LLMs evolve rapidly and continue training on all available data\u00a0(Villalobos et\u00a0al., 2022). Over time, unchanging benchmarks may become too easy for stronger LLMs or introduce data contamination issues.\n\nRecognizing this critical problem, contamination detectors have been developed to quantify contamination risks, and dynamic benchmarks have been proposed to mitigate these issues.\n\nTable 2: Summary of benchmarking applications.\n\n<!-- \ud83d\uddbc\ufe0f\u274c Image not available. Please use `PdfPipelineOptions(generate_picture_images=True)` -->\n", "topic": "The Importance of LLM Benchmarking in the Field of Natural Language Processing", "question": "What are the potential consequences of relying solely on static benchmarks for evaluating the performance of large language models, and how can contamination detectors and dynamic benchmarks help mitigate these issues?", "choices": {"C": "Contamination detectors can help identify and quantify contamination risks, but may not fully mitigate the issue.", "A": "Static benchmarks may become too easy for stronger LLMs, leading to inaccurate evaluations.", "B": "Dynamic benchmarks may not capture the full range of LLM performance, resulting in incomplete evaluations.", "D": "LLMs may adapt to static benchmarks, rendering them ineffective for evaluation."}, "answer": "A", "explanation": "This question requires the test-taker to think critically about the potential consequences of relying solely on static benchmarks and the importance of contamination detectors and dynamic benchmarks in mitigating these issues.", "question_token_count": 34, "answer_correctness_score": 10, "explanation_validity_score": 6, "question_clarity_score": 7, "question_groundedness_score": 8, "avg_answer_token_count": 17}
{"context": "###### Limitations\nThe quality of LLM-generated samples is often uncertain. For example, human annotation of LatestEval\u00a0(Li et\u00a0al., 2023d) shows that about 10% of samples suffer from faithfulness and answerability issues, compromising evaluation reliability. Moreover, in interactive evaluation, reliability largely depends on the interviewer LLM, which generates questions and assigns scores.\n\n\n##### 4.3.4 Hybrid Generation\nLatestEval\u00a0(Li et\u00a0al., 2023d) combines temporal cutoff and LLM-based generation to automatically generate reading comprehension datasets using LLMs on real-time content from sources such as BBC.\n\nDARG\u00a0(Zhang et\u00a0al., 2024b) integrates LLM-based and graph-based generation. It first extracts reasoning graphs from existing benchmarks and then perturbs them into new samples using predefined rules.\n\nC2LEVA\u00a0(Li et\u00a0al., 2024c) incorporates all three contamination-free construction methods to build a contamination-free bilingual evaluation.\n\n\n### 5 Discussions\nKey Insights.\n\nOur analysis highlights two key insights regarding data contamination in LLM benchmarking. First, static benchmarks become less effective as training corpora grow. The probability of contamination increases with Prcontam\u221d|\ud835\udc9ftrain|\u22c5|\ud835\udc9ftest|\u22121proportional-tosubscriptPrcontam\u22c5subscript\ud835\udc9ftrainsuperscriptsubscript\ud835\udc9ftest1\\text{Pr}\\_{\\text{contam}}\\propto|\\mathcal{D}\\_{\\text{train}}|\\cdot|\\mathcal{D}\\_%\n\n{\\text{test}}|^{-1}Pr start\\_POSTSUBSCRIPT contam end\\_POSTSUBSCRIPT \u221d | caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT | \u22c5 | caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT | start\\_POSTSUPERSCRIPT - 1 end\\_POSTSUPERSCRIPT, rendering traditional benchmarks outdated for models trained on web-scale data. Data privacy and commercial concerns further complicate contamination issues.\n\nSecond, traditional static methods fall short in preventing contamination, prompting the creation of dynamic benchmarks. However, our study identifies a lack of standardized criteria for evaluating these dynamic benchmarks. We hope our proposed criteria will offer valuable insights and guide the development of more effective benchmarks.\n\nCurrent Challenges.\n\nStatic benchmarking methods face challenges due to a lack of transparency (e.g., label protection) and high assumptions about contaminated models (e.g., post-hoc detection). Although dynamic benchmarks address these limitations, they introduce new issues, such as balancing correctness with scalability. We also observed that some dynamic benchmarks neglect complexity control, resulting in inefficiencies in evaluation.\n\nFuture Directions.\n\nDynamic benchmarking offers a new approach for evaluating LLMs, but our study reveals a lack of standardized criteria. Future efforts should focus on standardizing these criteria for dynamic benchmarks.\n", "topic": "The Need for Dynamic Benchmarks", "question": "Can dynamic benchmarks effectively address data contamination issues in evaluating Large Language Models (LLMs), and what criteria should be established for standardizing these benchmarks?", "choices": {"B": "No, dynamic benchmarks cannot address data contamination issues, and traditional static methods are more effective.", "A": "Yes, dynamic benchmarks can address data contamination issues, and standardized criteria can be established to ensure their effectiveness.", "C": "Dynamic benchmarks can address data contamination issues, but standardized criteria are still needed to ensure their effectiveness.", "D": "Dynamic benchmarks cannot address data contamination issues, but they can provide a more nuanced understanding of LLMs."}, "answer": "A", "explanation": "The correct answer is A, as dynamic benchmarks offer a more effective approach to addressing data contamination issues in evaluating LLMs. Standardized criteria can be established to ensure their effectiveness and balance correctness with scalability.", "question_token_count": 30, "answer_correctness_score": 9, "explanation_validity_score": 9, "question_clarity_score": 8, "question_groundedness_score": 9, "avg_answer_token_count": 20}
{"context": "#### 4.2 Evaluation Criteria\nWhile many dynamic benchmarking methods have been proposed to evaluate LLMs, the evaluation criteria for assessing these benchmarks themselves remain non-standardized. To this end, we propose the following evaluation criteria to assess the quality of a dynamic benchmarking algorithm.\n\n\n##### 4.2.1 Correctness\nThe first criterion for evaluating the quality of dynamic benchmarking is Correctness. If the correctness of the generated dataset cannot be guaranteed, the benchmark may provide a false sense of reliability when applied to benchmarking LLMs, leading to misleading evaluations.\n\nWe quantify the correctness of dynamic benchmarks as:\n\nwhere \ud835\udcb3isubscript\ud835\udcb3\ud835\udc56\\mathcal{X}\\_{i}caligraphic\\_X start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT and \ud835\udcb4isubscript\ud835\udcb4\ud835\udc56\\mathcal{Y}\\_{i}caligraphic\\_Y start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT represent the input and output of the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic\\_i start\\_POSTSUPERSCRIPT italic\\_t italic\\_h end\\_POSTSUPERSCRIPT transformation, respectively. The function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) is an oracle that returns the ground truth of its input, ensuring an objective reference for correctness evaluation. For example, the function \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic\\_G ( \u22c5 ) could be a domain-specific an annotator.\n\nThis equation can be interpreted as the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, measured using the scoring function \ud835\udcae\u2062(\u22c5)\ud835\udcae\u22c5\\mathcal{S}(\\cdot)caligraphic\\_S ( \u22c5 ). A higher correctness score indicates that the dynamic benchmark maintains correctness to the ground truth.\n", "topic": "Objectivity and Reliability", "question": "What is the primary measure of correctness used to evaluate the quality of dynamic benchmarking algorithms for Large Language Models (LLMs), as proposed in the given context?", "choices": {"A": "Alignment between transformed dataset outputs and their corresponding ground truth values", "B": "Error rate of the benchmarking algorithm", "C": "F1-score of the transformed dataset", "D": "Confidence interval of the oracle function"}, "answer": "A", "explanation": "The correctness of dynamic benchmarks is quantified using the equation, which measures the expected alignment between the transformed dataset\u2019s outputs and their corresponding ground truth values, ensuring an objective reference for correctness evaluation.", "question_token_count": 32, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 1, "question_groundedness_score": 10, "avg_answer_token_count": 8}
{"context": "##### 4.2.5 Diversity\nBesides the aforementioned criteria, another important factor is the diversity of the transformed dataset. This diversity can be categorized into two components: external diversity and internal diversity: External diversity measures the variation between the transformed dataset and the seed dataset. Internal diversity quantifies the differences between two transformation trials.\n\n|    | External Diversity   | =\ud835\udd3ci=1N\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9f)absentsuperscriptsubscript\ud835\udd3c\ud835\udc561\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56\ud835\udc9f\\displaystyle=\\mathbb{E}_{i=1}^{N}\\Theta(\\mathcal{D}_{i},\\mathcal{D})= blackboard_E start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D )                                                                                                                                                                                                                               |    |\n\n|----|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n\n|    | Internal Diversity   | =\ud835\udd3ci,j=1,i\u2260jN\u2062\u0398\u2062(\ud835\udc9fi,\ud835\udc9fj)absentsuperscriptsubscript\ud835\udd3cformulae-sequence\ud835\udc56\ud835\udc571\ud835\udc56\ud835\udc57\ud835\udc41\u0398subscript\ud835\udc9f\ud835\udc56subscript\ud835\udc9f\ud835\udc57\\displaystyle=\\mathbb{E}_{\\begin{subarray}{c}i,j=1,i\\neq j\\end{subarray}}^{N}% \\Theta(\\mathcal{D}_{i},\\mathcal{D}_{j})= blackboard_E start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_i , italic_j = 1 , italic_i \u2260 italic_j end_CELL end_ROW end_ARG end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_\u0398 ( caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) |    |\n\nwhere \u0398\u2062(\u22c5)\u0398\u22c5\\Theta(\\cdot)roman\\_\u0398 ( \u22c5 ) is a function that measures the diversity between two datasets. For example, it could be the N-gram metrics or the reference based metrics, such as BLEU scores.\n", "topic": "What is the difference between external diversity and internal diversity in the context of dataset transformation?", "question": "What is the primary distinction between external diversity and internal diversity in dataset transformation, as measured by the function \u0398(\u22c5)?", "choices": {"C": "C", "A": "C", "B": "B", "D": "A"}, "answer": "C", "explanation": "The correct answer, C, highlights the primary distinction between external diversity and internal diversity in dataset transformation. External diversity is the variation between the transformed dataset and the seed dataset, while internal diversity is the differences between two transformation trials. The function \u0398(\u22c5) measures this diversity. The other options are incorrect as they do not accurately capture the distinction between external diversity and internal diversity.", "question_token_count": 25, "answer_correctness_score": 10, "explanation_validity_score": 10, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 2}
{"context": "#### 2.1 Data Contamination\nData contamination occurs when LLM\u2019s training data \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT contains information that improperly overlaps with evaluation benchmark data \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT, compromising the validity of performance measurements. We summarize existing work and provide a formal definition of data contamination.\n\nExact contamination occurs when there is any exact duplicate in the benchmark dataset\n\nIn other word, there exist a data point d\ud835\udc51ditalic\\_d that both in \ud835\udc9ftrainsubscript\ud835\udc9ftrain\\mathcal{D}\\_{\\text{train}}caligraphic\\_D start\\_POSTSUBSCRIPT train end\\_POSTSUBSCRIPT and \ud835\udc9ftestsubscript\ud835\udc9ftest\\mathcal{D}\\_{\\text{test}}caligraphic\\_D start\\_POSTSUBSCRIPT test end\\_POSTSUBSCRIPT.\n\nCommon cases include verbatim test examples appearing in training corpora, code snippets from benchmark implementations, or documentation leaks.\n\nSyntactic contamination occurs when a test data point could be found in the training dataset after a syntactic transformation, such that\n\nwhere \u2131syntacticsubscript\u2131syntactic\\mathcal{F}\\_{\\text{syntactic}}caligraphic\\_F start\\_POSTSUBSCRIPT syntactic end\\_POSTSUBSCRIPT denotes syntactic transformations like punctuation normalization, whitespace modification, synonym substitution, morphological variations, or syntactic paraphrasing while preserving lexical meaning.\n", "topic": "Sources and Mitigation Strategies", "question": "Can exact contamination occur when a test data point is found in the training dataset after a syntactic transformation, such as punctuation normalization or synonym substitution?", "choices": {"C": "No, syntactic contamination can only occur when a test data point is found in the training dataset after a transformation that preserves lexical meaning.", "A": "No, exact contamination can only occur when there is an exact duplicate between the training and testing datasets.", "B": "Yes, exact contamination can occur when a test data point is found in the training dataset after a syntactic transformation, such as synonym substitution or morphological variations.", "D": "Yes, syntactic contamination can occur when a test data point is found in the training dataset after a transformation that affects the text's grammatical structure."}, "answer": "B", "explanation": "The correct answer is B, as syntactic contamination can indeed occur when a test data point is found in the training dataset after a transformation, such as synonym substitution or morphological variations. This type of contamination can be challenging to detect, as it may involve subtle changes to the text that do not affect its lexical meaning. The other options are incorrect because exact contamination requires an exact duplicate between the training and testing datasets, and syntactic contamination does not require the transformation to preserve lexical meaning.", "question_token_count": 29, "answer_correctness_score": 1, "explanation_validity_score": 4, "question_clarity_score": 6, "question_groundedness_score": 10, "avg_answer_token_count": 27}
